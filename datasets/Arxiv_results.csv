http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",ai consumer fraud online
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",ai consumer fraud online
http://arxiv.org/abs/1906.04272v2,"Given the magnitude of online auction transactions, it is difficult to
safeguard consumers from dishonest sellers, such as shill bidders. To date, the
application of machine learning to auction fraud detection has been limited.
Shill Bidding (SB) is a severe auction fraud, which is driven by modern-day
technologies and clever scammers. The difficulty of identifying the behavior of
sophisticated fraudsters and the unavailability of training datasets hinder the
research on SB detection. The aim of this study is to develop a high-quality SB
dataset. To do so, first, we crawled and preprocessed a large number of
commercial auctions and bidders' history as well. We thoroughly preprocessed
both datasets to make them usable for the computation of the SB metrics.
Nevertheless, this operation requires a deep understanding of the behavior of
auctions and bidders. Second, we introduced two new SB patterns and implemented
other existing ones. Finally, we removed outliers to improve the quality of the
training fraud data.",ai consumer fraud online
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",ai consumer fraud online
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",ai consumer fraud online
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",ai consumer fraud online
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",ai consumer fraud online
http://arxiv.org/abs/1906.10418v1,"The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process",ai consumer fraud online
http://arxiv.org/abs/1309.7262v1,"Fake websites have emerged as a major source of online fraud, accounting for
billions of dollars of loss by Internet users. We explore the process by which
salient design elements could increase the use of protective tools, thus
reducing the success rate of fake websites. Using the protection motivation
theory, we conceptualize a model to investigate how salient design elements of
detection tools could influence user perceptions of the tools, efficacy in
dealing with threats, and use of such tools. The research method was a
controlled lab experiment with a novel and extensive experimental design and
protocol. We found that trust in the detector is the pivotal coping mechanism
in dealing with security threats and is a major conduit for transforming
salient design elements into increased use. We also found that design elements
have profound and unexpected impacts on self-efficacy. The significant
theoretical and empirical implications of findings are discussed.",ai consumer fraud online
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",ai consumer fraud online
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",ai consumer fraud online
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",ai consumer fraud online
http://arxiv.org/abs/1510.07165v1,"Financial fraud is an issue with far reaching consequences in the finance
industry, government, corporate sectors, and for ordinary consumers. Increasing
dependence on new technologies such as cloud and mobile computing in recent
years has compounded the problem. Traditional methods of detection involve
extensive use of auditing, where a trained individual manually observes reports
or transactions in an attempt to discover fraudulent behaviour. This method is
not only time consuming, expensive and inaccurate, but in the age of big data
it is also impractical. Not surprisingly, financial institutions have turned to
automated processes using statistical and computational methods. This paper
presents a comprehensive investigation on financial fraud detection practices
using such data mining methods, with a particular focus on computational
intelligence-based techniques. Classification of the practices based on key
aspects such as detection algorithm used, fraud type investigated, and success
rate have been covered. Issues and challenges associated with the current
practices and potential future direction of research have also been identified.",ai consumer fraud online
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",ai consumer fraud online
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",ai consumer fraud online
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",ai consumer fraud online
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",ai consumer fraud online
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",ai consumer fraud online
http://arxiv.org/abs/1611.02260v1,"Food fraud has been an area of great concern due to its risk to public
health, reduction of food quality or nutritional value and for its economic
consequences. For this reason, it's been object of regulation in many countries
(e.g. [1], [2]). One type of food that has been frequently object of fraud
through the addition of water or an aqueous solution is bovine meat. The
traditional methods used to detect this kind of fraud are expensive,
time-consuming and depend on physicochemical analysis that require complex
laboratory techniques, specific for each added substance. In this paper, based
on digital images of histological cuts of adulterated and not-adulterated
(normal) bovine meat, we evaluate the of digital image analysis methods to
identify the aforementioned kind of fraud, with focus on the Local Binary
Pattern (LBP) algorithm.",ai consumer fraud online
http://arxiv.org/abs/1805.09741v2,"The Automobile Insurance Fraud is one of the main challenges for insurance
companies. This form of fraud is performed either opportunistic or professional
occurring through group cooperation that leads to greater financial losses,
while most presented methods thus far are unsuited for flagging these groups.
The article has put forward a new approach for identification, representation,
and analysis of organized fraudulent groups in automobile insurance through
focusing on structural aspects of networks, and cycles in particular, that
demonstrate the occurrence of potential fraud. Suspicious groups have been
detected by applying cycle detection algorithms (using both DFS, BFS trees),
afterward, the probability of being fraudulent for suspicious components were
investigated to reveal fraudulent groups with the maximum likelihood, and their
reviews were prioritized. The actual data of Iran Insurance Company is used for
evaluating the provided approach. As a result, the detection of cycles is not
only more efficient, accurate, but also less time-consuming in comparison with
previous methods for finding such groups.",ai consumer fraud online
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",ai consumer fraud online
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",ai consumer fraud online
http://arxiv.org/abs/1607.04451v4,"Emerging trends in smartphones, online maps, social media, and the resulting
geo-located data, provide opportunities to collect traces of people's
socio-economical activities in a much more granular and direct fashion,
triggering a revolution in empirical research. These vast mobile data offer new
perspectives and approaches for measurements of economic dynamics and are
broadening the research fields of social science and economics. In this paper,
we explore the potential of using mobile big data for measuring economic
activities of China. Firstly, We build indices for gauging employment and
consumer trends based on billions of geo-positioning data. Secondly, we advance
the estimation of store offline foot traffic via location search data derived
from Baidu Maps, which is then applied to predict revenues of Apple in China
and detect box-office fraud accurately. Thirdly, we construct consumption
indicators to track the trends of various industries in service sector, which
are verified by several existing indicators. To the best of our knowledge, we
are the first to measure the second largest economy by mining such
unprecedentedly large scale and fine granular spatial-temporal data. Our
research provides new approaches and insights on measuring economic activities.",ai consumer fraud online
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",ai consumer fraud online
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",ai consumer fraud online
http://arxiv.org/abs/1803.01798v2,"Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.",ai consumer fraud online
http://arxiv.org/abs/1809.04683v2,"Many online platforms have deployed anti-fraud systems to detect and prevent
fraudulent activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, which maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe the user suspended time instead of the
fraudulent activity time in the training data, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.",ai consumer fraud online
http://arxiv.org/abs/1811.08502v1,"The last decade has witnessed an explosion on the computational power and a
parallel increase of the access to large sets of data (the so called Big Data
paradigm) which is enabling to develop brand new quantitative strategies
underpinning description, understanding and control of complex scenarios. One
interesting area of application concerns fraud detection from online data, and
more particularly extracting meaningful information from massive digital
fingerprints of electoral activity to detect, a posteriori, evidence of
fraudulent behavior. In this short article we discuss a few quantitative
methodologies that have emerged in recent years on this respect, which
altogether form the nascent interdisciplinary field of election forensics.",ai consumer fraud online
http://arxiv.org/abs/0801.2700v1,"Labels and tags are accompanying us in almost each moment of our life and
everywhere we are going, in the form of electronic keys or money, or simply as
labels on products we are buying in shops and markets. The label diffusion,
rapidly increasing for logistic reasons in the actual global market, carries
huge amount of information but it is demanding security and anti-fraud systems.
The first crucial point, for the consumer and producer safety, is to ensure the
authenticity of the labelled products with systems against counterfeiting and
piracy. Recent anti-fraud techniques are based on a sophisticated use of
physical effects, from holograms till magnetic resonance or tunnel transitions
between atomic sublevels. In this paper we will discuss labels and anti-fraud
technologies as a new and very promising research field for applied physics.",ai consumer fraud online
http://arxiv.org/abs/1705.01010v3,"Image-based modeling techniques can now generate photo-realistic 3D models
from images. But it is up to users to provide high quality images with good
coverage and view overlap, which makes the data capturing process tedious and
time consuming. We seek to automate data capturing for image-based modeling.
The core of our system is an iterative linear method to solve the multi-view
stereo (MVS) problem quickly and plan the Next-Best-View (NBV) effectively. Our
fast MVS algorithm enables online model reconstruction and quality assessment
to determine the NBVs on the fly. We test our system with a toy unmanned aerial
vehicle (UAV) in simulated, indoor and outdoor experiments. Results show that
our system improves the efficiency of data acquisition and ensures the
completeness of the final model.",ai consumer fraud online
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",online shopping fraud
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",online shopping fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping fraud
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping fraud
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping fraud
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",online shopping fraud
http://arxiv.org/abs/1906.06977v1,"Machine learning and data mining techniques have been used extensively in
order to detect credit card frauds. However purchase behaviour and fraudster
strategies may change over time. This phenomenon is named dataset shift or
concept drift in the domain of fraud detection. In this paper, we present a
method to quantify day-by-day the dataset shift in our face-to-face credit card
transactions dataset (card holder located in the shop) . In practice, we
classify the days against each other and measure the efficiency of the
classification. The more efficient the classification, the more different the
buying behaviour between two days, and vice versa. Therefore, we obtain a
distance matrix characterizing the dataset shift. After an agglomerative
clustering of the distance matrix, we observe that the dataset shift pattern
matches the calendar events for this time period (holidays, week-ends, etc). We
then incorporate this dataset shift knowledge in the credit card fraud
detection task as a new feature. This leads to a small improvement of the
detection.",online shopping fraud
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",online shopping fraud
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",online shopping fraud
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping fraud
http://arxiv.org/abs/0801.2700v1,"Labels and tags are accompanying us in almost each moment of our life and
everywhere we are going, in the form of electronic keys or money, or simply as
labels on products we are buying in shops and markets. The label diffusion,
rapidly increasing for logistic reasons in the actual global market, carries
huge amount of information but it is demanding security and anti-fraud systems.
The first crucial point, for the consumer and producer safety, is to ensure the
authenticity of the labelled products with systems against counterfeiting and
piracy. Recent anti-fraud techniques are based on a sophisticated use of
physical effects, from holograms till magnetic resonance or tunnel transitions
between atomic sublevels. In this paper we will discuss labels and anti-fraud
technologies as a new and very promising research field for applied physics.",online shopping fraud
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",online shopping fraud
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",online shopping fraud
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",online shopping fraud
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping fraud
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",online shopping fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",online shopping fraud
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",online shopping fraud
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",online shopping fraud
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",online shopping fraud
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",online shopping fraud
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping fraud
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping fraud
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",online shopping fraud
http://arxiv.org/abs/1906.04272v2,"Given the magnitude of online auction transactions, it is difficult to
safeguard consumers from dishonest sellers, such as shill bidders. To date, the
application of machine learning to auction fraud detection has been limited.
Shill Bidding (SB) is a severe auction fraud, which is driven by modern-day
technologies and clever scammers. The difficulty of identifying the behavior of
sophisticated fraudsters and the unavailability of training datasets hinder the
research on SB detection. The aim of this study is to develop a high-quality SB
dataset. To do so, first, we crawled and preprocessed a large number of
commercial auctions and bidders' history as well. We thoroughly preprocessed
both datasets to make them usable for the computation of the SB metrics.
Nevertheless, this operation requires a deep understanding of the behavior of
auctions and bidders. Second, we introduced two new SB patterns and implemented
other existing ones. Finally, we removed outliers to improve the quality of the
training fraud data.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping law enforcement
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping law enforcement
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping law enforcement
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping law enforcement
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online shopping law enforcement
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping law enforcement
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping law enforcement
http://arxiv.org/abs/1807.05381v1,"Physical retailers, who once led the way in tracking with loyalty cards and
`reverse appends', now lag behind online competitors. Yet we might be seeing
these tables turn, as many increasingly deploy technologies ranging from simple
sensors to advanced emotion detection systems, even enabling them to tailor
prices and shopping experiences on a per-customer basis. Here, we examine these
in-store tracking technologies in the retail context, and evaluate them from
both technical and regulatory standpoints. We first introduce the relevant
technologies in context, before considering privacy impacts, the current
remedies individuals might seek through technology and the law, and those
remedies' limitations. To illustrate challenging tensions in this space we
consider the feasibility of technical and legal approaches to both a) the
recent `Go' store concept from Amazon which requires fine-grained, multi-modal
tracking to function as a shop, and b) current challenges in opting in or out
of increasingly pervasive passive Wi-Fi tracking. The `Go' store presents
significant challenges with its legality in Europe significantly unclear and
unilateral, technical measures to avoid biometric tracking likely ineffective.
In the case of MAC addresses, we see a difficult-to-reconcile clash between
privacy-as-confidentiality and privacy-as-control, and suggest a technical
framework which might help balance the two. Significant challenges exist when
seeking to balance personalisation with privacy, and researchers must work
together, including across the boundaries of preferred privacy definitions, to
come up with solutions that draw on both technology and the legal frameworks to
provide effective and proportionate protection. Retailers, simultaneously, must
ensure that their tracking is not just legal, but worthy of the trust of
concerned data subjects.",online shopping law enforcement
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping law enforcement
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping law enforcement
http://arxiv.org/abs/1505.07922v1,"We address the problem of cross-domain image retrieval, considering the
following practical application: given a user photo depicting a clothing image,
our goal is to retrieve the same or attribute-similar clothing items from
online shopping stores. This is a challenging problem due to the large
discrepancy between online shopping images, usually taken in ideal
lighting/pose/background conditions, and user photos captured in uncontrolled
conditions. To address this problem, we propose a Dual Attribute-aware Ranking
Network (DARN) for retrieval feature learning. More specifically, DARN consists
of two sub-networks, one for each domain, whose retrieval feature
representations are driven by semantic attribute learning. We show that this
attribute-guided learning is a key factor for retrieval accuracy improvement.
In addition, to further align with the nature of the retrieval problem, we
impose a triplet visual similarity constraint for learning to rank across the
two sub-networks. Another contribution of our work is a large-scale dataset
which makes the network learning feasible. We exploit customer review websites
to crawl a large set of online shopping images and corresponding offline user
photos with fine-grained clothing attributes, i.e., around 450,000 online
shopping images and about 90,000 exact offline counterpart images of those
online ones. All these images are collected from real-world consumer websites
reflecting the diversity of the data modality, which makes this dataset unique
and rare in the academic community. We extensively evaluate the retrieval
performance of networks in different configurations. The top-20 retrieval
accuracy is doubled when using the proposed DARN other than the current popular
solution using pre-trained CNN features only (0.570 vs. 0.268).",online shopping law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online shopping law enforcement
http://arxiv.org/abs/1811.02385v1,"The ability to correctly classify and retrieve apparel images has a variety
of applications important to e-commerce, online advertising and internet
search. In this work, we propose a robust framework for fine-grained apparel
classification, in-shop and cross-domain retrieval which eliminates the
requirement of rich annotations like bounding boxes and human-joints or
clothing landmarks, and training of bounding box/ key-landmark detector for the
same. Factors such as subtle appearance differences, variations in human poses,
different shooting angles, apparel deformations, and self-occlusion add to the
challenges in classification and retrieval of apparel items. Cross-domain
retrieval is even harder due to the presence of large variation between online
shopping images, usually taken in ideal lighting, pose, positive angle and
clean background as compared with street photos captured by users in
complicated conditions with poor lighting and cluttered scenes. Our framework
uses compact bilinear CNN with tensor sketch algorithm to generate embeddings
that capture local pairwise feature interactions in a translationally invariant
manner. For apparel classification, we pass the feature embeddings through a
softmax classifier, while, the in-shop and cross-domain retrieval pipelines use
a triplet-loss based optimization approach, such that squared Euclidean
distance between embeddings measures the dissimilarity between the images.
Unlike previous works that relied on bounding box, key clothing landmarks or
human joint detectors to assist the final deep classifier, proposed framework
can be trained directly on the provided category labels or generated triplets
for triplet loss optimization. Lastly, Experimental results on the DeepFashion
fine-grained categorization, and in-shop and consumer-to-shop retrieval
datasets provide a comparative analysis with previous work performed in the
domain.",online shopping law enforcement
http://arxiv.org/abs/1901.04140v1,"In the current field of computer vision, automatically generating texts from
given images has been a fully worked technique. Up till now, most works of this
area focus on image content describing, namely image-captioning. However, rare
researches focus on generating product review texts, which is ubiquitous in the
online shopping malls and is crucial for online shopping selection and
evaluation. Different from content describing, review texts include more
subjective information of customers, which may bring difference to the results.
Therefore, we aimed at a new field concerning generating review text from
customers based on images together with the ratings of online shopping
products, which appear as non-image attributes. We made several adjustments to
the existing image-captioning model to fit our task, in which we should also
take non-image features into consideration. We also did experiments based on
our model and get effective primary results.",online shopping law enforcement
http://arxiv.org/abs/1812.07143v1,"Shopping is difficult for people with motor impairments. This includes online
shopping. Proprietary software can emulate mouse and keyboard via head
tracking. However, such a solution is not common for smartphones. Unlike
desktop and laptop computers, they are also much easier to carry indoors and
outdoors.To address this, we implement and open source button that is sensitive
to head movements tracked from the front camera of iPhone X. This allows
developers to integrate in eCommerce applications easily without requiring
specialized knowledge. Other applications include gaming and use in hands-free
situations such as during cooking, auto-repair. We built a sample online
shopping application that allows users to easily browse between items from
various categories and take relevant action just by head movements. We present
results of user studies on this sample application and also include sensitivity
studies based on two independent tests performed at 3 different distances to
the screen.",online shopping law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online shopping law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online shopping law enforcement
http://arxiv.org/abs/1612.01603v1,"In this paper, we propose a SaaS service which prevents shoplifting using
image analysis and ERP. In Japan, total damage of shoplifting reaches 450
billion yen and more than 1000 small shops gave up their businesses because of
shoplifting. Based on recent cloud technology and data analysis technology, we
propose a shoplifting prevention service with image analysis of security camera
and ERP data check for small shops. We evaluated stream analysis of security
camera movie using online machine learining framework Jubatus.",online shopping law enforcement
http://arxiv.org/abs/1703.07371v1,"Today, huge amount of data is available on the web. Now there is a need to
convert that data in knowledge which can be useful for different purposes. This
paper depicts the use of data mining process, OLAP with the combination of
multi agent system to find the knowledge from data in cloud computing. For
this, I am also trying to explain one case study of online shopping of one
Bakery Shop. May be we can increase the sale of items by using the model, which
I am trying to represent.",online shopping law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online shopping law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online shopping law enforcement
http://arxiv.org/abs/1804.05287v2,"In recent years, both online retail and video hosting service are
exponentially growing. In this paper, we explore a new cross-domain task,
Video2Shop, targeting for matching clothes appeared in videos to the exact same
items in online shops. A novel deep neural network, called AsymNet, is proposed
to explore this problem. For the image side, well-established methods are used
to detect and extract features for clothing patches with arbitrary sizes. For
the video side, deep visual features are extracted from detected object regions
in each frame, and further fed into a Long Short-Term Memory (LSTM) framework
for sequence modeling, which captures the temporal dynamics in videos. To
conduct exact matching between videos and online shopping images, LSTM hidden
states, representing the video, and image features, which represent static
object images, are jointly modeled under the similarity network with
reconfigurable deep tree structure. Moreover, an approximate training method is
proposed to achieve the efficiency when training. Extensive experiments
conducted on a large cross-domain dataset have demonstrated the effectiveness
and efficiency of the proposed AsymNet, which outperforms the state-of-the-art
methods.",online shopping law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online shopping law enforcement
http://arxiv.org/abs/1610.05562v1,"An important goal of online comparison shopping services is to ""convert"" a
viewer from general product category pages (for example product groups such as
""smartphones"" or ""air-conditioners"") to detailed product pages and ultimately
to order pages. Comparison shopping websites provide a familiar web interface
as well as a chance for consumers to purchase items at competitive prices. In
return for providing access to a large market of potential consumers, the
comparison shopping service usually receives financial compensation for product
clicks and orders. This study looked at 2.5 million product listing visits at
price.com.hk to determine whether a modification in the way prices are
displayed on general category pages resulted in more ""conversions"" to product
detail pages. We found a statistically significant improvement over-all as a
result of the new price display resulting in 3.6% more product clicks over all
categories. Additional analysis showed that the effect is heterogeneous among
different categories, and in a few cases there may be some categories
negatively affected by the display modification.",online shopping law enforcement
http://arxiv.org/abs/1806.11423v1,"While shopping for fashion products, customers usually prefer to try-out
products to examine fit, material, overall look and feel. Due to lack of try
out options during online shopping, it becomes pivotal to provide customers
with as much of this information as possible to enhance their shopping
experience. Also it becomes essential to provide same experience for new
customers. Our work here focuses on providing a production ready size
recommendation system for shoes and address the challenge of providing
recommendation for users with no previous purchases on the platform. In our
work, we present a probabilistic approach based on user co-purchase data
facilitated by generating a brand-brand relationship graph. Specifically we
address two challenges that are commonly faced while implementing such
solution. 1. Sparse signals for less popular or new products in the system 2.
Extending the solution for new users. Further we compare and contrast this
approach with our previous work and show significant improvement both in
recommendation precision and coverage.",online shopping law enforcement
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online law enforcement
http://arxiv.org/abs/1612.05030v1,"Synchronous programming is a paradigm of choice for the design of
safety-critical reactive systems. Runtime enforcement is a technique to ensure
that the output of a black-box system satisfies some desired properties. This
paper deals with the problem of runtime enforcement in the context of
synchronous programs. We propose a framework where an enforcer monitors both
the inputs and the outputs of a synchronous program and (minimally) edits
erroneous inputs/outputs in order to guarantee that a given property holds. We
define enforceability conditions, develop an online enforcement algorithm, and
prove its correctness. We also report on an implementation of the algorithm on
top of the KIELER framework for the SCCharts synchronous language. Experimental
results show that enforcement has minimal execution time overhead, which
decreases proportionally with larger benchmarks.",online law enforcement
http://arxiv.org/abs/1701.01911v2,"Exemplar-based face sketch synthesis plays an important role in both digital
entertainment and law enforcement. It generally consists of two parts: neighbor
selection and reconstruction weight representation. The most time-consuming or
main computation complexity for exemplar-based face sketch synthesis methods
lies in the neighbor selection process. State-of-the-art face sketch synthesis
methods perform neighbor selection online in a data-driven manner by $K$
nearest neighbor ($K$-NN) searching. Actually, the online search increases the
time consuming for synthesis. Moreover, since these methods need to traverse
the whole training dataset for neighbor selection, the computational complexity
increases with the scale of the training database and hence these methods have
limited scalability. In this paper, we proposed a simple but effective offline
random sampling in place of online $K$-NN search to improve the synthesis
efficiency. Extensive experiments on public face sketch databases demonstrate
the superiority of the proposed method in comparison to state-of-the-art
methods, in terms of both synthesis quality and time consumption. The proposed
method could be extended to other heterogeneous face image transformation
problems such as face hallucination. We release the source codes of our
proposed methods and the evaluation metrics for future study online:
http://www.ihitworld.com/RSLCR.html.",online law enforcement
http://arxiv.org/abs/1509.06659v3,"Human trafficking is a challenging law enforcement problem, and a large
amount of such activity manifests itself on various online forums. Given the
large, heterogeneous and noisy structure of this data, building models to
predict instances of trafficking is an even more convolved a task. In this
paper we propose and entity resolution pipeline using a notion of proxy labels,
in order to extract clusters from this data with prior history of human
trafficking activity. We apply this pipeline to 5M records from backpage.com
and report on the performance of this approach, challenges in terms of
scalability, and some significant domain specific characteristics of our
resolved entities.",online law enforcement
http://arxiv.org/abs/1902.06961v1,"Cybercrime investigators face numerous challenges when policing online
crimes. Firstly, the methods and processes they use when dealing with
traditional crimes do not necessarily apply in the cyber-world. Additionally,
cyber criminals are usually technologically-aware and constantly adapting and
developing new tools that allow them to stay ahead of law enforcement
investigations. In order to provide adequate support for cybercrime
investigators, there needs to be a better understanding of the challenges they
face at both technical and socio-technical levels. In this paper, we
investigate this problem through an analysis of current practices and workflows
of investigators. We use interviews with experts from government and private
sectors who investigate cybercrimes as our main data gathering process. From an
analysis of the collected data, we identify several outstanding challenges
faced by investigators. These pertain to practical, technical, and social
issues such as systems availability, usability, and in computer-supported
collaborative work. Importantly, we use our findings to highlight research
areas where user-centric workflows and tools are desirable. We also define a
set of recommendations that can aid in providing a better foundation for future
research in the field and allow more effective combating of cybercrimes.",online law enforcement
http://arxiv.org/abs/1404.1295v1,"The study of criminal networks using traces from heterogeneous communication
media is acquiring increasing importance in nowadays society. The usage of
communication media such as phone calls and online social networks leaves
digital traces in the form of metadata that can be used for this type of
analysis. The goal of this work is twofold: first we provide a theoretical
framework for the problem of detecting and characterizing criminal
organizations in networks reconstructed from phone call records. Then, we
introduce an expert system to support law enforcement agencies in the task of
unveiling the underlying structure of criminal networks hidden in communication
data. This platform allows for statistical network analysis, community
detection and visual exploration of mobile phone network data. It allows
forensic investigators to deeply understand hierarchies within criminal
organizations, discovering members who play central role and provide connection
among sub-groups. Our work concludes illustrating the adoption of our
computational framework for a real-word criminal investigation.",online law enforcement
http://arxiv.org/abs/1603.07823v1,"Face sketch synthesis has wide applications ranging from digital
entertainments to law enforcements. Objective image quality assessment scores
and face recognition accuracy are two mainly used tools to evaluate the
synthesis performance. In this paper, we proposed a synthesized face sketch
recognition framework based on full-reference image quality assessment metrics.
Synthesized sketches generated from four state-of-the-art methods are utilized
to test the performance of the proposed recognition framework. For the image
quality assessment metrics, we employed the classical structured similarity
index metric and other three prevalent metrics: visual information fidelity,
feature similarity index metric and gradient magnitude similarity deviation.
Extensive experiments compared with baseline methods illustrate the
effectiveness of the proposed synthesized face sketch recognition framework.
Data and implementation code in this paper are available online at
www.ihitworld.com/WNN/IQA_Sketch.zip.",online law enforcement
http://arxiv.org/abs/1712.03086v1,"In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.",online law enforcement
http://arxiv.org/abs/1801.07207v1,"Security incidents such as targeted distributed denial of service (DDoS)
attacks on power grids and hacking of factory industrial control systems (ICS)
are on the increase. This paper unpacks where emerging security risks lie for
the industrial internet of things, drawing on both technical and regulatory
perspectives. Legal changes are being ushered by the European Union (EU)
Network and Information Security (NIS) Directive 2016 and the General Data
Protection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use
the case study of the emergent smart energy supply chain to frame, scope out
and consolidate the breadth of security concerns at play, and the regulatory
responses. We argue the industrial IoT brings four security concerns to the
fore, namely: appreciating the shift from offline to online infrastructure;
managing temporal dimensions of security; addressing the implementation gap for
best practice; and engaging with infrastructural complexity. Our goal is to
surface risks and foster dialogue to avoid the emergence of an Internet of
Insecure Industrial Things",online law enforcement
http://arxiv.org/abs/1810.03965v1,"We present an algorithm for realtime anomaly detection in low to medium
density crowd videos using trajectory-level behavior learning. Our formulation
combines online tracking algorithms from computer vision, non-linear pedestrian
motion models from crowd simulation, and Bayesian learning techniques to
automatically compute the trajectory-level pedestrian behaviors for each agent
in the video. These learned behaviors are used to segment the trajectories and
motions of different pedestrians or agents and detect anomalies. We demonstrate
the interactive performance on the PETS ARENA dataset as well as indoor and
outdoor crowd video benchmarks consisting of tens of human agents. We also
discuss the implications of recent public policy and law enforcement issues
relating to surveillance and our research.",online law enforcement
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",online law enforcement
http://arxiv.org/abs/1801.04565v1,"Data retrieval systems such as online search engines and online social
networks must comply with the privacy policies of personal and selectively
shared data items, regulatory policies regarding data retention and censorship,
and the provider's own policies regarding data use. Enforcing these policies is
difficult and error-prone. Systematic techniques to enforce policies are either
limited to type-based policies that apply uniformly to all data of the same
type, or incur significant runtime overhead.
  This paper presents Shai, the first system that systematically enforces
data-specific policies with near-zero overhead in the common case. Shai's key
idea is to push as many policy checks as possible to an offline, ahead-of-time
analysis phase, often relying on predicted values of runtime parameters such as
the state of access control lists or connected users' attributes. Runtime
interception is used sparingly, only to verify these predictions and to make
any remaining policy checks. Our prototype implementation relies on efficient,
modern OS primitives for sandboxing and isolation. We present the design of
Shai and quantify its overheads on an experimental data indexing and search
pipeline based on the popular search engine Apache Lucene.",online law enforcement
http://arxiv.org/abs/1302.3946v1,"We consider the classical online scheduling problem P||C_{max} in which jobs
are released over list and provide a nearly optimal online algorithm. More
precisely, an online algorithm whose competitive ratio is at most (1+\epsilon)
times that of an optimal online algorithm could be achieved in polynomial time,
where m, the number of machines, is a part of the input. It substantially
improves upon the previous results by almost closing the gap between the
currently best known lower bound of 1.88 (Rudin, Ph.D thesis, 2001) and the
best known upper bound of 1.92 (Fleischer, Wahl, Journal of Scheduling, 2000).
It has been known by folklore that an online problem could be viewed as a game
between an adversary and the online player. Our approach extensively explores
such a structure and builds up a completely new framework to show that, for the
online over list scheduling problem, given any \epsilon>0, there exists a
uniform threshold K which is polynomial in m such that if the competitive ratio
of an online algorithm is \rho<=2, then there exists a list of at most K jobs
to enforce the online algorithm to achieve a competitive ratio of at least
\rho-O(\epsilon). Our approach is substantially different from that of Gunther
et al. (Gunther et al., SODA 2013), in which an approximation scheme for online
over time scheduling problems is given, where the number of machines is fixed.
Our method could also be extended to several related online over list
scheduling models.",online law enforcement
http://arxiv.org/abs/1705.04480v1,"While online services emerge in all areas of life, the voting procedure in
many democracies remains paper-based as the security of current online voting
technology is highly disputed. We address the issue of trustworthy online
voting protocols and recall therefore their security concepts with its trust
assumptions. Inspired by the Bitcoin protocol, the prospects of distributed
online voting protocols are analysed. No trusted authority is assumed to ensure
ballot secrecy. Further, the integrity of the voting is enforced by all voters
themselves and without a weakest link, the protocol becomes more robust. We
introduce a taxonomy of notions of distribution in online voting protocols that
we apply on selected online voting protocols. Accordingly, blockchain-based
protocols seem to be promising for online voting due to their similarity with
paper-based protocols.",online law enforcement
http://arxiv.org/abs/1609.07602v1,"With an exponentially increasing usage of cloud services, the need for
forensic investigations of virtual space is equally in constantly increasing
demand, which includes as a very first approach, the gaining of access to it as
well as the data stored. This is an aspect that faces a number of challenges,
stemming not only from the technical difficulties and peculiarities, but
equally covers the interaction with an emerging line of businesses offering
cloud storage and services. Beyond the forensic aspects, it also covers to an
ever increasing amount the non-forensic considerations, such as the
availability of logs and archives, legal and data protection considerations
from a global perspective and the clashes in between, as well as the ever
competing interests between law enforcement to seize evidence which is
non-physical, and businesses who need to be able to continue to operate and
provide their hosted services, even if law enforcement seek to collect
evidence. The trend post-Snowden has been unequivocally towards default
encryption, and driven by market leaders such as Apple, motivated to a large
extent by the perceived demands for privacy of the consumer. The central
question to be explored in this paper is to what extent this trend towards
default encryption will have a negative impact on law enforcement
investigations and possibilities, and will at the end attempt to provide a
solution, which takes into account the needs of both law enforcement, but also
of the service providers. It is hoped that the recommendations from this paper
will be able to have an impact in the ability for law enforcement to continue
with their investigations in an efficient manner, whilst also safeguarding the
ability for business to thrive and continue to develop and offer new and
innovative solutions, which do not put law enforcement at risk.",online law enforcement
http://arxiv.org/abs/1401.5178v1,"The economics of an internet crime has newly developed into a field of
controlling black money. This economic approach not only provides estimated
technique of analyzing internet crimes but also gives details to analyzers of
system dependability and divergence. This paper will highlight on the subject
of online crime, which has formed its industry since. It all started from
amateur hackers who cracked websites and wrote malicious software in pursuit of
fun or achieving limited objectives to professional hacking. In the past days,
electronic fraud was main objective but now it has been changed into electronic
hacking. This study focuses the issue through an economic analysis of available
web forum to deals in malware and private information. The findings of this
survey research provide considerable in-depth sight into the functions of
malware economy spinning around computer impositions and compromise. In this
regard, the survey research paper may benefit particularly computer security
officials, the law enforcement agencies, and in general prospective anyone
involved in better understanding cybercrime from the offender standpoint.",online law enforcement
http://arxiv.org/abs/1403.6315v2,"The spread of rumors through social media and online social networks can not
only disrupt the daily lives of citizens but also result in loss of life and
property. A rumor spreads when individuals, who are unable decide the
authenticity of the information, mistake the rumor as genuine information and
pass it on to their acquaintances. We propose a solution where a set of
individuals (based on their degree) in the social network are trained and
provided resources to help them distinguish a rumor from genuine information.
By formulating an optimization problem we calculate the optimum set of
individuals, who must undergo training, and the quality of training that
minimizes the expected training cost and ensures an upper bound on the size of
the rumor outbreak. Our primary contribution is that although the optimization
problem turns out to be non convex, we show that the problem is equivalent to
solving a set of linear programs. This result also allows us to solve the
problem of minimizing the size of rumor outbreak for a given cost budget. The
optimum solution displays an interesting pattern which can be implemented as a
heuristic. These results can prove to be very useful for social planners and
law enforcement agencies for preventing dangerous rumors and misinformation
epidemics.",online law enforcement
http://arxiv.org/abs/1712.00846v1,"Web-based human trafficking activity has increased in recent years but it
remains sparsely dispersed among escort advertisements and difficult to
identify due to its often-latent nature. The use of intelligent systems to
detect trafficking can thus have a direct impact on investigative resource
allocation and decision-making, and, more broadly, help curb a widespread
social problem. Trafficking detection involves assigning a normalized score to
a set of escort advertisements crawled from the Web -- a higher score indicates
a greater risk of trafficking-related (involuntary) activities. In this paper,
we define and study the problem of trafficking detection and present a
trafficking detection pipeline architecture developed over three years of
research within the DARPA Memex program. Drawing on multi-institutional data,
systems, and experiences collected during this time, we also conduct post hoc
bias analyses and present a bias mitigation plan. Our findings show that, while
automatic trafficking detection is an important application of AI for social
good, it also provides cautionary lessons for deploying predictive machine
learning algorithms without appropriate de-biasing. This ultimately led to
integration of an interpretable solution into a search system that contains
over 100 million advertisements and is used by over 200 law enforcement
agencies to investigate leads.",online law enforcement
http://arxiv.org/abs/1504.01093v1,"We consider dynamic pricing schemes in online settings where selfish agents
generate online events. Previous work on online mechanisms has dealt almost
entirely with the goal of maximizing social welfare or revenue in an auction
settings. This paper deals with quite general settings and minimizing social
costs. We show that appropriately computed posted prices allow one to achieve
essentially the same performance as the best online algorithm. This holds in a
wide variety of settings. Unlike online algorithms that learn about the event,
and then make enforceable decisions, prices are posted without knowing the
future events or even the current event, and are thus inherently dominant
strategy incentive compatible.
  In particular we show that one can give efficient posted price mechanisms for
metrical task systems, some instances of the $k$-server problem, and metrical
matching problems. We give both deterministic and randomized algorithms. Such
posted price mechanisms decrease the social cost dramatically over selfish
behavior where no decision incurs a charge. One alluring application of this is
reducing the social cost of free parking exponentially.",online law enforcement
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",online law enforcement
http://arxiv.org/abs/1703.10764v1,"Global optimization algorithms have shown impressive performance in
data-association based multi-object tracking, but handling online data remains
a difficult hurdle to overcome. In this paper, we present a hybrid data
association framework with a min-cost multi-commodity network flow for robust
online multi-object tracking. We build local target-specific models interleaved
with global optimization of the optimal data association over multiple video
frames. More specifically, in the min-cost multi-commodity network flow, the
target-specific similarities are online learned to enforce the local
consistency for reducing the complexity of the global data association.
Meanwhile, the global data association taking multiple video frames into
account alleviates irrecoverable errors caused by the local data association
between adjacent frames. To ensure the efficiency of online tracking, we give
an efficient near-optimal solution to the proposed min-cost multi-commodity
flow problem, and provide the empirical proof of its sub-optimality. The
comprehensive experiments on real data demonstrate the superior tracking
performance of our approach in various challenging situations.",online law enforcement
http://arxiv.org/abs/1310.0505v1,"Online social networks such as Twitter and Facebook have gained tremendous
popularity for information exchange. The availability of unprecedented amounts
of digital data has accelerated research on information diffusion in online
social networks. However, the mechanism of information spreading in online
social networks remains elusive due to the complexity of social interactions
and rapid change of online social networks. Much of prior work on information
diffusion over online social networks has based on empirical and statistical
approaches. The majority of dynamical models arising from information diffusion
over online social networks involve ordinary differential equations which only
depend on time. In a number of recent papers, the authors propose to use
partial differential equations(PDEs) to characterize temporal and spatial
patterns of information diffusion over online social networks. Built on
intuitive cyber-distances such as friendship hops in online social networks,
the reaction-diffusion equations take into account influences from various
external out-of-network sources, such as the mainstream media, and provide a
new analytic framework to study the interplay of structural and topical
influences on information diffusion over online social networks. In this
survey, we discuss a number of PDE-based models that are validated with real
datasets collected from popular online social networks such as Digg and
Twitter. Some new developments including the conservation law of information
flow in online social networks and information propagation speeds based on
traveling wave solutions are presented to solidify the foundation of the PDE
models and highlight the new opportunities and challenges for mathematicians as
well as computer scientists and researchers in online social networks.",online law enforcement
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",e-commerce fraud
http://arxiv.org/abs/1709.01213v4,"Although mobile ad frauds have been widespread, state-of-the-art approaches
in the literature have mainly focused on detecting the so-called static
placement frauds, where only a single UI state is involved and can be
identified based on static information such as the size or location of ad
views. Other types of fraud exist that involve multiple UI states and are
performed dynamically while users interact with the app. Such dynamic
interaction frauds, although now widely spread in apps, have not yet been
explored nor addressed in the literature. In this work, we investigate a wide
range of mobile ad frauds to provide a comprehensive taxonomy to the research
community. We then propose, FraudDroid, a novel hybrid approach to detect ad
frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI
state transition graphs and collects their associated runtime network traffics,
which are then leveraged to check against a set of heuristic-based rules for
identifying ad fraudulent behaviours. We show empirically that FraudDroid
detects ad frauds with a high precision (93%) and recall (92%). Experimental
results further show that FraudDroid is capable of detecting ad frauds across
the spectrum of fraud types. By analysing 12,000 ad-supported Android apps,
FraudDroid identified 335 cases of fraud associated with 20 ad networks that
are further confirmed to be true positive results and are shared with our
fellow researchers to promote advanced ad fraud detection",e-commerce fraud
http://arxiv.org/abs/1309.3944v1,"With an upsurge in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection (FAFD) has become an
emerging topic of great importance for academic, research and industries. The
failure of internal auditing system of the organization in identifying the
accounting frauds has lead to use of specialized procedures to detect financial
accounting fraud, collective known as forensic accounting. Data mining
techniques are providing great aid in financial accounting fraud detection,
since dealing with the large data volumes and complexities of financial data
are big challenges for forensic accounting. This paper presents a comprehensive
review of the literature on the application of data mining techniques for the
detection of financial accounting fraud and proposes a framework for data
mining techniques based accounting fraud detection. The systematic and
comprehensive literature review of the data mining techniques applicable to
financial accounting fraud detection may provide a foundation to future
research in this field. The findings of this review show that data mining
techniques like logistic models, neural networks, Bayesian belief network, and
decision trees have been applied most extensively to provide primary solutions
to the problems inherent in the detection and classification of fraudulent
data.",e-commerce fraud
http://arxiv.org/abs/1907.03048v1,"Download fraud is a prevalent threat in mobile App markets, where fraudsters
manipulate the number of downloads of Apps via various cheating approaches.
Purchased fake downloads can mislead recommendation and search algorithms and
further lead to bad user experience in App markets. In this paper, we
investigate download fraud problem based on a company's App Market, which is
one of the most popular Android App markets. We release a honeypot App on the
App Market and purchase fake downloads from fraudster agents to track fraud
activities in the wild. Based on our interaction with the fraudsters, we
categorize download fraud activities into three types according to their
intentions: boosting front end downloads, optimizing App search ranking, and
enhancing user acquisition&retention rate. For the download fraud aimed at
optimizing App search ranking, we select, evaluate, and validate several
features in identifying fake downloads based on billions of download data. To
get a comprehensive understanding of download fraud, we further gather stances
of App marketers, fraudster agencies, and market operators on download fraud.
The followed analysis and suggestions shed light on the ways to mitigate
download fraud in App markets and other social platforms. To the best of our
knowledge, this is the first work that investigates the download fraud problem
in mobile App markets.",e-commerce fraud
http://arxiv.org/abs/1601.01228v1,"Financial fraud detection is an important problem with a number of design
aspects to consider. Issues such as algorithm selection and performance
analysis will affect the perceived ability of proposed solutions, so for
auditors and re-searchers to be able to sufficiently detect financial fraud it
is necessary that these issues be thoroughly explored. In this paper we will
revisit the key performance metrics used for financial fraud detection with a
focus on credit card fraud, critiquing the prevailing ideas and offering our
own understandings. There are many different performance metrics that have been
employed in prior financial fraud detection research. We will analyse several
of the popular metrics and compare their effectiveness at measuring the ability
of detection mechanisms. We further investigated the performance of a range of
computational intelligence techniques when applied to this problem domain, and
explored the efficacy of several binary classification methods.",e-commerce fraud
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",e-commerce fraud
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",e-commerce fraud
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",e-commerce fraud
http://arxiv.org/abs/0803.4058v3,"Typical arguments against scientific misconduct generally fail to support
current policies on research fraud: they may not prove wrong what is usually
considered research misconduct and they tend to make wrong things that are not
normally seen as scientific fraud, in particular honest errors. I also point
out that sanctions are not consistent with the reasons why scientific fraud is
supposed to be wrong either. Moreover honestly seeking truth should not be
contrived as a moral rule -- it is instead a necessary condition for work to
qualify as scientific.
  Keywords: cheating; ethics; fabrication; falsification; integrity;
plagiarism; research fraud; scientific misconduct.",e-commerce fraud
http://arxiv.org/abs/1510.07167v1,"Financial statement fraud detection is an important problem with a number of
design aspects to consider. Issues such as (i) problem representation, (ii)
feature selection, and (iii) choice of performance metrics all influence the
perceived performance of detection algorithms. Efficient implementation of
financial fraud detection methods relies on a clear understanding of these
issues. In this paper we present an analysis of the three key experimental
issues associated with financial statement fraud detection, critiquing the
prevailing ideas and providing new understandings.",e-commerce fraud
http://arxiv.org/abs/1808.07288v1,"Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.",e-commerce fraud
http://arxiv.org/abs/0910.2048v1,"This brief paper outlines how spreadsheets were used as one of the vehicles
for John Rusnak's fraud and the revenue control lessons this case gives us.",e-commerce fraud
http://arxiv.org/abs/1611.06439v1,"Credit card plays a very important rule in today's economy. It becomes an
unavoidable part of household, business and global activities. Although using
credit cards provides enormous benefits when used carefully and
responsibly,significant credit and financial damages may be caused by
fraudulent activities. Many techniques have been proposed to confront the
growth in credit card fraud. However, all of these techniques have the same
goal of avoiding the credit card fraud; each one has its own drawbacks,
advantages and characteristics. In this paper, after investigating difficulties
of credit card fraud detection, we seek to review the state of the art in
credit card fraud detection techniques, data sets and evaluation criteria.The
advantages and disadvantages of fraud detection methods are enumerated and
compared.Furthermore, a classification of mentioned techniques into two main
fraud detection approaches, namely, misuses (supervised) and anomaly detection
(unsupervised) is presented. Again, a classification of techniques is proposed
based on capability to process the numerical and categorical data sets.
Different data sets used in literature are then described and grouped into real
and synthesized data and the effective and common attributes are extracted for
further usage.Moreover, evaluation employed criterions in literature are
collected and discussed.Consequently, open issues for credit card fraud
detection are explained as guidelines for new researchers.",e-commerce fraud
http://arxiv.org/abs/1405.5704v1,"Contactless technologies such as RFID, NFC, and sensor networks are
vulnerable to mafia and distance frauds. Both frauds aim at passing an
authentication protocol by cheating on the actual distance between the prover
and the verifier. To cope these security issues, distance-bounding protocols
have been designed. However, none of the current proposals simultaneously
resists to these two frauds without requiring additional memory and
computation. The situation is even worse considering that just a few
distance-bounding protocols are able to deal with the inherent background noise
on the communication channels. This article introduces a noise-resilient
distance-bounding protocol that resists to both mafia and distance frauds. The
security of the protocol is analyzed with respect to these two frauds in both
scenarios, namely noisy and noiseless channels. Analytical expressions for the
adversary's success probabilities are provided, and are illustrated by
experimental results. The analysis, performed in an already existing framework
for fairness reasons, demonstrates the undeniable advantage of the introduced
lightweight design over the previous proposals.",e-commerce fraud
http://arxiv.org/abs/1304.6501v1,"Occupational fraud affects many companies worldwide causing them economic
loss and liability issues towards their customers and other involved entities.
Detecting internal fraud in a company requires significant effort and,
unfortunately cannot be entirely prevented. The internal auditors have to
process a huge amount of data produced by diverse systems, which are in most
cases in textual form, with little automated support. In this paper, we exploit
the advantages of information visualization and present a system that aims to
detect occupational fraud in systems which involve a pair of entities (e.g., an
employee and a client) and periodic activity. The main visualization is based
on a spiral system on which the events are drawn appropriately according to
their time-stamp. Suspicious events are considered those which appear along the
same radius or on close radii of the spiral. Before producing the
visualization, the system ranks both involved entities according to the
specifications of the internal auditor and generates a video file of the
activity such that events with strong evidence of fraud appear first in the
video. The system is also equipped with several different visualizations and
mechanisms in order to meet the requirements of an internal fraud detection
system.",e-commerce fraud
http://arxiv.org/abs/1804.07481v1,"Credit card fraud detection is a very challenging problem because of the
specific nature of transaction data and the labeling process. The transaction
data is peculiar because they are obtained in a streaming fashion, they are
strongly imbalanced and prone to non-stationarity. The labeling is the outcome
of an active learning process, as every day human investigators contact only a
small number of cardholders (associated to the riskiest transactions) and
obtain the class (fraud or genuine) of the related transactions. An adequate
selection of the set of cardholders is therefore crucial for an efficient fraud
detection process. In this paper, we present a number of active learning
strategies and we investigate their fraud detection accuracies. We compare
different criteria (supervised, semi-supervised and unsupervised) to query
unlabeled transactions. Finally, we highlight the existence of an
exploitation/exploration trade-off for active learning in the context of fraud
detection, which has so far been overlooked in the literature.",e-commerce fraud
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud pricing
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud pricing
http://arxiv.org/abs/cs/0110006v1,"This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud pricing
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud pricing
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud pricing
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud pricing
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud pricing
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",e-commerce fraud pricing
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud pricing
http://arxiv.org/abs/1707.03367v1,"In the e-commerce world, the follow-up of prices in detail web pages is of
great interest for things like buying a product when it falls below some
threshold. For doing this task, instead of bookmarking the pages and revisiting
them, in this paper we propose a novel web data extraction system, called
Wextractor. It consists of an extraction method and a web app for listing the
retrieved prices. As for the final user, the main feature of Wextractor is
usability because (s)he only has to signal the pages of interest and our system
automatically extracts the price from the page.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud pricing
http://arxiv.org/abs/1806.05799v2,"As the largest e-commerce platform, Taobao helps advertisers reach billions
of search queries each day via sponsored search, which has also contributed
considerable revenue to the platform. An efficient bidding strategy to cater to
diverse advertiser demands while balancing platform revenue and consumer
experience is significant to a healthy and sustainable marketing ecosystem. In
this paper we propose \emph{Customer Intelligent Agent (CIA)}, a bidding
optimization framework which implements an impression-level bidding to reflect
advertisers' conversion willingness and budget control. In this way, CIA is
capable of fulfilling various e-commerce advertiser demands on different
levels, such as Gross Merchandise Volume optimization, style comparison etc.
Additionally, a replay based simulation system is designed to predict the
performance of different take-rate. CIA unifies the benefits of three parties
in the marketing ecosystem without changing the Generalized Second Price
mechanism. Our extensive offline simulations and large-scale online experiments
on \emph{Taobao Search Advertising (TSA)} platform verify the high
effectiveness of the CIA framework. Moreover, CIA has been deployed online as a
major bidding tool in TSA.",e-commerce fraud pricing
http://arxiv.org/abs/1806.09793v1,"With the considerable development of customer-to-customer (C2C) e-commerce in
the recent years, there is a big demand for an effective recommendation system
that suggests suitable websites for users to sell their items with some
specified needs. Nonetheless, e-commerce recommendation systems are mostly
designed for business-to-customer (B2C) websites, where the systems offer the
consumers the products that they might like to buy. Almost none of the related
research works focus on choosing selling sites for target items. In this paper,
we introduce an approach that recommends the selling websites based upon the
item's description, category, and desired selling price. This approach employs
NoSQL data-based machine learning techniques for building and training topic
models and classification models. The trained models can then be used to rank
the websites dynamically with respect to the user needs. The experimental
results with real-world datasets from Vietnam C2C websites will demonstrate the
effectiveness of our proposed method.",e-commerce fraud pricing
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud pricing
http://arxiv.org/abs/physics/0608232v1,"We characterize the statistical properties of a large number of online
auctions run on eBay. Both stationary and dynamic properties, like
distributions of prices, number of bids etc., as well as relations between
these quantities are studied. The analysis of the data reveals surprisingly
simple distributions and relations, typically of power-law form. Based on these
findings we introduce a simple method to identify suspicious auctions that
could be influenced by a form of fraud known as shill bidding. Furthermore the
influence of bidding strategies is discussed. The results indicate that the
observed behavior is related to a mixture of agents using a variety of
strategies.",e-commerce fraud pricing
http://arxiv.org/abs/1905.04770v1,"Motivated by the dynamic assortment offerings and item pricings occurring in
e-commerce, we study a general problem of allocating finite inventories to
heterogeneous customers arriving sequentially. We analyze this problem under
the framework of competitive analysis, where the sequence of customers is
unknown and does not necessarily follow any pattern. Previous work in this
area, studying online matching, advertising, and assortment problems, has
focused on the case where each item can only be sold at a single price,
resulting in algorithms which achieve the best-possible competitive ratio of
1-1/e.
  In this paper, we extend all of these results to allow for items having
multiple feasible prices. Our algorithms achieve the best-possible
weight-dependent competitive ratios, which depend on the sets of feasible
prices given in advance. Our algorithms are also simple and intuitive; they are
based on constructing a class of universal ``value functions'' which integrate
the selection of items and prices offered.
  Finally, we test our algorithms on the publicly-available hotel data set of
Bodea et al. (2009), where there are multiple items (hotel rooms) each with
multiple prices (fares at which the room could be sold). We find that applying
our algorithms, as a ``hybrid'' with algorithms which attempt to forecast and
learn the future transactions, results in the best performance.",e-commerce fraud pricing
http://arxiv.org/abs/1711.02661v1,"In recent years, many new and interesting models of successful online
business have been developed, including competitive models such as auctions,
where the product price tends to rise, and group-buying, where users cooperate
obtaining a dynamic price that tends to go down. We propose the e-fair as a
business model for social commerce, where both sellers and buyers are grouped
to maximize benefits. e-Fairs extend the group-buying model aggregating demand
and supply for price optimization as well as consolidating shipments and
optimize withdrawals for guaranteeing additional savings. e-Fairs work upon
multiple dimensions: time to aggregate buyers, their geographical distribution,
price/quantity curves provided by sellers, and location of withdrawal points.
We provide an analytical model for time and spatial optimization and simulate
realistic scenarios using both real purchase data from an Italian marketplace
and simulated ones. Experimental results demonstrate the potentials offered by
e-fairs and show benefits for all the involved actors.",e-commerce fraud pricing
http://arxiv.org/abs/1211.3148v1,"Retailers in Saudi Arabia have been reserved in their adoption of
electronically delivered aspects of their business. This paper reports research
that identifies and explores key issues to enhance the diffusion of online
retailing in Saudi Arabia. Despite the fact that Saudi Arabia has the largest
and fastest growth of ICT marketplaces in the Arab region, e-commerce
activities are not progressing at the same speed. Only very few Saudi
companies, mostly medium and large companies from the manufacturing sector, are
involved in e-commerce implementation. Based on qualitative data collected by
conducting interviews with 16 retailers and 16 potential customers in Saudi
Arabia, 7 key drivers to online retailing diffusion in Saudi Arabia are
identified. These key drivers are government support, providing trustworthy and
secure online payments options, provision of individual house mailboxes,
providing high speed Internet connection at low cost, providing educational
programs, the success of bricks-and-clicks model, and competitive prices.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07607v3,"We study the problem of allocating impressions to sellers in e-commerce
websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue
generated by the platform. We employ a general framework of reinforcement
mechanism design, which uses deep reinforcement learning to design efficient
algorithms, taking the strategic behaviour of the sellers into account.
Specifically, we model the impression allocation problem as a Markov decision
process, where the states encode the history of impressions, prices,
transactions and generated revenue and the actions are the possible impression
allocations in each round. To tackle the problem of continuity and
high-dimensionality of states and actions, we adopt the ideas of the DDPG
algorithm to design an actor-critic policy gradient algorithm which takes
advantage of the problem domain in order to achieve convergence and stability.
We evaluate our proposed algorithm, coined IA(GRU), by comparing it against
DDPG, as well as several natural heuristics, under different rationality models
for the sellers - we assume that sellers follow well-known no-regret type
strategies which may vary in their degree of sophistication. We find that
IA(GRU) outperforms all algorithms in terms of the total revenue.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07946v1,"Sales forecast is an essential task in E-commerce and has a crucial impact on
making informed business decisions. It can help us to manage the workforce,
cash flow and resources such as optimizing the supply chain of manufacturers
etc. Sales forecast is a challenging problem in that sales is affected by many
factors including promotion activities, price changes, and user preferences
etc. Traditional sales forecast techniques mainly rely on historical sales data
to predict future sales and their accuracies are limited. Some more recent
learning-based methods capture more information in the model to improve the
forecast accuracy. However, these methods require case-by-case manual feature
engineering for specific commercial scenarios, which is usually a difficult,
time-consuming task and requires expert knowledge. To overcome the limitations
of existing methods, we propose a novel approach in this paper to learn
effective features automatically from the structured data using the
Convolutional Neural Network (CNN). When fed with raw log data, our approach
can automatically extract effective features from that and then forecast sales
using those extracted features. We test our method on a large real-world
dataset from CaiNiao.com and the experimental results validate the
effectiveness of our method.",e-commerce fraud pricing
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud pricing
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud pricing
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud pricing
http://arxiv.org/abs/1409.6559v1,"Online auctions are among the most influential e-business applications. Their
impact on trading for businesses, as well as consumers, is both remarkable and
inevitable. There have been considerable efforts in setting up market places,
but, with respects to market volume, online trading is still in its early
stages. This chapter discusses the benefits of the concept of Internet
marketplaces, with the highest impact on pricing strategies, namely, the
conduction of online business auctions. We discuss their benefits, problems and
possible solutions. In addition, we sketch actions for suppliers to achieve a
better strategic position in the upcoming Internet market places.",e-commerce fraud pricing
http://arxiv.org/abs/1809.09621v1,"Complementary products recommendation is an important problem in e-commerce.
Such recommendations increase the average order price and the number of
products in baskets. Complementary products are typically inferred from basket
data. In this study, we propose the BB2vec model. The BB2vec model learns
vector representations of products by analyzing jointly two types of data -
Baskets and Browsing sessions (visiting web pages of products). These vector
representations are used for making complementary products recommendation. The
proposed model alleviates the cold start problem by delivering better
recommendations for products having few or no purchases. We show that the
BB2vec model has better performance than other models which use only basket
data.",e-commerce fraud pricing
http://arxiv.org/abs/1709.07534v1,"E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.",e-commerce fraud pricing
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake review detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake review detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake review detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake review detection
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake review detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake review detection
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",fake review detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake review detection
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",fake review detection
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",fake review detection
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",fake review detection
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",fake review detection
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",fake review detection
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake review detection
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",fake review detection
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake review detection
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",fake review detection
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",fake review detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake review detection
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",fake review detection
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake review detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake review detection
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake review detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake review detection
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",fake review detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake review detection
http://arxiv.org/abs/1706.00884v1,"Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.",fake review detection
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake review detection
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake review detection
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake review detection
http://arxiv.org/abs/1407.3077v1,"A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.",consumer profiling algorithm
http://arxiv.org/abs/1411.3961v2,"Loyalty programs are promoted by vendors to incentivize loyalty in buyers.
Although such programs have become widespread, they have been criticized by
business experts and consumer associations: loyalty results in profiling and
hence in loss of privacy of consumers. We propose a protocol for
privacy-preserving loyalty programs that allows vendors and consumers to enjoy
the benefits of loyalty (returning customers and discounts, respectively),
while allowing consumers to stay anonymous and empowering them to decide how
much of their profile they reveal to the vendor. The vendor must offer
additional reward if he wants to learn more details on the consumer's profile.
Our protocol is based on partially blind signatures and generalization
techniques, and provides anonymity to consumers and their purchases, while
still allowing negotiated consumer profiling.",consumer profiling algorithm
http://arxiv.org/abs/1604.08330v1,"Server consolidation based on virtualization technology simplifies system
administration and improves energy efficiency by improving resource
utilizations and reducing the physical machine (PM) number in contemporary
service-oriented data centers. The elasticity of Internet applications changes
the consolidation technologies from addressing virtual machines (VMs) to PMs
mapping schemes which must know the VMs statuses, i.e. the number of VMs and
the profiling data of each VM, into providing the application-to-VM-to-PM
mapping. In this paper, we study on the consolidation of multiple Internet
applications, minimizing the number of PMs with required performance. We first
model the consolidation providing the application-to-VM-to-PM mapping to
minimize the number of PMs as an integer linear programming problem, and then
present a heuristic algorithm to solve the problem in polynomial time.
Extensive experimental results show that our heuristic algorithm consumes less
than 4.3% more resources than the optimal amounts with few overheads. Existing
consolidation technologies using the input of the VM statuses output by our
heuristic algorithm consume 1.06% more PMs.",consumer profiling algorithm
http://arxiv.org/abs/1208.4651v1,"In wireless networks, energy consumed for communication includes both the
transmission and the processing energy. In this paper, point-to-point
communication over a fading channel with an energy harvesting transmitter is
studied considering jointly the energy costs of transmission and processing.
Under the assumption of known energy arrival and fading profiles, optimal
transmission policy for throughput maximization is investigated. Assuming that
the transmitter has sufficient amount of data in its buffer at the beginning of
the transmission period, the average throughput by a given deadline is
maximized. Furthermore, a ""directional glue pouring algorithm"" that computes
the optimal transmission policy is described.",consumer profiling algorithm
http://arxiv.org/abs/1304.5197v1,"Identifying the hottest paths in the control flow graph of a routine can
direct optimizations to portions of the code where most resources are consumed.
This powerful methodology, called path profiling, was introduced by Ball and
Larus in the mid 90s and has received considerable attention in the last 15
years for its practical relevance. A shortcoming of Ball-Larus path profiling
was the inability to profile cyclic paths, making it difficult to mine
interesting execution patterns that span multiple loop iterations. Previous
results, based on rather complex algorithms, have attempted to circumvent this
limitation at the price of significant performance losses already for a small
number of iterations. In this paper, we present a new approach to multiple
iterations path profiling, based on data structures built on top of the
original Ball-Larus numbering technique. Our approach allows it to profile all
executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths,
where k is a user-defined parameter. An extensive experimental investigation on
a large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,
our approach can be even faster than Ball-Larus due to fewer operations on
smaller hash tables, producing compact representations of cyclic paths even for
large values of k.",consumer profiling algorithm
http://arxiv.org/abs/1907.12219v1,"JPEG is one of the popular image compression algorithms that provide
efficient storage and transmission capabilities in consumer electronics, and
hence it is the most preferred image format over the internet world. In the
present digital and Big-data era, a huge volume of JPEG compressed document
images are being archived and communicated through consumer electronics on
daily basis. Though it is advantageous to have data in the compressed form on
one side, however, on the other side processing with off-the-shelf methods
becomes computationally expensive because it requires decompression and
recompression operations. Therefore, it would be novel and efficient, if the
compressed data are processed directly in their respective compressed domains
of consumer electronics. In the present research paper, we propose to
demonstrate this idea taking the case study of printed text line segmentation.
Since, JPEG achieves compression by dividing the image into non overlapping 8x8
blocks in the pixel domain and using Discrete Cosine Transform (DCT); it is
very likely that the partitioned 8x8 DCT blocks overlap the contents of two
adjacent text-lines without leaving any clue for the line separator, thus
making text-line segmentation a challenging problem. Two approaches of
segmentation have been proposed here using the DC projection profile and AC
coefficients of each 8x8 DCT block. The first approach is based on the strategy
of partial decompression of selected DCT blocks, and the second approach is
with intelligent analysis of F10 and F11 AC coefficients and without using any
type of decompression. The proposed methods have been tested with variable font
sizes, font style and spacing between lines, and a good performance is
reported.",consumer profiling algorithm
http://arxiv.org/abs/1401.2440v1,"Future e-business models will rely on electronic contracts which are agreed
dynamically and adaptively by web services. Thus, the automatic negotiation of
Service Level Agreements (SLAs) between consumers and providers is key for
enabling service-based value chains.
  The process of finding appropriate providers for web services seems to be
simple. Consumers contact several providers and take the provider which offers
the best matching SLA. However, currently consumers are not able forecasting
the probability of finding a matching provider for their requested SLA. So
consumers contact several providers and check if their offers are matching. In
case of continuing faults, on the one hand consumers may adapt their Service
Level Objects (SLOs) of the required SLA or on the other hand simply accept
offered SLAs of the contacted providers.
  By forecasting the probability of finding a matching provider, consumers
could assess their chances of finding a provider offering the requested SLA. If
a low probability is predicted, consumers can immediately adapt their SLOs or
increase the numbers of providers to be contacted.
  Thus, this paper proposes an analytical forecast model, which allows
consumers to get a realistic assessment of the probability to find matching
providers. Additionally, we present an optimization algorithm based on the
forecast results, which allows adapting the SLO parameter ranges in order to
find at least one matching provider. Not only consumers, but also providers can
use this forecast model to predict the prospective demand. So providers are
able to assess the number of potential consumers based on their offers too.
  Justification of our approach is done by simulation of practical examples
checking our theoretical findings.",consumer profiling algorithm
http://arxiv.org/abs/1502.02125v2,"The last decade has witnessed a tremendous growth in the volume as well as
the diversity of multimedia content generated by a multitude of sources (news
agencies, social media, etc.). Faced with a variety of content choices,
consumers are exhibiting diverse preferences for content; their preferences
often depend on the context in which they consume content as well as various
exogenous events. To satisfy the consumers' demand for such diverse content,
multimedia content aggregators (CAs) have emerged which gather content from
numerous multimedia sources. A key challenge for such systems is to accurately
predict what type of content each of its consumers prefers in a certain
context, and adapt these predictions to the evolving consumers' preferences,
contexts and content characteristics. We propose a novel, distributed, online
multimedia content aggregation framework, which gathers content generated by
multiple heterogeneous producers to fulfill its consumers' demand for content.
Since both the multimedia content characteristics and the consumers'
preferences and contexts are unknown, the optimal content aggregation strategy
is unknown a priori. Our proposed content aggregation algorithm is able to
learn online what content to gather and how to match content and users by
exploiting similarities between consumer types. We prove bounds for our
proposed learning algorithms that guarantee both the accuracy of the
predictions as well as the learning speed. Importantly, our algorithms operate
efficiently even when feedback from consumers is missing or content and
preferences evolve over time. Illustrative results highlight the merits of the
proposed content aggregation system in a variety of settings.",consumer profiling algorithm
http://arxiv.org/abs/1110.5351v2,"We report on the implementation of a dynamically configurable, servomotor-
controlled, permanent magnet Zeeman slower for quantum optics experiments with
ultracold atoms and molecules. This atom slower allows for switching between
magnetic field profiles that are designed for different atomic species.
Additionally, through feedback on the atom trapping rate, we demonstrate that
computer-controlled genetic optimization algorithms applied to the magnet
positions can be used in situ to obtain field profiles that maximize the
trapping rate for any given experimental conditions. The device is lightweight,
remotely controlled, and consumes no power in steady state; it is a step toward
automated control of quantum optics experiments.",consumer profiling algorithm
http://arxiv.org/abs/1609.04053v1,"The arrival of small-scale distributed energy generation in the future smart
grid has led to the emergence of so-called prosumers, who can both consume as
well as produce energy. By using local generation from renewable energy
resources, the stress on power generation and supply system can be
significantly reduced during high demand periods. However, this also creates a
significant challenge for conventional power plants that suddenly need to ramp
up quickly when the renewable energy drops off. In this paper, we propose an
energy consumption scheduling problem for prosumers to minimize the peak ramp
of the system. The optimal schedule of prosumers can be obtained by solving the
centralized optimization problem. However, due to the privacy concerns and the
distributed topology of the power system, the centralized design is difficult
to implement in practice. Therefore, we propose the distributed algorithms to
efficiently solve the centralized problem using the alternating direction
method of multiplier (ADMM), in which each prosumer independently schedules its
energy consumption profile. The simulation results demonstrate the convergence
performance of the proposed algorithms as well as the capability of our model
in reducing the peak ramp of the system.",consumer profiling algorithm
http://arxiv.org/abs/1507.03328v1,"In this paper, we propose the amphibious influence maximization (AIM) model
that combines traditional marketing via content providers and viral marketing
to consumers in social networks in a single framework. In AIM, a set of content
providers and consumers form a bipartite network while consumers also form
their social network, and influence propagates from the content providers to
consumers and among consumers in the social network following the independent
cascade model. An advertiser needs to select a subset of seed content providers
and a subset of seed consumers, such that the influence from the seed providers
passing through the seed consumers could reach a large number of consumers in
the social network in expectation.
  We prove that the AIM problem is NP-hard to approximate to within any
constant factor via a reduction from Feige's k-prover proof system for 3-SAT5.
We also give evidence that even when the social network graph is trivial (i.e.
has no edges), a polynomial time constant factor approximation for AIM is
unlikely. However, when we assume that the weighted bi-adjacency matrix that
describes the influence of content providers on consumers is of constant rank,
a common assumption often used in recommender systems, we provide a
polynomial-time algorithm that achieves approximation ratio of
$(1-1/e-\epsilon)^3$ for any (polynomially small) $\epsilon > 0$. Our
algorithmic results still hold for a more general model where cascades in
social network follow a general monotone and submodular function.",consumer profiling algorithm
http://arxiv.org/abs/1809.05245v1,"Achieving a balance of supply and demand in a multi-agent system with many
individual self-interested and rational agents that act as suppliers and
consumers is a natural problem in a variety of real-life domains---smart power
grids, data centers, and others. In this paper, we address the
profit-maximization problem for a group of distributed supplier and consumer
agents, with no inter-agent communication. We simulate a scenario of a market
with $S$ suppliers and $C$ consumers such that at every instant, each supplier
agent supplies a certain quantity and simultaneously, each consumer agent
consumes a certain quantity. The information about the total amount supplied
and consumed is only kept with the center. The proposed algorithm is a
combination of the classical additive-increase multiplicative-decrease (AIMD)
algorithm in conjunction with a probabilistic rule for the agents to respond to
a capacity signal. This leads to a nonhomogeneous Markov chain and we show
almost sure convergence of this chain to the social optimum, for our market of
distributed supplier and consumer agents. Employing this AIMD-type algorithm,
the center sends a feedback message to the agents in the supplier side if there
is a scenario of excess supply, or to the consumer agents if there is excess
consumption. Each agent has a concave utility function whose derivative tends
to 0 when an optimum quantity is supplied/consumed. Hence when social
convergence is reached, each agent supplies or consumes a quantity which leads
to its individual maximum profit, without the need of any communication. So
eventually, each agent supplies or consumes a quantity which leads to its
individual maximum profit, without communicating with any other agents. Our
simulations show the efficacy of this approach.",consumer profiling algorithm
http://arxiv.org/abs/1803.03560v2,"In this paper, we propose a distributed control strategy for the design of an
energy market. The method relies on a hierarchical structure of aggregators for
the coordination of prosumers (agents which can produce and consume energy).
The hierarchy reflects the voltage level separations of the electrical grid and
allows aggregating prosumers in pools, while taking into account the grid
operational constraints. To reach optimal coordination, the prosumers
communicate their forecasted power profile to the upper level of the hierarchy.
Each time the information crosses upwards a level of the hierarchy, it is first
aggregated, both to strongly reduce the data flow and to preserve the privacy.
In the first part of the paper, the decomposition algorithm, which is based on
the alternating direction method of multipliers (ADMM), is presented. In the
second part, we explore how the proposed algorithm scales with increasing
number of prosumers and hierarchical levels, through extensive simulations
based on randomly generated scenarios.",consumer profiling algorithm
http://arxiv.org/abs/1805.11646v1,"Gradient index (GRIN) acoustic devices have spatially inhomogeneous
refractive index profile and allow flexible control of the propagation of
acoustic waves. Previous GRIN acoustic lenses are mostly inherently
two-dimensional designs that are difficult to be extended to all three
dimensions. Besides, manually designing the spatially inhomogeneous structure
is both time-consuming and error-prone. In this work, we proposed and
numerically verified an automated computer-aided design tool: GRadient Index
Pick-and-Place (GRIPP) algorithm, for generating three-dimensional GRIN
acoustic wave controlling devices with scalable and 3D printable structures.
The algorithm receives as inputs a spatial distribution of refractive index and
a pre-defined library of gradient index unit cells, and outputs a 3D model of
GRIN device that is ready to be 3D printed. The tool enables rapid design and
realization of a large variety of 3D GRIN acoustic devices, which can be useful
in areas such as speaker system design, airborne ultrasonic sensing, as well as
therapeutic ultrasound.",consumer profiling algorithm
http://arxiv.org/abs/1806.09542v1,"Mapping and translating professional but arcane clinical jargons to consumer
language is essential to improve the patient-clinician communication.
Researchers have used the existing biomedical ontologies and consumer health
vocabulary dictionary to translate between the languages. However, such
approaches are limited by expert efforts to manually build the dictionary,
which is hard to be generalized and scalable. In this work, we utilized the
embeddings alignment method for the word mapping between unparalleled clinical
professional and consumer language embeddings. To map semantically similar
words in two different word embeddings, we first independently trained word
embeddings on both the corpus with abundant clinical professional terms and the
other with mainly healthcare consumer terms. Then, we aligned the embeddings by
the Procrustes algorithm. We also investigated the approach with the
adversarial training with refinement. We evaluated the quality of the alignment
through the similar words retrieval both by computing the model precision and
as well as judging qualitatively by human. We show that the Procrustes
algorithm can be performant for the professional consumer language embeddings
alignment, whereas adversarial training with refinement may find some relations
between two languages.",consumer profiling algorithm
http://arxiv.org/abs/1802.08112v1,"A rational behavior of a consumer is analyzed when the user participates in a
Peak Time Rebate (PTR) mechanism, which is a demand response (DR) incentive
program based on a baseline. A multi-stage stochastic programming is proposed
from the demand side in order to understand the rational decisions. The
consumer preferences are modeled as a risk-averse function under additive
uncertainty. The user chooses the optimal consumption profile to maximize his
economic benefits for each period. The stochastic optimization problem is
solved backward in time. A particular situation is developed when the System
Operator (SO) uses consumption of the previous interval as the
household-specific baseline for the DR program. It is found that a rational
consumer alters the baseline in order to increase the well-being when there is
an economic incentive. As results, whether the incentive is lower than the
retail price, the user shifts his load requirement to the baseline setting
period. On the other hand, if the incentive is greater than the regular energy
price, the optimal decision is that the user spends the maximum possible energy
in the baseline setting period and reduces the consumption at the PTR time.
This consumer behavior produces more energy consumption in total considering
all periods. In addition, the user with high uncertainty level in his energy
pattern should spend less energy than a predictable consumer when the incentive
is lower than the retail price.",consumer profiling algorithm
http://arxiv.org/abs/1608.01244v1,"This paper introduces a new scheme for autonomous electricity cooperatives,
called predictive cooperative (PCP), which aggregates commercial and
residential electricity consumers and participates in the electricity market on
behalf of its members. An axiomatic approach is proposed to calculate the
day-ahead bid and to disaggregate the collective cost among participating
consumers. The resulting formulation is shown to keep the members incentivized
to both participate in the cooperative and remain truthful in reporting their
expected loads. The scheme is implemented using PJM (world's largest wholesale
electricity market) real-time and day-ahead price data for 2015 and a
collection of residential and commercial load profiles. The model performance
of this framework is compared to that of real-time pricing (RTP) scheme, in
which wholesale market prices are directly applied to individual consumers. The
results show truthful load announcement by consumers, reduction in electricity
price variation for all consumers, and comparative benefits for participants.",consumer profiling algorithm
http://arxiv.org/abs/1608.01658v1,"Metastatic presence in lymph nodes is one of the most important prognostic
variables of breast cancer. The current diagnostic procedure for manually
reviewing sentinel lymph nodes, however, is very time-consuming and subjective.
Pathologists have to manually scan an entire digital whole-slide image (WSI)
for regions of metastasis that are sometimes only detectable under high
resolution or entirely hidden from the human visual cortex. From October 2015
to April 2016, the International Symposium on Biomedical Imaging (ISBI) held
the Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for
automatic detection of lymph node metastasis. Using a generalizable stain
normalization technique and the Proscia Pathology Cloud computing platform, we
trained a deep convolutional neural network on millions of tissue and tumor
image tiles to perform slide-based evaluation on our testing set of whole-slide
images images, with a sensitivity of 0.96, specificity of 0.89, and AUC score
of 0.90. Our results indicate that our platform can automatically scan any WSI
for metastatic regions without institutional calibration to respective stain
profiles.",consumer profiling algorithm
http://arxiv.org/abs/1803.11560v1,"Learning through experience is time-consuming, inefficient and often bad for
your cortisol levels. To address this problem, a number of recently proposed
teacher-student methods have demonstrated the benefits of private tuition, in
which a single model learns from an ensemble of more experienced tutors.
Unfortunately, the cost of such supervision restricts good representations to a
privileged minority. Unsupervised learning can be used to lower tuition fees,
but runs the risk of producing networks that require extracurriculum learning
to strengthen their CVs and create their own LinkedIn profiles. Inspired by the
logo on a promotional stress ball at a local recruitment fair, we make the
following three contributions. First, we propose a novel almost no supervision
training algorithm that is effective, yet highly scalable in the number of
student networks being supervised, ensuring that education remains affordable.
Second, we demonstrate our approach on a typical use case: learning to bake,
developing a method that tastily surpasses the current state of the art.
Finally, we provide a rigorous quantitive analysis of our method, proving that
we have access to a calculator. Our work calls into question the long-held
dogma that life is the best teacher.",consumer profiling algorithm
http://arxiv.org/abs/1811.11272v1,"In recent years, service-oriented-based Internet of Things (IoT) has received
massive attention from research and industry. Integrating and composing smart
objects functionalities or their services is required to create and promote
more complex IoT applications with advanced features. When many smart objects
are deployed, selecting the most appropriate set of smart objects to compose a
service by considering both energy and quality of service (QoS) is an essential
and challenging task. In this paper, we reduced the problem of finding an
optimal balance between QoS level and the consumed energy of the IoT service
composition to a bi-objective shortest path optimization (BSPO) problem and
used an exact algorithm named pulse to solve the problem. The BSPO has two
objectives, minimizing the QoS including execution time, network latency, and
service price, and minimize the energy consumption of the composite service.
Experimental evaluations show that the proposed approach has short execution
time in various complex service profiles. Meanwhile, it can obtain good
performance in energy consumption and thus network lifetime while maintaining a
reasonable QoS level.",consumer profiling algorithm
http://arxiv.org/abs/1906.07840v1,"Porting code from CPU to GPU is costly and time-consuming; Unless much time
is invested in development and optimization, it is not obvious, a priori, how
much speed-up is achievable or how much room is left for improvement. Knowing
the potential speed-up a priori can be very useful: It can save hundreds of
engineering hours, help programmers with prioritization and algorithm
selection. We aim to address this problem using machine learning in a
supervised setting, using solely the single-threaded source code of the
program, without having to run or profile the code. We propose a static
analysis-based cross-architecture performance prediction framework (Static
XAPP) which relies solely on program properties collected using static analysis
of the CPU source code and predicts whether the potential speed-up is above or
below a given threshold. We offer preliminary results that show we can achieve
94% accuracy in binary classification, in average, across different thresholds",consumer profiling algorithm
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination algorithm
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination algorithm
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination algorithm
http://arxiv.org/abs/1708.00754v1,"Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.",price discrimination algorithm
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination algorithm
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination algorithm
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination algorithm
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination algorithm
http://arxiv.org/abs/1905.05922v1,"An effective way for a Mobile network operator (MNO) to improve its revenue
is price discrimination, i.e., providing different combinations of data caps
and subscription fees. Rollover data plan (allowing the unused data in the
current month to be used in the next month) is an innovative data mechanism
with time flexibility. In this paper, we study the MNO's optimal multi-cap data
plans with time flexibility in a realistic asymmetric information scenario.
Specifically, users are associated with multi-dimensional private information,
and the MNO designs a contract (with different data caps and subscription fees)
to induce users to truthfully reveal their private information. This problem is
quite challenging due to the multi-dimensional private information. We address
the challenge in two aspects. First, we find that a feasible contract
(satisfying incentive compatibility and individual rationality) should allocate
the data caps according to users' willingness-to-pay (captured by the slopes of
users' indifference curves). Second, for the non-convex data cap allocation
problem, we propose a Dynamic Quota Allocation Algorithm, which has a low
complexity and guarantees the global optimality. Numerical results show that
the time-flexible data mechanisms increase both the MNO's profit (25% on
average) and users' payoffs (8.2% on average) under price discrimination.",price discrimination algorithm
http://arxiv.org/abs/1508.05347v3,"Data as a commodity has always been purchased and sold. Recently, web
services that are data marketplaces have emerged that match data buyers with
data sellers. So far there are no guidelines how to price queries against a
database. We consider the recently proposed query-based pricing framework of
Koutris et al and ask the question of computing optimal input prices in this
framework by formulating a buyer utility model.
  We establish the interesting and deep equivalence between arbitrage-freeness
in the query-pricing framework and envy-freeness in pricing theory for
appropriately chosen buyer valuations. Given the approximation hardness results
from envy-free pricing we then develop logarithmic approximation pricing
algorithms exploiting the max flow interpretation of the arbitrage-free pricing
for the restricted query language proposed by Koutris et al. We propose a novel
polynomial-time logarithmic approximation pricing scheme and show that our new
scheme performs better than the existing envy-free pricing algorithms
instance-by-instance. We also present a faster pricing algorithm that is always
greater than the existing solutions, but worse than our previous scheme. We
experimentally show how our pricing algorithms perform with respect to the
existing envy-free pricing algorithms and to the optimal exponentially
computable solution, and our experiments show that our approximation algorithms
consistently arrive at about 99% of the optimal.",price discrimination algorithm
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination algorithm
http://arxiv.org/abs/1202.2840v2,"Consider the following toy problem. There are $m$ rectangles and $n$ points
on the plane. Each rectangle $R$ is a consumer with budget $B_R$, who is
interested in purchasing the cheapest item (point) inside R, given that she has
enough budget. Our job is to price the items to maximize the revenue. This
problem can also be defined on higher dimensions. We call this problem the
geometric pricing problem.
  In this paper, we study a new class of problems arising from a geometric
aspect of the pricing problem. It intuitively captures typical real-world
assumptions that have been widely studied in marketing research, healthcare
economics, etc. It also helps classify other well-known pricing problems, such
as the highway pricing problem and the graph vertex pricing problem on planar
and bipartite graphs. Moreover, this problem turns out to have close
connections to other natural geometric problems such as the geometric versions
of the unique coverage and maximum feasible subsystem problems.
  We show that the low dimensionality arising in this pricing problem does lead
to improved approximation ratios, by presenting sublinear-approximation
algorithms for two central versions of the problem: unit-demand uniform-budget
min-buying and single-minded pricing problems. Our algorithm is obtained by
combining algorithmic pricing and geometric techniques. These results suggest
that considering geometric aspect might be a promising research direction in
obtaining improved approximation algorithms for such pricing problems. To the
best of our knowledge, this is one of very few problems in the intersection
between geometry and algorithmic pricing areas. Thus its study may lead to new
algorithmic techniques that could benefit both areas.",price discrimination algorithm
http://arxiv.org/abs/0910.0110v1,"We consider the Stackelberg shortest-path pricing problem, which is defined
as follows. Given a graph G with fixed-cost and pricable edges and two distinct
vertices s and t, we may assign prices to the pricable edges. Based on the
predefined fixed costs and our prices, a customer purchases a cheapest s-t-path
in G and we receive payment equal to the sum of prices of pricable edges
belonging to the path. Our goal is to find prices maximizing the payment
received from the customer. While Stackelberg shortest-path pricing was known
to be APX-hard before, we provide the first explicit approximation threshold
and prove hardness of approximation within 2-o(1).",price discrimination algorithm
http://arxiv.org/abs/1709.07534v1,"E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.",price discrimination algorithm
http://arxiv.org/abs/0908.2834v1,"Most recent papers addressing the algorithmic problem of allocating
advertisement space for keywords in sponsored search auctions assume that
pricing is done via a first-price auction, which does not realistically model
the Generalized Second Price (GSP) auction used in practice. Towards the goal
of more realistically modeling these auctions, we introduce the Second-Price Ad
Auctions problem, in which bidders' payments are determined by the GSP
mechanism. We show that the complexity of the Second-Price Ad Auctions problem
is quite different than that of the more studied First-Price Ad Auctions
problem. First, unlike the first-price variant, for which small constant-factor
approximations are known, it is NP-hard to approximate the Second-Price Ad
Auctions problem to any non-trivial factor. Second, this discrepancy extends
even to the 0-1 special case that we call the Second-Price Matching problem
(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This stands
in contrast to the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a classic result on the
performance of the ""Ranking"" algorithm for online bipartite matching.",price discrimination algorithm
http://arxiv.org/abs/1611.02442v3,"We study approximation algorithms for revenue maximization based on static
item pricing, where a seller chooses prices for various goods in the market,
and then the buyers purchase utility-maximizing bundles at these given prices.
We formulate two somewhat general techniques for designing good pricing
algorithms for this setting: Price Doubling and Item Halving. Using these
techniques, we unify many of the existing results in the item pricing
literature under a common framework, as well as provide several new item
pricing algorithms for approximating both revenue and social welfare. More
specifically, for a variety of settings with item pricing, we show that it is
possible to deterministically obtain a log-approximation for revenue and a
constant-approximation for social welfare simultaneously: thus one need not
sacrifice revenue if the goal is to still have decent welfare guarantees. %In
addition, we provide a new black-box reduction from revenue to welfare based on
item pricing, which immediately gives us new revenue-approximation algorithms
(e.g., for gross substitutes valuations).
  The main technical contribution of this paper is a $O((\log m + \log
k)^2)$-approximation algorithm for revenue maximization based on the Item
Halving technique, for settings where buyers have XoS valuations, where $m$ is
the number of goods and $k$ is the average supply. Surprisingly, ours is the
first known item pricing algorithm with polylogarithmic approximations for such
general classes of valuations, and partially resolves an important open
question from the algorithmic pricing literature about the existence of item
pricing algorithms with logarithmic factors for general valuations. We also use
the Item Halving framework to form envy-free item pricing mechanisms for the
popular setting of multi-unit markets, providing a log-approximation to revenue
in this case.",price discrimination algorithm
http://arxiv.org/abs/1805.02574v1,"We study revenue optimization pricing algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation. We show that, in the case when both the seller and the
buyer have the same discounting in their cumulative utilities (revenue and
surplus), there exist two optimal algorithms. The first one constantly offers
the Myerson price, while the second pricing proposes a ""big deal"": pay for all
goods in advance (at the first round) or get nothing. However, when there is an
imbalance between the seller and the buyer in the patience to wait for utility,
we find that the constant pricing, surprisingly, is no longer optimal. First,
it is outperformed by the pricing algorithm ""big deal"", when the seller's
discount rate is lower than the one of the buyer. Second, in the inverse case
of a less patient buyer, we reduce the problem of finding an optimal algorithm
to a multidimensional optimization problem (a multivariate analogue of the
functional used to determine Myerson's price) that does not admit a closed form
solution in general, but can be solved by numerical optimization techniques
(e.g., gradient ones). We provide extensive analysis of numerically found
optimal algorithms to demonstrate that they are non-trivial, may be
non-consistent, and generate larger expected revenue than the constant pricing
with the Myerson price.",price discrimination algorithm
http://arxiv.org/abs/1610.08890v2,"Parking prices in cities are uniform over large areas and do not reflect
spatially heterogeneous parking supply and demand. Underpricing results in high
parking occupancy in the subareas where the demand exceeds supply and long
search for the vacant parking, whereas overpricing leads to low occupancy and
hampered economic vitality. We present Nearest Pocket for Prices Algorithm
(NPPA), a spatially explicit algorithm for establishing on-and off-street
parking prices that guarantee a predetermined uniform level of occupation over
the entire parking space. We apply NPPA for establishing heterogeneous parking
prices that guarantee 90% parking occupancy in the Israeli city of Bat Yam.",price discrimination algorithm
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination algorithm
http://arxiv.org/abs/1510.02377v3,"In a world where traditional notions of privacy are increasingly challenged
by the myriad companies that collect and analyze our data, it is important that
decision-making entities are held accountable for unfair treatments arising
from irresponsible data usage. Unfortunately, a lack of appropriate
methodologies and tools means that even identifying unfair or discriminatory
effects can be a challenge in practice. We introduce the unwarranted
associations (UA) framework, a principled methodology for the discovery of
unfair, discriminatory, or offensive user treatment in data-driven
applications. The UA framework unifies and rationalizes a number of prior
attempts at formalizing algorithmic fairness. It uniquely combines multiple
investigative primitives and fairness metrics with broad applicability,
granular exploration of unfair treatment in user subgroups, and incorporation
of natural notions of utility that may account for observed disparities. We
instantiate the UA framework in FairTest, the first comprehensive tool that
helps developers check data-driven applications for unfair user treatment. It
enables scalable and statistically rigorous investigation of associations
between application outcomes (such as prices or premiums) and sensitive user
attributes (such as race or gender). Furthermore, FairTest provides debugging
capabilities that let programmers rule out potential confounders for observed
unfair effects. We report on use of FairTest to investigate and in some cases
address disparate impact, offensive labeling, and uneven rates of algorithmic
error in four data-driven applications. As examples, our results reveal subtle
biases against older populations in the distribution of error in a predictive
health application and offensive racial labeling in an image tagger.",algorithm detect unfair pricing website
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",algorithm detect unfair pricing website
http://arxiv.org/abs/1503.05414v3,"Social networks help to bond people who share similar interests all over the
world. As a complement, the Facebook ""Like"" button is an efficient tool that
bonds people with the online information. People click on the ""Like"" button to
express their fondness of a particular piece of information and in turn tend to
visit webpages with high ""Like"" count. The important fact of the Like count is
that it reflects the number of actual users who ""liked"" this information.
However, according to our study, one can easily exploit the defects of the
""Like"" button to counterfeit a high ""Like"" count. We provide a proof-of-concept
implementation of these exploits, and manage to generate 100 fake Likes in 5
minutes with a single account. We also reveal existing counterfeiting
techniques used by some online sellers to achieve unfair advantage for
promoting their products. To address this fake Like problem, we study the
varying patterns of Like count and propose an innovative fake Like detection
method based on clustering. To evaluate the effectiveness of our algorithm, we
collect the Like count history of more than 9,000 websites. Our experiments
successfully uncover 16 suspicious fake Like buyers that show abnormal Like
count increase patterns.",algorithm detect unfair pricing website
http://arxiv.org/abs/1211.0963v1,"Online rating systems are subject to malicious behaviors mainly by posting
unfair rating scores. Users may try to individually or collaboratively promote
or demote a product. Collaborating unfair rating 'collusion' is more damaging
than individual unfair rating. Although collusion detection in general has been
widely studied, identifying collusion groups in online rating systems is less
studied and needs more investigation. In this paper, we study impact of
collusion in online rating systems and asses their susceptibility to collusion
attacks. The proposed model uses a frequent itemset mining algorithm to detect
candidate collusion groups. Then, several indicators are used for identifying
collusion groups and for estimating how damaging such colluding groups might
be. Also, we propose an algorithm for finding possible collusive subgroup
inside larger groups which are not identified as collusive. The model has been
implemented and we present results of experimental evaluation of our
methodology.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.10201v1,"High performance grid computing is a key enabler of large scale collaborative
computational science. With the promise of exascale computing, high performance
grid systems are expected to incur electricity bills that grow super-linearly
over time. In order to achieve cost effectiveness in these systems, it is
essential for the scheduling algorithms to exploit electricity price
variations, both in space and time, that are prevalent in the dynamic
electricity price markets. In this paper, we present a metascheduling algorithm
to optimize the placement of jobs in a compute grid which consumes electricity
from the day-ahead wholesale market. We formulate the scheduling problem as a
Minimum Cost Maximum Flow problem and leverage queue waiting time and
electricity price predictions to accurately estimate the cost of job execution
at a system. Using trace based simulation with real and synthetic workload
traces, and real electricity price data sets, we demonstrate our approach on
two currently operational grids, XSEDE and NorduGrid. Our experimental setup
collectively constitute more than 433K processors spread across 58 compute
systems in 17 geographically distributed locations. Experiments show that our
approach simultaneously optimizes the total electricity cost and the average
response time of the grid, without being unfair to users of the local batch
systems.",algorithm detect unfair pricing website
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",algorithm detect unfair pricing website
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",algorithm detect unfair pricing website
http://arxiv.org/abs/1408.1993v1,"Malicious websites are a major cyber attack vector, and effective detection
of them is an important cyber defense task. The main defense paradigm in this
regard is that the defender uses some kind of machine learning algorithms to
train a detection model, which is then used to classify websites in question.
Unlike other settings, the following issue is inherent to the problem of
malicious websites detection: the attacker essentially has access to the same
data that the defender uses to train its detection models. This 'symmetry' can
be exploited by the attacker, at least in principle, to evade the defender's
detection models. In this paper, we present a framework for characterizing the
evasion and counter-evasion interactions between the attacker and the defender,
where the attacker attempts to evade the defender's detection models by taking
advantage of this symmetry. Within this framework, we show that an adaptive
attacker can make malicious websites evade powerful detection models, but
proactive training can be an effective counter-evasion defense mechanism. The
framework is geared toward the popular detection model of decision tree, but
can be adapted to accommodate other classifiers.",algorithm detect unfair pricing website
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",algorithm detect unfair pricing website
http://arxiv.org/abs/1711.06955v1,"Web spam is a big problem for search engine users in World Wide Web. They use
deceptive techniques to achieve high rankings. Although many researchers have
presented the different approach for classification and web spam detection
still it is an open issue in computer science. Analyzing and evaluating these
websites can be an effective step for discovering and categorizing the features
of these websites. There are several methods and algorithms for detecting those
websites, such as decision tree algorithm. In this paper, we present a
systematic framework based on CHAID algorithm and a modified string matching
algorithm (KMP) for extract features and analysis of these websites. We
evaluated our model and other methods with a dataset of Alexa Top 500 Global
Sites and Bing search engine results in 500 queries.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",algorithm detect unfair pricing website
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",algorithm detect unfair pricing website
http://arxiv.org/abs/1808.07359v1,"Recent works showed that websites can detect browser extensions that users
install and websites they are logged into. This poses significant privacy
risks, since extensions and Web logins that reflect user's behavior, can be
used to uniquely identify users on the Web. This paper reports on the first
large-scale behavioral uniqueness study based on 16,393 users who visited our
website. We test and detect the presence of 16,743 Chrome extensions, covering
28% of all free Chrome extensions. We also detect whether the user is connected
to 60 different websites.
  We analyze how unique users are based on their behavior, and find out that
54.86% of users that have installed at least one detectable extension are
unique; 19.53% of users are unique among those who have logged into one or more
detectable websites; and 89.23% are unique among users with at least one
extension and one login. We use an advanced fingerprinting algorithm and show
that it is possible to identify a user in less than 625 milliseconds by
selecting the most unique combinations of extensions.
  Because privacy extensions contribute to the uniqueness of users, we study
the trade-off between the amount of trackers blocked by such extensions and how
unique the users of these extensions are. We have found that privacy extensions
should be considered more useful than harmful. The paper concludes with
possible countermeasures.",algorithm detect unfair pricing website
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",algorithm detect unfair pricing website
http://arxiv.org/abs/1708.01348v4,"While page views are often sold instantly through real-time auctions when
users visit websites, they can also be sold in advance via guaranteed
contracts. In this paper, we present a dynamic programming model to study how
an online publisher should optimally allocate and price page views between
guaranteed and spot markets. The problem is challenging because the allocation
and pricing of guaranteed contracts affect advertisers' purchase between the
two markets, and the terminal value of the model is endogenously determined by
the updated dual force of supply and demand in auctions. We take the
advertisers' purchasing behaviour into consideration, i.e., risk aversion and
stochastic demand arrivals, and present a scalable and efficient algorithm for
the optimal solution. The model is also empirically validated with a commercial
dataset. The experimental results show that selling page views via both
guaranteed contracts and auctions can increase the publisher's expected total
revenue, and the optimal pricing and allocation strategies are robust to
different market and advertiser types.",algorithm detect unfair pricing website
http://arxiv.org/abs/1308.1382v1,"We consider the optimal pricing problem for a model of the rich media
advertisement market, as well as other related applications. In this market,
there are multiple buyers (advertisers), and items (slots) that are arranged in
a line such as a banner on a website. Each buyer desires a particular number of
{\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on
the ad only) while each slot $j$ has a quality $q_j$ (dependent on the position
only such as click-through rate in position auctions). Hence, the valuation of
the buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and
the prices in order to maximize the total revenue of the market maker.
  A key difference from the traditional position auction is the advertiser's
requirement of a fixed number of consecutive slots. Consecutive slots may be
needed for a large size rich media ad. We study three major pricing mechanisms,
the Bayesian pricing model, the maximum revenue market equilibrium model and an
envy-free solution model. Under the Bayesian model, we design a polynomial time
computable truthful mechanism which is optimum in revenue. For the market
equilibrium paradigm, we find a polynomial time algorithm to obtain the maximum
revenue market equilibrium solution. In envy-free settings, an optimal solution
is presented when the buyers have the same demand for the number of consecutive
slots. We conduct a simulation that compares the revenues from the above
schemes and gives convincing results.",algorithm detect unfair pricing website
http://arxiv.org/abs/1907.12649v1,"In the last few years, Header Bidding (HB) has gained popularity among web
publishers and is challenging the status quo in the ad ecosystem. Contrary to
the traditional waterfall standard, HB aims to give back control of the ad
inventory to publishers, increase transparency, fairness and competition among
advertisers, thus, resulting in higher ad-slot prices. Although promising,
little is known about this new ad-tech protocol: How does it work internally
and what are the different implementations of HB? What is the performance
overhead, and how does it affect the user experience? Does it, indeed, provide
higher revenues to publishers than the waterfall model? Who are the dominating
entities in this new protocol?
  To respond to all these questions and shed light on this new, buzzing
ad-technology, we design and implement HBDetector: a holistic HB detection
mechanism that can capture HB auctions independently of the implementation
followed in a website. By running HBDetector across the top 35,000 Alexa
websites, we collect and analyze a dataset of 800k auctions. Our results show
that: (i) 14.28% of the top Alexa websites utilize HB. (ii) Publishers tend to
collaborate mostly with a relatively low number of demand partners, which are
already big players in waterfall standard, (iii) HB latency can be
significantly higher than waterfall, with up to 3x latency in the median cases.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0108015v1,"Recent trends reveal the search by companies for a legal hook to prevent the
undesired and unauthorized copying of information posted on websites. In the
center of this controversy are metasites, websites that display prices for a
variety of vendors. Metasites function by implementing shopbots, which extract
pricing data from other vendors' websites. Technological mechanisms have proved
unsuccessful in blocking shopbots, and in response, websites have asserted a
variety of legal claims. Two recent cases, which rely on the troublesome
trespass to chattels doctrine, suggest that contract law may provide a less
demanding legal method of preventing the search of websites by data robots. If
blocking collection of pricing data is as simple as posting an online contract,
the question arises whether this end result is desirable and legally viable.",algorithm detect unfair pricing website
http://arxiv.org/abs/1905.07026v1,"Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.",algorithm detect unfair pricing website
http://arxiv.org/abs/1511.06975v1,"Considering the level of competition prevailing in Business-to-Consumer (B2C)
E-Commerce domain and the huge investments required to attract new customers,
firms are now giving more focus to reduce their customer churn rate. Churn rate
is the ratio of customers who part away with the firm in a specific time
period. One of the best mechanism to retain current customers is to identify
any potential churn and respond fast to prevent it. Detecting early signs of a
potential churn, recognizing what the customer is looking for by the movement
and automating personalized win back campaigns are essential to sustain
business in this era of competition. E-Commerce firms normally possess large
volume of data pertaining to their existing customers like transaction history,
search history, periodicity of purchases, etc. Data mining techniques can be
applied to analyse customer behaviour and to predict the potential customer
attrition so that special marketing strategies can be adopted to retain them.
This paper proposes an integrated model that can predict customer churn and
also recommend personalized win back actions.",detecting e-commerce
http://arxiv.org/abs/1308.3559v1,"Man in the middle attacks are a significant threat to modern e-commerce and
online communications, even when such transactions are protected by TLS. We
intend to show that it is possible to detect man-in-the-middle attacks on SSL
and TLS by detecting timing differences between a standard SSL session and an
attack we created.",detecting e-commerce
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",detecting e-commerce
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",detecting e-commerce
http://arxiv.org/abs/1907.01284v1,"Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.",detecting e-commerce
http://arxiv.org/abs/1507.07382v1,"Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence.",detecting e-commerce
http://arxiv.org/abs/1512.04122v1,"The emergence of mobile platforms with increased storage and computing
capabilities and the pervasive use of these platforms for sensitive
applications such as online banking, e-commerce and the storage of sensitive
information on these mobile devices have led to increasing danger associated
with malware targeted at these devices. Detecting such malware presents
inimitable challenges as signature-based detection techniques available today
are becoming inefficient in detecting new and unknown malware. In this
research, a machine learning approach for the detection of malware on Android
platforms is presented. The detection system monitors and extracts features
from the applications while in execution and uses them to perform in-device
detection using a trained K-Nearest Neighbour classifier. Results shows high
performance in the detection rate of the classifier with accuracy of 93.75%,
low error rate of 6.25% and low false positive rate with ability of detecting
real Android malware.",detecting e-commerce
http://arxiv.org/abs/1510.05544v2,"Given a network with attributed edges, how can we identify anomalous
behavior? Networks with edge attributes are commonplace in the real world. For
example, edges in e-commerce networks often indicate how users rated products
and services in terms of number of stars, and edges in online social and
phonecall networks contain temporal information about when friendships were
formed and when users communicated with each other -- in such cases, edge
attributes capture information about how the adjacent nodes interact with other
entities in the network. In this paper, we aim to utilize exactly this
information to discern suspicious from typical node behavior. Our work has a
number of notable contributions, including (a) formulation: while most other
graph-based anomaly detection works use structural graph connectivity or node
information, we focus on the new problem of leveraging edge information, (b)
methodology: we introduce EdgeCentric, an intuitive and scalable
compression-based approach for detecting edge-attributed graph anomalies, and
(c) practicality: we show that EdgeCentric successfully spots numerous such
anomalies in several large, edge-attributed real-world graphs, including the
Flipkart e-commerce graph with over 3 million product reviews between 1.1
million users and 545 thousand products, where it achieved 0.87 precision over
the top 100 results.",detecting e-commerce
http://arxiv.org/abs/1905.02234v2,"In e-commerce, product content, especially product images have a significant
influence on a customer's journey from product discovery to evaluation and
finally, purchase decision. Since many e-commerce retailers sell items from
other third-party marketplace sellers besides their own, the content published
by both internal and external content creators needs to be monitored and
enriched, wherever possible. Despite guidelines and warnings, product listings
that contain offensive and non-compliant images continue to enter catalogs.
Offensive and non-compliant content can include a wide range of objects, logos,
and banners conveying violent, sexually explicit, racist, or promotional
messages. Such images can severely damage the customer experience, lead to
legal issues, and erode the company brand. In this paper, we present a computer
vision driven offensive and non-compliant image detection system for extremely
large image datasets. This paper delves into the unique challenges of applying
deep learning to real-world product image data from retail world. We
demonstrate how we resolve a number of technical challenges such as lack of
training data, severe class imbalance, fine-grained class definitions etc.
using a number of practical yet unique technical strategies. Our system
combines state-of-the-art image classification and object detection techniques
with budgeted crowdsourcing to develop a solution customized for a massive,
diverse, and constantly evolving product catalog.",detecting e-commerce
http://arxiv.org/abs/1811.04374v1,"We present an empirical study of applying deep Convolutional Neural Networks
(CNN) to the task of fashion and apparel image classification to improve
meta-data enrichment of e-commerce applications. Five different CNN
architectures were analyzed using clean and pre-trained models. The models were
evaluated in three different tasks person detection, product and gender
classification, on two small and large scale datasets.",detecting e-commerce
http://arxiv.org/abs/1202.1761v1,"Denial of Service (DoS) is a security threat which compromises the
confidentiality of information stored in Local Area Networks (LANs) due to
unauthorized access by spoofed IP addresses. SYN Flooding is a type of DoS
which is harmful to network as the flooding of packets may delay other users
from accessing the server and in severe cases, the server may need to be shut
down, wasting valuable resources, especially in critical real-time services
such as in e-commerce and the medical field. The objective of this paper is to
review the state-of-the art of detection mechanisms for SYN flooding. The
detection schemes for SYN Flooding attacks have been classified broadly into
three categories - detection schemes based on the router data structure,
detection schemes based on statistical analysis of the packet flow and
detection schemes based on artificial intelligence. The advantages and
disadvantages for various detection schemes under each category have been
critically examined. The performance measures of the categories have also been
compared.",detecting e-commerce
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",detecting e-commerce
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",detecting e-commerce
http://arxiv.org/abs/1411.5319v2,"In this work, we propose and address a new computer vision task, which we
call fashion item detection, where the aim is to detect various fashion items a
person in the image is wearing or carrying. The types of fashion items we
consider in this work include hat, glasses, bag, pants, shoes and so on. The
detection of fashion items can be an important first step of various e-commerce
applications for fashion industry. Our method is based on state-of-the-art
object detection method pipeline which combines object proposal methods with a
Deep Convolutional Neural Network. Since the locations of fashion items are in
strong correlation with the locations of body joints positions, we incorporate
contextual information from body poses in order to improve the detection
performance. Through the experiments, we demonstrate the effectiveness of the
proposed method.",detecting e-commerce
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",detecting e-commerce
http://arxiv.org/abs/1904.12574v2,"Learning product representations that reflect complementary relationship
plays a central role in modern recommender system for e-commerce platforms. A
notable challenge is that unlike many simple relationships such as similarity,
complementariness is often detected from customer purchase activities, which
are highly sparse and noisy. Also, standard usage of representation learning
emphasizes on only one set of embedding, which is problematic for modelling the
asymmetric property of complementariness. We propose using context-aware
multi-tasking learning with dual product embedding to solve the above
challenges. We encode contextual knowledge into product representation by
multi-task learning, in order to alleviate the sparsity issue. By explicitly
modelling with user bias terms, we take care of the noise induced by
customer-specific preferences. Furthermore, we adopt the dual embedding
framework to capture the intrinsic properties of complementariness and provide
geometric interpretation motivated by the classic separating hyperplane theory.
Finally, we propose a Bayesian network structure that unifies all the
components, which also concludes several popular models as special cases. The
proposed method compares favourably to state-of-art representation learning and
recommendation algorithms for e-commerce, in downstream classification and
recommendation tasks. We also develop an implementation that scales efficiently
to a dataset with millions of items and customers.",detecting e-commerce
http://arxiv.org/abs/1506.04584v1,"Collaborative filtering recommender systems (CFRSs) are the key components of
successful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks
since its openness. However, since attack size is far smaller than that of
genuine users, conventional supervised learning based detection methods could
be too ""dull"" to handle such imbalanced classification. In this paper, we
improve detection performance from following two aspects. First, we extract
well-designed features from user profiles based on the statistical properties
of the diverse attack models, making hard classification task becomes easier to
perform. Then, refer to the general idea of re-scale Boosting (RBoosting) and
AdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost
(RAdaBoost) as our detection method based on extracted features. RAdaBoost is
comparable to the optimal Boosting-type algorithm and can effectively improve
the performance in some hard scenarios. Finally, a series of experiments on the
MovieLens-100K data set are conducted to demonstrate the outperformance of
RAdaBoost comparing with some classical techniques such as SVM, kNN and
AdaBoost.",detecting e-commerce
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",detecting e-commerce
http://arxiv.org/abs/1002.3333v1,"In this paper, we describe an effective framework for adapting electronic
commerce or e-commerce services in developing countries like Bangladesh. The
internet has opened up a new horizon for commerce, namely electronic commerce
(e-commerce). It entails the use of the internet in the marketing,
identification, payment and delivery of goods and services. At present internet
facilities are available in Bangladesh. Slowly, but steadily these facilities
are holding a strong position in every aspects of our life. E-commerce is one
of those sectors which need more attention if we want to be a part of global
business. Bangladesh is far-far away to adapt the main stream of e-commerce
application. Though government is shouting to take the challenges of
e-commerce, but they do not take the right step, that is why e-commerce dose
not make any real contribution in our socio-economic life. Here we propose a
model which may develop the e-commerce infrastructure of Bangladesh.",detecting e-commerce
http://arxiv.org/abs/1807.04923v1,"Millions of people use online e-commerce platforms to search and buy
products. Identifying attributes in a query is a critical component in
connecting users to relevant items. However, in many cases, the queries have
multiple attributes, and some of them will be in conflict with each other. For
example, the query ""maroon 5 dvds"" has two candidate attributes, the color
""maroon"" or the band ""maroon 5"", where only one of the attributes can be
present. In this paper, we address the problem of resolving conflicting
attributes in e-commerce queries. A challenge in this problem is that knowledge
bases like Wikipedia that are used to understand web queries are not focused on
the e-commerce domain. E-commerce search engines, however, have access to the
catalog which contains detailed information about the items and its attributes.
We propose a framework that constructs knowledge graphs from catalog to resolve
conflicting attributes in e-commerce queries. Our experiments on real-world
queries on e-commerce platforms demonstrate that resolving conflicting
attributes by leveraging catalog information significantly improves attribute
identification, and also gives out more relevant search results.",detecting e-commerce
http://arxiv.org/abs/1602.07662v1,"Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.",detecting e-commerce
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",detecting e-commerce
http://arxiv.org/abs/1804.03836v3,"Anomaly Detection has several important applications. In this paper, our
focus is on detecting anomalies in seller-reviewer data using tensor
decomposition. While tensor-decomposition is mostly unsupervised, we formulate
Bayesian semi-supervised tensor decomposition to take advantage of sparse
labeled data. In addition, we use Polya-Gamma data augmentation for the
semi-supervised Bayesian tensor decomposition. Finally, we show that the
P\'olya-Gamma formulation simplifies calculation of the Fisher information
matrix for partial natural gradient learning. Our experimental results show
that our semi-supervised approach outperforms state of the art unsupervised
baselines. And that the partial natural gradient learning outperforms
stochastic gradient learning and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detecting e-commerce
http://arxiv.org/abs/1905.06112v1,"Recently, Vietnamese Natural Language Processing has been researched by
experts in academic and business. However, the existing papers have been
focused only on information classification or extraction from documents.
Nowadays, with quickly development of the e-commerce websites, forums and
social networks, the products, people, organizations or wonders are targeted of
comments or reviews of the network communities. Many people often use that
reviews to make their decision on something. Whereas, there are many people or
organizations use the reviews to mislead readers. Therefore, it is so necessary
to detect those bad behaviors in reviews. In this paper, we research this
problem and propose an appropriate method for detecting Vietnamese reviews
being spam or non-spam. The accuracy of our method is up to 90%.",detecting e-commerce
http://arxiv.org/abs/1905.06246v2,"Product reviews and ratings on e-commerce websites provide customers with
detailed insights about various aspects of the product such as quality,
usefulness, etc. Since they influence customers' buying decisions, product
reviews have become a fertile ground for abuse by sellers (colluding with
reviewers) to promote their own products or to tarnish the reputation of
competitor's products. In this paper, our focus is on detecting such abusive
entities (both sellers and reviewers) by applying tensor decomposition on the
product reviews data. While tensor decomposition is mostly unsupervised, we
formulate our problem as a semi-supervised binary multi-target tensor
decomposition, to take advantage of currently known abusive entities. We
empirically show that our multi-target semi-supervised model achieves higher
precision and recall in detecting abusive entities as compared to unsupervised
techniques. Finally, we show that our proposed stochastic partial natural
gradient inference for our model empirically achieves faster convergence than
stochastic gradient and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",detecting e-commerce
http://arxiv.org/abs/1506.05752v3,"Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to ""shilling"" attacks or ""profile injection"" attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
""shilling"" attacks or ""profile injection"" attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN.",detecting e-commerce
http://arxiv.org/abs/1102.0706v1,"E-commerce is an emerging technology. Impact of this new technology is
getting clearer with time and results are tangible to the user community. In
this paper we have tried to focus some of its issues like paradigms,
infrastructure integration, and security, which is considered to be the most
important issue in E-Commerce. At first we have elaborated the paradigms of
E-Commerce (Business-to-Business and Business-to-Consumer). Then comes the
necessity of infrastructure integration with the legacy system. Security
concerns comes next. Rest of the part contains conclusion and references.",detecting e-commerce
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie tracking
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie tracking
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",cookie tracking
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",cookie tracking
http://arxiv.org/abs/1510.01175v1,"The number of computers, tablets and smartphones is increasing rapidly, which
entails the ownership and use of multiple devices to perform online tasks. As
people move across devices to complete these tasks, their identities becomes
fragmented. Understanding the usage and transition between those devices is
essential to develop efficient applications in a multi-device world. In this
paper we present a solution to deal with the cross-device identification of
users based on semi-supervised machine learning methods to identify which
cookies belong to an individual using a device. The method proposed in this
paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections
challenge proving its good performance.",cookie tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",cookie tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",cookie tracking
http://arxiv.org/abs/1805.10505v2,"User data is the primary input of digital advertising, fueling the free
Internet as we know it. As a result, web companies invest a lot in elaborate
tracking mechanisms to acquire user data that can sell to data markets and
advertisers. However, with same-origin policy, and cookies as a primary
identification mechanism on the web, each tracker knows the same user with a
different ID. To mitigate this, Cookie Synchronization (CSync) came to the
rescue, facilitating an information sharing channel between third parties that
may or not have direct access to the website the user visits. In the
background, with CSync, they merge user data they own, but also reconstruct a
user's browsing history, bypassing the same origin policy. In this paper, we
perform a first to our knowledge in-depth study of CSync in the wild, using a
year-long weblog from 850 real mobile users. Through our study, we aim to
understand the characteristics of the CSync protocol and the impact it has on
web users' privacy. For this, we design and implement CONRAD, a holistic
mechanism to detect CSync events at real time, and the privacy loss on the user
side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97%
of the regular web users are exposed to CSync: most of them within the first
week of their browsing, and the median userID gets leaked, on average, to 3.5
different domains. Finally, we see that CSync increases the number of domains
that track the user by a factor of 6.75.",cookie tracking
http://arxiv.org/abs/1407.0803v1,"The popularity of mobile device has made people's lives more convenient, but
threatened people's privacy at the same time. As end users are becoming more
and more concerned on the protection of their private information, it is even
harder to track a specific user using conventional technologies. For example,
cookies might be cleared by users regularly. Apple has stopped apps accessing
UDIDs, and Android phones use some special permission to protect IMEI code. To
address this challenge, some recent studies have worked on tracing smart phones
using the hardware features resulted from the imperfect manufacturing process.
These works have demonstrated that different devices can be differentiated to
each other. However, it still has a long way to go in order to replace cookie
and be deployed in real world scenarios, especially in terms of properties like
uniqueness, robustness, etc. In this paper, we presented a novel method to
generate stable and unique device ID stealthy for smartphones by exploiting the
frequency response of the speaker. With carefully selected audio frequencies
and special sound wave patterns, we can reduce the impacts of non-linear
effects and noises, and keep our feature extraction process un-noticeable to
users. The extracted feature is not only very stable for a given smart phone
speaker, but also unique to that phone. The feature contains rich information
that is equivalent to around 40 bits of entropy, which is enough to identify
billions of different smart phones of the same model. We have built a prototype
to evaluate our method, and the results show that the generated device ID can
be used as a replacement of cookie.",cookie tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",cookie tracking
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",cookie tracking
http://arxiv.org/abs/1804.08491v1,"Internet users today are constantly giving away their personal information
and privacy through social media, tracking cookies, 'free' email, and single
sign-on authentication in order to access convenient online services.
Unfortunately, the elected officials who are supposed to be regulating these
technologies often know less about informed consent and data ownership than the
users themselves. This is why without changes, internet users may continue to
be exploited by companies offering free and convenient online services.",cookie tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",cookie tracking
http://arxiv.org/abs/1811.00920v1,"Numerous surveys have shown that Web users are concerned about the loss of
privacy associated with online tracking. Alarmingly, these surveys also reveal
that people are also unaware of the amount of data sharing that occurs between
ad exchanges, and thus underestimate the privacy risks associated with online
tracking.
  In reality, the modern ad ecosystem is fueled by a flow of user data between
trackers and ad exchanges. Although recent work has shown that ad exchanges
routinely perform cookie matching with other exchanges, these studies are based
on brittle heuristics that cannot detect all forms of information sharing,
especially under adversarial conditions.
  In this study, we develop a methodology that is able to detect client- and
server-side flows of information between arbitrary ad exchanges. Our key
insight is to leverage retargeted ads as a tool for identifying information
flows. Intuitively, our methodology works because it relies on the semantics of
how exchanges serve ads, rather than focusing on specific cookie matching
mechanisms. Using crawled data on 35,448 ad impressions, we show that our
methodology can successfully categorize four different kinds of information
sharing behavior between ad exchanges, including cases where existing heuristic
methods fail.
  We conclude with a discussion of how our findings and methodologies can be
leveraged to give users more control over what kind of ads they see and how
their information is shared between ad exchanges.",cookie tracking
http://arxiv.org/abs/1811.08660v1,"The European General Data Protection Regulation (GDPR), which went into
effect in May 2018, leads to important changes in this area: companies are now
required to ask for users' consent before collecting and sharing personal data
and by law users now have the right to gain access to the personal information
collected about them.
  In this paper, we study and evaluate the effect of the GDPR on the online
advertising ecosystem. In a first step, we measure the impact of the
legislation on the connections (regarding cookie syncing) between third-parties
and show that the general structure how the entities are arranged is not
affected by the GDPR. However, we find that the new regulation has a
statistically significant impact on the number of connections, which shrinks by
around 40%. Furthermore, we analyze the right to data portability by evaluating
the subject access right process of popular companies in this ecosystem and
observe differences between the processes implemented by the companies and how
they interpret the new legislation. We exercised our right of access under GDPR
with 36 companies that had tracked us online. Although 32 companies (89%) we
inquired replied within the period defined by law, only 21 (58%) finished the
process by the deadline set in the GDPR. Our work has implications regarding
the implementation of privacy law as well as what online tracking companies
should do to be more compliant with the new regulation.",cookie tracking
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie tracking
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie tracking
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",cookie tracking
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",cookie tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",cookie tracking
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie monitoring
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie monitoring
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie monitoring
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie monitoring
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie monitoring
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie monitoring
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie monitoring
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie monitoring
http://arxiv.org/abs/1803.10450v1,"Over the last decade, the number of devices per person has increased
substantially. This poses a challenge for cookie-based personalization
applications, such as online search and advertising, as it narrows the
personalization signal to a single device environment. A key task is to find
which cookies belong to the same person to recover a complete cross-device user
journey. Recent work on the topic has shown the benefits of using unsupervised
embeddings learned on user event sequences. In this paper, we extend this
approach to a supervised setting and introduce the Siamese Cookie Embedding
Network (SCEmNet), a siamese convolutional architecture that leverages the
multi-modal aspect of sequences, and show significant improvement over the
state-of-the-art.",cookie monitoring
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",website tracking
http://arxiv.org/abs/1703.07578v1,"Third party tracking is the practice by which third parties recognize users
accross different websites as they browse the web. Recent studies show that 90%
of websites contain third party content that is tracking its users across the
web. Website developers often need to include third party content in order to
provide basic functionality. However, when a developer includes a third party
content, she cannot know whether the third party contains tracking mechanisms.
If a website developer wants to protect her users from being tracked, the only
solution is to exclude any third-party content, thus trading functionality for
privacy. We describe and implement a privacy-preserving web architecture that
gives website developers a control over third party tracking: developers are
able to include functionally useful third party content, the same time ensuring
that the end users are not tracked by the third parties.",website tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",website tracking
http://arxiv.org/abs/1805.01392v1,"Web tracking technologies are pervasive and operated by a few large
technology companies. This technology, and the use of the collected data has
been implicated in influencing elections, fake news, discrimination, and even
health decisions. Little is known about how this technology is deployed on
hospital or other health related websites. The websites of the 210 public
hospitals in the state of Illinois, USA were evaluated with a web tracker
identification tool. Web trackers were identified on 94% of hospital webs
sites, with an average of 3.5 trackers on the websites of general hospitals.
The websites of smaller critical access hospitals used an average of 2 web
trackers. The most common web tracker identified was Google Analytics, found on
74% of Illinois hospital websites. Of the web trackers discovered, 88% were
operated by Google and 26% by Facebook. In light of revelations about how web
browsing profiles have been used and misused, search bubbles, and the potential
for algorithmic discrimination hospital leadership and policy makers must
carefully consider if it is appropriate to use third party tracking technology
on hospital web sites.",website tracking
http://arxiv.org/abs/1801.04829v2,"This paper presents a pilot study on developing an instrument to predict the
quality of e-commerce websites. The 8C model was adopted as the reference model
of the heuristic evaluation. Each dimension of the 8C was mapped into a set of
quantitative website elements, selected websites were scraped to get the
quantitative website elements, and the score of each dimension was calculated.
A software was developed in PHP for the experiments. In the training process,
10 experiments were conducted and quantitative analyses were regressively
conducted between the experiments. The conversion rate was used to verify the
heuristic evaluation of an e-commerce website after each experiment. The
results showed that the mapping revisions between the experiments improved the
performance of the evaluation instrument, therefore the experiment process and
the quantitative mapping revision guideline proposed was on the right track.
The software resulted from the experiment 10 can serve as the aimed e-commerce
website evaluation instrument. The experiment results and the future work have
been discussed.",website tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",website tracking
http://arxiv.org/abs/1607.07403v2,"We perform a large-scale analysis of third-party trackers on the World Wide
Web from more than 3.5 billion web pages of the CommonCrawl 2012 corpus. We
extract a dataset containing more than 140 million third-party embeddings in
over 41 million domains. To the best of our knowledge, this constitutes the
largest web tracking dataset collected so far, and exceeds related studies by
more than an order of magnitude in the number of domains and web pages
analyzed. We perform a large-scale study of online tracking, on three levels:
(1) On a global level, we give a precise figure for the extent of tracking,
give insights into the structure of the `online tracking sphere' and analyse
which trackers are used by how many websites. (2) On a country-specific level,
we analyse which trackers are used by websites in different countries, and
identify the countries in which websites choose significantly different
trackers than in the rest of the world. (3) We answer the question whether the
content of websites influences the choice of trackers they use, leveraging more
than 90 thousand categorized domains. In particular, we analyse whether highly
privacy-critical websites make different choices of trackers than other
websites. Based on the performed analyses, we confirm that trackers are
widespread (as expected), and that a small number of trackers dominates the web
(Google, Facebook and Twitter). In particular, the three tracking domains with
the highest PageRank are all owned by Google. The only exception to this
pattern are a few countries such as China and Russia. Our results suggest that
this dominance is strongly associated with country-specific political factors
such as freedom of the press. We also confirm that websites with highly
privacy-critical content are less likely to contain trackers (60% vs 90% for
other websites), even though the majority of them still do contain trackers.",website tracking
http://arxiv.org/abs/1511.00619v1,"This article provides a quantitative analysis of privacy-compromising
mechanisms on 1 million popular websites. Findings indicate that nearly 9 in 10
websites leak user data to parties of which the user is likely unaware; more
than 6 in 10 websites spawn third- party cookies; and more than 8 in 10
websites load Javascript code from external parties onto users' computers.
Sites that leak user data contact an average of nine external domains,
indicating that users may be tracked by multiple entities in tandem. By tracing
the unintended disclosure of personal browsing histories on the Web, it is
revealed that a handful of U.S. companies receive the vast bulk of user data.
Finally, roughly 1 in 5 websites are potentially vulnerable to known National
Security Agency spying techniques at the time of analysis.",website tracking
http://arxiv.org/abs/1502.00317v2,"Cursor tracking data contains information about website visitors which may
provide new ways to understand visitors and their needs. This paper presents an
Amazon Mechanical Turk study where participants were tracked as they used
modified variants of the Wikipedia and BBC News websites. Participants were
asked to complete reading and information-finding tasks. The results showed
that it was possible to differentiate between users reading content and users
looking for information based on cursor data. The effects of website
aesthetics, user interest and cursor hardware were also analysed which showed
it was possible to identify hardware from cursor data, but no relationship
between cursor data and engagement was found. The implications of these
results, from the impact on web analytics to the design of experiments to
assess user engagement, are discussed.",website tracking
http://arxiv.org/abs/1907.06520v1,"This paper explores tracking and privacy risks on pornography websites. Our
analysis of 22,484 pornography websites indicated that 93% leak user data to a
third party. Tracking on these sites is highly concentrated by a handful of
major companies, which we identify. We successfully extracted privacy policies
for 3,856 sites, 17% of the total. The policies were written such that one
might need a two-year college education to understand them. Our content
analysis of the sample's domains indicated 44.97% of them expose or suggest a
specific gender/sexual identity or interest likely to be linked to the user. We
identify three core implications of the quantitative results: 1) the
unique/elevated risks of porn data leakage versus other types of data, 2) the
particular risks/impact for vulnerable populations, and 3) the complications of
providing consent for porn site users and the need for affirmative consent in
these online sexual interactions.",website tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",website tracking
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",website tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",website tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",website tracking
http://arxiv.org/abs/1805.01187v1,"A dominant regulatory model for web privacy is ""notice and choice"". In this
model, users are notified of data collection and provided with options to
control it. To examine the efficacy of this approach, this study presents the
first large-scale audit of disclosure of third-party data collection in website
privacy policies. Data flows on one million websites are analyzed and over
200,000 websites' privacy policies are audited to determine if users are
notified of the names of the companies which collect their data. Policies from
25 prominent third-party data collectors are also examined to provide deeper
insights into the totality of the policy environment. Policies are additionally
audited to determine if the choice expressed by the ""Do Not Track"" browser
setting is respected.
  Third-party data collection is wide-spread, but fewer than 15% of attributed
data flows are disclosed. The third-parties most likely to be disclosed are
those with consumer services users may be aware of, those without consumer
services are less likely to be mentioned. Policies are difficult to understand
and the average time requirement to read both a given site{\guillemotright}s
policy and the associated third-party policies exceeds 84 minutes. Only 7% of
first-party site policies mention the Do Not Track signal, and the majority of
such mentions are to specify that the signal is ignored. Among third-party
policies examined, none offer unqualified support for the Do Not Track signal.
Findings indicate that current implementations of ""notice and choice"" fail to
provide notice or respect choice.",website tracking
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",website tracking
http://arxiv.org/abs/1812.01514v2,"Web tracking has been extensively studied over the last decade. To detect
tracking, previous studies and user tools rely on filter lists. However, there
has always been a suspicion that lists miss many trackers. In this paper, we
propose an alternative method to detect trackers inspired by analyzing behavior
of invisible pixels. By crawling 84,658 webpages from 8,744 domains, we detect
that third-party invisible pixels are widely deployed: they are present on more
than 94.51% of domains and constitute 35.66% of all third-party images. We
propose a fine-grained behavioral classification of tracking based on the
analysis of invisible pixels. We use this classification to detect new
categories of tracking and uncover new collaborations between domains on the
full dataset of 4,216,454 third-party requests. We demonstrate that two popular
methods to detect tracking, based on EasyList&EasyPrivacy and on Disconnect
lists respectively miss 25.22% and 30.34% of the trackers that we detect.
Moreover, we find that if we combine all three lists, 379,245 requests
originated from 8,744 domains still track users on 68.70% of websites.",website tracking
http://arxiv.org/abs/1802.02507v1,"Third-party networks collect vast amounts of data about users via web sites
and mobile applications. Consolidations among tracker companies can
significantly increase their individual tracking capabilities, prompting
scrutiny by competition regulators. Traditional measures of market share, based
on revenue or sales, fail to represent the tracking capability of a tracker,
especially if it spans both web and mobile. This paper proposes a new approach
to measure the concentration of tracking capability, based on the reach of a
tracker on popular websites and apps. Our results reveal that tracker
prominence and parent-subsidiary relationships have significant impact on
accurately measuring concentration.",website tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website tracking
http://arxiv.org/abs/1201.3783v1,"As the popularity of content sharing websites such as YouTube and Flickr has
increased, they have become targets for spam, phishing and the distribution of
malware. On YouTube, the facility for users to post comments can be used by
spam campaigns to direct unsuspecting users to bogus e-commerce websites. In
this paper, we demonstrate how such campaigns can be tracked over time using
network motif profiling, i.e. by tracking counts of indicative network motifs.
By considering all motifs of up to five nodes, we identify discriminating
motifs that reveal two distinctly different spam campaign strategies. One of
these strategies uses a small number of spam user accounts to comment on a
large number of videos, whereas a larger number of accounts is used with the
other. We present an evaluation that uses motif profiling to track two active
campaigns matching these strategies, and identify some of the associated user
accounts.",website tracking
http://arxiv.org/abs/1805.09155v2,"User demand for blocking advertising and tracking online is large and
growing. Existing tools, both deployed and described in research, have proven
useful, but lack either the completeness or robustness needed for a general
solution. Existing detection approaches generally focus on only one aspect of
advertising or tracking (e.g. URL patterns, code structure), making existing
approaches susceptible to evasion.
  In this work we present AdGraph, a novel graph-based machine learning
approach for detecting advertising and tracking resources on the web. AdGraph
differs from existing approaches by building a graph representation of the HTML
structure, network requests, and JavaScript behavior of a webpage, and using
this unique representation to train a classifier for identifying advertising
and tracking resources. Because AdGraph considers many aspects of the context a
network request takes place in, it is less susceptible to the single-factor
evasion techniques that flummox existing approaches.
  We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly
accurate, able to replicate the labels of human-generated filter lists with
95.33% accuracy, and can even identify many mistakes in filter lists. We
implement AdGraph as a modification to Chromium. AdGraph adds only minor
overhead to page loading and execution, and is actually faster than stock
Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we
conclude that AdGraph is both accurate enough and performant enough for online
use, breaking comparable or fewer websites than popular filter list based
approaches.",website tracking
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website tracking
http://arxiv.org/abs/1409.1066v1,"The presence of third-party tracking on websites has become customary.
However, our understanding of the third-party ecosystem is still very
rudimentary. We examine third-party trackers from a geographical perspective,
observing the third-party tracking ecosystem from 29 countries across the
globe. When examining the data by region (North America, South America, Europe,
East Asia, Middle East, and Oceania), we observe significant geographical
variation between regions and countries within regions. We find trackers that
focus on specific regions and countries, and some that are hosted in countries
outside their expected target tracking domain. Given the differences in
regulatory regimes between jurisdictions, we believe this analysis sheds light
on the geographical properties of this ecosystem and on the problems that these
may pose to our ability to track and manage the different data silos that now
store personal data about us all.",website tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",website tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",website tracking
http://arxiv.org/abs/1907.03892v2,"In this paper, we demonstrate a novel algorithm that uses ellipse fitting to
estimate the bounding box rotation angle and size with the segmentation(mask)
on the target for online and real-time visual object tracking. Our method,
SiamMask E, improves the bounding box fitting procedure of the state-of-the-art
object tracking algorithm SiamMask and still retains a fast-tracking frame rate
(80 fps) on a system equipped with GPU (GeForce GTX 1080 Ti or higher). We
tested our approach on the visual object tracking datasets (VOT2016, VOT2018,
and VOT2019) that were labeled with rotated bounding boxes. By comparing with
the original SiamMask, we achieved an improved Accuracy of 0.645 and 0.303 EAO
on VOT2019, which is 0.049 and 0.02 higher than the original SiamMask. Our
project website is available at
http://jtl.lassonde.yorku.ca/2019/07/siammask_e/.",website tracking
http://arxiv.org/abs/1908.02261v1,"We turn our attention to the elephant in the room of data protection, which
is none other than the simple and obvious question: ""Who's tracking sensitive
domains?"". Despite a fast-growing amount of work on more complex facets of the
interplay between privacy and the business models of the Web, the obvious
question of who collects data on domains where most people would prefer not be
seen, has received rather limited attention. First, we develop a methodology
for automatically annotating websites that belong to a sensitive category, e.g.
as defined by the General Data Protection Regulation (GDPR). Then, we extract
the third party tracking services included directly, or via recursive
inclusions, by the above mentioned sites. Having analyzed around 30k sensitive
domains, we show that such domains are tracked, albeit less intensely than the
mainstream ones. Looking in detail at the tracking services operating on them,
we find well known names, as well as some less known ones, including some
specializing on specific sensitive categories.",website tracking
http://arxiv.org/abs/1603.06289v1,"Numerous tools have been developed to aggressively block the execution of
popular JavaScript programs (JS) in Web browsers. Such blocking also affects
functionality of webpages and impairs user experience. As a consequence, many
privacy preserving tools (PP-Tools) that have been developed to limit online
tracking, often executed via JS, may suffer from poor performance and limited
uptake. A mechanism that can isolate JS necessary for proper functioning of the
website from tracking JS would thus be useful. Through the use of a manually
labelled dataset composed of 2,612 JS, we show how current PP-Tools are
ineffective in finding the right balance between blocking tracking JS and
allowing functional JS. To the best of our knowledge, this is the first study
to assess the performance of current web PP-Tools.
  To improve this balance, we examine the two classes of JS and hypothesize
that tracking JS share structural similarities that can be used to
differentiate them from functional JS. The rationale of our approach is that
web developers often borrow and customize existing pieces of code in order to
embed tracking (resp. functional) JS into their webpages. We then propose
one-class machine learning classifiers using syntactic and semantic features
extracted from JS. When trained only on samples of tracking JS, our classifiers
achieve an accuracy of 99%, where the best of the PP-Tools achieved an accuracy
of 78%.
  We further test our classifiers and several popular PP-Tools on a corpus of
4K websites with 135K JS. The output of our best classifier on this data is
between 20 to 64% different from the PP-Tools. We manually analyse a sample of
the JS for which our classifier is in disagreement with all other PP-Tools, and
show that our approach is not only able to enhance user web experience by
correctly classifying more functional JS, but also discovers previously unknown
tracking services.",website tracking
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",website tracking
http://arxiv.org/abs/1811.00918v1,"Web developers routinely rely on third-party Java-Script libraries such as
jQuery to enhance the functionality of their sites. However, if not properly
maintained, such dependencies can create attack vectors allowing a site to be
compromised.
  In this paper, we conduct the first comprehensive study of client-side
JavaScript library usage and the resulting security implications across the
Web. Using data from over 133 k websites, we show that 37% of them include at
least one library with a known vulnerability; the time lag behind the newest
release of a library is measured in the order of years. In order to better
understand why websites use so many vulnerable or outdated libraries, we track
causal inclusion relationships and quantify different scenarios. We observe
sites including libraries in ad hoc and often transitive ways, which can lead
to different versions of the same library being loaded into the same document
at the same time. Furthermore, we find that libraries included transitively, or
via ad and tracking code, are more likely to be vulnerable. This demonstrates
that not only website administrators, but also the dynamic architecture and
developers of third-party services are to blame for the Web's poor state of
library management.
  The results of our work underline the need for more thorough approaches to
dependency management, code maintenance and third-party code inclusion on the
Web.",website tracking
http://arxiv.org/abs/1109.0097v1,"Recent work in traffic analysis has shown that traffic patterns leaked
through side channels can be used to recover important semantic information.
For instance, attackers can find out which website, or which page on a website,
a user is accessing simply by monitoring the packet size distribution. We show
that traffic analysis is even a greater threat to privacy than previously
thought by introducing a new attack that can be carried out remotely. In
particular, we show that, to perform traffic analysis, adversaries do not need
to directly observe the traffic patterns. Instead, they can gain sufficient
information by sending probes from a far-off vantage point that exploits a
queuing side channel in routers. To demonstrate the threat of such remote
traffic analysis, we study a remote website detection attack that works against
home broadband users. Because the remotely observed traffic patterns are more
noisy than those obtained using previous schemes based on direct local traffic
monitoring, we take a dynamic time warping (DTW) based approach to detecting
fingerprints from the same website. As a new twist on website fingerprinting,
we consider a website detection attack, where the attacker aims to find out
whether a user browses a particular web site, and its privacy implications. We
show experimentally that, although the success of the attack is highly
variable, depending on the target site, for some sites very low error rates. We
also show how such website detection can be used to deanonymize message board
users.",website monitoring
http://arxiv.org/abs/1802.05409v1,"Traffic analysis attacks to identify which web page a client is browsing,
using only her packet metadata --- known as website fingerprinting --- has been
proven effective in closed-world experiments against privacy technologies like
Tor. However, due to the base rate fallacy, these attacks have failed in large
open-world settings against clients that visit sensitive pages with a low base
rate. We find that this is because they have poor precision as they were
designed to maximize recall.
  In this work, we argue that precision is more important than recall for
open-world website fingerprinting. For this reason, we develop three classes of
{\em precision optimizers}, based on confidence, distance, and ensemble
learning, that can be applied to any classifier to increase precision. We test
them on known website fingerprinting attacks and show significant improvements
in precision. Against a difficult scenario, where the attacker wants to monitor
and distinguish 100 sensitive pages each with a low mean base rate of 0.00001,
our best optimized classifier can achieve a precision of 0.78; the highest
precision of any known attack before optimization was 0.014. We use precise
classifiers to tackle realistic objectives in website fingerprinting, including
selection, identification, and defeating website fingerprinting defenses.",website monitoring
http://arxiv.org/abs/1509.00789v3,"Website fingerprinting enables an attacker to infer which web page a client
is browsing through encrypted or anonymized network connections. We present a
new website fingerprinting technique based on random decision forests and
evaluate performance over standard web pages as well as Tor hidden services, on
a larger scale than previous works. Our technique, k-fingerprinting, performs
better than current state-of-the-art attacks even against website
fingerprinting defenses, and we show that it is possible to launch a website
fingerprinting attack in the face of a large amount of noisy data. We can
correctly determine which of 30 monitored hidden services a client is visiting
with 85% true positive rate (TPR), a false positive rate (FPR) as low as 0.02%,
from a world size of 100,000 unmonitored web pages. We further show that error
rates vary widely between web resources, and thus some patterns of use will be
predictably more vulnerable to attack than others.",website monitoring
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website monitoring
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website monitoring
http://arxiv.org/abs/1601.07077v1,"Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.",website monitoring
http://arxiv.org/abs/1705.04437v1,"The browser history reveals highly sensitive information about users, such as
financial status, health conditions, or political views. Private browsing modes
and anonymity networks are consequently important tools to preserve the privacy
not only of regular users but in particular of whistleblowers and dissidents.
Yet, in this work we show how a malicious application can infer opened websites
from Google Chrome in Incognito mode and from Tor Browser by exploiting
hardware performance events (HPEs). In particular, we analyze the browsers'
microarchitectural footprint with the help of advanced Machine Learning
techniques: k-th Nearest Neighbors, Decision Trees, Support Vector Machines,
and in contrast to previous literature also Convolutional Neural Networks. We
profile 40 different websites, 30 of the top Alexa sites and 10 whistleblowing
portals, on two machines featuring an Intel and an ARM processor. By monitoring
retired instructions, cache accesses, and bus cycles for at most 5 seconds, we
manage to classify the selected websites with a success rate of up to 86.3%.
The results show that hardware performance events can clearly undermine the
privacy of web users. We therefore propose mitigation strategies that impede
our attacks and still allow legitimate use of HPEs.",website monitoring
http://arxiv.org/abs/1612.05318v1,"CUORE is a cryogenic experiment searching primarily for neutrinoless double
decay in $^{130}$Te. It will begin data-taking operations in 2016. To monitor
the cryostat and detector during commissioning and data taking, we have
designed and developed Slow Monitoring systems. In addition to real-time
systems using LabVIEW, we have an alarm, analysis, and archiving website that
uses MongoDB, AngularJS, and Bootstrap software. These modern, state of the art
software packages make the monitoring system transparent, easily maintainable,
and accessible on many platforms including mobile devices.",website monitoring
http://arxiv.org/abs/1704.04937v2,"Browsers can detect malicious websites that are provisioned with forged or
fake TLS/SSL certificates. However, they are not so good at detecting malicious
websites if they are provisioned with mistakenly issued certificates or
certificates that have been issued by a compromised certificate authority.
Google proposed certificate transparency which is an open framework to monitor
and audit certificates in real time. Thereafter, a few other certificate
transparency schemes have been proposed which can even handle revocation. All
currently known constructions use Merkle hash trees and have proof size
logarithmic in the number of certificates/domain owners.
  We present a new certificate transparency scheme with short (constant size)
proofs. Our construction makes use of dynamic bilinear-map accumulators. The
scheme has many desirable properties like efficient revocation, low
verification cost and update costs comparable to the existing schemes. We
provide proofs of security and evaluate the performance of our scheme.",website monitoring
http://arxiv.org/abs/1507.06562v1,"As of February, 2015, HTTP/2, the update to the 16-year-old HTTP 1.1, is
officially complete. HTTP/2 aims to improve the Web experience by solving
well-known problems (e.g., head of line blocking and redundant headers), while
introducing new features (e.g., server push and content priority). On paper
HTTP/2 represents the future of the Web. Yet, it is unclear whether the Web
itself will, and should, hop on board. To shed some light on these questions,
we built a measurement platform that monitors HTTP/2 adoption and performance
across the Alexa top 1 million websites on a daily basis. Our system is live
and up-to-date results can be viewed at http://isthewebhttp2yet.com/. In this
paper, we report our initial findings from a 6 month measurement campaign
(November 2014 - May 2015). We find 13,000 websites reporting HTTP/2 support,
but only 600, mostly hosted by Google and Twitter, actually serving content. In
terms of speed, we find no significant benefits from HTTP/2 under stable
network conditions. More benefits appear in a 3G network where current Web
development practices make HTTP/2 more resilient to losses and delay variation
than previously believed.",website monitoring
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",website monitoring
http://arxiv.org/abs/1804.01237v1,"With the rapid growth of mobile internet, mobile application, like website
navigation, searching, e-Shopping and app download, etc. are all popular in
worldwide. Meanwhile, it become more and more popular that traditional HTTP
protocol, which is also applying in not only web browsing but also
communication between mobile application clients and servers. Besides, it has
made HTTP Hijacking profitable. Furthermore, it has brought a lot of troubles
for users, network operators and ISP. We analyze the principle of HTTP spectral
Hijacking and present a mechanism of collaboratively detecting and locating
called Co HijackingMonitor. Experimental result shows that, Co HijackingMonitor
can solve the hijacking problem effectively.",website monitoring
http://arxiv.org/abs/1807.06373v1,"Predicting the popularity of online content has attracted much attention in
the past few years. In news rooms, for instance, journalists and editors are
keen to know, as soon as possible, the articles that will bring the most
traffic into their website. The relevant literature includes a number of
approaches and algorithms to perform this forecasting. Most of the proposed
methods require monitoring the popularity of content during some time after it
is posted, before making any longer-term prediction. In this paper, we propose
a new approach for predicting the popularity of news articles before they go
online. Our approach complements existing content-based methods, and is based
on a number of observations regarding article similarity and topicality. First,
the popularity of a new article is correlated with the popularity of similar
articles of recent publication. Second, the popularity of the new article is
related to the recent historical popularity of its main topic. Based on these
observations, we use time series forecasting to predict the number of visits an
article will receive. Our experiments, conducted on a real data collection of
articles in an international news website, demonstrate the effectiveness and
efficiency of the proposed method.",website monitoring
http://arxiv.org/abs/1811.11218v1,"Over the past years, literature has shown that attacks exploiting the
microarchitecture of modern processors pose a serious threat to the privacy of
mobile phone users. This is because applications leave distinct footprints in
the processor, which can be used by malware to infer user activities. In this
work, we show that these inference attacks are considerably more practical when
combined with advanced AI techniques. In particular, we focus on profiling the
activity in the last-level cache (LLC) of ARM processors. We employ a simple
Prime+Probe based monitoring technique to obtain cache traces, which we
classify with Deep Learning methods including Convolutional Neural Networks. We
demonstrate our approach on an off-the-shelf Android phone by launching a
successful attack from an unprivileged, zeropermission App in well under a
minute. The App thereby detects running applications with an accuracy of 98%
and reveals opened websites and streaming videos by monitoring the LLC for at
most 6 seconds. This is possible, since Deep Learning compensates measurement
disturbances stemming from the inherently noisy LLC monitoring and unfavorable
cache characteristics such as random line replacement policies. In summary, our
results show that thanks to advanced AI techniques, inference attacks are
becoming alarmingly easy to implement and execute in practice. This once more
calls for countermeasures that confine microarchitectural leakage and protect
mobile phone applications, especially those valuing the privacy of their users.",website monitoring
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",website monitoring
http://arxiv.org/abs/1811.09126v2,"Online monitoring user cardinalities (or degrees) in graph streams is
fundamental for many applications. For example in a bipartite graph
representing user-website visiting activities, user cardinalities (the number
of distinct visited websites) are monitored to report network anomalies. These
real-world graph streams may contain user-item duplicates and have a huge
number of distinct user-item pairs, therefore, it is infeasible to exactly
compute user cardinalities when memory and computation resources are
limited.Existing methods are designed to approximately estimate user
cardinalities, whose accuracy highly depends on parameters that are not easy to
set. Moreover, these methods cannot provide anytime-available estimation, as
the user cardinalities are computed at the end of the data stream. Real-time
applications such as anomaly detection require that user cardinalities are
estimated on the fly. To address these problems, we develop novel bit and
register sharing algorithms, which use a bit array and a register array to
build a compact sketch of all users' connected items respectively. Compared
with previous bit and register sharing methods, our algorithms exploit the
dynamic properties of the bit and register arrays (e.g., the fraction of zero
bits in the bit array at each time) to significantly improve the estimation
accuracy, and have low time complexity (O(1)) to update the estimations each
time they observe a new user-item pair. In addition, our algorithms are simple
and easy to use, without requirements to tune any parameter. We evaluate the
performance of our methods on real-world datasets. The experimental results
demonstrate that our methods are several times more accurate and faster than
state-of-the-art methods using the same amount of memory.",website monitoring
http://arxiv.org/abs/1105.1234v2,"Trojan virus attacks pose one of the most serious threats to computer
security. A Trojan horse is typically separated into two parts - a server and a
client. It is the client that is cleverly disguised as significant software and
positioned in peer-to-peer file sharing networks, or unauthorized download
websites. The most common means of infection is through email attachments. The
developer of the virus usually uses various spamming techniques in order to
distribute the virus to unsuspecting users. Malware developers use chat
software as another method to spread their Trojan horse viruses such as Yahoo
Messenger and Skype. The objective of this paper is to explore the network
packet information and detect the behavior of Trojan attacks to monitoring
operating systems such as Windows and Linux. This is accomplished by detecting
and analyzing the Trojan infected packet from a network segment -which passes
through email attachment- before attacking a host computer. The results that
have been obtained to detect information and to store infected packets through
monitoring when using the web browser also compare the behaviors of Linux and
Windows using the payload size after implementing the Wireshark sniffer packet
results. Conclusions of the figures analysis from the packet captured data to
analyze the control bit, and check the behavior of the control bits, and the
usability of the operating systems Linux and Windows.",website monitoring
http://arxiv.org/abs/1203.4099v1,"The aim of the Karlsruhe Tritium Neutrino experiment (KATRIN) is the direct
(model-independent) measurement of the neutrino mass. For that purpose a
windowless gaseous tritium source is used, with a tritium throughput of 40
g/day. In order to reach the design sensitivity of 0.2 eV/c^{2} (90% C.L.) the
key parameters of the tritium source, i.e. the gas inlet rate and the gas
composition, have to be stabilized and monitored at the 0.1% level (1 sigma).
Any small change of the tritium gas composition will manifest itself in
non-negligible effects on the KATRIN measurements; therefore, Laser Raman
spectroscopy (LARA) is the method of choice for the monitoring of the gas
composition because it is a non-invasive and fast in-line measurement
technique. In these proceedings, the requirements of KATRIN for statistical and
systematical uncertainties of this method are discussed. An overview of the
current performance of the LARA system in regard to precision will be given. In
addition, two complementary approaches of intensity calibration are presented.",website monitoring
http://arxiv.org/abs/1610.02065v1,"After carefully considering the scalability problem in Tor and exhaustively
evaluating related works on AS-level adversaries, the author proposes
ASmoniTor, which is an autonomous system monitor for mitigating correlation
attacks in the Tor network. In contrast to prior works, which often released
offline packets, including the source code of a modified Tor client and a
snapshot of the Internet topology, ASmoniTor is an online system that assists
end users with mitigating the threat of AS-level adversaries in a near
real-time fashion. For Tor clients proposed in previous works, users need to
compile the source code on their machine and continually update the snapshot of
the Internet topology in order to obtain accurate AS-path inferences. On the
contrary, ASmoniTor is an online platform that can be utilized easily by not
only technical users, but also by users without a technical background, because
they only need to access it via Tor and input two parameters to execute an
AS-aware path selection algorithm. With ASmoniTor, the author makes three key
technical contributions to the research against AS-level adversaries in the Tor
network. First, ASmoniTor does not require the users to initiate complicated
source code compilations. Second, it helps to reduce errors in AS-path
inferences by letting users input a set of suspected ASes obtained directly
from their own traceroute measurements. Third, the Internet topology database
at the back-end of ASmoniTor is periodically updated to assure near real-time
AS-path inferences between Tor exit nodes and the most likely visited websites.
Finally, in addition to its convenience, ASmoniTor gives users full control
over the information they want to input, thus preserving their privacy.",website monitoring
http://arxiv.org/abs/1709.05628v1,"Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.",website monitoring
http://arxiv.org/abs/1902.03937v2,"Identifying and monitoring Open Access (OA) publications might seem a trivial
task while practical efforts prove otherwise. Contradictory information arise
often depending on metadata employed. We strive to assign OA status to
publications in Web of Science (WOS) and Scopus while complementing it with
different sources of OA information to resolve contradicting cases. We linked
publications from WOS and Scopus via DOIs and ISSNs to Unpaywall, Crossref,
DOAJ and ROAD. Only about 50% of articles and reviews from WOS and Scopus could
be matched via a DOI to Unpaywall. Matching with Crossref brought 56 distinct
licences, which define in many cases the legally binding access status of
publications. But only 44% of publications hold only a single licence on
Crossref, while more than 50% have no licence information submitted to
Crossref. Contrasting OA information from Crossref licences with Unpaywall we
found contradictory cases overall amounting to more than 25%, which might be
partially explained by (ex-)including green OA. A further manual check found
about 17% of OA publications that are not accessible and 15% non-OA
publications that are accessible through publishers' websites. These
preliminary results suggest that identification of OA state of publications
denotes a difficult and currently unfulfilled task.",website monitoring
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",illegal advertising
http://arxiv.org/abs/1710.02546v1,"The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.",illegal advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",illegal advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",illegal advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",illegal advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",illegal advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",illegal advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",illegal advertising
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",illegal advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",illegal advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",illegal advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",illegal advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",illegal advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",illegal advertising
http://arxiv.org/abs/1907.07275v1,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",illegal advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",illegal advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",illegal advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",illegal advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",illegal advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",illegal advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",illegal advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",illegal advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",illegal advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",illegal advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",illegal advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",illegal advertising
http://arxiv.org/abs/1809.10948v1,"In Named Data Networking (NDN), there is a need for routing protocols to
populate Forwarding Information Base (FIB) tables so that the Interest messages
can be forwarded. To populate FIBs, clients and routers require some routing
information. One method to obtain this information is that network nodes
exchange routing information by each node advertising the available content
objects. Bloom Filter-based Routing approaches like BFR [1], use Bloom Filters
(BFs) to advertise all provided content objects, which consumes valuable
bandwidth and storage resources. This strategy is inefficient as clients
request only a small number of the provided content objects and they do not
need the content advertisement information for all provided content objects. In
this paper, we propose a novel routing algorithm for NDN called pull-based BFR
in which servers only advertise the demanded file names. We compare the
performance of pull-based BFR with original BFR and with a flooding-assisted
routing protocol. Our experimental evaluations show that pull-based BFR
outperforms original BFR in terms of communication overhead needed for content
advertisements, average roundtrip delay, memory resources needed for storing
content advertisements at clients and routers, and the impact of false positive
reports on routing. The comparisons also show that pull-based BFR outperforms
flooding-assisted routing in terms of average round-trip delay.",false advertising
http://arxiv.org/abs/1808.09218v4,"Targeted advertising is meant to improve the efficiency of matching
advertisers to their customers. However, targeted advertising can also be
abused by malicious advertisers to efficiently reach people susceptible to
false stories, stoke grievances, and incite social conflict. Since targeted ads
are not seen by non-targeted and non-vulnerable people, malicious ads are
likely to go unreported and their effects undetected. This work examines a
specific case of malicious advertising, exploring the extent to which political
ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
elections exploited Facebook's targeted advertising infrastructure to
efficiently target ads on divisive or polarizing topics (e.g., immigration,
race-based policing) at vulnerable sub-populations. In particular, we do the
following: (a) We conduct U.S. census-representative surveys to characterize
how users with different political ideologies report, approve, and perceive
truth in the content of the IRA ads. Our surveys show that many ads are
""divisive"": they elicit very different reactions from people belonging to
different socially salient groups. (b) We characterize how these divisive ads
are targeted to sub-populations that feel particularly aggrieved by the status
quo. Our findings support existing calls for greater transparency of content
and targeting of political ads. (c) We particularly focus on how the Facebook
ad API facilitates such targeting. We show how the enormous amount of personal
data Facebook aggregates about users and makes available to advertisers enables
such malicious targeting.",false advertising
http://arxiv.org/abs/1702.00340v1,"Locating the demanded content is one of the major challenges in
Information-Centric Networking (ICN). This process is known as content
discovery. To facilitate content discovery, in this paper we focus on Named
Data Networking (NDN) and propose a novel routing scheme for content discovery,
called Bloom Filter-based Routing (BFR), which is fully distributed, content
oriented, and topology agnostic at the intra-domain level. In BFR, origin
servers advertise their content objects using Bloom filters. We compare the
performance of the proposed BFR with flooding and shortest path content
discovery approaches. BFR outperforms its counterparts in terms of the average
round-trip delay, while it is shown to be very robust to false positive reports
from Bloom filters. Also, BFR is much more robust than shortest path routing to
topology changes. BFR strongly outperforms flooding and performs almost equal
with shortest path routing with respect to the normalized communication costs
for data retrieval and total communication overhead for forwarding Interests.
All the three approaches achieve similar mean hit distance. The signalling
overhead for content advertisement in BFR is much lower than the signalling
overhead for calculating shortest paths in the shortest path approach. Finally,
BFR requires small storage overhead for maintaining content advertisements.",false advertising
http://arxiv.org/abs/1305.4045v1,"In recent years, there has been rapid growth in mobile devices such as
smartphones, and a number of applications are developed specifically for the
smartphone market. In particular, there are many applications that are ``free''
to the user, but depend on advertisement services for their revenue. Such
applications include an advertisement module - a library provided by the
advertisement service - that can collect a user's sensitive information and
transmit it across the network. Users accept this business model, but in most
cases the applications do not require the user's acknowledgment in order to
transmit sensitive information. Therefore, such applications' behavior becomes
an invasion of privacy. In our analysis of 1,188 Android applications' network
traffic and permissions, 93% of the applications we analyzed connected to
multiple destinations when using the network. 61% required a permission
combination that included both access to sensitive information and use of
networking services. These applications have the potential to leak the user's
sensitive information. In an effort to enable users to control the transmission
of their private information, we propose a system which, using a novel
clustering method based on the HTTP packet destination and content distances,
generates signatures from the clustering result and uses them to detect
sensitive information leakage from Android applications. Our system does not
require an Android framework modification or any special privileges. Thus users
can easily introduce our system to their devices, and manage suspicious
applications' network behavior in a fine grained manner. Our system accurately
detected 94% of the sensitive information leakage from the applications
evaluated and produced only 5% false negative results, and less than 3% false
positive results.",false advertising
http://arxiv.org/abs/1903.00733v2,"Advertising is a primary means for revenue generation for millions of
websites and smartphone apps (publishers). Naturally, a fraction of publishers
abuse the ad-network to systematically defraud advertisers of their money.
Defenses have matured to overcome some forms of click fraud but are inadequate
against the threat of organic click fraud attacks. Malware detection systems
including honeypots fail to stop click fraud apps; ad-network filters are
better but measurement studies have reported that a third of the clicks
supplied by ad-networks are fake; collaborations between ad-networks and app
stores that bad-lists malicious apps works better still, but fails to prevent
criminals from writing fraudulent apps which they monetise until they get
banned and start over again. This work develops novel inference techniques that
can isolate click fraud attacks using their fundamental properties. In the {\em
mimicry defence}, we leverage the observation that organic click fraud involves
the re-use of legitimate clicks. Thus we can isolate fake-clicks by detecting
patterns of click-reuse within ad-network clickstreams with historical
behaviour serving as a baseline. Second, in {\em bait-click defence}. we
leverage the vantage point of an ad-network to inject a pattern of bait clicks
into the user's device, to trigger click fraud-apps that are gated on
user-behaviour. Our experiments show that the mimicry defence detects around
81\% of fake-clicks in stealthy (low rate) attacks with a false-positive rate
of 110110 per hundred thousand clicks. Bait-click defence enables further
improvements in detection rates of 95\% and reduction in false-positive rates
of between 0 and 30 clicks per million, a substantial improvement over current
approaches.",false advertising
http://arxiv.org/abs/1602.07128v2,"It is known that some network operators inject false content into users'
network traffic. Yet all previous works that investigate this practice focus on
edge ISPs (Internet Service Providers), namely, those that provide Internet
access to end users. Edge ISPs that inject false content affect their customers
only. However, in this work we show that not only edge ISPs may inject false
content, but also core network operators. These operators can potentially alter
the traffic of \emph{all} Internet users who visit predetermined websites. We
expose this practice by inspecting a large amount of traffic originating from
several networks. Our study is based on the observation that the forged traffic
is injected in an out-of-band manner: the network operators do not update the
network packets in-path, but rather send the forged packets \emph{without}
dropping the legitimate ones. This creates a race between the forged and the
legitimate packets as they arrive to the end user. This race can be identified
and analyzed. Our analysis shows that the main purpose of content injection is
to increase the network operators' revenue by inserting advertisements to
websites. Nonetheless, surprisingly, we have also observed numerous cases of
injected malicious content. We publish representative samples of the injections
to facilitate continued analysis of this practice by the security community.",false advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",false advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",false advertising
http://arxiv.org/abs/1209.2557v1,"A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.",false advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",false advertising
http://arxiv.org/abs/1508.01843v2,"Background: Twitter has become the ""wild-west"" of marketing and promotional
strategies for advertisement agencies. Electronic cigarettes have been heavily
marketed across Twitter feeds, offering discounts, ""kid-friendly"" flavors,
algorithmically generated false testimonials, and free samples. Methods:All
electronic cigarette keyword related tweets from a 10% sample of Twitter
spanning January 2012 through December 2014 (approximately 850,000 total
tweets) were identified and categorized as Automated or Organic by combining a
keyword classification and a machine trained Human Detection algorithm. A
sentiment analysis using Hedonometrics was performed on Organic tweets to
quantify the change in consumer sentiments over time. Commercialized tweets
were topically categorized with key phrasal pattern matching. Results:The
overwhelming majority (80%) of tweets were classified as automated or
promotional in nature. The majority of these tweets were coded as
commercialized (83.65% in 2013), up to 33% of which offered discounts or free
samples and appeared on over a billion twitter feeds as impressions. The
positivity of Organic (human) classified tweets has decreased over time (5.84
in 2013 to 5.77 in 2014) due to a relative increase in the negative words
ban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the
positive words like haha,good,cool. Automated tweets are more positive than
organic (6.17 versus 5.84) due to a relative increase in the marketing words
best,win,buy,sale,health,discount and a relative decrease in negative words
like bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter
and the clinical uncertainty of the long term health complications of
electronic cigarette consumption, the protection of public health warrants
scrutiny and potential regulation of social media marketing.",false advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",false advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",false advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",false advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",false advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",false advertising
http://arxiv.org/abs/1111.0387v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms,absence of centralized
monitoring points, and lack of clear lines of defense. Most of the routing
protocols for MANETs are thus vulnerable to various types of attacks. Ad hoc
on-demand distance vector routing (AODV) is a very popular routing algorithm.
However, it is vulnerable to the well-known black hole attack, where a
malicious node falsely advertises good paths to a destination node during the
route discovery process. This attack becomes more sever when a group of
malicious nodes cooperate each other. In this paper, a defense mechanism is
presented against a coordinated attack by multiple black hole nodes in a MANET.
The simulation carried out on the proposed scheme has produced results that
demonstrate the effectiveness of the mechanism in detection of the attack while
maintaining a reasonable level of throughput in the network.",false advertising
http://arxiv.org/abs/1302.4882v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms, absence of centralized
monitoring points, and lack of clear lines of defense. Protecting the network
layer of a MANET from malicious attacks is an important and challenging
security issue, since most of the routing protocols for MANETs are vulnerable
to various types of attacks. Ad hoc on-demand distance vector routing (AODV) is
a very popular routing algorithm. However, it is vulnerable to the well-known
black hole attack, where a malicious node falsely advertises good paths to a
destination node during the route discovery process but drops all packets in
the data forwarding phase. This attack becomes more severe when a group of
malicious nodes cooperate each other. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of the nodes.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead and computation overhead in the nodes.",false advertising
http://arxiv.org/abs/1709.10388v1,"The online ads trading platform plays a crucial role in connecting publishers
and advertisers and generates tremendous value in facilitating the convenience
of our lives. It has been evolving into a more and more complicated structure.
In this paper, we consider the problem of maximizing the revenue for the seller
side via utilizing proper reserve price for the auctions in a dynamical way.
  Predicting the optimal reserve price for each auction in the repeated auction
marketplaces is a non-trivial problem. However, we were able to come up with an
efficient method of improving the seller revenue by mainly focusing on
adjusting the reserve price for those high-value inventories. Previously, no
dedicated work has been performed from this perspective. Inspired by Paul and
Michael, our model first identifies the value of the inventory by predicting
the top bid price bucket using a cascade of classifiers. The cascade is
essential in significantly reducing the false positive rate of a single
classifier. Based on the output of the first step, we build another cluster of
classifiers to predict the price separations between the top two bids. We
showed that although the high-value auctions are only a small portion of all
the traffic, successfully identifying them and setting correct reserve price
would result in a significant revenue lift. Moreover, our optimization is
compatible with all other reserve price models in the system and does not
impact their performance. In other words, when combined with other models, the
enhancement on exchange revenue will be aggregated. Simulations on randomly
sampled Yahoo ads exchange (YAXR) data showed stable and expected lift after
applying our model.",false advertising
http://arxiv.org/abs/1807.07741v1,"Employers actively look for talents having not only specific hard skills but
also various soft skills. To analyze the soft skill demands on the job market,
it is important to be able to detect soft skill phrases from job advertisements
automatically. However, a naive matching of soft skill phrases can lead to
false positive matches when a soft skill phrase, such as friendly, is used to
describe a company, a team, or another entity, rather than a desired candidate.
  In this paper, we propose a phrase-matching-based approach which
differentiates between soft skill phrases referring to a candidate vs.
something else. The disambiguation is formulated as a binary text
classification problem where the prediction is made for the potential soft
skill based on the context where it occurs. To inform the model about the soft
skill for which the prediction is made, we develop several approaches,
including soft skill masking and soft skill tagging.
  We compare several neural network based approaches, including CNN, LSTM and
Hierarchical Attention Model. The proposed tagging-based input representation
using LSTM achieved the highest recall of 83.92% on the job dataset when fixing
a precision to 95%.",false advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",false advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",false advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",false advertising
http://arxiv.org/abs/1907.07275v1,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",false advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",false advertising
http://arxiv.org/abs/1802.08365v6,"Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.",misleading advertising
http://arxiv.org/abs/1901.00546v1,"Adversarial examples are delicately perturbed inputs, which aim to mislead
machine learning models towards incorrect outputs. While most of the existing
work focuses on generating adversarial perturbations in multi-class
classification problems, many real-world applications fall into the multi-label
setting in which one instance could be associated with more than one label. For
example, a spammer may generate adversarial spams with malicious advertising
while maintaining the other labels such as topic labels unchanged. To analyze
the vulnerability and robustness of multi-label learning models, we investigate
the generation of multi-label adversarial perturbations. This is a challenging
task due to the uncertain number of positive labels associated with one
instance, as well as the fact that multiple labels are usually not mutually
exclusive with each other. To bridge this gap, in this paper, we propose a
general attacking framework targeting on multi-label classification problem and
conduct a premier analysis on the perturbations for deep neural networks.
Leveraging the ranking relationships among labels, we further design a
ranking-based framework to attack multi-label ranking algorithms. We specify
the connection between the two proposed frameworks and separately design two
specific methods grounded on each of them to generate targeted multi-label
perturbations. Experiments on real-world multi-label image classification and
ranking problems demonstrate the effectiveness of our proposed frameworks and
provide insights of the vulnerability of multi-label deep learning models under
diverse targeted attacking strategies. Several interesting findings including
an unpolished defensive strategy, which could potentially enhance the
interpretability and robustness of multi-label deep learning models, are
further presented and discussed at the end.",misleading advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",misleading advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",misleading advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",misleading advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",misleading advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",misleading advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",misleading advertising
http://arxiv.org/abs/1811.03194v2,"Perceptual ad-blocking is a novel approach that detects online advertisements
using visual cues. Compared to traditional filter lists, perceptual ad-blocking
is believed to be less prone to an arms race with web publishers and ad
networks. In this work, we use techniques from adversarial machine learning to
demonstrate that this may not be the case. We show that perceptual ad-blocking
engenders a new arms race that likely disfavors ad-blockers. Unexpectedly,
perceptual ad-blocking can also introduce new vulnerabilities that let an
attacker bypass web security boundaries and mount DDoS attacks. We first
analyze the design space of perceptual ad-blockers and present a unified
architecture that incorporates prior academic and commercial work. We then
explore a variety of attacks on the ad-blocker's visual detection pipeline,
that enable publishers or ad networks to evade or detect ad-blocking, and at
times even abuse its high privilege level to bypass web security boundaries.
Our attacks exploit the unreasonably strong threat model that perceptual
ad-blockers must resist. Finally, we present a concrete set of attacks on an
ad-blocker's internal ad-classifier by constructing adversarial examples in a
real Web page context. For six ad-detection techniques, we create perturbed
ads, perturbed ad-disclosure logos, and native web content that misleads
perceptual ad-blocking with 100% success rates. In one of our attacks, we
demonstrate how a malicious user can upload adversarial content, such as a
perturbed image in a Facebook post, that fools the ad-blocker into removing
another users' non-ad content. Moving beyond the Web and visual domain, we also
build adversarial examples for AdblockRadio, an open source radio client that
uses machine learning to detects ads in raw audio.",misleading advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",misleading advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",misleading advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",misleading advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",misleading advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",misleading advertising
http://arxiv.org/abs/1907.07275v1,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",misleading advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",misleading advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",misleading advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",misleading advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",misleading advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",misleading advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",misleading advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",misleading advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",misleading advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",misleading advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",misleading advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",misleading advertising
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",unfair terms
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",unfair terms
http://arxiv.org/abs/1705.08804v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1706.09838v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1002.4833v1,"The number of users using wireless Local Area Network is increasing
exponentially and their behavior is changing day after day. Nowadays, users of
wireless LAN are using huge amount of bandwidth because of the explosive growth
of some services and applications such as video sharing. This situation imposes
massive pressure on the wireless LAN performance especially in term of fairness
among wireless stations. The limited resources are not distributed fairly in
saturated conditions. The most important resource is the access point buffer
space. This importance is a result of access point being the bottleneck between
two different types of networks. These two types are wired network with
relatively huge bandwidth and wireless network with much smaller bandwidth.
Also the unfairness problem is keep getting worse because of the greedy nature
Transmission Control Protocol (TCP). In this paper, we conduct a comprehensive
study on wireless LAN dynamics and proposed a new mathematical model that
describes the performance and effects of its behavior. We validate the proposed
model by using the simulation technique. The proposed model was able to produce
very good approximation in most of the cases. It also gave us a great insight
into the effective variables in the wireless LAN behavior and what are the
dimensions of the unfairness problem.",unfair terms
http://arxiv.org/abs/1607.07021v1,"We consider single-hop topologies with saturated transmitting nodes, using
IEEE~802.11 DCF for medium access. However, unlike the conventional WiFi, we
study systems where one or more of the protocol parameters are different from
the standard, and/or where the propagation delays among the nodes are not
negligible compared to the duration of a backoff slot. We observe that for
several classes of protocol parameters, and for large propagation delays, such
systems exhibit a certain performance anomaly known as short term unfairness,
which may lead to severe performance degradation. The standard fixed point
analysis technique (and its simple extensions) do not predict the system
behavior well in such cases; a mean field model based asymptotic approach also
is not adequate to predict the performance for networks of practical sizes in
such cases. We provide a detailed stochastic model that accurately captures the
system evolution. Since an exact analysis of this model is computationally
intractable, we develop a novel approximate, but accurate, analysis that uses a
parsimonious state representation for computational tractability. Apart from
providing insights into the system behavior, the analytical method is also able
to quantify the extent of short term unfairness in the system, and can
therefore be used for tuning the protocol parameters to achieve desired
throughput and fairness objectives.",unfair terms
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",unfair terms
http://arxiv.org/abs/0806.1093v1,"We present the station-based unfair access problem among the uplink and the
downlink stations in the IEEE 802.11e infrastructure Basic Service Set (BSS)
when the default settings of the Enhanced Distributed Channel Access (EDCA)
parameters are used. We discuss how the transport layer protocol
characteristics alleviate the unfairness problem. We design a simple,
practical, and standard-compliant framework to be employed at the Access Point
(AP) for fair and efficient access provisioning. A dynamic measurement-based
EDCA parameter adaptation block lies in the core of this framework. The
proposed framework is unique in the sense that it considers the characteristic
differences of Transmission Control Protocol (TCP) and User Datagram Protocol
(UDP) flows and the coexistence of stations with varying bandwidth or
Quality-of-Service (QoS) requirements. Via simulations, we show that our
solution provides short- and long-term fair access for all stations in the
uplink and downlink employing TCP and UDP flows with non-uniform packet rates
in a wired-wireless heterogeneous network. In the meantime, the QoS
requirements of coexisting real-time flows are also maintained.",unfair terms
http://arxiv.org/abs/1510.01125v1,"-Performance of Vehicular Adhoc Networks (VANETs) in high node density
situation has long been a major field of studies. Particular attention has been
paid to the frequent exchange of Cooperative Awareness Messages (CAMs) on which
many road safety applications rely. In the present paper, se focus on the
European Telecommunications Standard Institute (ETSI) Decentralized Congestion
Control (DCC) mechanism, particularly on the evaluation of its facility layers
component when applied in the context of dense networks. For this purpose, a
set of simulations has been conducted over several scenarios, considering rural
highway and urban mobility in order to investigate unfairness and oscillation
issues, and analyze the triggering factors. The experimental results show that
the latest technical specification of the ETSI DCC presents a significant
enhancement in terms of fairness. In contrast, the stability criterion leaves
room for improvement as channel load measurement presents (i) considerable
fluctuations when only the facility layer control is applied and (i.i) severe
state oscillation when different DCC control methods are combined.",unfair terms
http://arxiv.org/abs/1509.03815v1,"In this paper, we revisit two fundamental results of the self-stabilizing
literature about silent BFS spanning tree constructions: the Dolev et al
algorithm and the Huang and Chen's algorithm. More precisely, we propose in the
composite atomicity model three straightforward adaptations inspired from those
algorithms. We then present a deep study of these three algorithms. Our results
are related to both correctness (convergence and closure, assuming a
distributed unfair daemon) and complexity (analysis of the stabilization time
in terms of rounds and steps).",unfair terms
http://arxiv.org/abs/0806.1089v1,"When the stations in an IEEE 802.11 infrastructure Basic Service Set (BSS)
employ Transmission Control Protocol (TCP) in the transport layer, this
exacerbates per-flow unfair access which is a direct result of uplink/downlink
bandwidth asymmetry in the BSS. We propose a novel and simple analytical model
to approximately calculate the per-flow TCP congestion window limit that
provides fair and efficient TCP access in a heterogeneous wired-wireless
scenario. The proposed analysis is unique in that it considers the effects of
varying number of uplink and downlink TCP flows, differing Round Trip Times
(RTTs) among TCP connections, and the use of delayed TCP Acknowledgment (ACK)
mechanism. Motivated by the findings of this analysis, we design a link layer
access control block to be employed only at the Access Point (AP) in order to
resolve the unfair access problem. The novel and simple idea of the proposed
link layer access control block is employing a congestion control and filtering
algorithm on TCP ACK packets of uplink flows, thereby prioritizing the access
of TCP data packets of downlink flows at the AP. Via simulations, we show that
short- and long-term fair access can be provisioned with the introduction of
the proposed link layer access control block to the protocol stack of the AP
while improving channel utilization and access delay.",unfair terms
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",unfair terms
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",unfair terms
http://arxiv.org/abs/1903.01209v2,"Most existing notions of algorithmic fairness are one-shot: they ensure some
form of allocative equality at the time of decision making, but do not account
for the adverse impact of the algorithmic decisions today on the long-term
welfare and prosperity of certain segments of the population. We take a broader
perspective on algorithmic fairness. We propose an effort-based measure of
fairness and present a data-driven framework for characterizing the long-term
impact of algorithmic policies on reshaping the underlying population.
Motivated by the psychological literature on \emph{social learning} and the
economic literature on equality of opportunity, we propose a micro-scale model
of how individuals may respond to decision-making algorithms. We employ
existing measures of segregation from sociology and economics to quantify the
resulting macro-scale population-level change. Importantly, we observe that
different models may shift the group-conditional distribution of qualifications
in different directions. Our findings raise a number of important questions
regarding the formalization of fairness for decision-making models.",unfair terms
http://arxiv.org/abs/1511.06035v7,"Applications running in modern multithreaded environments are sometimes
\emph{over-threaded}. The excess threads do not improve performance, and in
fact may act to degrade performance via \emph{scalability collapse}. Often,
such software also has highly contended locks. We opportunistically leverage
the existence of such locks by modifying the lock admission policy so as to
intentionally limit the number of threads circulating over the lock in a given
period. Specifically, if there are more threads circulating than are necessary
to keep the lock saturated, our approach will selectively cull and passivate
some of those threads. We borrow the concept of \emph{swapping} from the field
of memory management and intentionally impose \emph{concurrency restriction}
(CR) if a lock is oversubscribed. In the worst case CR does no harm, but it
often yields performance benefits. The resultant admission order is unfair over
the short term but we explicitly provide long-term fairness by periodically
shifting threads between the set of passivated threads and those actively
circulating. Our approach is palliative, but often effective.",unfair terms
http://arxiv.org/abs/1905.11260v3,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called r-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and learning algorithm used as a black-box. We provide a
fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves O(log(T)) r-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1907.10516v1,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called $r$-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and the learning algorithm used as a black-box. We provide
a fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves $O(\ln T)$ $r$-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1403.4357v1,"High speed railways (HSRs) have been deployed widely all over the world in
recent years. Different from traditional cellular communication, its high
mobility makes it essential to implement power allocation along the time. In
the HSR case, the transmission rate depends greatly on the distance between the
base station (BS) and the train. As a result, the train receives a time varying
data rate service when passing by a BS. It is clear that the most efficient
power allocation will spend all the power when the train is nearest from the
BS, which will cause great unfairness along the time. On the other hand, the
channel inversion allocation achieves the best fairness in terms of constant
rate transmission. However, its power efficiency is much lower. Therefore, the
power efficiency and the fairness along time are two incompatible objects. For
the HSR cellular system considered in this paper, a trade-off between the two
is achieved by proposing a temporal proportional fair power allocation scheme.
Besides, near optimal closed form solution and one algorithm finding the
$\epsilon$-optimal allocation are presented.",unfair terms
http://arxiv.org/abs/1805.12572v3,"We compare and contrast fourteen measures that have been proposed for the
purpose of quantifying partisan gerrymandering. We consider measures that,
rather than examining the shapes of districts, utilize only the partisan vote
distribution among districts. The measures considered are two versions of
partisan bias; the efficiency gap and several of its variants; the mean-median
difference and the equal vote weight standard; the declination and one variant;
and the lopsided-means test. Our primary means of evaluating these measures is
a suite of hypothetical elections we classify from the start as fair or unfair.
We conclude that the declination is the most successful measure in terms of
avoiding false positives and false negatives on the elections considered. We
include in an appendix the most extreme outliers for each measure among
historical congressional and state legislative elections.",unfair terms
http://arxiv.org/abs/1806.09936v1,"Black box systems for automated decision making, often based on machine
learning over (big) data, map a user's features into a class or a score without
exposing the reasons why. This is problematic not only for lack of
transparency, but also for possible biases hidden in the algorithms, due to
human prejudices and collection artifacts hidden in the training data, which
may lead to unfair or wrong decisions. We introduce the local-to-global
framework for black box explanation, a novel approach with promising early
results, which paves the road for a wide spectrum of future developments along
three dimensions: (i) the language for expressing explanations in terms of
highly expressive logic-based rules, with a statistical and causal
interpretation; (ii) the inference of local explanations aimed at revealing the
logic of the decision adopted for a specific instance by querying and auditing
the black box in the vicinity of the target instance; (iii), the bottom-up
generalization of the many local explanations into simple global ones, with
algorithms that optimize the quality and comprehensibility of explanations.",unfair terms
http://arxiv.org/abs/1905.12535v1,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",unfair terms
http://arxiv.org/abs/1302.5491v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a study of Sri Lankan hotel websites.
A web survey and a web content analysis were conducted as the methods to elicit
data on web accessibility. Factors preventing accessibility were hypothesized
as an initial experiment. Affecting design elements are identified through web
content analysis, the results of which are utilized to develop specific
implications for improving web accessibility. The hypothesis tests show that
there is no significant correlation between accessibility and geographical or
economic factors. However, physical impairments of users have a considerable
influence on the accessibility of web page user interface if it has been
designed without full consideration of the needs of all users. Especially,
visual and mobility impaired users experience poor accessibility. Poor
readability and less navigable page designs are two observable issues, which
pose threats to accessibility. The lack of conformance to W3C accessibility
guidelines and the poor design process are the specific shortcomings which
reduce the overall accessibility. Guidelines aim to improve the accessibility
of sites with a strategic focus. Further enhancements are suggested with
adherence to principles, user centered design and developing customizable web
portals compatible for connections with differing speeds. Re-ordering search
results has been suggested as one of the finest step towards making the web
content accessible for users with differing needs. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1112.5728v1,"Web Accessibility for disabled people has posed a challenge to the civilized
societies that claim to uphold the principles of equal opportunity and
nondiscrimination. Certain concrete measures have been taken to narrow down the
digital divide between normal and disabled users of Internet technology. The
efforts have resulted in enactment of legislations and laws and mass awareness
about the discriminatory nature of the accessibility issue, besides the efforts
have resulted in the development of commensurate technological tools to develop
and test the Web accessibility. World Wide Web consortium's (W3C) Web
Accessibility Initiative (WAI) has framed a comprehensive document comprising
of set of guidelines to make the Web sites accessible to the users with
disabilities. This paper is about the issues and aspects surrounding Web
Accessibility. The details and scope are kept limited to comply with the aim of
the paper which is to create awareness and to provide basis for in-depth
investigation.",web accessibility
http://arxiv.org/abs/1405.7868v1,"Today most of the information in all areas is available over the web. It
increases the web utilization as well as attracts the interest of researchers
to improve the effectiveness of web access and web utilization. As the number
of web clients gets increased, the bandwidth sharing is performed that
decreases the web access efficiency. Web page prefetching improves the
effectiveness of web access by availing the next required web page before the
user demand. It is an intelligent predictive mining that analyze the user web
access history and predict the next page. In this work, vague improved markov
model is presented to perform the prediction. In this work, vague rules are
suggested to perform the pruning at different levels of markov model. Once the
prediction table is generated, the association mining will be implemented to
identify the most effective next page. In this paper, an integrated model is
suggested to improve the prediction accuracy and effectiveness.",web accessibility
http://arxiv.org/abs/1004.1257v1,"World Wide Web is a huge repository of web pages and links. It provides
abundance of information for the Internet users. The growth of web is
tremendous as approximately one million pages are added daily. Users' accesses
are recorded in web logs. Because of the tremendous usage of web, the web log
files are growing at a faster rate and the size is becoming huge. Web data
mining is the application of data mining techniques in web data. Web Usage
Mining applies mining techniques in log data to extract the behavior of users
which is used in various applications like personalized services, adaptive web
sites, customer profiling, prefetching, creating attractive web sites etc., Web
usage mining consists of three phases preprocessing, pattern discovery and
pattern analysis. Web log data is usually noisy and ambiguous and preprocessing
is an important process before mining. For discovering patterns sessions are to
be constructed efficiently. This paper reviews existing work done in the
preprocessing stage. A brief overview of various data mining techniques for
discovering patterns, and pattern analysis are discussed. Finally a glimpse of
various applications of web usage mining is also presented.",web accessibility
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",web accessibility
http://arxiv.org/abs/1302.5198v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a subjective study of Sri Lankan
hotel websites. A web survey and a web content analysis were conducted as the
methods to elicit data on web accessibility. Factors preventing accessibility
were hypothesized as an initial experiment. Hazardous design elements are
identified through web content analysis, the results of which are utilized to
develop specific implications for improving web accessibility. The hypothesis
tests show that there is no significant correlation between accessibility and
geographical or economic factors. However, physical impairments of users have a
considerable influence on the accessibility. Especially, visual and mobility
impaired users experience poor accessibility. Poor readability and less
navigable page designs are two observable issues, which pose threats to
accessibility. The lack of conformance to W3C accessibility guidelines and the
poor design process are the specific shortcomings which reduce the overall
accessibility. Guidelines aim to improve the accessibility of sites with a
strategic focus. Further enhancements are suggested with adherence to
principles and user centered design and developing customizable web portals
compatible for connections with differing speeds. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1105.0141v1,"Semantic Web is an open, distributed, and dynamic environment where access to
resources cannot be controlled in a safe manner unless the access decision
takes into account during discovery of web services. Security becomes the
crucial factor for the adoption of the semantic based web services. An access
control means that the users must fulfill certain conditions in order to gain
access over web services. Access control is important in both perspectives i.e.
legal and security point of view. This paper discusses important requirements
for effective access control in semantic web services which have been extracted
from the literature surveyed. I have also discussed open research issues in
this context, focusing on access control policies and models in this paper.",web accessibility
http://arxiv.org/abs/1511.04493v1,"Mobile devices that connect to the Internet via cellular networks are rapidly
becoming the primary medium for accessing Web content. Cellular service
providers (CSPs) commonly deploy Web proxies and other middleboxes for
security, performance optimization and traffic engineering reasons. However,
the prevalence and policies of these Web proxies are generally opaque to users
and difficult to measure without privileged access to devices and servers. In
this paper, we present a methodology to detect the presence of Web proxies
without requiring access to low-level packet traces on a device, nor access to
servers being contacted. We demonstrate the viability of this technique using
controlled experiments, and present the results of running our approach on
several production networks and popular Web sites. Next, we characterize the
behaviors of these Web proxies, including caching, redirecting, and content
rewriting. Our analysis can identify how Web proxies impact network
performance, and inform policies for future deployments. Last, we release an
Android app called Proxy Detector on the Google Play Store, allowing average
users with unprivileged (non-rooted) devices to understand Web proxy
deployments and contribute to our IRB-approved study. We report on results of
using this app on 11 popular carriers from the US, Canada, Austria, and China.",web accessibility
http://arxiv.org/abs/1710.07899v1,"Web portals are being considered as excellent means for conducting teaching
and learning activities electronically. The number of online services such as
course enrollment, tutoring through online course materials, evaluation and
even certification through web portals is increasing day by day. However, the
effectiveness of an educational web portal depends on its accessibility to a
wide range of students irrespective of their age, and physical abilities.
Accessibility of web portals largely depends on their userfriendliness in terms
of design, contents, assistive features, and online support. In this paper, we
have critically analyzed the web portals of thirty Indian Universities of
different categories based on the WCAG 2.0 guidelines. The purpose of this
study is to point out the deficiencies that are commonly observed in web
portals and help web designers to remove such deficiencies from the academic
web portals with a view to enhance their accessibility.",web accessibility
http://arxiv.org/abs/0907.5433v1,"World Wide Web is a huge data repository and is growing with the explosive
rate of about 1 million pages a day. As the information available on World Wide
Web is growing the usage of the web sites is also growing. Web log records each
access of the web page and number of entries in the web logs is increasing
rapidly. These web logs, when mined properly can provide useful information for
decision-making. The designer of the web site, analyst and management
executives are interested in extracting this hidden information from web logs
for decision making. Web access pattern, which is the frequently used sequence
of accesses, is one of the important information that can be mined from the web
logs. This information can be used to gather business intelligence to improve
sales and advertisement, personalization for a user, to analyze system
performance and to improve the web site organization. There exist many
techniques to mine access patterns from the web logs. This paper describes the
powerful algorithm that mines the web logs efficiently. Proposed algorithm
firstly converts the web access data available in a special doubly linked tree.
Each access is called an event. This tree keeps the critical mining related
information in very compressed form based on the frequent event count. Proposed
recursive algorithm uses this tree to efficiently find all access patterns that
satisfy user specified criteria. To prove that our algorithm is efficient from
the other GSP (Generalized Sequential Pattern) algorithms we have done
experimental studies on sample data.",web accessibility
http://arxiv.org/abs/1305.5959v2,"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.",web accessibility
http://arxiv.org/abs/1408.5460v1,"The World Wide Web is the most wide known information source that is easily
available and searchable. It consists of billions of interconnected documents
Web pages are authored by millions of people. Accesses made by various users to
pages are recorded inside web logs. These log files exist in various formats.
Because of increase in usage of web, size of web log files is increasing at a
much faster rate. Web mining is application of data mining technique to these
log files. It can be of three types Web usage mining, Web structure mining and
Web content mining. Web Usage mining is mining of usage patterns of users which
can then be used to personalize web sites and create attractive web sites. It
consists of three main phases: Preprocessing, Pattern discovery and Pattern
analysis. In this paper we focus on Data cleaning and IP Address identification
stages of preprocessing. Methodology has been proposed for both the stages. At
the end conclusion is made about number of users left after IP address
identification.",web accessibility
http://arxiv.org/abs/1204.2225v1,"This paper support the concept of a community Web directory, as a Web
directory that is constructed according to the needs and interests of
particular user communities. Furthermore, it presents the complete method for
the construction of such directories by using web usage data. User community
models take the form of thematic hierarchies and are constructed by employing
clustering approach. We applied our methodology to the ODP directory and also
to an artificial Web directory, which was generated by clustering Web pages
that appear in the access log of an Internet Service Provider. For the
discovery of the community models, we introduced a new criterion that combines
a priori thematic informativeness of the Web directory categories with the
level of interest observed in the usage data. In this context, we introduced
and evaluated new clustering method. We have tested the methodology using
access log files which are collected from the proxy servers of an Internet
Service Provider and provided results that indicates the usability of the
community Web directories. The proposed clustering methodology is evaluated
both on a specialized artificial and a community Web directory, indicating its
value to the user of the web.",web accessibility
http://arxiv.org/abs/1312.3060v1,"Expert System is developed as consulting service for users spread or public
requires affordable access. The Internet has become a medium for such services,
but presence of mobile devices make the access becomes more widespread by
utilizing mobile web and WAP (Wireless Application Protocol). Applying expert
systems applications over the web and WAP requires a knowledge base
representation that can be accessed simultaneously. This paper proposes single
database to accommodate the knowledge representation with decision tree mapping
approach. Because of the database exist, consulting application through both
web and WAP can access it to provide expert system services options for more
affordable for public.",web accessibility
http://arxiv.org/abs/1803.09585v1,"The paper presents a flexible and efficient method to secure the access to a
Web site implemented in PHP script language. The algorithm is based on the PHP
session mechanism. The proposed method is a general one and offers the
possibility to implement a PHP based secured access to a Web site, through a
portal page and using an additional script included in any site page, which is
required to be accessed only by registered users. This paper presents the
design, implementation and integration of the algorithm on any generic WEB
site.",web accessibility
http://arxiv.org/abs/1506.05628v1,"We analyze 18 million rows of Wi-Fi access logs collected over a one year
period from over 120,000 anonymized users at an inner-city shopping mall. The
anonymized dataset gathered from an opt-in system provides users' approximate
physical location, as well as Web browsing and some search history. Such data
provides a unique opportunity to analyze the interaction between people's
behavior in physical retail spaces and their Web behavior, serving as a proxy
to their information needs. We find: (1) the use of Wi-Fi network maps the
opening hours of the mall; (2) there is a weekly periodicity in users' visits
to the mall; (3) around 60% of registered Wi-Fi users actively browse the Web
and around 10% of them use Wi-Fi for accessing Web search engines; (4) people
are likely to spend a relatively constant amount of time browsing the Web while
their visiting duration may vary; (5) people tend to visit similar mall
locations and Web content during their repeated visits to the mall; (6) the
physical spatial context has a small but significant influence on the Web
content that indoor users browse; (7) accompanying users tend to access
resources from the same Web domains.",web accessibility
http://arxiv.org/abs/1208.1679v1,"Colors play a particularly important role in both designing and accessing Web
pages. A well-designed color scheme improves Web pages' visual aesthetic and
facilitates user interactions. As far as we know, existing color assessment
studies focus on images; studies on color assessment and editing for Web pages
are rare. This paper investigates color assessment for Web pages based on
existing online color theme-rating data sets and applies this assessment to Web
color edit. This study consists of three parts. First, we study the extraction
of a Web page's color theme. Second, we construct color assessment models that
score the color compatibility of a Web page by leveraging machine learning
techniques. Third, we incorporate the learned color assessment model into a new
application, namely, color transfer for Web pages. Our study combines
techniques from computer graphics, Web mining, computer vision, and machine
learning. Experimental results suggest that our constructed color assessment
models are effective, and useful in the color transfer for Web pages, which has
received little attention in both Web mining and computer graphics communities.",web accessibility
http://arxiv.org/abs/1111.2530v1,"With the rapid growth of internet technologies, Web has become a huge
repository of information and keeps growing exponentially under no editorial
control. However the human capability to read, access and understand Web
content remains constant. This motivated researchers to provide Web
personalized online services such as Web recommendations to alleviate the
information overload problem and provide tailored Web experiences to the Web
users. Recent studies show that Web usage mining has emerged as a popular
approach in providing Web personalization. However conventional Web usage based
recommender systems are limited in their ability to use the domain knowledge of
the Web application. The focus is only on Web usage data. As a consequence the
quality of the discovered patterns is low. In this paper, we propose a novel
framework integrating semantic information in the Web usage mining process.
Sequential Pattern Mining technique is applied over the semantic space to
discover the frequent sequential patterns. The frequent navigational patterns
are extracted in the form of Ontology instances instead of Web page views and
the resultant semantic patterns are used for generating Web page
recommendations to the user. Experimental results shown are promising and
proved that incorporating semantic information into Web usage mining process
can provide us with more interesting patterns which consequently make the
recommendation system more functional, smarter and comprehensive.",web accessibility
http://arxiv.org/abs/1204.5267v1,"Though World Wide Web is the single largest source of information, it is
ill-equipped to serve the people with vision related problems. With the
prolific increase in the interest to make the web accessible to all sections of
the society, solving this accessibility problem becomes mandatory. This paper
presents a technique for making web pages accessible for people with low vision
issues. A model for making web pages accessible, WILI (Web Interface for people
with Low-vision Issues) has been proposed. The approach followed in this work
is to automatically replace the existing display style of a web page with a new
skin following the guidelines given by Clear Print Booklet provided by Royal
National Institute of Blind. ""Single Click Solution"" is one of the primary
advantages provided by WILI. A prototype using the WILI model is implemented
and various experiments are conducted. The results of experiments conducted on
WILI indicate 82% effective conversion rate.",web accessibility
http://arxiv.org/abs/1309.1792v1,"The broad proliferation of mobile devices in recent years has drastically
changed the means of accessing the World Wide Web. Describing a shift away from
the desktop computer era for content consumption, predictions indicate that the
main access of web-based content will come from mobile devices. Concurrently,
the manner of content presentation has changed as well; web artifacts are
allowing for richer media and higher levels of user interaction which is
enabled through increasing access networks speeds. This article provides an
overview of more than two years of high level web page characteristics by
comparing the desktop and mobile client versions. Our study is the first
long-term evaluation of differences as seen by desktop and mobile web browser
clients. We showcase the main differentiating factors with respect to the
number of web page object requests, their sizes, relationships, and web page
object caching. We additionally highlight long-term trends and discuss their
future implications.",web accessibility
http://arxiv.org/abs/1408.2695v1,"This paper addresses web object size which is one of important performance
measures and affects to service time in multiple access environment. Since
packets arrive according to Poission distribution and web service time has
arbitrary distribution, M/G/1 model can be used to describe the behavior of the
web server system. In the time division multiplexing (TDM), we can use M/D/1
with vacations model, because service time is constant and server may have a
vacation. We derive the mean web object size satisfying the constraint such
that mean waiting time by round-robin scheduling in multiple access environment
is equal to the mean queueing delay of M/D/1 with vacations model in TDM and
M/H2/1 model, respectively. Performance evaluation shows that the mean web
object size increases as the link utilization increases at the given maximum
segment size (MSS), but converges on the lower bound when the number of
embedded objects included in a web page is beyond the threshold. Our results
can be applied to the economic design and maintenance of web service.",web accessibility
http://arxiv.org/abs/1310.2375v1,"Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.",web accessibility
http://arxiv.org/abs/1508.02127v1,"Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results.",web accessibility
http://arxiv.org/abs/1103.5046v1,"The Semantic Web initiative puts emphasis not primarily on putting data on
the Web, but rather on creating links in a way that both humans and machines
can explore the Web of data. When such users access the Web, they leave a trail
as Web servers maintain a history of requests. Web usage mining approaches have
been studied since the beginning of the Web given the log's huge potential for
purposes such as resource annotation, personalization, forecasting etc.
However, the impact of any such efforts has not really gone beyond generating
statistics detailing who, when, and how Web pages maintained by a Web server
were visited.",web accessibility
http://arxiv.org/abs/1806.00871v1,"Personal and private Web archives are proliferating due to the increase in
the tools to create them and the realization that Internet Archive and other
public Web archives are unable to capture personalized (e.g., Facebook) and
private (e.g., banking) Web pages. We introduce a framework to mitigate issues
of aggregation in private, personal, and public Web archives without
compromising potential sensitive information contained in private captures. We
amend Memento syntax and semantics to allow TimeMap enrichment to account for
additional attributes to be expressed inclusive of the requirements for
dereferencing private Web archive captures. We provide a method to involve the
user further in the negotiation of archival captures in dimensions beyond time.
We introduce a model for archival querying precedence and short-circuiting, as
needed when aggregating private and personal Web archive captures with those
from public Web archives through Memento. Negotiation of this sort is novel to
Web archiving and allows for the more seamless aggregation of various types of
Web archives to convey a more accurate picture of the past Web.",web accessibility
http://arxiv.org/abs/1103.5002v1,"The paper proposes an approach to modeling users of large Web sites based on
combining different data sources: access logs and content of the accessed pages
are combined with semantic information about the Web pages, the users and the
accesses of the users to the Web site. The assumption is that we are dealing
with a large Web site providing content to a large number of users accessing
the site. The proposed approach represents each user by a set of features
derived from the different data sources, where some feature values may be
missing for some users. It further enables user modeling based on the provided
characteristics of the targeted user subset. The approach is evaluated on
real-world data where we compare performance of the automatic assignment of a
user to a predefined user segment when different data sources are used to
represent the users.",web accessibility
http://arxiv.org/abs/1104.1892v1,"In this paper we present clustering method is very sensitive to the initial
center values, requirements on the data set too high, and cannot handle noisy
data the proposal method is using information entropy to initialize the cluster
centers and introduce weighting parameters to adjust the location of cluster
centers and noise problems.The navigation datasets which are sequential in
nature, Clustering web data is finding the groups which share common interests
and behavior by analyzing the data collected in the web servers, this improves
clustering on web data efficiently using improved fuzzy c-means(FCM)
clustering. Web usage mining is the application of data mining techniques to
web log data repositories. It is used in finding the user access patterns from
web access log. Web data Clusters are formed using on MSNBC web navigation
dataset.",web accessibility
http://arxiv.org/abs/1405.0749v1,"Web crawlers visit internet applications, collect data, and learn about new
web pages from visited pages. Web crawlers have a long and interesting history.
Early web crawlers collected statistics about the web. In addition to
collecting statistics about the web and indexing the applications for search
engines, modern crawlers can be used to perform accessibility and vulnerability
checks on the application. Quick expansion of the web, and the complexity added
to web applications have made the process of crawling a very challenging one.
Throughout the history of web crawling many researchers and industrial groups
addressed different issues and challenges that web crawlers face. Different
solutions have been proposed to reduce the time and cost of crawling.
Performing an exhaustive crawl is a challenging question. Additionally
capturing the model of a modern web application and extracting data from it
automatically is another open question. What follows is a brief history of
different technique and algorithms used from the early days of crawling up to
the recent days. We introduce criteria to evaluate the relative performance of
web crawlers. Based on these criteria we plot the evolution of web crawlers and
compare their performance",web accessibility
http://arxiv.org/abs/1105.1929v1,"The World Wide Web no longer consists just of HTML pages. Our work sheds
light on a number of trends on the Internet that go beyond simple Web pages.
The hidden Web provides a wealth of data in semi-structured form, accessible
through Web forms and Web services. These services, as well as numerous other
applications on the Web, commonly use XML, the eXtensible Markup Language. XML
has become the lingua franca of the Internet that allows customized markups to
be defined for specific domains. On top of XML, the Semantic Web grows as a
common structured data source. In this work, we first explain each of these
developments in detail. Using real-world examples from scientific domains of
great interest today, we then demonstrate how these new developments can assist
the managing, harvesting, and organization of data on the Web. On the way, we
also illustrate the current research avenues in these domains. We believe that
this effort would help bridge multiple database tracks, thereby attracting
researchers with a view to extend database technology.",web accessibility
http://arxiv.org/abs/1108.0748v2,"Web mining is the nontrivial process to discover valid, novel, potentially
useful knowledge from web data using the data mining techniques or methods. It
may give information that is useful for improving the services offered by web
portals and information access and retrieval tools. With the rapid development
of biclustering, more researchers have applied the biclustering technique to
different fields in recent years. When biclustering approach is applied to the
web usage data it automatically captures the hidden browsing patterns from it
in the form of biclusters. In this work, swarm intelligent technique is
combined with biclustering approach to propose an algorithm called Binary
Particle Swarm Optimization (BPSO) based Biclustering for Web Usage Data. The
main objective of this algorithm is to retrieve the global optimal bicluster
from the web usage data. These biclusters contain relationships between web
users and web pages which are useful for the E-Commerce applications like web
advertising and marketing. Experiments are conducted on real dataset to prove
the efficiency of the proposed algorithms.",web accessibility
http://arxiv.org/abs/1811.02293v1,"3GPP Release 15, the first 5G standard, includes protection of user identity
privacy against IMSI catchers. These protection mechanisms are based on public
key encryption. Despite this protection, IMSI catching is still possible in LTE
networks which opens the possibility of a downgrade attack on user identity
privacy, where a fake LTE base station obtains the identity of a 5G user
equipment. We propose (i) to use an existing pseudonym-based solution to
protect user identity privacy of 5G user equipment against IMSI catchers in LTE
and (ii) to include a mechanism for updating LTE pseudonyms in the public key
encryption based 5G identity privacy procedure. The latter helps to recover
from a loss of synchronization of LTE pseudonyms. Using this mechanism,
pseudonyms in the user equipment and home network are automatically
synchronized when the user equipment connects to 5G. Our mechanisms utilize
existing LTE and 3GPP Release 15 messages and require modifications only in the
user equipment and home network in order to provide identity privacy.
Additionally, lawful interception requires minor patching in the serving
network.",identity protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",identity protection
http://arxiv.org/abs/1806.05943v1,"Anonymous Identity-Based Encryption can protect privacy of the receiver.
However, there are some situations that we need to recover the identity of the
receiver, for example a dispute occurs or the privacy mechanism is abused. In
this paper, we propose a new concept, referred to as Anonymous Identity-Based
Encryption with Identity Recovery(AIBEIR), which is an anonymous IBE with
identity recovery property. There is a party called the Identity Recovery
Manager(IRM) who has a secret key to recover the identity from the ciphertext
in our scheme. We construct it with an anonymous IBE and a special IBE which we
call it testable IBE. In order to ensure the semantic security in the case
where the identity recovery manager is an adversary, we define a stronger
semantic security model in which the adversary is given the secret key of the
identity recovery manager. To our knowledge, we propose the first AIBEIR scheme
and prove the security in our defined model.",identity protection
http://arxiv.org/abs/1710.03317v1,"Research use of sensitive information -- personally identifiable information
(PII), protected health information (PHI), commercial or proprietary data, and
the like -- is increasing as researchers' skill with ""big data"" matures. Duke
University's Protected Network is an environment with technical controls in
place that provide research groups with essential pieces of security measures
needed for studies using sensitive information. The environment uses
virtualization and authorization groups extensively to isolate data, provide
elasticity of resources, and flexibly meet a range of computational
requirements within tightly controlled network boundaries. Since its beginning
in 2011, the environment has supported about 200 research projects and groups
and has served as a foundation for specialized and protected IT infrastructures
in the social sciences, population studies, and medical research. This article
lays out key features of the development of the Protected Network and outlines
the IT infrastructure design and organizational features that Duke has used in
establishing this resource for researchers. It consists of four sections: 1.
Context, 2. Infrastructure, 3. Authentication and identity management, and 4.
The infrastructure as a ""platform.""",identity protection
http://arxiv.org/abs/1208.3192v1,"One of the most important issues in peer-to-peer networks is anonymity. The
major anonymity for peer-to-peer users concerned with the users' identities and
actions which can be revealed by any other members. There are many approaches
proposed to provide anonymous peer-to-peer communications. An intruder can get
information about the content of the data, the sender's and receiver's
identities. Anonymous approaches are designed with the following three goals:
to protect the identity of provider, to protect the identity of requester and
to protect the contents of transferred data between them. This article presents
a new peer-to-peer approach to achieve anonymity between a requester and a
provider in peer-to-peer networks with trusted servers called suppernode so
that the provider will not be able to identify the requester and no other peers
can identify the two communicating parties with certainty. This article shows
that the proposed algorithm improved reliability and has more security. This
algorithm, based on onion routing and randomization, protects transferring data
against traffic analysis attack. The ultimate goal of this anonymous
communications algorithm is to allow a requester to communicate with a provider
in such a manner that nobody can determine the requester's identity and the
content of transferred data.",identity protection
http://arxiv.org/abs/1701.00436v1,"Privacy has become a serious concern for modern Information Societies. The
sensitive nature of much of the data that are daily exchanged or released to
untrusted parties requires that responsible organizations undertake appropriate
privacy protection measures. Nowadays, much of these data are texts (e.g.,
emails, messages posted in social media, healthcare outcomes, etc.) that,
because of their unstructured and semantic nature, constitute a challenge for
automatic data protection methods. In fact, textual documents are usually
protected manually, in a process known as document redaction or sanitization.
To do so, human experts identify sensitive terms (i.e., terms that may reveal
identities and/or confidential information) and protect them accordingly (e.g.,
via removal or, preferably, generalization). To relieve experts from this
burdensome task, in a previous work we introduced the theoretical basis of
C-sanitization, an inherently semantic privacy model that provides the basis to
the development of automatic document redaction/sanitization algorithms and
offers clear and a priori privacy guarantees on data protection; even though
its potential benefits C-sanitization still presents some limitations when
applied to practice (mainly regarding flexibility, efficiency and accuracy). In
this paper, we propose a new more flexible model, named (C, g(C))-sanitization,
which enables an intuitive configuration of the trade-off between the desired
level of protection (i.e., controlled information disclosure) and the
preservation of the utility of the protected data (i.e., amount of semantics to
be preserved). Moreover, we also present a set of technical solutions and
algorithms that provide an efficient and scalable implementation of the model
and improve its practical accuracy, as we also illustrate through empirical
experiments.",identity protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",identity protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",identity protection
http://arxiv.org/abs/1312.7511v1,"In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.",identity protection
http://arxiv.org/abs/1401.0092v1,"In identity management system, commonly used biometric recognition system
needs attention towards issue of biometric template protection as far as more
reliable solution is concerned. In view of this biometric template protection
algorithm should satisfy security, discriminability and cancelability. As no
single template protection method is capable of satisfying the basic
requirements, a novel technique for face template generation and protection is
proposed. The novel approach is proposed to provide security and accuracy in
new user enrollment as well as authentication process. This novel technique
takes advantage of both the hybrid approach and the binary discriminant
analysis algorithm. This algorithm is designed on the basis of random
projection, binary discriminant analysis and fuzzy commitment scheme. Three
publicly available benchmark face databases are used for evaluation. The
proposed novel technique enhances the discriminability and recognition accuracy
by 80% in terms of matching score of the face images and provides high
security.",identity protection
http://arxiv.org/abs/cs/0701144v1,"Trusted Computing is a security base technology that will perhaps be
ubiquitous in a few years in personal computers and mobile devices alike.
Despite its neutrality with respect to applications, it has raised some privacy
concerns. We show that trusted computing can be applied for service access
control in a manner protecting users' privacy. We construct a ticket system --
a concept which is at the heart of Identity Management -- relying solely on the
capabilities of the trusted platform module and the standards specified by the
Trusted Computing Group. Two examples show how it can be used for pseudonymous
and protected service access.",identity protection
http://arxiv.org/abs/1506.00950v1,"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange system has been
introduced as a simple, very low cost and efficient classical physical
alternative to quantum key distribution systems. The ideal system uses only a
few electronic components - identical resistor pairs, switches and
interconnecting wires - to guarantee perfectly protected data transmission. We
show that a generalized KLJN system can provide unconditional security even if
it is used with significantly less limitations. The more universal conditions
ease practical realizations considerably and support more robust protection
against attacks. Our theoretical results are confirmed by numerical
simulations.",identity protection
http://arxiv.org/abs/1603.00182v1,"A marketplace is defined where the private data of suppliers (e.g.,
prosumers) are protected, so that neither their identity nor their level of
stock is made known to end customers, while they can sell their products at a
reduced price. A broker acts as an intermediary, which takes care of providing
the items missing to meet the customers' demand and allows end customers to
take advantages of reduced prices through the subscription of option contracts.
Formulas are provided for the option price under three different probability
models for the availability of items. Option pricing allows the broker to
partially transfer its risk on end customers.",identity protection
http://arxiv.org/abs/1907.12221v1,"We increasingly live in a world where there is a balance between the rights
to privacy and the requirements for consent, and the rights of society to
protect itself. Within this world, there is an ever-increasing requirement to
protect the identities involved within financial transactions, but this makes
things increasingly difficult for law enforcement agencies, especially in terms
of financial fraud and money laundering. This paper reviews the
state-of-the-art in terms of the methods of privacy that are being used within
cryptocurrency transactions, and in the challenges that law enforcement face.",identity protection
http://arxiv.org/abs/1506.06996v1,"Using communication services is a common part of everyday life in a personal
or business context. Communication services include Internet services like
voice services, chat service, and web 2.0 technologies (wikis, blogs, etc), but
other usage areas like home energy management and eMobility are will be
increasingly tackled. Such communication services typically authenticate
participants. For this identities of some kind are used to identify the
communication peer to the user of a service or to the service itself. Calling
line identification used in the Session Initiation Protocol (SIP) used for
Voice over IP (VoIP) is just one example. Authentication and identification of
eCar users for accounting during charging of the eCar is another example. Also,
further mechanisms rely on identities, e.g., whitelists defining allowed
communication peers. Trusted identities prevent identity spoofing, hence are a
basic building block for the protection of communication. However, providing
trusted identities in a practical way is still a difficult problem and too
often application specific identities are used, making identity handling a
hassle. Nowadays, many countries introduced electronic identity cards, e.g.,
the German ""Elektronischer Personalausweis"" (ePA). As many German citizens will
possess an ePA soon, it can be used as security token to provide trusted
identities. Especially new usage areas (like eMobility) should from the start
be based on the ubiquitous availability of trusted identities. This paper
describes how identity cards can be integrated within three domains: home
energy management, vehicle-2-grid communication, and SIP-based voice over IP
telephony. In all three domains, identity cards are used to reliably identify
users and authenticate participants. As an example for an electronic identity
card, this paper focuses on the German ePA.",identity protection
http://arxiv.org/abs/1207.0135v1,"In this work, we focus on protection against identity disclosure in the
publication of sparse multidimensional data. Existing multidimensional
anonymization techniquesa) protect the privacy of users either by altering the
set of quasi-identifiers of the original data (e.g., by generalization or
suppression) or by adding noise (e.g., using differential privacy) and/or (b)
assume a clear distinction between sensitive and non-sensitive information and
sever the possible linkage. In many real world applications the above
techniques are not applicable. For instance, consider web search query logs.
Suppressing or generalizing anonymization methods would remove the most
valuable information in the dataset: the original query terms. Additionally,
web search query logs contain millions of query terms which cannot be
categorized as sensitive or non-sensitive since a term may be sensitive for a
user and non-sensitive for another. Motivated by this observation, we propose
an anonymization technique termed disassociation that preserves the original
terms but hides the fact that two or more different terms appear in the same
record. We protect the users' privacy by disassociating record terms that
participate in identifying combinations. This way the adversary cannot
associate with high probability a record with a rare combination of terms. To
the best of our knowledge, our proposal is the first to employ such a technique
to provide protection against identity disclosure. We propose an anonymization
algorithm based on our approach and evaluate its performance on real and
synthetic datasets, comparing it against other state-of-the-art methods based
on generalization and differential privacy.",identity protection
http://arxiv.org/abs/1907.03710v1,"Data exfiltration attacks have led to huge data breaches. Recently, the
Equifax attack affected 147M users and a third-party library - Apache Struts -
was alleged to be responsible for it. These attacks often exploit the fact that
sensitive data are stored unencrypted in process memory and can be accessed by
any function executing within the same process, including untrusted third-party
library functions. This paper presents StackVault, a kernel-based system to
prevent sensitive stack-based data from being accessed in an unauthorized
manner by intra-process functions. Stack-based data includes data on stack as
well as data pointed to by pointer variables on stack. StackVault consists of
three components: (1) a set of programming APIs to allow users to specify which
data needs to be protected, (2) a kernel module which uses unforgeable function
identities to reliably carry out the sensitive data protection, and (3) an LLVM
compiler extension that enables transparent placement of stack protection
operations. The StackVault system automatically enforces stack protection
through spatial and temporal access monitoring and control over both sensitive
stack data and untrusted functions. We implemented StackVault and evaluated it
using a number of popular real-world applications, including gRPC. The results
show that StackVault is effective and efficient, incurring only up to 2.4%
runtime overhead.",identity protection
http://arxiv.org/abs/1806.04410v1,"A legally valid identification document allows impartial arbitration of the
identification of individuals. It protects individuals from a violation of
their dignity, justice, liberty and equality. It protects the nation from a
destruction of its republic, democratic, sovereign status. In order to test the
ability of an identification document to establish impartial identification of
individuals, it must be evaluated for its ability to establish identity,
undertake identification and build confidence to impartial, reliable and valid
identification. The processes of issuing, using and validating identification
documents alter the ability of the document to establish identity, undertake
identification and build confidence to impartial and valid identification.
These processes alter the ability of the document to serve as proof of
identity, proof of address, proof of being a resident, or even the proof of
existence of a person. We examine the ability of the UID number to serve as an
identification document with the ability to impartially arbitrate the
identification of individuals and serve as proof of identity, address, and
demonstrate existence of a person. We evaluate the implications of the
continued use UID system on our ability to undertake legally valid
identification ensure integrity of the identity and address databases across
the world.",identity protection
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",web beacon
http://arxiv.org/abs/1507.03509v1,"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger.""",web beacon
http://arxiv.org/abs/cs/0201003v1,"Random beacons-information sources that broadcast a stream of random digits
unknown by anyone beforehand-are useful for various cryptographic purposes. But
such beacons can be easily and undetectably sabotaged, so that their output is
known beforehand by a dishonest party, who can use this information to defeat
the cryptographic protocols supposedly protected by the beacon. We explore a
strategy to reduce this hazard by combining the outputs from several
noninteracting (eg spacelike-separated) beacons by XORing them together to
produce a single digit stream which is more trustworthy than any individual
beacon, being random and unpredictable if at least one of the contributing
beacons is honest. If the contributing beacons are not spacelike separated, so
that a dishonest beacon can overhear and adapt to earlier outputs of other
beacons, the beacons' trustworthiness can still be enhanced to a lesser extent
by a time sharing strategy. We point out some disadvantages of alternative
trust amplification methods based on one-way hash functions.",web beacon
http://arxiv.org/abs/1605.07329v1,"Vehicular communication requires vehicles to self-organize through the
exchange of periodic beacons. Recent analysis on beaconing indicates that the
standards for beaconing restrict the desired performance of vehicular
applications. This situation can be attributed to the quality of the available
transmission medium, persistent change in the traffic situation and the
inability of standards to cope with application requirements. To this end, this
paper is motivated by the classifications and capability evaluations of
existing adaptive beaconing approaches. To begin with, we explore the anatomy
and the performance requirements of beaconing. Then, the beaconing design is
analyzed to introduce a design-based beaconing taxonomy. A survey of the
state-of-the-art is conducted with an emphasis on the salient features of the
beaconing approaches. We also evaluate the capabilities of beaconing approaches
using several key parameters. A comparison among beaconing approaches is
presented, which is based on the architectural and implementation
characteristics. The paper concludes by discussing open challenges in the
field.",web beacon
http://arxiv.org/abs/1505.05106v1,"We establish tight bounds for beacon-based coverage problems, and improve the
bounds for beacon-based routing problems in simple rectilinear polygons.
Specifically, we show that $\lfloor \frac{n}{6} \rfloor$ beacons are always
sufficient and sometimes necessary to cover a simple rectilinear polygon $P$
with $n$ vertices. We also prove tight bounds for the case where $P$ is
monotone, and we present an optimal linear-time algorithm that computes the
beacon-based kernel of $P$. For the routing problem, we show that $\lfloor
\frac{3n-4}{8} \rfloor - 1$ beacons are always sufficient, and $\lceil
\frac{n}{4}\rceil-1$ beacons are sometimes necessary to route between all pairs
of points in $P$.",web beacon
http://arxiv.org/abs/1712.07416v1,"A beacon is a point-like object which can be enabled to exert a magnetic pull
on other point-like objects in space. Those objects then move towards the
beacon in a greedy fashion until they are either stuck at an obstacle or reach
the beacon's location. Beacons placed inside polyhedra can be used to route
point-like objects from one location to another. A second use case is to cover
a polyhedron such that every point-like object at an arbitrary location in the
polyhedron can reach at least one of the beacons once the latter is activated.
  The notion of beacon-based routing and guarding was introduced by Biro et al.
[FWCG'11] in 2011 and covered in detail by Biro in his PhD thesis [SUNY-SB'13],
which focuses on the two-dimensional case.
  We extend Biro's result to three dimensions by considering beacon routing in
polyhedra. We show that $\lfloor\frac{m+1}{3}\rfloor$ beacons are always
sufficient and sometimes necessary to route between any pair of points in a
given polyhedron $P$, where $m$ is the number of tetrahedra in a tetrahedral
decomposition of $P$. This is one of the first results that show that beacon
routing is also possible in three dimensions.",web beacon
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",web beacon
http://arxiv.org/abs/1802.05735v1,"Traditionally, there have been few options for navigational aids for the
blind and visually impaired (BVI) in large indoor spaces. Some recent indoor
navigation systems allow users equipped with smartphones to interact with low
cost Bluetoothbased beacons deployed strategically within the indoor space of
interest to navigate their surroundings. A major challenge in deploying such
beacon-based navigation systems is the need to employ a time and
labor-expensive beacon planning process to identify potential beacon placement
locations and arrive at a topological structure representing the indoor space.
This work presents a technique called IBeaconMap for creating such topological
structures to use with beacon-based navigation that only needs the floor plans
of the indoor spaces of interest. IBeaconMap employs a combination of computer
vision and machine learning techniques to arrive at the required set of beacon
locations and a weighted connectivity graph (with directional orientations) for
subsequent navigational needs. Evaluations show IBeaconMap to be both fast and
reasonably accurate, potentially proving to be an essential tool to be utilized
before mass deployments of beacon-based indoor wayfinding systems of the
future.",web beacon
http://arxiv.org/abs/1503.08404v1,"Beacon node placement, node-to-node measurement, and target node positioning
are the three key steps for a localization process. However, compared with the
other two steps, beacon node placement still lacks a comprehensive, systematic
study in research literatures. To fill this gap, we address the Beacon Node
Placment (BNP) problem that deploys beacon nodes for minimal localization error
in this paper. BNP is difficult in that the localization error is determined by
a complicated combination of factors, i.e., the localization error differing
greatly under a different environment, with a different algorithm applied, or
with a different type of beacon node used. In view of the hardness of BNP, we
propose an approximate function to reduce time cost in localization error
calculation, and also prove its time complexity and error bound. By
approximation, a sub-optimal distribution of beacon nodes could be found within
acceptable time cost for placement. In the experiment, we test our method and
compare it with other node placement methods under various settings and
environments. The experimental results show feasibility and effectiveness of
our method in practice.",web beacon
http://arxiv.org/abs/1709.10237v1,"Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.",web beacon
http://arxiv.org/abs/0810.3966v3,"What would SETI Beacon transmitters be like if built by civilizations with a
variety of motivations, but who cared about cost? We studied in a companion
paper how, for fixed power density in the far field, we could build a
cost-optimum interstellar Beacon system. Here we consider, if someone like us
were to produce a Beacon, how should we look for it? High-power transmitters
might be built for wide variety of motives other than twoway communication;
Beacons built to be seen over thousands of light years are such. Altruistic
Beacon builders will have to contend with other altruistic causes, just as
humans do, so may select for economy of effort. Cost, spectral lines near 1 GHz
and interstellar scintillation favor radiating frequencies substantially above
the classic water hole. Therefore the transmission strategy for a distant,
cost-conscious Beacon will be a rapid scan of the galactic plane, to cover the
angular space. Such pulses will be infrequent events for the receiver. Such
Beacons built by distant advanced, wealthy societies will have very different
characteristics from what SETI researchers seek. Future searches should pay
special attention to areas along the galactic disk where SETI searches have
seen coherent signals that have not recurred on the limited listening time
intervals we have used. We will need to wait for recurring events that may
arrive in intermittent bursts. Several new SETI search strategies emerge from
these ideas. We propose a new test for SETI Beacons, based on the Life Plane
hypotheses.",web beacon
http://arxiv.org/abs/1803.05946v1,"The beacon model is a recent paradigm for guiding the trajectory of messages
or small robotic agents in complex environments. A beacon is a fixed point with
an attraction pull that can move points within a given polygon. Points move
greedily towards a beacon: if unobstructed, they move along a straight line to
the beacon, and otherwise they slide on the edges of the polygon. The Euclidean
distance from a moving point to a beacon is monotonically decreasing. A given
beacon attracts a point if the point eventually reaches the beacon.
  The problem of attracting all points within a polygon with a set of beacons
can be viewed as a variation of the art gallery problem. Unlike most
variations, the beacon attraction has the intriguing property of being
asymmetric, leading to separate definitions of attraction region and inverse
attraction region. The attraction region of a beacon is the set of points that
it attracts. It is connected and can be computed in linear time for simple
polygons. By contrast, it is known that the inverse attraction region of a
point---the set of beacon positions that attract it---could have $\Omega(n)$
disjoint connected components.
  In this paper, we prove that, in spite of this, the total complexity of the
inverse attraction region of a point in a simple polygon is linear, and present
a $O(n \log n)$ time algorithm to construct it. This improves upon the best
previous algorithm which required $O(n^3)$ time and $O(n^2)$ space. Furthermore
we prove a matching $\Omega(n\log n)$ lower bound for this task in the
algebraic computation tree model of computation, even if the polygon is
monotone.",web beacon
http://arxiv.org/abs/1504.07192v1,"This work presents a mobile sign-on scheme, which utilizes Bluetooth Low
Energy beacons for location awareness and Attribute-Based Encryption for
expressive, broadcast-style key exchange. Bluetooth Low Energy beacons
broadcast encrypted messages with encoded access policies. Within range of the
beacons, a user with appropriate attributes is able to decrypt the broadcast
message and obtain parameters that allow the user to perform a short or
simplified login. The effect is a ""traveling"" sign-on that accompanies the user
throughout different locations.",web beacon
http://arxiv.org/abs/0810.3964v2,"This paper considers galactic scale Beacons from the point of view of expense
to a builder on Earth. For fixed power density in the far field, what is the
cost-optimum interstellar Beacon system? Experience shows an optimum tradeoff,
depending on transmission frequency and on antenna size and power. This emerges
by minimizing the cost of producing a desired effective isotropic radiated
power, which in turn determines the maximum range of detectability of a
transmitted signal. We derive general relations for cost-optimal aperture and
power. For linear dependence of capital cost on transmitter power and antenna
area, minimum capital cost occurs when the cost is equally divided between
antenna gain and radiated power. For non-linear power law dependence a similar
simple division occurs. This is validated in cost data for many systems;
industry uses this cost optimum as a rule-of-thumb. Costs of pulsed
cost-efficient transmitters are estimated from these relations using current
cost parameters ($/W, $/m2) as a basis. Galactic-scale Beacons demand effective
isotropic radiated power >1017 W, emitted powers are >1 GW, with antenna areas
> km2. We show the scaling and give examples of such Beacons. Thrifty beacon
systems would be large and costly, have narrow searchlight beams and short
dwell times when the Beacon would be seen by an alien oberver at target areas
in the sky. They may revisit an area infrequently and will likely transmit at
higher microwave frequencies, ~10 GHz. The natural corridor to broadcast is
along the galactic spiral radius or along the spiral galactic arm we are in.
Our second paper argues that nearly all SETI searches to date had little chance
of seeing such Beacons.",web beacon
http://arxiv.org/abs/1407.6965v3,"Cooperative inter-vehicular applications rely on the exchange of broadcast
single-hop status messages among vehicles, called beacons. The aggregated load
on the wireless channel due to periodic beacons can prevent the transmission of
other types of messages, what is called channel congestion due to beaconing
activity. In this paper we approach the problem of controlling the beaconing
rate on each vehicle by modeling it as a Network Utility Maximization (NUM)
problem. This allows us to formally apply the notion of fairness of a beaconing
rate allocation in vehicular networks and to control the trade-off between
efficiency and fairness. The NUM methodology provides a rigorous framework to
design a broad family of simple and decentralized algorithms, with proved
convergence guarantees to a fair allocation solution. In this context, we focus
exclusively in beaconing rate control and propose the Fair Adaptive Beaconing
Rate for Intervehicular Communications (FABRIC) algorithm, which uses a
particular scaled gradient projection algorithm to solve the dual of the NUM
problem. The desired fairness notion in the allocation can be established with
an algorithm parameter. Simulation results validate our approach and show that
FABRIC converges to fair rate allocations in multi-hop and dynamic scenarios.",web beacon
http://arxiv.org/abs/1805.04548v1,"The DFINITY blockchain computer provides a secure, performant and flexible
consensus mechanism. At its core, DFINITY contains a decentralized randomness
beacon which acts as a verifiable random function (VRF) that produces a stream
of outputs over time. The novel technique behind the beacon relies on the
existence of a unique-deterministic, non-interactive, DKG-friendly threshold
signatures scheme. The only known examples of such a scheme are pairing-based
and derived from BLS.
  The DFINITY blockchain is layered on top of the DFINITY beacon and uses the
beacon as its source of randomness for leader selection and leader ranking. A
""weight"" is attributed to a chain based on the ranks of the leaders who propose
the blocks in the chain, and that weight is used to select between competing
chains. The DFINITY blockchain is layered on top of the DFINITY beacon and uses
the beacon as its source of randomness for leader selection and leader ranking
blockchain is further hardened by a notarization process which dramatically
improves the time to finality and eliminates the nothing-at-stake and selfish
mining attacks.
  DFINITY consensus algorithm is made to scale through continuous quorum
selections driven by the random beacon. In practice, DFINITY achieves block
times of a few seconds and transaction finality after only two confirmations.
The system gracefully handles temporary losses of network synchrony including
network splits, while it is provably secure under synchrony.",web beacon
http://arxiv.org/abs/1208.2403v1,"IEEE 802.15.4 standard is designed for low power and low data rate
applications with high reliability. It operates in beacon enable and non-beacon
enable modes. In this work, we analyze delay, throughput, load, and end-to-end
delay of nonbeacon enable mode. Analysis of these parameters are performed at
varying data rates. Evaluation of non beacon enabled mode is done in a 10 node
network. We limit our analysis to non beacon or unslotted version because, it
performs better than other. Protocol performance is examined by changing
different Medium Access Control (MAC) parameters. We consider a full size MAC
packet with payload size of 114 bytes. In this paper we show that maximum
throughput and lowest delay is achieved at highest data rate.",web beacon
http://arxiv.org/abs/1702.05116v2,"Cyclic pursuit frameworks, which are built upon pursuit interactions between
neighboring agents in a cycle graph, provide an efficient way to create useful
global behaviors in a collective of autonomous robots. Previous work had
considered cyclic pursuit with a constant bearing (CB) pursuit law, and
demonstrated the existence of circling equilibria for the corresponding
dynamics. In this work, we propose a beacon-referenced version of the CB
pursuit law, wherein a stationary beacon provides an additional reference for
the individual agents in a collective. When implemented in a cyclic framework,
we show that the resulting dynamics admit relative equilibria corresponding to
a circling orbit around the beacon, with the circling radius and the
distribution of agents along the orbit determined by parameters of the proposed
pursuit law. We also derive necessary conditions for stability of the circling
equilibria, which provides a guide for parameter selection. Finally, by
introducing a change of variables, we demonstrate the existence of a family of
invariant manifolds related to spiraling motions around the beacon which
preserve the ""pure shape"" of the collective, and study the reduced dynamics on
a representative manifold.",web beacon
http://arxiv.org/abs/1703.08612v2,"The ability of robots to estimate their location is crucial for a wide
variety of autonomous operations. In settings where GPS is unavailable,
measurements of transmissions from fixed beacons provide an effective means of
estimating a robot's location as it navigates. The accuracy of such a
beacon-based localization system depends both on how beacons are distributed in
the environment, and how the robot's location is inferred based on noisy and
potentially ambiguous measurements. We propose an approach for making these
design decisions automatically and without expert supervision, by explicitly
searching for the placement and inference strategies that, together, are
optimal for a given environment. Since this search is computationally
expensive, our approach encodes beacon placement as a differential neural layer
that interfaces with a neural network for inference. This formulation allows us
to employ standard techniques for training neural networks to carry out the
joint optimization. We evaluate this approach on a variety of environments and
settings, and find that it is able to discover designs that enable high
localization accuracy.",web beacon
http://arxiv.org/abs/1503.03388v1,"This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.",web beacon
http://arxiv.org/abs/1212.2404v1,"Vehicular Ad-Hoc Networks (VANETs) are special forms of Mobile Ad-Hoc
Networks (MANETs) that allows vehicles to communicate together in the absence
of fixed infrastructure.In this type of network beaconing is the means used to
discover the nodes in its eighborhood.For routing protocol successful delivery
of beacons containing speed, direction and position of a car is extremely
important.Otherwise, routing information should not be modified/manipulated
during transmission without detection, in order to ensure the routing
information, messages must be signed and provided with a certificate to attest
valid network participants. In this work we present a beaconing protocol with
key exchange to prepare the generation of a signature to protect the routing
information protocol 'Greedy Perimeter Stateless Routing'.",web beacon
http://arxiv.org/abs/1703.04150v1,"Location sensing is a key enabling technology for Ubicomp to support
contextual interaction. However, the laboratories where calibrated testing of
location technologies is done are very different to the domestic situations
where `context' is a problematic social construct. This study reports
measurements of Bluetooth beacons, informed by laboratory studies, but done in
diverse domestic settings. The design of these surveys has been motivated by
the natural environment implied in the Bluetooth beacon standards - relating
the technical environment of the beacon to the function of spaces within the
home. This research method can be considered as a situated, `ethnographic'
technical response to the study of physical infrastructure that arises through
social processes. The results offer insights for the future design of `seamful'
approaches to indoor location sensing, and to the ways that context might be
constructed and interpreted in a seamful manner.",web beacon
http://arxiv.org/abs/1806.02325v1,"We consider a sensing application where the sensor nodes are wirelessly
powered by an energy beacon. We focus on the problem of jointly optimizing the
energy allocation of the energy beacon to different sensors and the data
transmission powers of the sensors in order to minimize the field
reconstruction error at the sink. In contrast to the standard ideal linear
energy harvesting (EH) model, we consider practical non-linear EH models. We
investigate this problem under two different frameworks: i) an optimization
approach where the energy beacon knows the utility function of the nodes,
channel state information and the energy harvesting characteristics of the
devices; hence optimal power allocation strategies can be designed using an
optimization problem and ii) a learning approach where the energy beacon
decides on its strategies adaptively with battery level information and
feedback on the utility function. Our results illustrate that deep
reinforcement learning approach can obtain the same error levels with the
optimization approach and provides a promising alternative to the optimization
framework.",web beacon
http://arxiv.org/abs/1812.02349v2,"An ultrasonic Positioning System (UPS) has outperformed RF-based systems in
terms of its accuracy for years. However, few of the developed solutions have
been deployed in practice to satisfy the localization demand of today's smart
devices, which lack ultrasonic sensors and were considered as being `deaf' to
ultrasound. A recent finding demonstrates that ultrasound may be audible to the
smart devices under certain conditions due to their microphone's nonlinearity.
Inspired by this insight, this work revisits the ultrasonic positioning
technique and builds a practical UPS, called UPS+ for ultrasound-incapable
smart devices. The core concept is to deploy two types of indoor beacon
devices, which will advertise ultrasonic beacons at two different ultrasonic
frequencies respectively. Their superimposed beacons are shifted to a
low-frequency by virtue of the nonlinearity effect at the receiver's
microphone. This underlying property functions as an implicit ultrasonic
downconverter without throwing harm to the hearing system of humans. We
demonstrate UPS+, a fully functional UPS prototype, with centimeter-level
localization accuracy using custom-made beacon hardware and well-designed
algorithms.",web beacon
http://arxiv.org/abs/1807.07468v1,"A text mining approach is proposed based on latent Dirichlet allocation (LDA)
to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints.
The proposed approach aims to extract latent topics in the CFPB complaint
narratives, and explores their associated trends over time. The time trends
will then be used to evaluate the effectiveness of the CFPB regulations and
expectations on financial institutions in creating a consumer oriented culture
that treats consumers fairly and prioritizes consumer protection in their
decision making processes. The proposed approach can be easily operationalized
as a decision support system to automate detection of emerging topics in
consumer complaints. Hence, the technology-human partnership between the
proposed approach and the CFPB team could certainly improve consumer
protections from unfair, deceptive or abusive practices in the financial
markets by providing more efficient and effective investigations of consumer
complaint narratives.",consumer protection
http://arxiv.org/abs/1907.11717v1,"The benefits of the ubiquitous caching in ICN are profound, such features
make ICN promising for content distribution, but it also introduces a challenge
to content protection against the unauthorized access. The protection of a
content against unauthorized access requires consumer authentication and
involves the conventional end-to-end encryption. However, in
information-centric networking (ICN), such end-to-end encryption makes the
content caching ineffective since encrypted contents stored in a cache are
useless for any consumers except those who know the encryption key. For
effective caching of encrypted contents in ICN, we propose a secure
distribution of protected content (SDPC) scheme, which ensures that only
authenticated consumers can access the content. SDPC is lightweight and allows
consumers to verify the originality of the published content by using a
symmetric key encryption. SDPC also provides protection against privacy
leakage. The security of SDPC was proved with the BAN logic and Scyther tool
verification, and simulation results show that SDPC can reduce the content
download delay.",consumer protection
http://arxiv.org/abs/1705.06809v1,"The growing market for smart home IoT devices promises new conveniences for
consumers while presenting novel challenges for preserving privacy within the
home. Specifically, Internet service providers or neighborhood WiFi
eavesdroppers can measure Internet traffic rates from smart home devices and
infer consumers' private in-home behaviors. Here we propose four strategies
that device manufacturers and third parties can take to protect consumers from
side-channel traffic rate privacy threats: 1) blocking traffic, 2) concealing
DNS, 3) tunneling traffic, and 4) shaping and injecting traffic. We hope that
these strategies, and the implementation nuances we discuss, will provide a
foundation for the future development of privacy-sensitive smart homes.",consumer protection
http://arxiv.org/abs/1703.00518v1,"Consumer protection agencies are charged with safeguarding the public from
hazardous products, but the thousands of products under their jurisdiction make
it challenging to identify and respond to consumer complaints quickly. From the
consumer's perspective, online reviews can provide evidence of product defects,
but manually sifting through hundreds of reviews is not always feasible. In
this paper, we propose a system to mine Amazon.com reviews to identify products
that may pose safety or health hazards. Since labeled data for this task are
scarce, our approach combines positive unlabeled learning with domain
adaptation to train a classifier from consumer complaints submitted to the U.S.
Consumer Product Safety Commission. On a validation set of manually annotated
Amazon product reviews, we find that our approach results in an absolute F1
score improvement of 8% over the best competing baseline. Furthermore, we apply
the classifier to Amazon reviews of known recalled products; the classifier
identifies reviews reporting safety hazards prior to the recall date for 45% of
the products. This suggests that the system may be able to provide an early
warning system to alert consumers to hazardous products before an official
recall is announced.",consumer protection
http://arxiv.org/abs/1405.3342v1,"In the event that a bacteriological or chemical toxin is intro- duced to a
water distribution network, a large population of consumers may become exposed
to the contaminant. A contamination event may be poorly predictable dynamic
process due to the interactions of consumers and utility managers during an
event. Consumers that become aware of a threat may select protective actions
that change their water demands from typical demand patterns, and new hydraulic
conditions can arise that differ from conditions that are predicted when
demands are considered as exogenous inputs. Consequently, the movement of the
contaminant plume in the pipe network may shift from its expected trajectory. A
sociotechnical model is developed here to integrate agent-based models of
consumers with an engineering water distribution system model and capture the
dynamics between consumer behaviors and the water distribution system for
predicting contaminant transport and public exposure. Consumers are simulated
as agents with behaviors defined for water use activities, mobility,
word-of-mouth communication, and demand reduction, based on a set of rules
representing an agents autonomy and reaction to health impacts, the
environment, and the actions of other agents. As consumers decrease their water
use, the demand exerted on the water distribution system is updated; as the
flow directions and volumes shift in response, the location of the contaminant
plume is updated and the amount of contaminant consumed by each agent is
calculated. The framework is tested through simulating realistic contamination
scenarios for a virtual city and water distribution system.",consumer protection
http://arxiv.org/abs/1808.03289v1,"The secure distribution of protected content requires consumer authentication
and involves the conventional method of end-to-end encryption. However, in
information-centric networking (ICN) the end-to-end encryption makes the
content caching ineffective since encrypted content stored in a cache is
useless for any consumer except those who know the encryption key. For
effective caching of encrypted content in ICN, we propose a novel scheme,
called the Secure Distribution of Protected Content (SDPC). SDPC ensures that
only authenticated consumers can access the content. The SDPC is a lightweight
authentication and key distribution protocol; it allows consumer nodes to
verify the originality of the published article by using a symmetric key
encryption. The security of the SDPC was proved with BAN logic and Scyther tool
verification.",consumer protection
http://arxiv.org/abs/1711.07220v1,"The AN.ON-Next project aims to integrate privacy-enhancing technologies into
the internet's infrastructure and establish them in the consumer mass market.
  The technologies in focus include a basis protection at internet service
provider level, an improved overlay network-based protection and a concept for
privacy protection in the emerging 5G mobile network. A crucial success factor
will be the viable adjustment and development of standards, business models and
pricing strategies for those new technologies.",consumer protection
http://arxiv.org/abs/1806.08274v1,"Pre-configured cycle (p-Cycle) method has been studied in literature
extensively for optical network protection. A large p-cycle has high capacity
efficiency and can protect a large number of nodes against the single link
failure scenarios. All the links protected by such a p-cycle lose protection
when the p-cycle is consumed to restore traffic after a failure. As the
probability of multiple link failure is high for a large network, it also means
that with higher probability, on the second failure, protection may not be
there for the failed link. Thus, if the number of links protected by a p-cycle
is large, it makes the network unprotected with high probability on the advent
of the second failure. In this paper, we study the impact zone due to a first
link failure in the various configurations of the p-cycles. The study gives
insight into how to choose the p-cycle configuration to reduce the impact zone
while using minimum spare capacity. We propose few methods and compare them to
show how the impact zone analysis can be used to improve the fault tolerance in
an optical network.",consumer protection
http://arxiv.org/abs/cs/9908012v1,"E-business, information serving, and ubiquitous computing will create heavy
request traffic from strangers or even incognitos. Such requests must be
managed automatically. Two ways of doing this are well known: giving every
incognito consumer the same treatment, and rendering service in return for
money. However, different behavior will be often wanted, e.g., for a university
library with different access policies for undergraduates, graduate students,
faculty, alumni, citizens of the same state, and everyone else.
  For a data or process server contacted by client machines on behalf of users
not previously known, we show how to provide reliable automatic access
administration conforming to service agreements. Implementations scale well
from very small collections of consumers and producers to immense client/server
networks. Servers can deliver information, effect state changes, and control
external equipment.
  Consumer privacy is easily addressed by the same protocol. We support
consumer privacy, but allow servers to deny their resources to incognitos. A
protocol variant even protects against statistical attacks by consortia of
service organizations.
  One e-commerce application would put the consumer's tokens on a smart card
whose readers are in vending kiosks. In e-business we can simplify supply chain
administration. Our method can also be used in sensitive networks without
introducing new security loopholes.",consumer protection
http://arxiv.org/abs/1805.02722v1,"Data encryption is the primary method of protecting the privacy of consumer
device Internet communications from network observers. The ability to
automatically detect unencrypted data in network traffic is therefore an
essential tool for auditing Internet-connected devices. Existing methods
identify network packets containing cleartext but cannot differentiate packets
containing encrypted data from packets containing compressed unencrypted data,
which can be easily recovered by reversing the compression algorithm. This
makes it difficult for consumer protection advocates to identify devices that
risk user privacy by sending sensitive data in a compressed unencrypted format.
Here, we present the first technique to automatically distinguish encrypted
from compressed unencrypted network transmissions on a per-packet basis. We
apply three machine learning models and achieve a maximum 66.9% accuracy with a
convolutional neural network trained on raw packet data. This result is a
baseline for this previously unstudied machine learning problem, which we hope
will motivate further attention and accuracy improvements. To facilitate
continuing research on this topic, we have made our training and test datasets
available to the public.",consumer protection
http://arxiv.org/abs/1902.08712v1,"Resource allocation is the process of optimizing the rare resources. In the
area of security, how to allocate limited resources to protect a massive number
of targets is especially challenging. This paper addresses this resource
allocation issue by constructing a game theoretic model. A defender and an
attacker are players and the interaction is formulated as a trade-off between
protecting targets and consuming resources. The action cost which is a
necessary role of consuming resource, is considered in the proposed model.
Additionally, a bounded rational behavior model (Quantal Response, QR), which
simulates a human attacker of the adversarial nature, is introduced to improve
the proposed model. To validate the proposed model, we compare the different
utility functions and resource allocation strategies. The comparison results
suggest that the proposed resource allocation strategy performs better than
others in the perspective of utility and resource effectiveness.",consumer protection
http://arxiv.org/abs/1612.05120v4,"The roll-out of smart meters in electricity networks introduces risks for
consumer privacy due to increased measurement frequency and granularity.
Through various Non-Intrusive Load Monitoring techniques, consumer behavior may
be inferred from their metering data. In this paper, we propose an energy
management method that reduces energy cost and protects privacy through the
minimization of information leakage. The method is based on a Model Predictive
Controller that utilizes energy storage and local generation, and that predicts
the effects of its actions on the statistics of the actual energy consumption
of a consumer and that seen by the grid. Computationally, the method requires
solving a Mixed-Integer Quadratic Program of manageable size whenever new meter
readings are available. We simulate the controller on generated residential
load profiles with different privacy costs in a two-tier time-of-use energy
pricing environment. Results show that information leakage is effectively
reduced at the expense of increased energy cost. The results also show that
with the proposed controller the consumer load profile seen by the grid
resembles a mixture between that obtained with Non-Intrusive Load Leveling and
Lazy Stepping.",consumer protection
http://arxiv.org/abs/1708.02629v1,"In the post-genomic era, large-scale personal DNA sequences are produced and
collected for genetic medical diagnoses and new drug discovery, which, however,
simultaneously poses serious challenges to the protection of personal genomic
privacy. Existing genomic privacy-protection methods are either time-consuming
or with low accuracy. To tackle these problems, this paper proposes a sequence
similarity-based obfuscation method, namely IterMegaBLAST, for fast and
reliable protection of personal genomic privacy. Specifically, given a randomly
selected sequence from a dataset of DNA sequences, we first use MegaBLAST to
find its most similar sequence from the dataset. These two aligned sequences
form a cluster, for which an obfuscated sequence was generated via a DNA
generalization lattice scheme. These procedures are iteratively performed until
all of the sequences in the dataset are clustered and their obfuscated
sequences are generated. Experimental results on two benchmark datasets
demonstrate that under the same degree of anonymity, IterMegaBLAST
significantly outperforms existing state-of-the-art approaches in terms of both
utility accuracy and time complexity.",consumer protection
http://arxiv.org/abs/1310.1551v1,"Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.",consumer protection
http://arxiv.org/abs/1709.09614v1,"Power grids are undergoing major changes due to rapid growth in renewable
energy resources and improvements in battery technology. While these changes
enhance sustainability and efficiency, they also create significant management
challenges as the complexity of power systems increases. To tackle these
challenges, decentralized Internet-of-Things (IoT) solutions are emerging,
which arrange local communities into transactive microgrids. Within a
transactive microgrid, ""prosumers"" (i.e., consumers with energy generation and
storage capabilities) can trade energy with each other, thereby smoothing the
load on the main grid using local supply. It is hard, however, to provide
security, safety, and privacy in a decentralized and transactive energy system.
On the one hand, prosumers' personal information must be protected from their
trade partners and the system operator. On the other hand, the system must be
protected from careless or malicious trading, which could destabilize the
entire grid. This paper describes Privacy-preserving Energy Transactions
(PETra), which is a secure and safe solution for transactive microgrids that
enables consumers to trade energy without sacrificing their privacy. PETra
builds on distributed ledgers, such as blockchains, and provides anonymity for
communication, bidding, and trading.",consumer protection
http://arxiv.org/abs/1803.10099v1,"Ad targeting is getting more powerful with introduction of new tools, such as
Custom Audiences, behavioral targeting, and Audience Insights. Although this is
beneficial for businesses as it enables people to receive more relevant
advertising, the power of the tools has downsides. In this paper, we focus on
three downsides: privacy violations, microtargeting (i.e., the ability to reach
a specific individual or individuals without their explicit knowledge that they
are the only ones an ad reaches) and ease of reaching marginalized groups.
Using Facebook's ad system as a case study, we demonstrate the feasibility of
such downsides. We then discuss Facebook's response to our responsible
disclosures of the findings and call for additional policy, science, and
engineering work to protect consumers in the rapidly evolving ecosystem of ad
targeting.",consumer protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",consumer protection
http://arxiv.org/abs/1601.06372v1,"Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.",consumer protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",consumer protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",consumer protection
http://arxiv.org/abs/1901.03603v1,"Billions of users rely on the security of the Android platform to protect
phones, tablets, and many different types of consumer electronics. While
Android's permission model is well studied, the enforcement of the protection
policy has received relatively little attention. Much of this enforcement is
spread across system services, taking the form of hard-coded checks within
their implementations. In this paper, we propose Authorization Check Miner
(ACMiner), a framework for evaluating the correctness of Android's access
control enforcement through consistency analysis of authorization checks.
ACMiner combines program and text analysis techniques to generate a rich set of
authorization checks, mines the corresponding protection policy for each
service entry point, and uses association rule mining at a service granularity
to identify inconsistencies that may correspond to vulnerabilities. We used
ACMiner to study the AOSP version of Android 7.1.1 to identify 28
vulnerabilities relating to missing authorization checks. In doing so, we
demonstrate ACMiner's ability to help domain experts process thousands of
authorization checks scattered across millions of lines of code.",consumer protection
http://arxiv.org/abs/0712.2587v1,"The code that combines channel estimation and error protection has received
general attention recently, and has been considered a promising methodology to
compensate multi-path fading effect. It has been shown by simulations that such
code design can considerably improve the system performance over the
conventional design with separate channel estimation and error protection
modules under the same code rate. Nevertheless, the major obstacle that
prevents from the practice of the codes is that the existing codes are mostly
searched by computers, and hence exhibit no good structure for efficient
decoding. Hence, the time-consuming exhaustive search becomes the only decoding
choice, and the decoding complexity increases dramatically with the codeword
length. In this paper, by optimizing the signal-tonoise ratio, we found a
systematic construction for the codes for combined channel estimation and error
protection, and confirmed its equivalence in performance to the
computer-searched codes by simulations. Moreover, the structural codes that we
construct by rules can now be maximum-likelihoodly decodable in terms of a
newly derived recursive metric for use of the priority-first search decoding
algorithm. Thus,the decoding complexity reduces significantly when compared
with that of the exhaustive decoder. The extension code design for fast-fading
channels is also presented. Simulations conclude that our constructed extension
code is robust in performance even if the coherent period is shorter than the
codeword length.",consumer protection
http://arxiv.org/abs/cs/0611102v1,"We present a method to secure the complete path between a server and the
local human user at a network node. This is useful for scenarios like internet
banking, electronic signatures, or online voting. Protection of input
authenticity and output integrity and authenticity is accomplished by a
combination of traditional and novel technologies, e.g., SSL, ActiveX, and
DirectX. Our approach does not require administrative privileges to deploy and
is hence suitable for consumer applications. Results are based on the
implementation of a proof-of-concept application for the Windows platform.",consumer protection
http://arxiv.org/abs/1004.4732v2,"In this paper, we calculate energy required to copy one bit of useful
information in the presence of thermal noise. For this purpose, we consider a
quantum system capable of storing one bit of classical information, which is
initially in a mixed state corresponding to temperature T. We calculate how
many of these systems must be used to store useful information and control bits
protecting the content against transmission errors. Finally, we analyze how
adding these extra bits changes the total energy consumed during the copying.",consumer protection
http://arxiv.org/abs/1201.0949v1,"Phishing (password + fishing) is a form of cyber crime based on social
engineering and site spoofing techniques. The name of 'phishing' is a conscious
misspelling of the word 'fishing' and involves stealing confidential data from
a user's computer and subsequently using the data to steal the user's money. In
this paper, we study, discuss and propose the phishing attack stages and types,
technologies for detection of phishing web pages, and conclude our paper with
some important recommendations for preventing phishing for both consumer and
company.",consumer protection
http://arxiv.org/abs/1607.06377v1,"Advanced Metering Infrastructure (AMI) have rapidly become a topic of
international interest as governments have sponsored their deployment for the
purposes of utility service reliability and efficiency, e.g., water and
electricity conservation. Two problems plague such deployments. First is the
protection of consumer privacy. Second is the problem of huge amounts of data
from such deployments. A new architecture is proposed to address these problems
through the use of Aggregators, which incorporate temporary data buffering and
the modularization of utility grid analysis. These Aggregators are used to
deliver anonymized summary data to the central utility while preserving billing
and automated connection services.",consumer protection
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",consumer protection
http://arxiv.org/abs/1201.4376v2,"Participatory Sensing is an emerging computing paradigm that enables the
distributed collection of data by self-selected participants. It allows the
increasing number of mobile phone users to share local knowledge acquired by
their sensor-equipped devices, e.g., to monitor temperature, pollution level or
consumer pricing information. While research initiatives and prototypes
proliferate, their real-world impact is often bounded to comprehensive user
participation. If users have no incentive, or feel that their privacy might be
endangered, it is likely that they will not participate. In this article, we
focus on privacy protection in Participatory Sensing and introduce a suitable
privacy-enhanced infrastructure. First, we provide a set of definitions of
privacy requirements for both data producers (i.e., users providing sensed
information) and consumers (i.e., applications accessing the data). Then, we
propose an efficient solution designed for mobile phone users, which incurs
very low overhead. Finally, we discuss a number of open problems and possible
research directions.",consumer protection
