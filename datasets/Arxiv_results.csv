http://arxiv.org/abs/1709.09302v3,"We consider a market in which capacity-constrained generators compete in
scalar-parameterized supply functions to serve an inelastic demand spread
throughout a transmission constrained power network. The market clears
according to a locational marginal pricing mechanism, in which the independent
system operator (ISO) determines the generators' production quantities to
minimize the revealed cost of meeting demand, while ensuring that network
transmission and generator capacity constraints are met. Under the stylizing
assumption that both the ISO and generators choose their strategies
simultaneously, we establish the existence of Nash equilibria for the
underlying market, and derive an upper bound on the allocative efficiency loss
at Nash equilibrium relative to the socially optimal level. We also
characterize an upper bound on the markup of locational marginal prices at Nash
equilibrium above their perfectly competitive levels. Of particular relevance
to ex ante market power monitoring, these bounds reveal the role of certain
market structures---specifically, the \emph{market share} and \emph{residual
supply index} of a producer---in predicting the degree to which that producer
is able to exercise market power to influence the market outcome to its
advantage. Finally, restricting our attention to the simpler setting of a
two-node power network, we provide a characterization of market structures
under which a Braess-like paradox occurs due to the exercise of market
power---that is to say, we provide a necessary and sufficient condition on
market structure under which the strengthening of the network's transmission
line capacity results in the (counterintuitive) increase in the total cost of
generation at Nash equilibrium.",market monitoring
http://arxiv.org/abs/cs/0306024v1,"The DESY Computer Center is the home of O(1000) computers supplying a wide
range of different services Monitoring such a large installation is a
challenge. After a long time running a SNMP based commercial Network Management
System, the evaluation of a new System was started. There are a lot of
different commercial and freeware products on the market, but none of them
fully satisfied all our requirements. After re-valuating our original
requirements we selected NAGIOS as our monitoring and alarming tool. After a
successful test we are in production since autumn 2002 and are extending the
service to fully support a distributed monitoring and alarming.",market monitoring
http://arxiv.org/abs/1607.03583v1,"This paper analyzes repeated multimarket contact with observation errors
where two players operate in multiple markets simultaneously. Multimarket
contact has received much attention in economics, management, and so on.
Despite vast empirical studies that examine whether multimarket contact fosters
cooperation or collusion, little is theoretically known as to how players
behave in an equilibrium when each player receives a noisy and different
observation or signal indicating other firms' actions (private monitoring). To
the best of our knowledge, we are the first to construct a strategy designed
for multiple markets whose per-market equilibrium payoffs exceed one for a
single market, in our setting. We first construct an entirely novel strategy
whose behavior is specified by a non-linear function of the signal
configurations. We then show that the per-market equilibrium payoff improves
when the number of markets is sufficiently large.",market monitoring
http://arxiv.org/abs/1809.01640v1,"The paper presents web based information system for heat supply monitoring.
The proposed model and information system for gathering data from heating
station heat-flow meters and regulators is software realized. The novel system
with proved functionality can be commercialized at the cost of minimal
investments, finding wildly use on Bulgarian market as cheap and quality
alternative of the western systems.",market monitoring
http://arxiv.org/abs/1602.07063v2,"This article investigates graph analysis for intelligent marketing in smart
cities, where metatrails are crowdsourced by mobile sensing for marketing
strategies. Unlike most works that focused on client sides, this study is
intended for market planning, from the perspective of enterprises. Several
novel crowdsourced features based on metatrails, including hotspot networks,
crowd transitions, affinity subnetworks, and sequential visiting patterns, are
discussed in the article. These smart footprints can reflect crowd preferences
and the topology of a site of interest. Marketers can utilize such information
for commercial resource planning and deployment. Simulations were conducted to
demonstrate the performance. At the end, this study also discusses different
scenarios for practical geo-conquesting applications.",market monitoring
http://arxiv.org/abs/1608.04143v2,"We investigate the problem of market mechanism design for wind energy. We
consider a dynamic two-step model with one strategic seller with wind
generation and one buyer, who trade energy through a mechanism determined by a
designer. The seller has private information about his technology and wind
condition, which he learns dynamically over time. We consider (static) forward
and real-time mechanisms that take place at time T=1 and T=2, respectively. We
also propose a dynamic mechanism that provides a coupling between the outcomes
of the forward and real-time markets. We show that the dynamic mechanism
outperforms the forward and real-time mechanisms for a general objective of the
designer. Therefore, we demonstrate the advantage of adopting dynamic market
mechanisms over static market mechanisms for wind energy. The dynamic mechanism
reveals information about wind generation in advance, and also provides
flexibility for incorporation of new information arriving over time. We discuss
how our results generalize to environments with many strategic sellers. We also
study two variants of the dynamic mechanism that guarantee no penalty risk for
the seller, and/or monitor the wind condition. We illustrate our results with a
numerical example.",market monitoring
http://arxiv.org/abs/cs/0405028v1,"In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.",market monitoring
http://arxiv.org/abs/cs/0109111v2,"We propose a quality of service (QoS) monitoring program for broadband access
to measure the impact of proprietary network spaces. Our paper surveys other
QoS policy initiatives, including those in the airline, and wireless and
wireline telephone industries, to situate broadband in the context of other
markets undergoing regulatory devolution. We illustrate how network
architecture can create impediments to open communications, and how QoS
monitoring can detect such effects. We present data from a field test of
QoS-monitoring software now in development. We suggest QoS metrics to gauge
whether information ""walled gardens"" represent a real threat for dividing the
Internet into proprietary spaces.
  To demonstrate our proposal, we are placing our software on the computers of
a sample of broadband subscribers. The software periodically conducts a battery
of tests that assess the quality of connections from the subscriber's computer
to various content sites. Any systematic differences in connection quality
between affiliated and non-affiliated content sites would warrant research into
the behavioral implications of those differences.
  QoS monitoring is timely because the potential for the Internet to break into
a loose network of proprietary content domains appears stronger than ever.
Recent court rulings and policy statements suggest a growing trend towards
relaxed scrutiny of mergers and the easing or elimination of content ownership
rules. This policy environment could lead to a market with a small number of
large, vertically integrated network operators, each pushing its proprietary
content on subscribers.",market monitoring
http://arxiv.org/abs/1806.05914v1,"Cloud services are becoming increasingly popular: 60\% of information
technology spending in 2016 was Cloud-based, and the size of the public Cloud
service market will reach \$236B by 2020. To ensure reliable operation of the
Cloud services, one must monitor their health.
  While a number of research challenges in the area of Cloud monitoring have
been solved, problems are remaining. This prompted us to highlight three areas,
which cause problems to practitioners and require further research. These three
areas are as follows: A) defining health states of Cloud systems, B) creating
unified monitoring environments, and C) establishing high availability
strategies.
  In this paper we provide details of these areas and suggest a number of
potential solutions to the challenges. We also show that Cloud monitoring
presents exciting opportunities for novel research and practice.",market monitoring
http://arxiv.org/abs/1201.2207v1,"We consider the problem of information fusion from multiple sensors of
different types with the objective of improving the confidence of inference
tasks, such as object classification, performed from the data collected by the
sensors. We propose a novel technique based on distributed belief aggregation
using a multi-agent prediction market to solve this information fusion problem.
To monitor the improvement in the confidence of the object classification as
well as to dis-incentivize agents from misreporting information, we have
introduced a market maker that rewards the agents instantaneously as well as at
the end of the inference task, based on the quality of the submitted reports.
We have implemented the market maker's reward calculation in the form of a
scoring rule and have shown analytically that it incentivizes truthful
revelation or accurate reporting by each agent. We have experimentally verified
our technique for multi-sensor information fusion for an automated landmine
detection scenario. Our experimental results show that, for identical data
distributions and settings, using our information aggregation technique
increases the accuracy of object classification favorably as compared to two
other commonly used techniques for information fusion for landmine detection.",market monitoring
http://arxiv.org/abs/1507.02043v1,"Today's cellular telecommunications markets require continuous monitoring and
intervention by regulators in order to balance the interests of various
stakeholders. In order to reduce the extent of regulatory involvements in the
day-to-day business of cellular operators, the present paper proposes a
""self-regulating"" spectrum market regime named ""society spectrum"". This regime
provides a market-inherent and automatic self-balancing of stakeholder powers,
which at the same time provides a series of coordination and fairness assurance
functions that clearly distinguish it from ""spectrum as a commons"" solutions.
The present paper will introduce the fundamental regulatory design and will
elaborate on mechanisms to assure fairness among stakeholders and individuals.
This work further puts the society spectrum into the context of contemporary
radio access technologies and cognitive radio approaches.",market monitoring
http://arxiv.org/abs/1606.06510v1,"Aggregators are playing an increasingly crucial role in the integration of
renewable generation in power systems. However, the intermittent nature of
renewable generation makes market interactions of aggregators difficult to
monitor and regulate, raising concerns about potential market manipulation by
aggregators. In this paper, we study this issue by quantifying the profit an
aggregator can obtain through strategic curtailment of generation in an
electricity market. We show that, while the problem of maximizing the benefit
from curtailment is hard in general, efficient algorithms exist when the
topology of the network is radial (acyclic). Further, we highlight that
significant increases in profit are possible via strategic curtailment in
practical settings.",market monitoring
http://arxiv.org/abs/1705.03233v4,"Managing the prediction of metrics in high-frequency financial markets is a
challenging task. An efficient way is by monitoring the dynamics of a limit
order book to identify the information edge. This paper describes the first
publicly available benchmark dataset of high-frequency limit order markets for
mid-price prediction. We extracted normalized data representations of time
series data for five stocks from the NASDAQ Nordic stock market for a time
period of ten consecutive days, leading to a dataset of ~4,000,000 time series
samples in total. A day-based anchored cross-validation experimental protocol
is also provided that can be used as a benchmark for comparing the performance
of state-of-the-art methodologies. Performance of baseline approaches are also
provided to facilitate experimental comparisons. We expect that such a
large-scale dataset can serve as a testbed for devising novel solutions of
expert systems for high-frequency limit order book data analysis.",market monitoring
http://arxiv.org/abs/1802.02996v1,"The difficulty of large scale monitoring of app markets affects our
understanding of their dynamics. This is particularly true for dimensions such
as app update frequency, control and pricing, the impact of developer actions
on app popularity, as well as coveted membership in top app lists. In this
paper we perform a detailed temporal analysis on two datasets we have collected
from the Google Play Store, one consisting of 160,000 apps and the other of
87,223 newly released apps. We have monitored and collected data about these
apps over more than 6 months. Our results show that a high number of these apps
have not been updated over the monitoring interval. Moreover, these apps are
controlled by a few developers that dominate the total number of app downloads.
We observe that infrequently updated apps significantly impact the median app
price. However, a changing app price does not correlate with the download
count. Furthermore, we show that apps that attain higher ranks have better
stability in top app lists. We show that app market analytics can help detect
emerging threat vectors, and identify search rank fraud and even malware.
Further, we discuss the research implications of app market analytics on
improving developer and user experiences.",market monitoring
http://arxiv.org/abs/1510.07385v1,"Twitter is now a gold marketing tool for entities concerned with online
reputation. To automatically monitor online reputation of entities , systems
have to deal with ambiguous entity names, polarity detection and topic
detection. We propose three approaches to tackle the first issue: monitoring
Twitter in order to find relevant tweets about a given entity. Evaluated within
the framework of the RepLab-2013 Filtering task, each of them has been shown
competitive with state-of-the-art approaches. Mainly we investigate on how much
merging strategies may impact performances on a filtering task according to the
evaluation measure.",market monitoring
http://arxiv.org/abs/1211.6512v1,"Recent research has focused on the monitoring of global-scale online data for
improved detection of epidemics, mood patterns, movements in the stock market,
political revolutions, box-office revenues, consumer behaviour and many other
important phenomena. However, privacy considerations and the sheer scale of
data available online are quickly making global monitoring infeasible, and
existing methods do not take full advantage of local network structure to
identify key nodes for monitoring. Here, we develop a model of the contagious
spread of information in a global-scale, publicly-articulated social network
and show that a simple method can yield not just early detection, but advance
warning of contagious outbreaks. In this method, we randomly choose a small
fraction of nodes in the network and then we randomly choose a ""friend"" of each
node to include in a group for local monitoring. Using six months of data from
most of the full Twittersphere, we show that this friend group is more central
in the network and it helps us to detect viral outbreaks of the use of novel
hashtags about 7 days earlier than we could with an equal-sized randomly chosen
group. Moreover, the method actually works better than expected due to network
structure alone because highly central actors are both more active and exhibit
increased diversity in the information they transmit to others. These results
suggest that local monitoring is not just more efficient, it is more effective,
and it is possible that other contagious processes in global-scale networks may
be similarly monitored.",market monitoring
http://arxiv.org/abs/1502.00206v2,"Cloud computing provides on-demand access to affordable hardware (multi-core
CPUs, GPUs, disks, and networking equipment) and software (databases,
application servers and data processing frameworks) platforms with features
such as elasticity, pay-per-use, low upfront investment and low time to market.
This has led to the proliferation of business critical applications that
leverage various cloud platforms. Such applications hosted on single or
multiple cloud provider platforms have diverse characteristics requiring
extensive monitoring and benchmarking mechanisms to ensure run-time Quality of
Service (QoS) (e.g., latency and throughput). This paper proposes, develops and
validates CLAMBS:Cross-Layer Multi-Cloud Application Monitoring and
Benchmarking as-a-Service for efficient QoS monitoring and benchmarking of
cloud applications hosted on multi-clouds environments. The major highlight of
CLAMBS is its capability of monitoring and benchmarking individual application
components such as databases and web servers, distributed across cloud layers,
spread among multiple cloud providers. We validate CLAMBS using prototype
implementation and extensive experimentation and show that CLAMBS efficiently
monitors and benchmarks application components on multi-cloud platforms
including Amazon EC2 and Microsoft Azure.",market monitoring
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",market monitoring
http://arxiv.org/abs/1301.1444v2,"We study an economic decision problem where the actors are two firms and the
Antitrust Authority whose main task is to monitor and prevent firms' potential
anti-competitive behaviour and its effect on the market. The Antitrust
Authority's decision process is modelled using a Bayesian network where both
the relational structure and the parameters of the model are estimated from a
data set provided by the Authority itself. A number of economic variables that
influence this decision process are also included in the model. We analyse how
monitoring by the Antitrust Authority affects firms' strategies about
cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma
using object-oriented Bayesian networks. We show how the integration of firms'
decision process and external market information can be modelled in this way.
Various decision scenarios and strategies are illustrated.",market monitoring
http://arxiv.org/abs/1503.01061v4,"We investigate a hierarchically organized cloud infrastructure and compare
distributed hierarchical control based on resource monitoring with market
mechanisms for resource management. The latter do not require a model of the
system, incur a low overhead, are robust, and satisfy several other desiderates
of autonomic computing. We introduce several performance measures and report on
simulation studies which show that a straightforward bidding scheme supports an
effective admission control mechanism, while reducing the communication
complexity by several orders of magnitude and also increasing the acceptance
rate compared to hierarchical control and monitoring mechanisms. Resource
management based on market-based mechanisms can be seen as an intermediate step
towards cloud self-organization, an ideal alternative to current mechanisms for
cloud resource management.",market monitoring
http://arxiv.org/abs/1407.3077v1,"A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.",consumer profiling algorithm
http://arxiv.org/abs/1411.3961v2,"Loyalty programs are promoted by vendors to incentivize loyalty in buyers.
Although such programs have become widespread, they have been criticized by
business experts and consumer associations: loyalty results in profiling and
hence in loss of privacy of consumers. We propose a protocol for
privacy-preserving loyalty programs that allows vendors and consumers to enjoy
the benefits of loyalty (returning customers and discounts, respectively),
while allowing consumers to stay anonymous and empowering them to decide how
much of their profile they reveal to the vendor. The vendor must offer
additional reward if he wants to learn more details on the consumer's profile.
Our protocol is based on partially blind signatures and generalization
techniques, and provides anonymity to consumers and their purchases, while
still allowing negotiated consumer profiling.",consumer profiling algorithm
http://arxiv.org/abs/1604.08330v1,"Server consolidation based on virtualization technology simplifies system
administration and improves energy efficiency by improving resource
utilizations and reducing the physical machine (PM) number in contemporary
service-oriented data centers. The elasticity of Internet applications changes
the consolidation technologies from addressing virtual machines (VMs) to PMs
mapping schemes which must know the VMs statuses, i.e. the number of VMs and
the profiling data of each VM, into providing the application-to-VM-to-PM
mapping. In this paper, we study on the consolidation of multiple Internet
applications, minimizing the number of PMs with required performance. We first
model the consolidation providing the application-to-VM-to-PM mapping to
minimize the number of PMs as an integer linear programming problem, and then
present a heuristic algorithm to solve the problem in polynomial time.
Extensive experimental results show that our heuristic algorithm consumes less
than 4.3% more resources than the optimal amounts with few overheads. Existing
consolidation technologies using the input of the VM statuses output by our
heuristic algorithm consume 1.06% more PMs.",consumer profiling algorithm
http://arxiv.org/abs/1208.4651v1,"In wireless networks, energy consumed for communication includes both the
transmission and the processing energy. In this paper, point-to-point
communication over a fading channel with an energy harvesting transmitter is
studied considering jointly the energy costs of transmission and processing.
Under the assumption of known energy arrival and fading profiles, optimal
transmission policy for throughput maximization is investigated. Assuming that
the transmitter has sufficient amount of data in its buffer at the beginning of
the transmission period, the average throughput by a given deadline is
maximized. Furthermore, a ""directional glue pouring algorithm"" that computes
the optimal transmission policy is described.",consumer profiling algorithm
http://arxiv.org/abs/1304.5197v1,"Identifying the hottest paths in the control flow graph of a routine can
direct optimizations to portions of the code where most resources are consumed.
This powerful methodology, called path profiling, was introduced by Ball and
Larus in the mid 90s and has received considerable attention in the last 15
years for its practical relevance. A shortcoming of Ball-Larus path profiling
was the inability to profile cyclic paths, making it difficult to mine
interesting execution patterns that span multiple loop iterations. Previous
results, based on rather complex algorithms, have attempted to circumvent this
limitation at the price of significant performance losses already for a small
number of iterations. In this paper, we present a new approach to multiple
iterations path profiling, based on data structures built on top of the
original Ball-Larus numbering technique. Our approach allows it to profile all
executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths,
where k is a user-defined parameter. An extensive experimental investigation on
a large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,
our approach can be even faster than Ball-Larus due to fewer operations on
smaller hash tables, producing compact representations of cyclic paths even for
large values of k.",consumer profiling algorithm
http://arxiv.org/abs/1907.12219v1,"JPEG is one of the popular image compression algorithms that provide
efficient storage and transmission capabilities in consumer electronics, and
hence it is the most preferred image format over the internet world. In the
present digital and Big-data era, a huge volume of JPEG compressed document
images are being archived and communicated through consumer electronics on
daily basis. Though it is advantageous to have data in the compressed form on
one side, however, on the other side processing with off-the-shelf methods
becomes computationally expensive because it requires decompression and
recompression operations. Therefore, it would be novel and efficient, if the
compressed data are processed directly in their respective compressed domains
of consumer electronics. In the present research paper, we propose to
demonstrate this idea taking the case study of printed text line segmentation.
Since, JPEG achieves compression by dividing the image into non overlapping 8x8
blocks in the pixel domain and using Discrete Cosine Transform (DCT); it is
very likely that the partitioned 8x8 DCT blocks overlap the contents of two
adjacent text-lines without leaving any clue for the line separator, thus
making text-line segmentation a challenging problem. Two approaches of
segmentation have been proposed here using the DC projection profile and AC
coefficients of each 8x8 DCT block. The first approach is based on the strategy
of partial decompression of selected DCT blocks, and the second approach is
with intelligent analysis of F10 and F11 AC coefficients and without using any
type of decompression. The proposed methods have been tested with variable font
sizes, font style and spacing between lines, and a good performance is
reported.",consumer profiling algorithm
http://arxiv.org/abs/1401.2440v1,"Future e-business models will rely on electronic contracts which are agreed
dynamically and adaptively by web services. Thus, the automatic negotiation of
Service Level Agreements (SLAs) between consumers and providers is key for
enabling service-based value chains.
  The process of finding appropriate providers for web services seems to be
simple. Consumers contact several providers and take the provider which offers
the best matching SLA. However, currently consumers are not able forecasting
the probability of finding a matching provider for their requested SLA. So
consumers contact several providers and check if their offers are matching. In
case of continuing faults, on the one hand consumers may adapt their Service
Level Objects (SLOs) of the required SLA or on the other hand simply accept
offered SLAs of the contacted providers.
  By forecasting the probability of finding a matching provider, consumers
could assess their chances of finding a provider offering the requested SLA. If
a low probability is predicted, consumers can immediately adapt their SLOs or
increase the numbers of providers to be contacted.
  Thus, this paper proposes an analytical forecast model, which allows
consumers to get a realistic assessment of the probability to find matching
providers. Additionally, we present an optimization algorithm based on the
forecast results, which allows adapting the SLO parameter ranges in order to
find at least one matching provider. Not only consumers, but also providers can
use this forecast model to predict the prospective demand. So providers are
able to assess the number of potential consumers based on their offers too.
  Justification of our approach is done by simulation of practical examples
checking our theoretical findings.",consumer profiling algorithm
http://arxiv.org/abs/1110.5351v2,"We report on the implementation of a dynamically configurable, servomotor-
controlled, permanent magnet Zeeman slower for quantum optics experiments with
ultracold atoms and molecules. This atom slower allows for switching between
magnetic field profiles that are designed for different atomic species.
Additionally, through feedback on the atom trapping rate, we demonstrate that
computer-controlled genetic optimization algorithms applied to the magnet
positions can be used in situ to obtain field profiles that maximize the
trapping rate for any given experimental conditions. The device is lightweight,
remotely controlled, and consumes no power in steady state; it is a step toward
automated control of quantum optics experiments.",consumer profiling algorithm
http://arxiv.org/abs/1502.02125v2,"The last decade has witnessed a tremendous growth in the volume as well as
the diversity of multimedia content generated by a multitude of sources (news
agencies, social media, etc.). Faced with a variety of content choices,
consumers are exhibiting diverse preferences for content; their preferences
often depend on the context in which they consume content as well as various
exogenous events. To satisfy the consumers' demand for such diverse content,
multimedia content aggregators (CAs) have emerged which gather content from
numerous multimedia sources. A key challenge for such systems is to accurately
predict what type of content each of its consumers prefers in a certain
context, and adapt these predictions to the evolving consumers' preferences,
contexts and content characteristics. We propose a novel, distributed, online
multimedia content aggregation framework, which gathers content generated by
multiple heterogeneous producers to fulfill its consumers' demand for content.
Since both the multimedia content characteristics and the consumers'
preferences and contexts are unknown, the optimal content aggregation strategy
is unknown a priori. Our proposed content aggregation algorithm is able to
learn online what content to gather and how to match content and users by
exploiting similarities between consumer types. We prove bounds for our
proposed learning algorithms that guarantee both the accuracy of the
predictions as well as the learning speed. Importantly, our algorithms operate
efficiently even when feedback from consumers is missing or content and
preferences evolve over time. Illustrative results highlight the merits of the
proposed content aggregation system in a variety of settings.",consumer profiling algorithm
http://arxiv.org/abs/1609.04053v1,"The arrival of small-scale distributed energy generation in the future smart
grid has led to the emergence of so-called prosumers, who can both consume as
well as produce energy. By using local generation from renewable energy
resources, the stress on power generation and supply system can be
significantly reduced during high demand periods. However, this also creates a
significant challenge for conventional power plants that suddenly need to ramp
up quickly when the renewable energy drops off. In this paper, we propose an
energy consumption scheduling problem for prosumers to minimize the peak ramp
of the system. The optimal schedule of prosumers can be obtained by solving the
centralized optimization problem. However, due to the privacy concerns and the
distributed topology of the power system, the centralized design is difficult
to implement in practice. Therefore, we propose the distributed algorithms to
efficiently solve the centralized problem using the alternating direction
method of multiplier (ADMM), in which each prosumer independently schedules its
energy consumption profile. The simulation results demonstrate the convergence
performance of the proposed algorithms as well as the capability of our model
in reducing the peak ramp of the system.",consumer profiling algorithm
http://arxiv.org/abs/1507.03328v1,"In this paper, we propose the amphibious influence maximization (AIM) model
that combines traditional marketing via content providers and viral marketing
to consumers in social networks in a single framework. In AIM, a set of content
providers and consumers form a bipartite network while consumers also form
their social network, and influence propagates from the content providers to
consumers and among consumers in the social network following the independent
cascade model. An advertiser needs to select a subset of seed content providers
and a subset of seed consumers, such that the influence from the seed providers
passing through the seed consumers could reach a large number of consumers in
the social network in expectation.
  We prove that the AIM problem is NP-hard to approximate to within any
constant factor via a reduction from Feige's k-prover proof system for 3-SAT5.
We also give evidence that even when the social network graph is trivial (i.e.
has no edges), a polynomial time constant factor approximation for AIM is
unlikely. However, when we assume that the weighted bi-adjacency matrix that
describes the influence of content providers on consumers is of constant rank,
a common assumption often used in recommender systems, we provide a
polynomial-time algorithm that achieves approximation ratio of
$(1-1/e-\epsilon)^3$ for any (polynomially small) $\epsilon > 0$. Our
algorithmic results still hold for a more general model where cascades in
social network follow a general monotone and submodular function.",consumer profiling algorithm
http://arxiv.org/abs/1809.05245v1,"Achieving a balance of supply and demand in a multi-agent system with many
individual self-interested and rational agents that act as suppliers and
consumers is a natural problem in a variety of real-life domains---smart power
grids, data centers, and others. In this paper, we address the
profit-maximization problem for a group of distributed supplier and consumer
agents, with no inter-agent communication. We simulate a scenario of a market
with $S$ suppliers and $C$ consumers such that at every instant, each supplier
agent supplies a certain quantity and simultaneously, each consumer agent
consumes a certain quantity. The information about the total amount supplied
and consumed is only kept with the center. The proposed algorithm is a
combination of the classical additive-increase multiplicative-decrease (AIMD)
algorithm in conjunction with a probabilistic rule for the agents to respond to
a capacity signal. This leads to a nonhomogeneous Markov chain and we show
almost sure convergence of this chain to the social optimum, for our market of
distributed supplier and consumer agents. Employing this AIMD-type algorithm,
the center sends a feedback message to the agents in the supplier side if there
is a scenario of excess supply, or to the consumer agents if there is excess
consumption. Each agent has a concave utility function whose derivative tends
to 0 when an optimum quantity is supplied/consumed. Hence when social
convergence is reached, each agent supplies or consumes a quantity which leads
to its individual maximum profit, without the need of any communication. So
eventually, each agent supplies or consumes a quantity which leads to its
individual maximum profit, without communicating with any other agents. Our
simulations show the efficacy of this approach.",consumer profiling algorithm
http://arxiv.org/abs/1908.10713v1,"Non-intrusive load monitoring (NILM) has been extensively researched over the
last decade. The objective of NILM is to identify the power consumption of
individual appliances and to detect when particular devices are on or off from
measuring the power consumption of an entire house. This information allows
households to receive customized advice on how to better manage their
electrical consumption. In this paper, we present an alternative NILM method
that breaks down the aggregated power signal into categories of appliances. The
ultimate goal is to use this approach for demand-side management to estimate
potential flexibility within the electricity consumption of households. Our
method is implemented as an algorithm combining NILM and load profile
simulation. This algorithm, based on a Markov model, allocates an activity
chain to each inhabitant of the household, deduces from the whole-house power
measurement and statistical data the appliance usage, generate the power
profile accordingly and finally returns the share of energy consumed by each
appliance category over time. To analyze its performance, the algorithm was
benchmarked against several state-of-the-art NILM algorithms and tested on
three public datasets. The proposed algorithm is unsupervised; hence it does
not require any labeled data, which are expensive to acquire. Although better
performance is shown for the supervised algorithms, our proposed unsupervised
algorithm achieves a similar range of uncertainty while saving on the cost of
acquiring labeled data. Additionally, our method requires lower computational
power compared to most of the tested NILM algorithms. It was designed for
low-sampling-rate power measurement (every 15 min), which corresponds to the
frequency range of most common smart meters.",consumer profiling algorithm
http://arxiv.org/abs/1803.03560v2,"In this paper, we propose a distributed control strategy for the design of an
energy market. The method relies on a hierarchical structure of aggregators for
the coordination of prosumers (agents which can produce and consume energy).
The hierarchy reflects the voltage level separations of the electrical grid and
allows aggregating prosumers in pools, while taking into account the grid
operational constraints. To reach optimal coordination, the prosumers
communicate their forecasted power profile to the upper level of the hierarchy.
Each time the information crosses upwards a level of the hierarchy, it is first
aggregated, both to strongly reduce the data flow and to preserve the privacy.
In the first part of the paper, the decomposition algorithm, which is based on
the alternating direction method of multipliers (ADMM), is presented. In the
second part, we explore how the proposed algorithm scales with increasing
number of prosumers and hierarchical levels, through extensive simulations
based on randomly generated scenarios.",consumer profiling algorithm
http://arxiv.org/abs/1805.11646v1,"Gradient index (GRIN) acoustic devices have spatially inhomogeneous
refractive index profile and allow flexible control of the propagation of
acoustic waves. Previous GRIN acoustic lenses are mostly inherently
two-dimensional designs that are difficult to be extended to all three
dimensions. Besides, manually designing the spatially inhomogeneous structure
is both time-consuming and error-prone. In this work, we proposed and
numerically verified an automated computer-aided design tool: GRadient Index
Pick-and-Place (GRIPP) algorithm, for generating three-dimensional GRIN
acoustic wave controlling devices with scalable and 3D printable structures.
The algorithm receives as inputs a spatial distribution of refractive index and
a pre-defined library of gradient index unit cells, and outputs a 3D model of
GRIN device that is ready to be 3D printed. The tool enables rapid design and
realization of a large variety of 3D GRIN acoustic devices, which can be useful
in areas such as speaker system design, airborne ultrasonic sensing, as well as
therapeutic ultrasound.",consumer profiling algorithm
http://arxiv.org/abs/1806.09542v1,"Mapping and translating professional but arcane clinical jargons to consumer
language is essential to improve the patient-clinician communication.
Researchers have used the existing biomedical ontologies and consumer health
vocabulary dictionary to translate between the languages. However, such
approaches are limited by expert efforts to manually build the dictionary,
which is hard to be generalized and scalable. In this work, we utilized the
embeddings alignment method for the word mapping between unparalleled clinical
professional and consumer language embeddings. To map semantically similar
words in two different word embeddings, we first independently trained word
embeddings on both the corpus with abundant clinical professional terms and the
other with mainly healthcare consumer terms. Then, we aligned the embeddings by
the Procrustes algorithm. We also investigated the approach with the
adversarial training with refinement. We evaluated the quality of the alignment
through the similar words retrieval both by computing the model precision and
as well as judging qualitatively by human. We show that the Procrustes
algorithm can be performant for the professional consumer language embeddings
alignment, whereas adversarial training with refinement may find some relations
between two languages.",consumer profiling algorithm
http://arxiv.org/abs/1802.08112v1,"A rational behavior of a consumer is analyzed when the user participates in a
Peak Time Rebate (PTR) mechanism, which is a demand response (DR) incentive
program based on a baseline. A multi-stage stochastic programming is proposed
from the demand side in order to understand the rational decisions. The
consumer preferences are modeled as a risk-averse function under additive
uncertainty. The user chooses the optimal consumption profile to maximize his
economic benefits for each period. The stochastic optimization problem is
solved backward in time. A particular situation is developed when the System
Operator (SO) uses consumption of the previous interval as the
household-specific baseline for the DR program. It is found that a rational
consumer alters the baseline in order to increase the well-being when there is
an economic incentive. As results, whether the incentive is lower than the
retail price, the user shifts his load requirement to the baseline setting
period. On the other hand, if the incentive is greater than the regular energy
price, the optimal decision is that the user spends the maximum possible energy
in the baseline setting period and reduces the consumption at the PTR time.
This consumer behavior produces more energy consumption in total considering
all periods. In addition, the user with high uncertainty level in his energy
pattern should spend less energy than a predictable consumer when the incentive
is lower than the retail price.",consumer profiling algorithm
http://arxiv.org/abs/1608.01244v1,"This paper introduces a new scheme for autonomous electricity cooperatives,
called predictive cooperative (PCP), which aggregates commercial and
residential electricity consumers and participates in the electricity market on
behalf of its members. An axiomatic approach is proposed to calculate the
day-ahead bid and to disaggregate the collective cost among participating
consumers. The resulting formulation is shown to keep the members incentivized
to both participate in the cooperative and remain truthful in reporting their
expected loads. The scheme is implemented using PJM (world's largest wholesale
electricity market) real-time and day-ahead price data for 2015 and a
collection of residential and commercial load profiles. The model performance
of this framework is compared to that of real-time pricing (RTP) scheme, in
which wholesale market prices are directly applied to individual consumers. The
results show truthful load announcement by consumers, reduction in electricity
price variation for all consumers, and comparative benefits for participants.",consumer profiling algorithm
http://arxiv.org/abs/1608.01658v1,"Metastatic presence in lymph nodes is one of the most important prognostic
variables of breast cancer. The current diagnostic procedure for manually
reviewing sentinel lymph nodes, however, is very time-consuming and subjective.
Pathologists have to manually scan an entire digital whole-slide image (WSI)
for regions of metastasis that are sometimes only detectable under high
resolution or entirely hidden from the human visual cortex. From October 2015
to April 2016, the International Symposium on Biomedical Imaging (ISBI) held
the Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for
automatic detection of lymph node metastasis. Using a generalizable stain
normalization technique and the Proscia Pathology Cloud computing platform, we
trained a deep convolutional neural network on millions of tissue and tumor
image tiles to perform slide-based evaluation on our testing set of whole-slide
images images, with a sensitivity of 0.96, specificity of 0.89, and AUC score
of 0.90. Our results indicate that our platform can automatically scan any WSI
for metastatic regions without institutional calibration to respective stain
profiles.",consumer profiling algorithm
http://arxiv.org/abs/1803.11560v1,"Learning through experience is time-consuming, inefficient and often bad for
your cortisol levels. To address this problem, a number of recently proposed
teacher-student methods have demonstrated the benefits of private tuition, in
which a single model learns from an ensemble of more experienced tutors.
Unfortunately, the cost of such supervision restricts good representations to a
privileged minority. Unsupervised learning can be used to lower tuition fees,
but runs the risk of producing networks that require extracurriculum learning
to strengthen their CVs and create their own LinkedIn profiles. Inspired by the
logo on a promotional stress ball at a local recruitment fair, we make the
following three contributions. First, we propose a novel almost no supervision
training algorithm that is effective, yet highly scalable in the number of
student networks being supervised, ensuring that education remains affordable.
Second, we demonstrate our approach on a typical use case: learning to bake,
developing a method that tastily surpasses the current state of the art.
Finally, we provide a rigorous quantitive analysis of our method, proving that
we have access to a calculator. Our work calls into question the long-held
dogma that life is the best teacher.",consumer profiling algorithm
http://arxiv.org/abs/1606.01403v1,"Mass-market mobile security threats have increased recently due to the growth
of mobile technologies and the popularity of mobile devices. Accordingly,
techniques have been introduced for identifying, classifying, and defending
against mobile threats utilizing static, dynamic, on-device, off-device, and
hybrid approaches. In this paper, we contribute to the mobile security defense
posture by introducing Andro-profiler, a hybrid behavior based analysis and
classification system for mobile malware. Andro-profiler classifies malware by
exploiting the behavior profiling extracted from the integrated system logs
including system calls, which are implicitly equivalent to distinct behavior
characteristics. Andro-profiler executes a malicious application on an emulator
in order to generate the integrated system logs, and creates human-readable
behavior profiles by analyzing the integrated system logs. By comparing the
behavior profile of malicious application with representative behavior profile
for each malware family, Andro-profiler detects and classifies it into malware
families. The experiment results demonstrate that Andro-profiler is scalable,
performs well in detecting and classifying malware with accuracy greater than
$98\%$, outperforms the existing state-of-the-art work, and is capable of
identifying zero-day mobile malware samples.",behavioral profiling
http://arxiv.org/abs/1506.01675v1,"Data aggregators collect large amount of information about individual users
and create detailed online behavioral profiles of individuals. Behavioral
profiles benefit users by improving products and services. However, they have
also raised concerns regarding user privacy, transparency of collection
practices and accuracy of data in the profiles. To improve transparency, some
companies are allowing users to access their behavioral profiles. In this work,
we investigated behavioral profiles of users by utilizing these access
mechanisms. Using in-person interviews (n=8), we analyzed the data shown in the
profiles, elicited user concerns, and estimated accuracy of profiles. We
confirmed our interview findings via an online survey (n=100). To assess the
claim of improving transparency, we compared data shown in profiles with the
data that companies have about users. More than 70% of the participants
expressed concerns about collection of sensitive data such as credit and health
information, level of detail and how their data may be used. We found a large
gap between the data shown in profiles and the data possessed by companies. A
large number of profiles were inaccurate with as much as 80% inaccuracy. We
discuss implications for public policy management.",behavioral profiling
http://arxiv.org/abs/1909.10012v1,"Users on Twitter are identified with the help of their profile attributes
that consists of username, display name, profile image, to name a few. The
profile attributes that users adopt can reflect their interests, belief, or
thematic inclinations. Literature has proposed the implications and
significance of profile attribute change for a random population of users.
However, the use of profile attribute for endorsements and to start a movement
have been under-explored. In this work, we consider #LokSabhaElections2019 as a
movement and perform a large-scale study of the profile of users who actively
made changes to profile attributes centered around #LokSabhaElections2019. We
collect the profile metadata for 49.4M users for a period of 2 months from
April 5, 2019 to June 5, 2019 amid #LokSabhaElections2019. We investigate how
the profile changes vary for the influential leaders and their followers over
the social movement. We further differentiate the organic and inorganic ways to
show the political inclination from the prism of profile changes. We report how
the addition of election campaign related keywords lead to spread of behavior
contagion and further investigate it with respect to ""Chowkidar Movement"" in
detail.",behavioral profiling
http://arxiv.org/abs/0807.1153v1,"We propose behavior-oriented services as a new paradigm of communication in
mobile human networks. Our study is motivated by the tight user-network
coupling in future mobile societies. In such a paradigm, messages are sent to
inferred behavioral profiles, instead of explicit IDs. Our paper provides a
systematic framework in providing such services. First, user behavioral
profiles are constructed based on traces collected from two large wireless
networks, and their spatio-temporal stability is analyzed. The implicit
relationship discovered between mobile users could be utilized to provide a
service for message delivery and discovery in various network environments. As
an example application, we provide a detailed design of such a service in
challenged opportunistic network architecture, named CSI. We provide a fully
distributed solution using behavioral profile space gradients and small world
structures.
  Our analysis shows that user behavioral profiles are surprisingly stable,
i.e., the similarity of the behavioral profile of a user to its future
behavioral profile is above 0.8 for two days and 0.75 for one week, and remains
above 0.6 for five weeks. The correlation coefficient of the similarity metrics
between a user pair at different time instants is above 0.7 for four days, 0.62
for a week, and remains above 0.5 for two weeks. Leveraging such a stability in
user behaviors, the CSI service achieves delivery rate very close to the
delay-optimal strategy (above 94%), with minimal overhead (less than 84% of the
optimal). We believe that this new paradigm will act as an enabler of multiple
new services in mobile societies, and is potentially applicable in
server-based, heterogeneous or infrastructure-less wireless environments.",behavioral profiling
http://arxiv.org/abs/1703.09745v2,"Users of electronic devices, e.g., laptop, smartphone, etc. have
characteristic behaviors while surfing the Web. Profiling this behavior can
help identify the person using a given device. In this paper, we introduce a
technique to profile users based on their web transactions. We compute several
features extracted from a sequence of web transactions and use them with
one-class classification techniques to profile a user. We assess the efficacy
and speed of our method at differentiating 25 users on a dataset representing 6
months of web traffic monitoring from a small company network.",behavioral profiling
http://arxiv.org/abs/1003.0466v1,"Analysing Online Social Networks (OSN), voluntarily maintained and
automatically exploitable databases of electronic personal information,
promises a wealth of insight into their users' behavior, interest, and
utilization of these currently predominant services on the Internet. To
understand popularity in OSN, we monitored a large sample of profiles from a
highly popular network for three months, and analysed the relation between
profile properties and their impression frequency. Evaluating the data
indicates a strong relation between both the number of accepted contacts and
the diligence of updating contacts versus the frequency of requests for a
profile. Counter intuitively, the overall activity, gender, as well as
participation span of users have no remarkable impact on their profile's
popularity.",behavioral profiling
http://arxiv.org/abs/1407.3950v1,"The analysis of user behavior in digital games has been aided by the
introduction of user telemetry in game development, which provides
unprecedented access to quantitative data on user behavior from the installed
game clients of the entire population of players. Player behavior telemetry
datasets can be exceptionally complex, with features recorded for a varying
population of users over a temporal segment that can reach years in duration.
Categorization of behaviors, whether through descriptive methods (e.g.
segmention) or unsupervised/supervised learning techniques, is valuable for
finding patterns in the behavioral data, and developing profiles that are
actionable to game developers. There are numerous methods for unsupervised
clustering of user behavior, e.g. k-means/c-means, Non-negative Matrix
Factorization, or Principal Component Analysis. Although all yield behavior
categorizations, interpretation of the resulting categories in terms of actual
play behavior can be difficult if not impossible. In this paper, a range of
unsupervised techniques are applied together with Archetypal Analysis to
develop behavioral clusters from playtime data of 70,014 World of Warcraft
players, covering a five year interval. The techniques are evaluated with
respect to their ability to develop actionable behavioral profiles from the
dataset.",behavioral profiling
http://arxiv.org/abs/1802.03500v1,"Electricity users are the major players of the electric systems, and
electricity consumption is growing at an extraordinary rate. The research on
electricity consumption behaviors is becoming increasingly important to design
and deployment of the electric systems. Unfortunately, electricity load
profiles are difficult to acquire. Data synthesis is one of the best approaches
to solving the lack of data, and the key is the model that preserves the real
electricity consumption behaviors. In this paper, we propose a hierarchical
multi-matrices Markov Chain (HMMC) model to synthesize scalable electricity
load profiles that preserve the real consumption behavior on three time scales:
per day, per week, and per year. To promote the research on the electricity
consumption behavior, we use the HMMC approach to model two distinctive raw
electricity load profiles. One is collected from the resident sector, and the
other is collected from the non-resident sectors, including different
industries such as education, finance, and manufacturing. The experiments show
our model performs much better than the classical Markov Chain model. We
publish two trained models online, and researchers can directly use these
trained models to synthesize scalable electricity load profiles for further
researches.",behavioral profiling
http://arxiv.org/abs/1908.06869v1,"The world sees a proliferation of machine learning/deep learning (ML) models
and their wide adoption in different application domains recently. This has
made the profiling and characterization of ML models an increasingly pressing
task for both hardware designers and system providers, as they would like to
offer the best possible computing system to serve ML models with the desired
latency, throughput, and energy requirements while maximizing resource
utilization. Such an endeavor is challenging as the characteristics of an ML
model depend on the interplay between the model, framework, system libraries,
and the hardware (or the HW/SW stack). A thorough characterization requires
understanding the behavior of the model execution across the HW/SW stack
levels. Existing profiling tools are disjoint, however, and only focus on
profiling within a particular level of the stack. This paper proposes a leveled
profiling design that leverages existing profiling tools to perform
across-stack profiling. The design does so in spite of the profiling overheads
incurred from the profiling providers. We coupled the profiling capability with
an automatic analysis pipeline to systematically characterize 65
state-of-the-art ML models. Through this characterization, we show that our
across-stack profiling solution provides insights (which are difficult to
discern otherwise) on the characteristics of ML models, ML frameworks, and GPU
hardware.",behavioral profiling
http://arxiv.org/abs/1603.07728v1,"Wang and Castillo have developed empirical parameters for scaling the
temperature profile of the turbulent boundary layer flowing over a heated wall
in the paper X. Wang and L. Castillo, J. Turbul., 4, 1(2003). They presented
experimental data plots that showed similarity type behavior when scaled with
their new scaling parameters. However, what was actually plotted, and what
actually showed similarity type behavior, was not the temperature profile but
the defect profile formed by subtracting the temperature in the boundary layer
from the temperature in the bulk flow. We show that if the same data and same
scaling is replotted as just the scaled temperature profile, similarity is no
longer prevalent. This failure to show both defect profile similarity and
temperature profile similarity is indicative of false similarity. The nature of
this false similarity problem is discussed in detail.",behavioral profiling
http://arxiv.org/abs/1507.06951v4,"Zagarola and Smits developed an empirical velocity parameter for scaling the
outer region of the turbulent boundary layer velocity profile that has been
widely applied to experimental datasets. Plots of the scaled defect profiles
indicate that most datasets display similar-like behavior using the Zagarola
and Smits scaling parameter. In the work herein, it is shown that the common
practice of finding similarity behavior using the defect profile is often
incomplete in the sense that not all of the criteria for similarity have been
checked for compliance. When full compliance is checked, it is found that most
of the datasets which display defect similarity do not satisfy all the criteria
required for similarity. The nature of this contradiction and noncompliance is
described in detail. It is shown that the original datasets used by Zagarola
and Smits display this flawed similarity behavior. Hence, a careful
reassessment of any claims in the literature is required for those groups that
attempted to use the defect profile and the Zagarola and Smits type of velocity
scaling parameter to assert similarity of the velocity profile.",behavioral profiling
http://arxiv.org/abs/1109.1421v1,"The behavior of parallel programs is even harder to understand than the
behavior of sequential programs. Parallel programs may suffer from any of the
performance problems affecting sequential programs, as well as from several
problems unique to parallel systems. Many of these problems are quite hard (or
even practically impossible) to diagnose without help from specialized tools.
We present a proposal for a tool for profiling the parallel execution of
Mercury programs, a proposal whose implementation we have already started. This
tool is an adaptation and extension of the ThreadScope profiler that was first
built to help programmers visualize the execution of parallel Haskell programs.",behavioral profiling
http://arxiv.org/abs/1002.2202v1,"Currently, criminals profile (CP) is obtained from investigators or forensic
psychologists interpretation, linking crime scene characteristics and an
offenders behavior to his or her characteristics and psychological profile.
This paper seeks an efficient and systematic discovery of nonobvious and
valuable patterns between variables from a large database of solved cases via a
probabilistic network (PN) modeling approach. The PN structure can be used to
extract behavioral patterns and to gain insight into what factors influence
these behaviors. Thus, when a new case is being investigated and the profile
variables are unknown because the offender has yet to be identified, the
observed crime scene variables are used to infer the unknown variables based on
their connections in the structure and the corresponding numerical
(probabilistic) weights. The objective is to produce a more systematic and
empirical approach to profiling, and to use the resulting PN model as a
decision tool.",behavioral profiling
http://arxiv.org/abs/1705.09444v1,"Sequential allocation is a simple mechanism for sharing multiple indivisible
items. We study strategic behavior in sequential allocation. In particular, we
consider Nash dynamics, as well as the computation and Pareto optimality of
pure equilibria, and Stackelberg strategies. We first demonstrate that, even
for two agents, better responses can cycle. We then present a linear-time
algorithm that returns a profile (which we call the ""bluff profile"") that is in
pure Nash equilibrium. Interestingly, the outcome of the bluff profile is the
same as that of the truthful profile and the profile is in pure Nash
equilibrium for \emph{all} cardinal utilities consistent with the ordinal
preferences. We show that the outcome of the bluff profile is Pareto optimal
with respect to pairwise comparisons. In contrast, we show that an assignment
may not be Pareto optimal with respect to pairwise comparisons even if it is a
result of a preference profile that is in pure Nash equilibrium for all
utilities consistent with ordinal preferences. Finally, we present a dynamic
program to compute an optimal Stackelberg strategy for two agents, where the
second agent has a constant number of distinct values for the items.",behavioral profiling
http://arxiv.org/abs/1902.02484v1,"IoT devices are increasingly being implicated in cyber-attacks, raising
community concern about the risks they pose to critical infrastructure,
corporations, and citizens. In order to reduce this risk, the IETF is pushing
IoT vendors to develop formal specifications of the intended purpose of their
IoT devices, in the form of a Manufacturer Usage Description (MUD), so that
their network behavior in any operating environment can be locked down and
verified rigorously. This paper aims to assist IoT manufacturers in developing
and verifying MUD profiles, while also helping adopters of these devices to
ensure they are compatible with their organizational policies and track devices
network behavior based on their MUD profile. Our first contribution is to
develop a tool that takes the traffic trace of an arbitrary IoT device as input
and automatically generates the MUD profile for it. We contribute our tool as
open source, apply it to 28 consumer IoT devices, and highlight insights and
challenges encountered in the process. Our second contribution is to apply a
formal semantic framework that not only validates a given MUD profile for
consistency, but also checks its compatibility with a given organizational
policy. We apply our framework to representative organizations and selected
devices, to demonstrate how MUD can reduce the effort needed for IoT acceptance
testing. Finally, we show how operators can dynamically identify IoT devices
using known MUD profiles and monitor their behavioral changes on their network.",behavioral profiling
http://arxiv.org/abs/1310.4399v1,"In this work we present an in-depth analysis of the user behaviors on
different Social Sharing systems. We consider three popular platforms, Flickr,
Delicious and StumbleUpon, and, by combining techniques from social network
analysis with techniques from semantic analysis, we characterize the tagging
behavior as well as the tendency to create friendship relationships of the
users of these platforms. The aim of our investigation is to see if (and how)
the features and goals of a given Social Sharing system reflect on the behavior
of its users and, moreover, if there exists a correlation between the social
and tagging behavior of the users. We report our findings in terms of the
characteristics of user profiles according to three different dimensions: (i)
intensity of user activities, (ii) tag-based characteristics of user profiles,
and (iii) semantic characteristics of user profiles.",behavioral profiling
http://arxiv.org/abs/1506.02289v1,"Matching the profiles of a user across multiple online social networks brings
opportunities for new services and applications as well as new insights on user
online behavior, yet it raises serious privacy concerns. Prior literature has
proposed methods to match profiles and showed that it is possible to do it
accurately, but using evaluations that focused on sampled datasets only. In
this paper, we study the extent to which we can reliably match profiles in
practice, across real-world social networks, by exploiting public attributes,
i.e., information users publicly provide about themselves. Today's social
networks have hundreds of millions of users, which brings completely new
challenges as a reliable matching scheme must identify the correct matching
profile out of the millions of possible profiles. We first define a set of
properties for profile attributes--Availability, Consistency,
non-Impersonability, and Discriminability (ACID)--that are both necessary and
sufficient to determine the reliability of a matching scheme. Using these
properties, we propose a method to evaluate the accuracy of matching schemes in
real practical cases. Our results show that the accuracy in practice is
significantly lower than the one reported in prior literature. When considering
entire social networks, there is a non-negligible number of profiles that
belong to different users but have similar attributes, which leads to many
false matches. Our paper sheds light on the limits of matching profiles in the
real world and illustrates the correct methodology to evaluate matching schemes
in realistic scenarios.",behavioral profiling
http://arxiv.org/abs/1902.09154v1,"With the increasing variety of services that e-commerce platforms provide,
criteria for evaluating their success become also increasingly multi-targeting.
This work introduces a multi-target optimization framework with Bayesian
modeling of the target events, called Deep Bayesian Multi-Target Learning
(DBMTL). In this framework, target events are modeled as forming a Bayesian
network, in which directed links are parameterized by hidden layers, and
learned from training samples. The structure of Bayesian network is determined
by model selection. We applied the framework to Taobao live-streaming
recommendation, to simultaneously optimize (and strike a balance) on targets
including click-through rate, user stay time in live room, purchasing behaviors
and interactions. Significant improvement has been observed for the proposed
method over other MTL frameworks and the non-MTL model. Our practice shows that
with an integrated causality structure, we can effectively make the learning of
a target benefit from other targets, creating significant synergy effects that
improve all targets. The neural network construction guided by DBMTL fits in
with the general probabilistic model connecting features and multiple targets,
taking weaker assumption than the other methods discussed in this paper. This
theoretical generality brings about practical generalization power over various
targets distributions, including sparse targets and continuous-value ones.",behavioral targeting
http://arxiv.org/abs/1805.09436v1,"Dyadic interactions among humans are marked by speakers continuously
influencing and reacting to each other in terms of responses and behaviors,
among others. Understanding how interpersonal dynamics affect behavior is
important for successful treatment in psychotherapy domains. Traditional
schemes that automatically identify behavior for this purpose have often looked
at only the target speaker. In this work, we propose a Markov model of how a
target speaker's behavior is influenced by their own past behavior as well as
their perception of their partner's behavior, based on lexical features. Apart
from incorporating additional potentially useful information, our model can
also control the degree to which the partner affects the target speaker. We
evaluate our proposed model on the task of classifying Negative behavior in
Couples Therapy and show that it is more accurate than the single-speaker
model. Furthermore, we investigate the degree to which the optimal influence
relates to how well a couple does on the long-term, via relating to
relationship outcomes",behavioral targeting
http://arxiv.org/abs/cs/0607143v1,"In this paper we consider and analyze the behavior of two combinational rules
for temporal (sequential) attribute data fusion for target type estimation. Our
comparative analysis is based on Dempster's fusion rule proposed in
Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution
rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We
show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a
very efficient Target Type Tracking and reduces drastically the latency delay
for correct Target Type decision with respect to Demspter's rule. For cases
presenting some short Target Type switches, Demspter's rule is proved to be
unable to detect the switches and thus to track correctly the Target Type
changes. The approach proposed here is totally new, efficient and promising to
be incorporated in real-time Generalized Data Association - Multi Target
Tracking systems (GDA-MTT) and provides an important result on the behavior of
PCR5 with respect to Dempster's rule. The MatLab source code is provided in",behavioral targeting
http://arxiv.org/abs/1303.5903v1,"We identify influential early adopters that achieve a target behavior
distribution for a resource constrained social network with multiple costly
behaviors. This problem is important for applications ranging from collective
behavior change to corporate viral marketing campaigns. In this paper, we
propose a model of diffusion of multiple behaviors when individual participants
have resource constraints. Individuals adopt the set of behaviors that maximize
their utility subject to available resources. We show that the problem of
influence maximization for multiple behaviors is NP-complete. Thus we propose
heuristics, which are based on node degree and expected immediate adoption, to
select early adopters. We evaluate the effectiveness under three metrics:
unique number of participants, total number of active behaviors and network
resource utilization. We also propose heuristics to distribute the behaviors
amongst the early adopters to achieve a target distribution in the population.
We test our approach on synthetic and real-world topologies with excellent
results. Our heuristics produce 15-51\% increase in resource utilization over
the na\""ive approach.",behavioral targeting
http://arxiv.org/abs/1305.3384v1,"In this paper we present a new approach to content-based transfer learning
for solving the data sparsity problem in cases when the users' preferences in
the target domain are either scarce or unavailable, but the necessary
information on the preferences exists in another domain. We show that training
a system to use such information across domains can produce better performance.
Specifically, we represent users' behavior patterns based on topological graph
structures. Each behavior pattern represents the behavior of a set of users,
when the users' behavior is defined as the items they rated and the items'
rating values. In the next step we find a correlation between behavior patterns
in the source domain and behavior patterns in the target domain. This mapping
is considered a bridge between the two domains. Based on the correlation and
content-attributes of the items, we train a machine learning model to predict
users' ratings in the target domain. When we compare our approach to the
popularity approach and KNN-cross-domain on a real world dataset, the results
show that on an average of 83$%$ of the cases our approach outperforms both
methods.",behavioral targeting
http://arxiv.org/abs/1809.00289v1,"In Twitter, there is a rising trend in abusive behavior which often leads to
incivility. This trend is affecting users mentally and as a result they tend to
leave Twitter and other such social networking sites thus depleting the active
user base. In this paper, we study factors associated with incivility. We
observe that the act of incivility is highly correlated with the opinion
differences between the account holder (i.e., the user writing the incivil
tweet) and the target (i.e., the user for whom the incivil tweet is meant for
or targeted), toward a named entity. We introduce a character level CNN model
and incorporate the entity-specific sentiment information for efficient
incivility detection which significantly outperforms multiple baseline methods
achieving an impressive accuracy of 93.3% (4.9% improvement over the best
baseline). In a post-hoc analysis, we also study the behavioral aspects of the
targets and account holders and try to understand the reasons behind the
incivility incidents. Interestingly, we observe that there are strong signals
of repetitions in incivil behavior. In particular, we find that there are a
significant fraction of account holders who act as repeat offenders - attacking
the targets even more than 10 times. Similarly, there are also targets who get
targeted multiple times. In general, the targets are found to have higher
reputation scores than the account holders.",behavioral targeting
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",behavioral targeting
http://arxiv.org/abs/1906.08025v1,"This paper describes a software-based tool that tracks mobile node roaming
and infers the time-to-handover as well as the preferential handover target,
based on behavior inference solely derived from regular usage data captured in
visited wireless networks. The paper presents the tool architecture;
computational background for mobility estimation; operational guidelines
concerning how the tool is being used to track several aspects of roaming
behavior in the context of wireless networks. Target selection accuracy is
validated having as baseline traces obtained in realistic scenarios.",behavioral targeting
http://arxiv.org/abs/1808.01075v1,"In the modern e-commerce, the behaviors of customers contain rich
information, e.g., consumption habits, the dynamics of preferences. Recently,
session-based recommendations are becoming popular to explore the temporal
characteristics of customers' interactive behaviors. However, existing works
mainly exploit the short-term behaviors without fully taking the customers'
long-term stable preferences and evolutions into account. In this paper, we
propose a novel Behavior-Intensive Neural Network (BINN) for next-item
recommendation by incorporating both users' historical stable preferences and
present consumption motivations. Specifically, BINN contains two main
components, i.e., Neural Item Embedding, and Discriminative Behaviors Learning.
Firstly, a novel item embedding method based on user interactions is developed
for obtaining an unified representation for each item. Then, with the embedded
items and the interactive behaviors over item sequences, BINN discriminatively
learns the historical preferences and present motivations of the target users.
Thus, BINN could better perform recommendations of the next items for the
target users. Finally, for evaluating the performances of BINN, we conduct
extensive experiments on two real-world datasets, i.e., Tianchi and JD. The
experimental results clearly demonstrate the effectiveness of BINN compared
with several state-of-the-art methods.",behavioral targeting
http://arxiv.org/abs/1809.08793v1,"This paper addresses a novel architecture for person-following robots using
active search. The proposed system can be applied in real-time to general
mobile robots for learning features of a human, detecting and tracking, and
finally navigating towards that person. To succeed at person-following,
perception, planning, and robot behavior need to be integrated properly. Toward
this end, an active target searching capability, including prediction and
navigation toward vantage locations for finding human targets, is proposed. The
proposed capability aims at improving the robustness and efficiency for
tracking and following people under dynamic conditions such as crowded
environments. A multi-modal sensor information approach including fusing an
RGB-D sensor and a laser scanner, is pursued to robustly track and identify
human targets. Bayesian filtering for keeping track of human and a regression
algorithm to predict the trajectory of people are investigated. In order to
make the robot autonomous, the proposed framework relies on a behavior-tree
structure. Using Toyota Human Support Robot (HSR), real-time experiments
demonstrate that the proposed architecture can generate fast, efficient
person-following behaviors.",behavioral targeting
http://arxiv.org/abs/1503.08048v1,"We consider the effect of inducement to vaccinate during the spread of an
infectious disease on complex networks. Suppose that public resources are
finite and that only a small proportion of individuals can be vaccinated freely
(complete subsidy), for the remainder of the population vaccination is a
voluntary behavior --- and each vaccinated individual carries a perceived cost.
We ask whether the classical targeted subsidy strategy is definitely better
than the random strategy: does targeting subsidy at individuals perceived to be
with the greatest risk actually help? With these questions, we propose a model
to investigate the \emph{interaction effects} of the subsidy policies and
individuals responses when facing subsidy policies on the epidemic dynamics on
complex networks. In the model, a small proportion of individuals are freely
vaccinated according to either the targeted or random subsidy policy, the
remainder choose to vaccinate (or not) based on voluntary principle and update
their vaccination decision via an imitation rule. Our findings show that the
targeted strategy is only advantageous when individuals prefer to imitate the
subsidized individuals' strategy. Otherwise, the effect of the targeted policy
is worse than the random immunization, since individuals preferentially select
non-subsidized individuals as the imitation objects. More importantly, we find
that under the targeted subsidy policy, increasing the proportion of subsidized
individuals may increase the final epidemic size. We further define social cost
as the sum of the costs of vaccination and infection, and study how each of the
two policies affect the social cost. Our result shows that there exist some
optimal intermediate regions leading to the minimal social cost.",behavioral targeting
http://arxiv.org/abs/1607.07647v1,"We propose a method for tracking an unknown number of targets based on
measurements provided by multiple sensors. Our method achieves low
computational complexity and excellent scalability by running belief
propagation on a suitably devised factor graph. A redundant formulation of data
association uncertainty and the use of ""augmented target states"" including
binary target indicators make it possible to exploit statistical independencies
for a drastic reduction of complexity. An increase in the number of targets,
sensors, or measurements leads to additional variable nodes in the factor graph
but not to higher dimensions of the messages. As a consequence, the complexity
of our method scales only quadratically in the number of targets, linearly in
the number of sensors, and linearly in the number of measurements per sensors.
The performance of the method compares well with that of previously proposed
methods, including methods with a less favorable scaling behavior. In
particular, our method can outperform multisensor versions of the probability
hypothesis density (PHD) filter, the cardinalized PHD filter, and the
multi-Bernoulli filter.",behavioral targeting
http://arxiv.org/abs/1212.4305v1,"The two-frequency problem of synchronization of the pulse train of a
passively mode locked soliton laser to an externally injected pulse train is
solved in the weak injection regime. The source and target frequency combs are
distinguished by the spacing and offset frequency mismatches. Locking diagrams
map the domain in the mismatch parameter space where stable locking of the
combs is possible. We analyze the dependence of the locking behavior on the
relative frequency and chirp of the source and target pulses, and the
conditions where the relative offset frequency has to be actively stabilized.
Locked steady states are characterized by a fixed source-target time and phase
shifts that map the locking domain.",behavioral targeting
http://arxiv.org/abs/1908.03597v1,"Gene regulation is one of the most important fundamental biological processes
in living cells. It involves multiple protein molecules that locate specific
sites on DNA and assemble gene initiation or gene repression multi-molecular
complexes. While the protein search dynamics for DNA targets has been
intensively investigated, the role of inter-molecular interactions during the
genetic activation or repression remains not well quantified. Here we present a
simple one-dimensional model of target search for two interacting molecules
that can reversibly form a dimer molecular complex, which also participates in
the search process. In addition, the proteins have finite residence times on
specific target sites, and the gene is activated or repressed when both
proteins are simultaneously present at the target. The model is analyzed using
first-passage analytical calculations and Monte Carlo computer simulations. It
is shown that the search dynamics exhibits a complex behavior depending on the
strength of inter-molecular interactions and on the target residence times. We
also found that the search time shows a non-monotonic behavior as a function of
the dissociation rate for the molecular complex. Physical-chemical arguments to
explain these observations are presented. Our theoretical approach highlights
the importance of molecular interactions in the complex process of gene
activation/repression by multiple transcription factor proteins.",behavioral targeting
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",targeted advertising
http://arxiv.org/abs/1907.01862v2,"Being able to check whether an online advertisement has been targeted is
essential for resolving privacy controversies and implementing in practice data
protection regulations like GDPR, CCPA, and COPPA. In this paper we describe
the design, implementation, and deployment of an advertisement auditing system
called iWnder that uses crowdsourcing to reveal in real time whether a display
advertisement has been targeted or not. Crowdsourcing simplifies the detection
of targeted advertising, but requires reporting to a central repository the
impressions seen by different users, thereby jeopardising their privacy. We
break this deadlock with a privacy preserving data sharing protocol that allows
iWnder to compute global statistics required to detect targeting, while keeping
the advertisements seen by individual users and their browsing history private.
We conduct a simulation study to explore the effect of different parameters and
a live validation to demonstrate the accuracy of our approach. Unlike previous
solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns
that promote a product or service whose description bears no semantic overlap
with its targeted audience.",targeted advertising
http://arxiv.org/abs/1407.3338v1,"We undertake a formal study of the value of targeting data to an advertiser.
As expected, this value is increasing in the utility difference between
realizations of the targeting data and the accuracy of the data, and depends on
the distribution of competing bids. However, this value may vary
non-monotonically with an advertiser's budget. Similarly, modeling the values
as either private or correlated, or allowing other advertisers to also make use
of the data, leads to unpredictable changes in the value of data. We address
questions related to multiple data sources, show that utility of additional
data may be non-monotonic, and provide tradeoffs between the quality and the
price of data sources. In a game-theoretic setting, we show that advertisers
may be worse off than if the data had not been available at all. We also ask
whether a publisher can infer the value an advertiser would place on targeting
data from the advertiser's bidding behavior and illustrate that this is
impossible.",targeted advertising
http://arxiv.org/abs/1510.04031v1,"In the last decade, the advertisement market spread significantly in the web
and mobile app system. Its effectiveness is also due thanks to the possibility
to target the advertisement on the specific interests of the actual user, other
than on the content of the website hosting the advertisement. In this scenario,
became of great value services that collect and hence can provide information
about the browsing user, like Facebook and Google. In this paper, we show how
to maliciously exploit the Google Targeted Advertising system to infer personal
information in Google user profiles. In particular, the attack we consider is
external from Google and relies on combining data from Google AdWords with
other data collected from a website of the Google Display Network. We validate
the effectiveness of our proposed attack, also discussing possible application
scenarios. The result of our research shows a significant practical privacy
issue behind such type of targeted advertising service, and call for further
investigation and the design of more privacy-aware solutions, possibly without
impeding the current business model involved in online advertisement.",targeted advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",targeted advertising
http://arxiv.org/abs/1508.03080v1,"We study how privacy technologies affect user and advertiser behavior in a
simple economic model of targeted advertising. In our model, a consumer first
decides whether or not to buy a good, and then an advertiser chooses an
advertisement to show the consumer. The consumer's value for the good is
correlated with her type, which determines which ad the advertiser would prefer
to show to her---and hence, the advertiser would like to use information about
the consumer's purchase decision to target the ad that he shows.
  In our model, the advertiser is given only a differentially private signal
about the consumer's behavior---which can range from no signal at all to a
perfect signal, as we vary the differential privacy parameter. This allows us
to study equilibrium behavior as a function of the level of privacy provided to
the consumer. We show that this behavior can be highly counter-intuitive, and
that the effect of adding privacy in equilibrium can be completely different
from what we would expect if we ignored equilibrium incentives. Specifically,
we show that increasing the level of privacy can actually increase the amount
of information about the consumer's type contained in the signal the advertiser
receives, lead to decreased utility for the consumer, and increased profit for
the advertiser, and that generally these quantities can be non-monotonic and
even discontinuous in the privacy level of the signal.",targeted advertising
http://arxiv.org/abs/1703.02091v4,"Taobao, as the largest online retail platform in the world, provides billions
of online display advertising impressions for millions of advertisers every
day. For commercial purposes, the advertisers bid for specific spots and target
crowds to compete for business traffic. The platform chooses the most suitable
ads to display in tens of milliseconds. Common pricing methods include cost per
mille (CPM) and cost per click (CPC). Traditional advertising systems target
certain traits of users and ad placements with fixed bids, essentially regarded
as coarse-grained matching of bid and traffic quality. However, the fixed bids
set by the advertisers competing for different quality requests cannot fully
optimize the advertisers' key requirements. Moreover, the platform has to be
responsible for the business revenue and user experience. Thus, we proposed a
bid optimizing strategy called optimized cost per click (OCPC) which
automatically adjusts the bid to achieve finer matching of bid and traffic
quality of page view (PV) request granularity. Our approach optimizes
advertisers' demands, platform business revenue and user experience and as a
whole improves traffic allocation efficiency. We have validated our approach in
Taobao display advertising system in production. The online A/B test shows our
algorithm yields substantially better results than previous fixed bid manner.",targeted advertising
http://arxiv.org/abs/1411.5281v3,"Online Behavioural targeted Advertising (OBA) has risen in prominence as a
method to increase the effectiveness of online advertising. OBA operates by
associating tags or labels to users based on their online activity and then
using these labels to target them. This rise has been accompanied by privacy
concerns from researchers, regulators and the press. In this paper, we present
a novel methodology for measuring and understanding OBA in the online
advertising market. We rely on training artificial online personas representing
behavioural traits like 'cooking', 'movies', 'motor sports', etc. and build a
measurement system that is automated, scalable and supports testing of multiple
configurations. We observe that OBA is a frequent practice and notice that
categories valued more by advertisers are more intensely targeted. In addition,
we provide evidences showing that the advertising market targets sensitive
topics (e.g, religion or health) despite the existence of regulation that bans
such practices. We also compare the volume of OBA advertising for our personas
in two different geographical locations (US and Spain) and see little
geographic bias in terms of intensity of OBA targeting. Finally, we check for
targeting with do-not-track (DNT) enabled and discovered that DNT is not yet
enforced in the web.",targeted advertising
http://arxiv.org/abs/1909.02156v1,"Interactions between bids to show ads online can lead to an advertiser's ad
being shown to more men than women even when the advertiser does not target
towards men. We design bidding strategies that advertisers can use to avoid
such emergent discrimination without having to modify the auction mechanism. We
mathematically analyze the strategies to determine the additional cost to the
advertiser for avoiding discrimination, proving our strategies to be optimal in
some settings. We use simulations to understand other settings.",targeted advertising
http://arxiv.org/abs/1603.07768v1,"We study the online budgeted allocation (also called ADWORDS) problem, where
a set of impressions arriving online are allocated to a set of
budget-constrained advertisers to maximize revenue. Motivated by connections to
Internet advertising, several variants of this problem have been studied since
the seminal work of Mehta, Saberi, Vazirani, and Vazirani (FOCS 2005). However,
this entire body of work focuses on a single budget for every advertising
campaign, whereas in order to fully represent the actual agenda of an
advertiser, an advertising budget should be expressible over multiple tiers of
user-attribute granularity. A simple example is an advertising campaign that is
constrained by an overall budget but is also accompanied by a set of
sub-budgets for each target demographic. In such a contract scheme, an
advertiser can specify their true user-targeting goals, allowing the publisher
to fulfill them through relevant allocations.
  In this paper, we give a complete characterization of the ADWORDS problem for
general advertising budgets. In the most general setting, we show that, unlike
in the single-budget ADWORDS problem, obtaining a constant competitive ratio is
impossible and give asymptotically tight upper and lower bounds. However for
our main result, we observe that in many real-world scenarios (as in the above
example), multi-tier budgets have a laminar structure, since most relevant
consumer or product classifications are hierarchical. For laminar budgets, we
obtain a competitive ratio of e/(e-1) in the small bids case, which matches the
best known ADWORDS result for single budgets. Our algorithm has a primal-dual
structure and generalizes the primal-dual analysis for single- budget ADWORDS
first given by Buchbinder, Jain, and Naor (ESA 2007).",targeted advertising
http://arxiv.org/abs/1907.12118v1,"Sponsored search has more than 20 years of history, and it has been proven to
be a successful business model for online advertising. Based on the
pay-per-click pricing model and the keyword targeting technology, the sponsored
system runs online auctions to determine the allocations and prices of search
advertisements. In the traditional setting, advertisers should manually create
lots of ad creatives and bid on some relevant keywords to target their
audience. Due to the huge amount of search traffic and a wide variety of ad
creations, the limits of manual optimizations from advertisers become the main
bottleneck for improving the efficiency of this market. Moreover, as many
emerging advertising forms and supplies are growing, it's crucial for sponsored
search platform to pay more attention to the ROI metrics of ads for getting the
marketing budgets of advertisers. In this paper, we present the AiAds system
developed at Baidu, which use machine learning techniques to build an automated
and intelligent advertising system. By designing and implementing the automated
bidding strategy, the intelligent targeting and the intelligent creation
models, the AiAds system can transform the manual optimizations into multiple
automated tasks and optimize these tasks in advanced methods. AiAds is a
brand-new architecture of sponsored search system which changes the bidding
language and allocation mechanism, breaks the limit of keyword targeting with
end-to-end ad retrieval framework and provides global optimization of ad
creation. This system can increase the advertiser's campaign performance, the
user experience and the revenue of the advertising platform simultaneously and
significantly. We present the overall architecture and modeling techniques for
each module of the system and share our lessons learned in solving several key
challenges.",targeted advertising
http://arxiv.org/abs/1808.09218v4,"Targeted advertising is meant to improve the efficiency of matching
advertisers to their customers. However, targeted advertising can also be
abused by malicious advertisers to efficiently reach people susceptible to
false stories, stoke grievances, and incite social conflict. Since targeted ads
are not seen by non-targeted and non-vulnerable people, malicious ads are
likely to go unreported and their effects undetected. This work examines a
specific case of malicious advertising, exploring the extent to which political
ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
elections exploited Facebook's targeted advertising infrastructure to
efficiently target ads on divisive or polarizing topics (e.g., immigration,
race-based policing) at vulnerable sub-populations. In particular, we do the
following: (a) We conduct U.S. census-representative surveys to characterize
how users with different political ideologies report, approve, and perceive
truth in the content of the IRA ads. Our surveys show that many ads are
""divisive"": they elicit very different reactions from people belonging to
different socially salient groups. (b) We characterize how these divisive ads
are targeted to sub-populations that feel particularly aggrieved by the status
quo. Our findings support existing calls for greater transparency of content
and targeting of political ads. (c) We particularly focus on how the Facebook
ad API facilitates such targeting. We show how the enormous amount of personal
data Facebook aggregates about users and makes available to advertisers enables
such malicious targeting.",targeted advertising
http://arxiv.org/abs/1907.02178v3,"Firms implementing digital advertising campaigns face a complex problem in
determining the right match between their advertising creatives and target
audiences. Typical solutions to the problem have leveraged non-experimental
methods, or used ""split-testing"" strategies that have not explicitly addressed
the complexities induced by targeted audiences that can potentially overlap
with one another. This paper presents an adaptive algorithm that addresses the
problem via online experimentation. The algorithm is set up as a contextual
bandit and addresses the overlap issue by partitioning the target audiences
into disjoint, non-overlapping sub-populations. It learns an optimal creative
display policy in the disjoint space, while assessing in parallel which
creative has the best match in the space of possibly overlapping target
audiences. Experiments show that the proposed method is more efficient compared
to naive ""split-testing"" or non-adaptive ""A/B/n"" testing based methods. We also
describe a testing product we built that uses the algorithm. The product is
currently deployed on the advertising platform of JD.com, an eCommerce company
and a publisher of digital ads in China.",targeted advertising
http://arxiv.org/abs/1206.1754v2,"Internet advertising is a fast growing business which has proved to be
significantly important in digital economics. It is vitally important for both
web search engines and online content providers and publishers because web
advertising provides them with major sources of revenue. Its presence is
increasingly important for the whole media industry due to the influence of the
Web. For advertisers, it is a smarter alternative to traditional marketing
media such as TVs and newspapers. As the web evolves and data collection
continues, the design of methods for more targeted, interactive, and friendly
advertising may have a major impact on the way our digital economy evolves, and
to aid societal development.
  Towards this goal mathematically well-grounded Computational Advertising
methods are becoming necessary and will continue to develop as a fundamental
tool towards the Web. As a vibrant new discipline, Internet advertising
requires effort from different research domains including Information
Retrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,
and even Psychology to predict and understand user behaviours. In this paper,
we provide a comprehensive survey on Internet advertising, discussing and
classifying the research issues, identifying the recent technologies, and
suggesting its future directions. To have a comprehensive picture, we first
start with a brief history, introduction, and classification of the industry
and present a schematic view of the new advertising ecosystem. We then
introduce four major participants, namely advertisers, online publishers, ad
exchanges and web users; and through analysing and discussing the major
research problems and existing solutions from their perspectives respectively,
we discover and aggregate the fundamental problems that characterise the
newly-formed research field and capture its potential future prospects.",targeted advertising
http://arxiv.org/abs/1902.02429v1,"Vehicle service providers can display commercial ads in their vehicles based
on passengers' origins and destinations to create a new revenue stream. In this
work, we study a vehicle service provider who can generate different ad
revenues when displaying ads on different arcs (i.e., origin-destination
pairs). The provider needs to ensure the vehicle flow balance at each location,
which makes it challenging to analyze the provider's vehicle assignment and
pricing decisions for different arcs. For example, the provider's price for its
service on an arc depends on the ad revenues on other arcs as well as on the
arc in question. To tackle the problem, we show that the traffic network
corresponds to an electrical network. When the effective resistance between two
locations is small, there are many paths between the two locations and the
provider can easily route vehicles between them. We characterize the dependence
of an arc's optimal price on any other arc's ad revenue using the effective
resistances between these two arcs' origins and destinations. Furthermore, we
study the provider's optimal selection of advertisers when it can only display
ads for a limited number of advertisers. If each advertiser has one target arc
for advertising, the provider should display ads for the advertiser whose
target arc has a small effective resistance. We investigate the performance of
our advertiser selection strategy based on a real-world dataset.",targeted advertising
http://arxiv.org/abs/1711.11175v1,"In online advertising, our aim is to match the advertisers with the most
relevant users to optimize the campaign performance. In the pursuit of
achieving this goal, multiple data sources provided by the advertisers or
third-party data providers are utilized to choose the set of users according to
the advertisers' targeting criteria. In this paper, we present a framework that
can be applied to assess the quality of such data sources in large scale. This
framework efficiently evaluates the similarity of a specific data source
categorization to that of the ground truth, especially for those cases when the
ground truth is accessible only in aggregate, and the user-level information is
anonymized or unavailable due to privacy reasons. We propose multiple
methodologies within this framework, present some preliminary assessment
results, and evaluate how the methodologies compare to each other. We also
present two use cases where we can utilize the data quality assessment results:
the first use case is targeting specific user categories, and the second one is
forecasting the desirable audiences we can reach for an online advertising
campaign with pre-set targeting criteria.",targeted advertising
http://arxiv.org/abs/1101.3400v1,"We present a new algorithm for behavioral targeting of banner advertisements.
We record different user's actions such as clicks, search queries and page
views. We use the collected information on the user to estimate in real time
the probability of a click on a banner. A banner is displayed if it either has
the highest probability of being clicked or if it is the one that generates the
highest average profit.",targeted advertising
http://arxiv.org/abs/1112.5396v2,"With more than four billion usage of cellular phones worldwide, mobile
advertising has become an attractive alternative to online advertisements. In
this paper, we propose a new targeted advertising policy for Wireless Service
Providers (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSP
charges the advertisers for showing their ads. Each advertiser has a valuation
for specific types of customers in various times and locations and has a limit
on the maximum available budget. Each query is in the form of time and location
and is associated with one individual customer. In order to achieve a
non-intrusive delivery, only a limited number of ads can be sent to each
customer. Recently, new services have been introduced that offer location-based
advertising over cellular network that fit in our model (e.g., ShopAlerts by
AT&T) .
  We consider both online and offline version of the AdCell problem and develop
approximation algorithms with constant competitive ratio. For the online
version, we assume that the appearances of the queries follow a stochastic
distribution and thus consider a Bayesian setting. Furthermore, queries may
come from different distributions on different times. This model generalizes
several previous advertising models such as online secretary problem
\cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords
\cite{saberi05}. ...",targeted advertising
http://arxiv.org/abs/1305.3014v1,"Online advertising has been introduced as one of the most efficient methods
of advertising throughout the recent years. Yet, advertisers are concerned
about the efficiency of their online advertising campaigns and consequently,
would like to restrict their ad impressions to certain websites and/or certain
groups of audience. These restrictions, known as targeting criteria, limit the
reachability for better performance. This trade-off between reachability and
performance illustrates a need for a forecasting system that can quickly
predict/estimate (with good accuracy) this trade-off. Designing such a system
is challenging due to (a) the huge amount of data to process, and, (b) the need
for fast and accurate estimates. In this paper, we propose a distributed fault
tolerant system that can generate such estimates fast with good accuracy. The
main idea is to keep a small representative sample in memory across multiple
machines and formulate the forecasting problem as queries against the sample.
The key challenge is to find the best strata across the past data, perform
multivariate stratified sampling while ensuring fuzzy fall-back to cover the
small minorities. Our results show a significant improvement over the uniform
and simple stratified sampling strategies which are currently widely used in
the industry.",targeted advertising
http://arxiv.org/abs/1502.06657v1,"Budget allocation in online advertising deals with distributing the campaign
(insertion order) level budgets to different sub-campaigns which employ
different targeting criteria and may perform differently in terms of
return-on-investment (ROI). In this paper, we present the efforts at Turn on
how to best allocate campaign budget so that the advertiser or campaign-level
ROI is maximized. To do this, it is crucial to be able to correctly determine
the performance of sub-campaigns. This determination is highly related to the
action-attribution problem, i.e. to be able to find out the set of ads, and
hence the sub-campaigns that provided them to a user, that an action should be
attributed to. For this purpose, we employ both last-touch (last ad gets all
credit) and multi-touch (many ads share the credit) attribution methodologies.
We present the algorithms deployed at Turn for the attribution problem, as well
as their parallel implementation on the large advertiser performance datasets.
We conclude the paper with our empirical comparison of last-touch and
multi-touch attribution-based budget allocation in a real online advertising
setting.",targeted advertising
http://arxiv.org/abs/1909.13221v2,"Online advertising in E-commerce platforms provides sellers an opportunity to
achieve potential audiences with different target goals. Ad serving systems
(like display and search advertising systems) that assign ads to pages should
satisfy objectives such as plenty of audience for branding advertisers, clicks
or conversions for performance-based advertisers, at the same time try to
maximize overall revenue of the platform. In this paper, we propose an approach
based on linear programming subjects to constraints in order to optimize the
revenue and improve different performance goals simultaneously. We have
validated our algorithm by implementing an offline simulation system in Alibaba
E-commerce platform and running the auctions from online requests which takes
system performance, ranking and pricing schemas into account. We have also
compared our algorithm with related work, and the results show that our
algorithm can effectively improve campaign performance and revenue of the
platform.",targeted advertising
http://arxiv.org/abs/1910.02358v1,"Assessing aesthetic preference is a fundamental task related to human
cognition. It can also contribute to various practical applications such as
image creation for online advertisements. Despite crucial influences of image
quality, auxiliary information of ad images such as tags and target subjects
can also determine image preference. Existing studies mainly focus on images
and thus are less useful for advertisement scenarios where rich auxiliary data
are available. Here we propose a modality fusion-based neural network that
evaluates the aesthetic preference of images with auxiliary information. Our
method fully utilizes auxiliary data by introducing multi-step modality fusion
using both conditional batch normalization-based low-level and attention-based
high-level fusion mechanisms, inspired by the findings from statistical
analyses on real advertisement data. Our approach achieved state-of-the-art
performance on the AVA dataset, a widely used dataset for aesthetic assessment.
Besides, the proposed method is evaluated on large-scale real-world
advertisement image data with rich auxiliary attributes, providing promising
preference prediction results. Through extensive experiments, we investigate
how image and auxiliary information together influence click-through rate.",targeted advertising
http://arxiv.org/abs/1904.02095v5,"The enormous financial success of online advertising platforms is partially
due to the precise targeting features they offer. Although researchers and
journalists have found many ways that advertisers can target---or
exclude---particular groups of users seeing their ads, comparatively little
attention has been paid to the implications of the platform's ad delivery
process, comprised of the platform's choices about which users see which ads.
  It has been hypothesized that this process can ""skew"" ad delivery in ways
that the advertisers do not intend, making some users less likely than others
to see particular ads based on their demographic characteristics. In this
paper, we demonstrate that such skewed delivery occurs on Facebook, due to
market and financial optimization effects as well as the platform's own
predictions about the ""relevance"" of ads to different groups of users. We find
that both the advertiser's budget and the content of the ad each significantly
contribute to the skew of Facebook's ad delivery. Critically, we observe
significant skew in delivery along gender and racial lines for ""real"" ads for
employment and housing opportunities despite neutral targeting parameters.
  Our results demonstrate previously unknown mechanisms that can lead to
potentially discriminatory ad delivery, even when advertisers set their
targeting parameters to be highly inclusive. This underscores the need for
policymakers and platforms to carefully consider the role of the ad delivery
optimization run by ad platforms themselves---and not just the targeting
choices of advertisers---in preventing discrimination in digital advertising.",targeted advertising
http://arxiv.org/abs/1606.07189v1,"As one of the leading platforms for creative content, Tumblr offers
advertisers a unique way of creating brand identity. Advertisers can tell their
story through images, animation, text, music, video, and more, and promote that
content by sponsoring it to appear as an advertisement in the streams of Tumblr
users. In this paper we present a framework that enabled one of the key
targeted advertising components for Tumblr, specifically gender and interest
targeting. We describe the main challenges involved in development of the
framework, which include creating the ground truth for training gender
prediction models, as well as mapping Tumblr content to an interest taxonomy.
For purposes of inferring user interests we propose a novel semi-supervised
neural language model for categorization of Tumblr content (i.e., post tags and
post keywords). The model was trained on a large-scale data set consisting of
6.8 billion user posts, with very limited amount of categorized keywords, and
was shown to have superior performance over the bag-of-words model. We
successfully deployed gender and interest targeting capability in Yahoo
production systems, delivering inference for users that cover more than 90% of
daily activities at Tumblr. Online performance results indicate advantages of
the proposed approach, where we observed 20% lift in user engagement with
sponsored posts as compared to untargeted campaigns.",targeted advertising
http://arxiv.org/abs/1001.2735v4,"Internet advertising is a sophisticated game in which the many advertisers
""play"" to optimize their return on investment. There are many ""targets"" for the
advertisements, and each ""target"" has a collection of games with a potentially
different set of players involved. In this paper, we study the problem of how
advertisers allocate their budget across these ""targets"". In particular, we
focus on formulating their best response strategy as an optimization problem.
Advertisers have a set of keywords (""targets"") and some stochastic information
about the future, namely a probability distribution over scenarios of cost vs
click combinations. This summarizes the potential states of the world assuming
that the strategies of other players are fixed. Then, the best response can be
abstracted as stochastic budget optimization problems to figure out how to
spread a given budget across these keywords to maximize the expected number of
clicks.
  We present the first known non-trivial poly-logarithmic approximation for
these problems as well as the first known hardness results of getting better
than logarithmic approximation ratios in the various parameters involved. We
also identify several special cases of these problems of practical interest,
such as with fixed number of scenarios or with polynomial-sized parameters
related to cost, which are solvable either in polynomial time or with improved
approximation ratios. Stochastic budget optimization with scenarios has
sophisticated technical structure. Our approximation and hardness results come
from relating these problems to a special type of (0/1, bipartite) quadratic
programs inherent in them. Our research answers some open problems raised by
the authors in (Stochastic Models for Budget Optimization in Search-Based
Advertising, Algorithmica, 58 (4), 1022-1044, 2010).",targeted advertising
http://arxiv.org/abs/1601.02377v1,"User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.",targeted advertising
http://arxiv.org/abs/1610.03013v2,"The most significant progress in recent years in online display advertising
is what is known as the Real-Time Bidding (RTB) mechanism to buy and sell ads.
RTB essentially facilitates buying an individual ad impression in real time
while it is still being generated from a user's visit. RTB not only scales up
the buying process by aggregating a large amount of available inventories
across publishers but, most importantly, enables direct targeting of individual
users. As such, RTB has fundamentally changed the landscape of digital
marketing. Scientifically, the demand for automation, integration and
optimisation in RTB also brings new research opportunities in information
retrieval, data mining, machine learning and other related fields. In this
monograph, an overview is given of the fundamental infrastructure, algorithms,
and technical solutions of this new frontier of computational advertising. The
covered topics include user response prediction, bid landscape forecasting,
bidding algorithms, revenue optimisation, statistical arbitrage, dynamic
pricing, and ad fraud detection.",targeted advertising
http://arxiv.org/abs/1109.0097v1,"Recent work in traffic analysis has shown that traffic patterns leaked
through side channels can be used to recover important semantic information.
For instance, attackers can find out which website, or which page on a website,
a user is accessing simply by monitoring the packet size distribution. We show
that traffic analysis is even a greater threat to privacy than previously
thought by introducing a new attack that can be carried out remotely. In
particular, we show that, to perform traffic analysis, adversaries do not need
to directly observe the traffic patterns. Instead, they can gain sufficient
information by sending probes from a far-off vantage point that exploits a
queuing side channel in routers. To demonstrate the threat of such remote
traffic analysis, we study a remote website detection attack that works against
home broadband users. Because the remotely observed traffic patterns are more
noisy than those obtained using previous schemes based on direct local traffic
monitoring, we take a dynamic time warping (DTW) based approach to detecting
fingerprints from the same website. As a new twist on website fingerprinting,
we consider a website detection attack, where the attacker aims to find out
whether a user browses a particular web site, and its privacy implications. We
show experimentally that, although the success of the attack is highly
variable, depending on the target site, for some sites very low error rates. We
also show how such website detection can be used to deanonymize message board
users.",website monitoring
http://arxiv.org/abs/1802.05409v1,"Traffic analysis attacks to identify which web page a client is browsing,
using only her packet metadata --- known as website fingerprinting --- has been
proven effective in closed-world experiments against privacy technologies like
Tor. However, due to the base rate fallacy, these attacks have failed in large
open-world settings against clients that visit sensitive pages with a low base
rate. We find that this is because they have poor precision as they were
designed to maximize recall.
  In this work, we argue that precision is more important than recall for
open-world website fingerprinting. For this reason, we develop three classes of
{\em precision optimizers}, based on confidence, distance, and ensemble
learning, that can be applied to any classifier to increase precision. We test
them on known website fingerprinting attacks and show significant improvements
in precision. Against a difficult scenario, where the attacker wants to monitor
and distinguish 100 sensitive pages each with a low mean base rate of 0.00001,
our best optimized classifier can achieve a precision of 0.78; the highest
precision of any known attack before optimization was 0.014. We use precise
classifiers to tackle realistic objectives in website fingerprinting, including
selection, identification, and defeating website fingerprinting defenses.",website monitoring
http://arxiv.org/abs/1509.00789v3,"Website fingerprinting enables an attacker to infer which web page a client
is browsing through encrypted or anonymized network connections. We present a
new website fingerprinting technique based on random decision forests and
evaluate performance over standard web pages as well as Tor hidden services, on
a larger scale than previous works. Our technique, k-fingerprinting, performs
better than current state-of-the-art attacks even against website
fingerprinting defenses, and we show that it is possible to launch a website
fingerprinting attack in the face of a large amount of noisy data. We can
correctly determine which of 30 monitored hidden services a client is visiting
with 85% true positive rate (TPR), a false positive rate (FPR) as low as 0.02%,
from a world size of 100,000 unmonitored web pages. We further show that error
rates vary widely between web resources, and thus some patterns of use will be
predictably more vulnerable to attack than others.",website monitoring
http://arxiv.org/abs/1908.02548v1,"The automated detection of corrosion from images (i.e., photographs) or video
(i.e., drone footage) presents significant advantages in terms of corrosion
monitoring. Such advantages include access to remote locations, mitigation of
risk to inspectors, cost savings and monitoring speed. The automated detection
of corrosion requires deep learning to approach human level artificial
intelligence (A.I.). The training of a deep learning model requires intensive
image labelling, and in order to generate a large database of labelled images,
crowd sourced labelling via a dedicated website was sought. The website
(corrosiondetector.com) permits any user to label images, with such labelling
then contributing to the training of a cloud based A.I. model - with such a
cloud-based model then capable of assessing any fresh (or uploaded) image for
the presence of corrosion. In other words, the website includes both the crowd
sourced training process, but also the end use of the evolving model. Herein,
the results and findings from the website (corrosiondetector.com) over the
period of approximately one month, are reported.",website monitoring
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website monitoring
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website monitoring
http://arxiv.org/abs/1601.07077v1,"Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.",website monitoring
http://arxiv.org/abs/1705.04437v1,"The browser history reveals highly sensitive information about users, such as
financial status, health conditions, or political views. Private browsing modes
and anonymity networks are consequently important tools to preserve the privacy
not only of regular users but in particular of whistleblowers and dissidents.
Yet, in this work we show how a malicious application can infer opened websites
from Google Chrome in Incognito mode and from Tor Browser by exploiting
hardware performance events (HPEs). In particular, we analyze the browsers'
microarchitectural footprint with the help of advanced Machine Learning
techniques: k-th Nearest Neighbors, Decision Trees, Support Vector Machines,
and in contrast to previous literature also Convolutional Neural Networks. We
profile 40 different websites, 30 of the top Alexa sites and 10 whistleblowing
portals, on two machines featuring an Intel and an ARM processor. By monitoring
retired instructions, cache accesses, and bus cycles for at most 5 seconds, we
manage to classify the selected websites with a success rate of up to 86.3%.
The results show that hardware performance events can clearly undermine the
privacy of web users. We therefore propose mitigation strategies that impede
our attacks and still allow legitimate use of HPEs.",website monitoring
http://arxiv.org/abs/1612.05318v1,"CUORE is a cryogenic experiment searching primarily for neutrinoless double
decay in $^{130}$Te. It will begin data-taking operations in 2016. To monitor
the cryostat and detector during commissioning and data taking, we have
designed and developed Slow Monitoring systems. In addition to real-time
systems using LabVIEW, we have an alarm, analysis, and archiving website that
uses MongoDB, AngularJS, and Bootstrap software. These modern, state of the art
software packages make the monitoring system transparent, easily maintainable,
and accessible on many platforms including mobile devices.",website monitoring
http://arxiv.org/abs/1507.06562v1,"As of February, 2015, HTTP/2, the update to the 16-year-old HTTP 1.1, is
officially complete. HTTP/2 aims to improve the Web experience by solving
well-known problems (e.g., head of line blocking and redundant headers), while
introducing new features (e.g., server push and content priority). On paper
HTTP/2 represents the future of the Web. Yet, it is unclear whether the Web
itself will, and should, hop on board. To shed some light on these questions,
we built a measurement platform that monitors HTTP/2 adoption and performance
across the Alexa top 1 million websites on a daily basis. Our system is live
and up-to-date results can be viewed at http://isthewebhttp2yet.com/. In this
paper, we report our initial findings from a 6 month measurement campaign
(November 2014 - May 2015). We find 13,000 websites reporting HTTP/2 support,
but only 600, mostly hosted by Google and Twitter, actually serving content. In
terms of speed, we find no significant benefits from HTTP/2 under stable
network conditions. More benefits appear in a 3G network where current Web
development practices make HTTP/2 more resilient to losses and delay variation
than previously believed.",website monitoring
http://arxiv.org/abs/1704.04937v2,"Browsers can detect malicious websites that are provisioned with forged or
fake TLS/SSL certificates. However, they are not so good at detecting malicious
websites if they are provisioned with mistakenly issued certificates or
certificates that have been issued by a compromised certificate authority.
Google proposed certificate transparency which is an open framework to monitor
and audit certificates in real time. Thereafter, a few other certificate
transparency schemes have been proposed which can even handle revocation. All
currently known constructions use Merkle hash trees and have proof size
logarithmic in the number of certificates/domain owners.
  We present a new certificate transparency scheme with short (constant size)
proofs. Our construction makes use of dynamic bilinear-map accumulators. The
scheme has many desirable properties like efficient revocation, low
verification cost and update costs comparable to the existing schemes. We
provide proofs of security and evaluate the performance of our scheme.",website monitoring
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",website monitoring
http://arxiv.org/abs/1908.02900v1,"Viral diseases are major sources of poor yields for cassava, the 2nd largest
provider of carbohydrates in Africa.At least 80% of small-holder farmer
households in Sub-Saharan Africa grow cassava. Since many of these farmers have
smart phones, they can easily obtain photos of dis-eased and healthy cassava
leaves in their farms, allowing the opportunity to use computer vision
techniques to monitor the disease type and severity and increase yields.
How-ever, annotating these images is extremely difficult as ex-perts who are
able to distinguish between highly similar dis-eases need to be employed. We
provide a dataset of labeled and unlabeled cassava leaves and formulate a
Kaggle challenge to encourage participants to improve the performance of their
algorithms using semi-supervised approaches. This paper describes our dataset
and challenge which is part of the Fine-Grained Visual Categorization workshop
at CVPR2019.",website monitoring
http://arxiv.org/abs/1804.01237v1,"With the rapid growth of mobile internet, mobile application, like website
navigation, searching, e-Shopping and app download, etc. are all popular in
worldwide. Meanwhile, it become more and more popular that traditional HTTP
protocol, which is also applying in not only web browsing but also
communication between mobile application clients and servers. Besides, it has
made HTTP Hijacking profitable. Furthermore, it has brought a lot of troubles
for users, network operators and ISP. We analyze the principle of HTTP spectral
Hijacking and present a mechanism of collaboratively detecting and locating
called Co HijackingMonitor. Experimental result shows that, Co HijackingMonitor
can solve the hijacking problem effectively.",website monitoring
http://arxiv.org/abs/1807.06373v1,"Predicting the popularity of online content has attracted much attention in
the past few years. In news rooms, for instance, journalists and editors are
keen to know, as soon as possible, the articles that will bring the most
traffic into their website. The relevant literature includes a number of
approaches and algorithms to perform this forecasting. Most of the proposed
methods require monitoring the popularity of content during some time after it
is posted, before making any longer-term prediction. In this paper, we propose
a new approach for predicting the popularity of news articles before they go
online. Our approach complements existing content-based methods, and is based
on a number of observations regarding article similarity and topicality. First,
the popularity of a new article is correlated with the popularity of similar
articles of recent publication. Second, the popularity of the new article is
related to the recent historical popularity of its main topic. Based on these
observations, we use time series forecasting to predict the number of visits an
article will receive. Our experiments, conducted on a real data collection of
articles in an international news website, demonstrate the effectiveness and
efficiency of the proposed method.",website monitoring
http://arxiv.org/abs/1811.11218v1,"Over the past years, literature has shown that attacks exploiting the
microarchitecture of modern processors pose a serious threat to the privacy of
mobile phone users. This is because applications leave distinct footprints in
the processor, which can be used by malware to infer user activities. In this
work, we show that these inference attacks are considerably more practical when
combined with advanced AI techniques. In particular, we focus on profiling the
activity in the last-level cache (LLC) of ARM processors. We employ a simple
Prime+Probe based monitoring technique to obtain cache traces, which we
classify with Deep Learning methods including Convolutional Neural Networks. We
demonstrate our approach on an off-the-shelf Android phone by launching a
successful attack from an unprivileged, zeropermission App in well under a
minute. The App thereby detects running applications with an accuracy of 98%
and reveals opened websites and streaming videos by monitoring the LLC for at
most 6 seconds. This is possible, since Deep Learning compensates measurement
disturbances stemming from the inherently noisy LLC monitoring and unfavorable
cache characteristics such as random line replacement policies. In summary, our
results show that thanks to advanced AI techniques, inference attacks are
becoming alarmingly easy to implement and execute in practice. This once more
calls for countermeasures that confine microarchitectural leakage and protect
mobile phone applications, especially those valuing the privacy of their users.",website monitoring
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",website monitoring
http://arxiv.org/abs/1811.09126v2,"Online monitoring user cardinalities (or degrees) in graph streams is
fundamental for many applications. For example in a bipartite graph
representing user-website visiting activities, user cardinalities (the number
of distinct visited websites) are monitored to report network anomalies. These
real-world graph streams may contain user-item duplicates and have a huge
number of distinct user-item pairs, therefore, it is infeasible to exactly
compute user cardinalities when memory and computation resources are
limited.Existing methods are designed to approximately estimate user
cardinalities, whose accuracy highly depends on parameters that are not easy to
set. Moreover, these methods cannot provide anytime-available estimation, as
the user cardinalities are computed at the end of the data stream. Real-time
applications such as anomaly detection require that user cardinalities are
estimated on the fly. To address these problems, we develop novel bit and
register sharing algorithms, which use a bit array and a register array to
build a compact sketch of all users' connected items respectively. Compared
with previous bit and register sharing methods, our algorithms exploit the
dynamic properties of the bit and register arrays (e.g., the fraction of zero
bits in the bit array at each time) to significantly improve the estimation
accuracy, and have low time complexity (O(1)) to update the estimations each
time they observe a new user-item pair. In addition, our algorithms are simple
and easy to use, without requirements to tune any parameter. We evaluate the
performance of our methods on real-world datasets. The experimental results
demonstrate that our methods are several times more accurate and faster than
state-of-the-art methods using the same amount of memory.",website monitoring
http://arxiv.org/abs/1105.1234v2,"Trojan virus attacks pose one of the most serious threats to computer
security. A Trojan horse is typically separated into two parts - a server and a
client. It is the client that is cleverly disguised as significant software and
positioned in peer-to-peer file sharing networks, or unauthorized download
websites. The most common means of infection is through email attachments. The
developer of the virus usually uses various spamming techniques in order to
distribute the virus to unsuspecting users. Malware developers use chat
software as another method to spread their Trojan horse viruses such as Yahoo
Messenger and Skype. The objective of this paper is to explore the network
packet information and detect the behavior of Trojan attacks to monitoring
operating systems such as Windows and Linux. This is accomplished by detecting
and analyzing the Trojan infected packet from a network segment -which passes
through email attachment- before attacking a host computer. The results that
have been obtained to detect information and to store infected packets through
monitoring when using the web browser also compare the behaviors of Linux and
Windows using the payload size after implementing the Wireshark sniffer packet
results. Conclusions of the figures analysis from the packet captured data to
analyze the control bit, and check the behavior of the control bits, and the
usability of the operating systems Linux and Windows.",website monitoring
http://arxiv.org/abs/1203.4099v1,"The aim of the Karlsruhe Tritium Neutrino experiment (KATRIN) is the direct
(model-independent) measurement of the neutrino mass. For that purpose a
windowless gaseous tritium source is used, with a tritium throughput of 40
g/day. In order to reach the design sensitivity of 0.2 eV/c^{2} (90% C.L.) the
key parameters of the tritium source, i.e. the gas inlet rate and the gas
composition, have to be stabilized and monitored at the 0.1% level (1 sigma).
Any small change of the tritium gas composition will manifest itself in
non-negligible effects on the KATRIN measurements; therefore, Laser Raman
spectroscopy (LARA) is the method of choice for the monitoring of the gas
composition because it is a non-invasive and fast in-line measurement
technique. In these proceedings, the requirements of KATRIN for statistical and
systematical uncertainties of this method are discussed. An overview of the
current performance of the LARA system in regard to precision will be given. In
addition, two complementary approaches of intensity calibration are presented.",website monitoring
http://arxiv.org/abs/1610.02065v1,"After carefully considering the scalability problem in Tor and exhaustively
evaluating related works on AS-level adversaries, the author proposes
ASmoniTor, which is an autonomous system monitor for mitigating correlation
attacks in the Tor network. In contrast to prior works, which often released
offline packets, including the source code of a modified Tor client and a
snapshot of the Internet topology, ASmoniTor is an online system that assists
end users with mitigating the threat of AS-level adversaries in a near
real-time fashion. For Tor clients proposed in previous works, users need to
compile the source code on their machine and continually update the snapshot of
the Internet topology in order to obtain accurate AS-path inferences. On the
contrary, ASmoniTor is an online platform that can be utilized easily by not
only technical users, but also by users without a technical background, because
they only need to access it via Tor and input two parameters to execute an
AS-aware path selection algorithm. With ASmoniTor, the author makes three key
technical contributions to the research against AS-level adversaries in the Tor
network. First, ASmoniTor does not require the users to initiate complicated
source code compilations. Second, it helps to reduce errors in AS-path
inferences by letting users input a set of suspected ASes obtained directly
from their own traceroute measurements. Third, the Internet topology database
at the back-end of ASmoniTor is periodically updated to assure near real-time
AS-path inferences between Tor exit nodes and the most likely visited websites.
Finally, in addition to its convenience, ASmoniTor gives users full control
over the information they want to input, thus preserving their privacy.",website monitoring
http://arxiv.org/abs/1709.05628v1,"Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.",website monitoring
http://arxiv.org/abs/1902.03937v2,"Identifying and monitoring Open Access (OA) publications might seem a trivial
task while practical efforts prove otherwise. Contradictory information arise
often depending on metadata employed. We strive to assign OA status to
publications in Web of Science (WOS) and Scopus while complementing it with
different sources of OA information to resolve contradicting cases. We linked
publications from WOS and Scopus via DOIs and ISSNs to Unpaywall, Crossref,
DOAJ and ROAD. Only about 50% of articles and reviews from WOS and Scopus could
be matched via a DOI to Unpaywall. Matching with Crossref brought 56 distinct
licences, which define in many cases the legally binding access status of
publications. But only 44% of publications hold only a single licence on
Crossref, while more than 50% have no licence information submitted to
Crossref. Contrasting OA information from Crossref licences with Unpaywall we
found contradictory cases overall amounting to more than 25%, which might be
partially explained by (ex-)including green OA. A further manual check found
about 17% of OA publications that are not accessible and 15% non-OA
publications that are accessible through publishers' websites. These
preliminary results suggest that identification of OA state of publications
denotes a difficult and currently unfulfilled task.",website monitoring
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",website tracking
http://arxiv.org/abs/1703.07578v1,"Third party tracking is the practice by which third parties recognize users
accross different websites as they browse the web. Recent studies show that 90%
of websites contain third party content that is tracking its users across the
web. Website developers often need to include third party content in order to
provide basic functionality. However, when a developer includes a third party
content, she cannot know whether the third party contains tracking mechanisms.
If a website developer wants to protect her users from being tracked, the only
solution is to exclude any third-party content, thus trading functionality for
privacy. We describe and implement a privacy-preserving web architecture that
gives website developers a control over third party tracking: developers are
able to include functionally useful third party content, the same time ensuring
that the end users are not tracked by the third parties.",website tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",website tracking
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",website tracking
http://arxiv.org/abs/1805.01392v1,"Web tracking technologies are pervasive and operated by a few large
technology companies. This technology, and the use of the collected data has
been implicated in influencing elections, fake news, discrimination, and even
health decisions. Little is known about how this technology is deployed on
hospital or other health related websites. The websites of the 210 public
hospitals in the state of Illinois, USA were evaluated with a web tracker
identification tool. Web trackers were identified on 94% of hospital webs
sites, with an average of 3.5 trackers on the websites of general hospitals.
The websites of smaller critical access hospitals used an average of 2 web
trackers. The most common web tracker identified was Google Analytics, found on
74% of Illinois hospital websites. Of the web trackers discovered, 88% were
operated by Google and 26% by Facebook. In light of revelations about how web
browsing profiles have been used and misused, search bubbles, and the potential
for algorithmic discrimination hospital leadership and policy makers must
carefully consider if it is appropriate to use third party tracking technology
on hospital web sites.",website tracking
http://arxiv.org/abs/1801.04829v2,"This paper presents a pilot study on developing an instrument to predict the
quality of e-commerce websites. The 8C model was adopted as the reference model
of the heuristic evaluation. Each dimension of the 8C was mapped into a set of
quantitative website elements, selected websites were scraped to get the
quantitative website elements, and the score of each dimension was calculated.
A software was developed in PHP for the experiments. In the training process,
10 experiments were conducted and quantitative analyses were regressively
conducted between the experiments. The conversion rate was used to verify the
heuristic evaluation of an e-commerce website after each experiment. The
results showed that the mapping revisions between the experiments improved the
performance of the evaluation instrument, therefore the experiment process and
the quantitative mapping revision guideline proposed was on the right track.
The software resulted from the experiment 10 can serve as the aimed e-commerce
website evaluation instrument. The experiment results and the future work have
been discussed.",website tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",website tracking
http://arxiv.org/abs/1607.07403v2,"We perform a large-scale analysis of third-party trackers on the World Wide
Web from more than 3.5 billion web pages of the CommonCrawl 2012 corpus. We
extract a dataset containing more than 140 million third-party embeddings in
over 41 million domains. To the best of our knowledge, this constitutes the
largest web tracking dataset collected so far, and exceeds related studies by
more than an order of magnitude in the number of domains and web pages
analyzed. We perform a large-scale study of online tracking, on three levels:
(1) On a global level, we give a precise figure for the extent of tracking,
give insights into the structure of the `online tracking sphere' and analyse
which trackers are used by how many websites. (2) On a country-specific level,
we analyse which trackers are used by websites in different countries, and
identify the countries in which websites choose significantly different
trackers than in the rest of the world. (3) We answer the question whether the
content of websites influences the choice of trackers they use, leveraging more
than 90 thousand categorized domains. In particular, we analyse whether highly
privacy-critical websites make different choices of trackers than other
websites. Based on the performed analyses, we confirm that trackers are
widespread (as expected), and that a small number of trackers dominates the web
(Google, Facebook and Twitter). In particular, the three tracking domains with
the highest PageRank are all owned by Google. The only exception to this
pattern are a few countries such as China and Russia. Our results suggest that
this dominance is strongly associated with country-specific political factors
such as freedom of the press. We also confirm that websites with highly
privacy-critical content are less likely to contain trackers (60% vs 90% for
other websites), even though the majority of them still do contain trackers.",website tracking
http://arxiv.org/abs/1511.00619v1,"This article provides a quantitative analysis of privacy-compromising
mechanisms on 1 million popular websites. Findings indicate that nearly 9 in 10
websites leak user data to parties of which the user is likely unaware; more
than 6 in 10 websites spawn third- party cookies; and more than 8 in 10
websites load Javascript code from external parties onto users' computers.
Sites that leak user data contact an average of nine external domains,
indicating that users may be tracked by multiple entities in tandem. By tracing
the unintended disclosure of personal browsing histories on the Web, it is
revealed that a handful of U.S. companies receive the vast bulk of user data.
Finally, roughly 1 in 5 websites are potentially vulnerable to known National
Security Agency spying techniques at the time of analysis.",website tracking
http://arxiv.org/abs/1502.00317v2,"Cursor tracking data contains information about website visitors which may
provide new ways to understand visitors and their needs. This paper presents an
Amazon Mechanical Turk study where participants were tracked as they used
modified variants of the Wikipedia and BBC News websites. Participants were
asked to complete reading and information-finding tasks. The results showed
that it was possible to differentiate between users reading content and users
looking for information based on cursor data. The effects of website
aesthetics, user interest and cursor hardware were also analysed which showed
it was possible to identify hardware from cursor data, but no relationship
between cursor data and engagement was found. The implications of these
results, from the impact on web analytics to the design of experiments to
assess user engagement, are discussed.",website tracking
http://arxiv.org/abs/1907.06520v1,"This paper explores tracking and privacy risks on pornography websites. Our
analysis of 22,484 pornography websites indicated that 93% leak user data to a
third party. Tracking on these sites is highly concentrated by a handful of
major companies, which we identify. We successfully extracted privacy policies
for 3,856 sites, 17% of the total. The policies were written such that one
might need a two-year college education to understand them. Our content
analysis of the sample's domains indicated 44.97% of them expose or suggest a
specific gender/sexual identity or interest likely to be linked to the user. We
identify three core implications of the quantitative results: 1) the
unique/elevated risks of porn data leakage versus other types of data, 2) the
particular risks/impact for vulnerable populations, and 3) the complications of
providing consent for porn site users and the need for affirmative consent in
these online sexual interactions.",website tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",website tracking
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",website tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",website tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",website tracking
http://arxiv.org/abs/1805.01187v1,"A dominant regulatory model for web privacy is ""notice and choice"". In this
model, users are notified of data collection and provided with options to
control it. To examine the efficacy of this approach, this study presents the
first large-scale audit of disclosure of third-party data collection in website
privacy policies. Data flows on one million websites are analyzed and over
200,000 websites' privacy policies are audited to determine if users are
notified of the names of the companies which collect their data. Policies from
25 prominent third-party data collectors are also examined to provide deeper
insights into the totality of the policy environment. Policies are additionally
audited to determine if the choice expressed by the ""Do Not Track"" browser
setting is respected.
  Third-party data collection is wide-spread, but fewer than 15% of attributed
data flows are disclosed. The third-parties most likely to be disclosed are
those with consumer services users may be aware of, those without consumer
services are less likely to be mentioned. Policies are difficult to understand
and the average time requirement to read both a given site{\guillemotright}s
policy and the associated third-party policies exceeds 84 minutes. Only 7% of
first-party site policies mention the Do Not Track signal, and the majority of
such mentions are to specify that the signal is ignored. Among third-party
policies examined, none offer unqualified support for the Do Not Track signal.
Findings indicate that current implementations of ""notice and choice"" fail to
provide notice or respect choice.",website tracking
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",website tracking
http://arxiv.org/abs/1812.01514v3,"Web tracking has been extensively studied over the last decade. To detect
tracking, previous studies and user tools rely on filter lists. However, it has
been shown that filter lists miss trackers. In this paper, we propose an
alternative method to detect trackers inspired by analyzing behavior of
invisible pixels. By crawling 84,658 webpages from 8,744 domains, we detect
that third-party invisible pixels are widely deployed: they are present on more
than 94.51% of domains and constitute 35.66% of all third-party images. We
propose a fine-grained behavioral classification of tracking based on the
analysis of invisible pixels. We use this classification to detect new
categories of tracking and uncover new collaborations between domains on the
full dataset of 4,216,454 third-party requests. We demonstrate that two popular
methods to detect tracking, based on EasyList&EasyPrivacy and on Disconnect
lists respectively miss 25.22% and 30.34% of the trackers that we detect.
Moreover, we find that if we combine all three lists 379,245 requests
originated from 8,744 domains still track users on 68.70% of websites.",website tracking
http://arxiv.org/abs/1802.02507v1,"Third-party networks collect vast amounts of data about users via web sites
and mobile applications. Consolidations among tracker companies can
significantly increase their individual tracking capabilities, prompting
scrutiny by competition regulators. Traditional measures of market share, based
on revenue or sales, fail to represent the tracking capability of a tracker,
especially if it spans both web and mobile. This paper proposes a new approach
to measure the concentration of tracking capability, based on the reach of a
tracker on popular websites and apps. Our results reveal that tracker
prominence and parent-subsidiary relationships have significant impact on
accurately measuring concentration.",website tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website tracking
http://arxiv.org/abs/1201.3783v1,"As the popularity of content sharing websites such as YouTube and Flickr has
increased, they have become targets for spam, phishing and the distribution of
malware. On YouTube, the facility for users to post comments can be used by
spam campaigns to direct unsuspecting users to bogus e-commerce websites. In
this paper, we demonstrate how such campaigns can be tracked over time using
network motif profiling, i.e. by tracking counts of indicative network motifs.
By considering all motifs of up to five nodes, we identify discriminating
motifs that reveal two distinctly different spam campaign strategies. One of
these strategies uses a small number of spam user accounts to comment on a
large number of videos, whereas a larger number of accounts is used with the
other. We present an evaluation that uses motif profiling to track two active
campaigns matching these strategies, and identify some of the associated user
accounts.",website tracking
http://arxiv.org/abs/1805.09155v2,"User demand for blocking advertising and tracking online is large and
growing. Existing tools, both deployed and described in research, have proven
useful, but lack either the completeness or robustness needed for a general
solution. Existing detection approaches generally focus on only one aspect of
advertising or tracking (e.g. URL patterns, code structure), making existing
approaches susceptible to evasion.
  In this work we present AdGraph, a novel graph-based machine learning
approach for detecting advertising and tracking resources on the web. AdGraph
differs from existing approaches by building a graph representation of the HTML
structure, network requests, and JavaScript behavior of a webpage, and using
this unique representation to train a classifier for identifying advertising
and tracking resources. Because AdGraph considers many aspects of the context a
network request takes place in, it is less susceptible to the single-factor
evasion techniques that flummox existing approaches.
  We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly
accurate, able to replicate the labels of human-generated filter lists with
95.33% accuracy, and can even identify many mistakes in filter lists. We
implement AdGraph as a modification to Chromium. AdGraph adds only minor
overhead to page loading and execution, and is actually faster than stock
Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we
conclude that AdGraph is both accurate enough and performant enough for online
use, breaking comparable or fewer websites than popular filter list based
approaches.",website tracking
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",website tracking
http://arxiv.org/abs/1409.1066v1,"The presence of third-party tracking on websites has become customary.
However, our understanding of the third-party ecosystem is still very
rudimentary. We examine third-party trackers from a geographical perspective,
observing the third-party tracking ecosystem from 29 countries across the
globe. When examining the data by region (North America, South America, Europe,
East Asia, Middle East, and Oceania), we observe significant geographical
variation between regions and countries within regions. We find trackers that
focus on specific regions and countries, and some that are hosted in countries
outside their expected target tracking domain. Given the differences in
regulatory regimes between jurisdictions, we believe this analysis sheds light
on the geographical properties of this ecosystem and on the problems that these
may pose to our ability to track and manage the different data silos that now
store personal data about us all.",website tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",website tracking
http://arxiv.org/abs/1908.02261v1,"We turn our attention to the elephant in the room of data protection, which
is none other than the simple and obvious question: ""Who's tracking sensitive
domains?"". Despite a fast-growing amount of work on more complex facets of the
interplay between privacy and the business models of the Web, the obvious
question of who collects data on domains where most people would prefer not be
seen, has received rather limited attention. First, we develop a methodology
for automatically annotating websites that belong to a sensitive category, e.g.
as defined by the General Data Protection Regulation (GDPR). Then, we extract
the third party tracking services included directly, or via recursive
inclusions, by the above mentioned sites. Having analyzed around 30k sensitive
domains, we show that such domains are tracked, albeit less intensely than the
mainstream ones. Looking in detail at the tracking services operating on them,
we find well known names, as well as some less known ones, including some
specializing on specific sensitive categories.",website tracking
http://arxiv.org/abs/1603.06289v1,"Numerous tools have been developed to aggressively block the execution of
popular JavaScript programs (JS) in Web browsers. Such blocking also affects
functionality of webpages and impairs user experience. As a consequence, many
privacy preserving tools (PP-Tools) that have been developed to limit online
tracking, often executed via JS, may suffer from poor performance and limited
uptake. A mechanism that can isolate JS necessary for proper functioning of the
website from tracking JS would thus be useful. Through the use of a manually
labelled dataset composed of 2,612 JS, we show how current PP-Tools are
ineffective in finding the right balance between blocking tracking JS and
allowing functional JS. To the best of our knowledge, this is the first study
to assess the performance of current web PP-Tools.
  To improve this balance, we examine the two classes of JS and hypothesize
that tracking JS share structural similarities that can be used to
differentiate them from functional JS. The rationale of our approach is that
web developers often borrow and customize existing pieces of code in order to
embed tracking (resp. functional) JS into their webpages. We then propose
one-class machine learning classifiers using syntactic and semantic features
extracted from JS. When trained only on samples of tracking JS, our classifiers
achieve an accuracy of 99%, where the best of the PP-Tools achieved an accuracy
of 78%.
  We further test our classifiers and several popular PP-Tools on a corpus of
4K websites with 135K JS. The output of our best classifier on this data is
between 20 to 64% different from the PP-Tools. We manually analyse a sample of
the JS for which our classifier is in disagreement with all other PP-Tools, and
show that our approach is not only able to enhance user web experience by
correctly classifying more functional JS, but also discovers previously unknown
tracking services.",website tracking
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",website tracking
http://arxiv.org/abs/1811.00918v1,"Web developers routinely rely on third-party Java-Script libraries such as
jQuery to enhance the functionality of their sites. However, if not properly
maintained, such dependencies can create attack vectors allowing a site to be
compromised.
  In this paper, we conduct the first comprehensive study of client-side
JavaScript library usage and the resulting security implications across the
Web. Using data from over 133 k websites, we show that 37% of them include at
least one library with a known vulnerability; the time lag behind the newest
release of a library is measured in the order of years. In order to better
understand why websites use so many vulnerable or outdated libraries, we track
causal inclusion relationships and quantify different scenarios. We observe
sites including libraries in ad hoc and often transitive ways, which can lead
to different versions of the same library being loaded into the same document
at the same time. Furthermore, we find that libraries included transitively, or
via ad and tracking code, are more likely to be vulnerable. This demonstrates
that not only website administrators, but also the dynamic architecture and
developers of third-party services are to blame for the Web's poor state of
library management.
  The results of our work underline the need for more thorough approaches to
dependency management, code maintenance and third-party code inclusion on the
Web.",website tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie monitoring
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie monitoring
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie monitoring
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie monitoring
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie monitoring
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie monitoring
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie monitoring
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie monitoring
http://arxiv.org/abs/1803.10450v1,"Over the last decade, the number of devices per person has increased
substantially. This poses a challenge for cookie-based personalization
applications, such as online search and advertising, as it narrows the
personalization signal to a single device environment. A key task is to find
which cookies belong to the same person to recover a complete cross-device user
journey. Recent work on the topic has shown the benefits of using unsupervised
embeddings learned on user event sequences. In this paper, we extend this
approach to a supervised setting and introduce the Siamese Cookie Embedding
Network (SCEmNet), a siamese convolutional architecture that leverages the
multi-modal aspect of sequences, and show significant improvement over the
state-of-the-art.",cookie monitoring
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie tracking
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie tracking
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",cookie tracking
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",cookie tracking
http://arxiv.org/abs/1510.01175v1,"The number of computers, tablets and smartphones is increasing rapidly, which
entails the ownership and use of multiple devices to perform online tasks. As
people move across devices to complete these tasks, their identities becomes
fragmented. Understanding the usage and transition between those devices is
essential to develop efficient applications in a multi-device world. In this
paper we present a solution to deal with the cross-device identification of
users based on semi-supervised machine learning methods to identify which
cookies belong to an individual using a device. The method proposed in this
paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections
challenge proving its good performance.",cookie tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",cookie tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",cookie tracking
http://arxiv.org/abs/1805.10505v2,"User data is the primary input of digital advertising, fueling the free
Internet as we know it. As a result, web companies invest a lot in elaborate
tracking mechanisms to acquire user data that can sell to data markets and
advertisers. However, with same-origin policy, and cookies as a primary
identification mechanism on the web, each tracker knows the same user with a
different ID. To mitigate this, Cookie Synchronization (CSync) came to the
rescue, facilitating an information sharing channel between third parties that
may or not have direct access to the website the user visits. In the
background, with CSync, they merge user data they own, but also reconstruct a
user's browsing history, bypassing the same origin policy. In this paper, we
perform a first to our knowledge in-depth study of CSync in the wild, using a
year-long weblog from 850 real mobile users. Through our study, we aim to
understand the characteristics of the CSync protocol and the impact it has on
web users' privacy. For this, we design and implement CONRAD, a holistic
mechanism to detect CSync events at real time, and the privacy loss on the user
side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97%
of the regular web users are exposed to CSync: most of them within the first
week of their browsing, and the median userID gets leaked, on average, to 3.5
different domains. Finally, we see that CSync increases the number of domains
that track the user by a factor of 6.75.",cookie tracking
http://arxiv.org/abs/1407.0803v1,"The popularity of mobile device has made people's lives more convenient, but
threatened people's privacy at the same time. As end users are becoming more
and more concerned on the protection of their private information, it is even
harder to track a specific user using conventional technologies. For example,
cookies might be cleared by users regularly. Apple has stopped apps accessing
UDIDs, and Android phones use some special permission to protect IMEI code. To
address this challenge, some recent studies have worked on tracing smart phones
using the hardware features resulted from the imperfect manufacturing process.
These works have demonstrated that different devices can be differentiated to
each other. However, it still has a long way to go in order to replace cookie
and be deployed in real world scenarios, especially in terms of properties like
uniqueness, robustness, etc. In this paper, we presented a novel method to
generate stable and unique device ID stealthy for smartphones by exploiting the
frequency response of the speaker. With carefully selected audio frequencies
and special sound wave patterns, we can reduce the impacts of non-linear
effects and noises, and keep our feature extraction process un-noticeable to
users. The extracted feature is not only very stable for a given smart phone
speaker, but also unique to that phone. The feature contains rich information
that is equivalent to around 40 bits of entropy, which is enough to identify
billions of different smart phones of the same model. We have built a prototype
to evaluate our method, and the results show that the generated device ID can
be used as a replacement of cookie.",cookie tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",cookie tracking
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",cookie tracking
http://arxiv.org/abs/1804.08491v1,"Internet users today are constantly giving away their personal information
and privacy through social media, tracking cookies, 'free' email, and single
sign-on authentication in order to access convenient online services.
Unfortunately, the elected officials who are supposed to be regulating these
technologies often know less about informed consent and data ownership than the
users themselves. This is why without changes, internet users may continue to
be exploited by companies offering free and convenient online services.",cookie tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",cookie tracking
http://arxiv.org/abs/1811.00920v1,"Numerous surveys have shown that Web users are concerned about the loss of
privacy associated with online tracking. Alarmingly, these surveys also reveal
that people are also unaware of the amount of data sharing that occurs between
ad exchanges, and thus underestimate the privacy risks associated with online
tracking.
  In reality, the modern ad ecosystem is fueled by a flow of user data between
trackers and ad exchanges. Although recent work has shown that ad exchanges
routinely perform cookie matching with other exchanges, these studies are based
on brittle heuristics that cannot detect all forms of information sharing,
especially under adversarial conditions.
  In this study, we develop a methodology that is able to detect client- and
server-side flows of information between arbitrary ad exchanges. Our key
insight is to leverage retargeted ads as a tool for identifying information
flows. Intuitively, our methodology works because it relies on the semantics of
how exchanges serve ads, rather than focusing on specific cookie matching
mechanisms. Using crawled data on 35,448 ad impressions, we show that our
methodology can successfully categorize four different kinds of information
sharing behavior between ad exchanges, including cases where existing heuristic
methods fail.
  We conclude with a discussion of how our findings and methodologies can be
leveraged to give users more control over what kind of ads they see and how
their information is shared between ad exchanges.",cookie tracking
http://arxiv.org/abs/1811.08660v1,"The European General Data Protection Regulation (GDPR), which went into
effect in May 2018, leads to important changes in this area: companies are now
required to ask for users' consent before collecting and sharing personal data
and by law users now have the right to gain access to the personal information
collected about them.
  In this paper, we study and evaluate the effect of the GDPR on the online
advertising ecosystem. In a first step, we measure the impact of the
legislation on the connections (regarding cookie syncing) between third-parties
and show that the general structure how the entities are arranged is not
affected by the GDPR. However, we find that the new regulation has a
statistically significant impact on the number of connections, which shrinks by
around 40%. Furthermore, we analyze the right to data portability by evaluating
the subject access right process of popular companies in this ecosystem and
observe differences between the processes implemented by the companies and how
they interpret the new legislation. We exercised our right of access under GDPR
with 36 companies that had tracked us online. Although 32 companies (89%) we
inquired replied within the period defined by law, only 21 (58%) finished the
process by the deadline set in the GDPR. Our work has implications regarding
the implementation of privacy law as well as what online tracking companies
should do to be more compliant with the new regulation.",cookie tracking
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie tracking
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie tracking
http://arxiv.org/abs/1909.02638v1,"Since the adoption of the General Data Protection Regulation (GDPR) in May
2018 more than 60 % of popular websites in Europe display cookie consent
notices to their visitors. This has quickly led to users becoming fatigued with
privacy notifications and contributed to the rise of both browser extensions
that block these banners and demands for a solution that bundles consent across
multiple websites or in the browser.
  In this work, we identify common properties of the graphical user interface
of consent notices and conduct three experiments with more than 80,000 unique
users on a German website to investigate the influence of notice position, type
of choice, and content framing on consent. We find that users are more likely
to interact with a notice shown in the lower (left) part of the screen. Given a
binary choice, more users are willing to accept tracking compared to mechanisms
that require them to allow cookie use for each category or company
individually. We also show that the wide-spread practice of nudging has a large
effect on the choices users make. Our experiments show that seemingly small
implementation decisions can substantially impact whether and how people
interact with consent notices. Our findings demonstrate the importance for
regulation to not just require consent, but also provide clear requirements or
guidance for how this consent has to be obtained in order to ensure that users
can make free and informed choices.",cookie tracking
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",cookie tracking
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",cookie tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",cookie tracking
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",ad blocking
http://arxiv.org/abs/1605.05841v1,"The rise of ad-blockers is viewed as an economic threat by online publishers,
especially those who primarily rely on ad- vertising to support their services.
To address this threat, publishers have started retaliating by employing
ad-block detectors, which scout for ad-blocker users and react to them by
restricting their content access and pushing them to whitelist the website or
disabling ad-blockers altogether. The clash between ad-blockers and ad-block
detectors has resulted in a new arms race on the web. In this paper, we present
the first systematic measurement and analysis of ad-block detection on the web.
We have designed and implemented a machine learning based tech- nique to
automatically detect ad-block detection, and use it to study the deployment of
ad-block detectors on Alexa top- 100K websites. The approach is promising with
precision of 94.8% and recall of 93.1%. We characterize the spectrum of
different strategies used by websites for ad-block detection. We find that most
of publishers use fairly simple passive ap- proaches for ad-block detection.
However, we also note that a few websites use third-party services, e.g.
PageFair, for ad-block detection and response. The third-party services use
active deception and other sophisticated tactics to de- tect ad-blockers. We
also find that the third-party services can successfully circumvent ad-blockers
and display ads on publisher websites.",ad blocking
http://arxiv.org/abs/1905.07444v2,"Online advertising has been a long-standing concern for user privacy and
overall web experience. Several techniques have been proposed to block ads,
mostly based on filter-lists and manually-written rules. While a typical ad
blocker relies on manually-curated block lists, these inevitably get
out-of-date, thus compromising the ultimate utility of this ad blocking
approach. In this paper we present Percival, a browser-embedded, lightweight,
deep learning-powered ad blocker. Percival embeds itself within the browser's
image rendering pipeline, which makes it possible to intercept every image
obtained during page execution and to perform blocking based on applying
machine learning for image classification to flag potential ads. Our
implementation inside both Chromium and Brave browsers shows only a minor
rendering performance overhead of 4.55%, demonstrating the feasibility of
deploying traditionally heavy models (i.e. deep neural networks) inside the
critical path of the rendering engine of a browser. We show that our
image-based ad blocker can replicate EasyList rules with an accuracy of 96.76%.
To show the versatility of the Percival's approach we present case studies that
demonstrate that Percival 1) does surprisingly well on ads in languages other
than English; 2) Percival also performs well on blocking first-party Facebook
ads, which have presented issues for other ad blockers. Percival proves that
image-based perceptual ad blocking is an attractive complement to today's
dominant approach of block lists",ad blocking
http://arxiv.org/abs/1811.03194v3,"Perceptual ad-blocking is a novel approach that detects online advertisements
based on their visual content. Compared to traditional filter lists, the use of
perceptual signals is believed to be less prone to an arms race with web
publishers and ad networks. We demonstrate that this may not be the case. We
describe attacks on multiple perceptual ad-blocking techniques, and unveil a
new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual
ad-blocking can also introduce new vulnerabilities that let an attacker bypass
web security boundaries and mount DDoS attacks.
  We first analyze the design space of perceptual ad-blockers and present a
unified architecture that incorporates prior academic and commercial work. We
then explore a variety of attacks on the ad-blocker's detection pipeline, that
enable publishers or ad networks to evade or detect ad-blocking, and at times
even abuse its high privilege level to bypass web security boundaries.
  On one hand, we show that perceptual ad-blocking must visually classify
rendered web content to escape an arms race centered on obfuscation of page
markup. On the other, we present a concrete set of attacks on visual
ad-blockers by constructing adversarial examples in a real web page context.
For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and
native web content that misleads perceptual ad-blocking with 100% success
rates. In one of our attacks, we demonstrate how a malicious user can upload
adversarial content, such as a perturbed image in a Facebook post, that fools
the ad-blocker into removing another users' non-ad content.
  Moving beyond the Web and visual domain, we also build adversarial examples
for AdblockRadio, an open source radio client that uses machine learning to
detects ads in raw audio streams.",ad blocking
http://arxiv.org/abs/1705.08568v1,"We present a systematic study of ad blocking - and the associated ""arms race""
- as a security problem. We model ad blocking as a state space with four states
and six state transitions, which correspond to techniques that can be deployed
by either publishers or ad blockers. We argue that this is a complete model of
the system. We propose several new ad blocking techniques, including ones that
borrow ideas from rootkits to prevent detection by anti-ad blocking scripts.
Another technique uses the insight that ads must be recognizable by humans to
comply with laws and industry self-regulation. We have built prototype
implementations of three of these techniques, successfully blocking ads and
evading detection. We systematically evaluate our proposed techniques, along
with existing ones, in terms of security, practicality, and legality. We
characterize the order of growth of the development effort required to
create/maintain ad blockers as a function of the growth of the web. Based on
our state-space model, our new techniques, and this systematization, we offer
insights into the likely ""end game"" of the arms race. We challenge the
widespread assumption that the arms race will escalate indefinitely, and
instead identify a combination of evolving technical and legal factors that
will determine the outcome.",ad blocking
http://arxiv.org/abs/0807.3383v4,"we will present an estimation for the upper-bound of the amount of 16-bytes
plaintexts for English texts, which indicates that the block ciphers with block
length no more than 16-bytes will be subject to recover plaintext attacks in
the occasions of plaintext -known or plaintext-chosen attacks.",ad blocking
http://arxiv.org/abs/1507.05753v2,"Wall-clock-time is minimized for a solution to a linear-program with
block-diagonal-structure, by decomposing the linear-program into as many
small-sized subproblems as possible, each block resulting in a separate
subproblem, when the number of available parallel-processing-units is at least
equal to the number of blocks. This is not necessarily the case when the
parallel processing capability is limited, causing multiple subproblems to be
serially solved on the same processing-unit. In such a situation, it might be
better to aggregate blocks into larger sized subproblems. The optimal
aggregation strategy depends on the computing-platform used, and minimizes the
average-case running time for the set of subproblems. We show that optimal
aggregation is NP-hard when blocks are of unequal size, and that optimal
aggregation can be achieved within polynomial-time when blocks are of equal
size.",ad blocking
http://arxiv.org/abs/1701.07184v1,"We define multi-block interleaved codes as codes that allow reading
information from either a small sub-block or from a larger full block. The
former offers faster access, while the latter provides better reliability. We
specify the correction capability of the sub-block code through its gap $t$
from optimal minimum distance, and look to have full-block minimum distance
that grows with the parameter $t$. We construct two families of such codes when
the number of sub-blocks is $3$. The codes match the distance properties of
known integrated-interleaving codes, but with the added feature of mapping the
same number of information symbols to each sub-block. As such, they are the
first codes that provide read access in multiple size granularities and
correction capabilities.",ad blocking
http://arxiv.org/abs/1903.11434v2,"We present a general consensus framework that allows to easily introduce a
customizable Byzantine fault tolerant consensus algorithm to an existing
(delegated) proof-of-stake blockchain. We prove the safety of the protocol
under the assumption that less than 1/3 of the block proposers are Byzantine.
The framework further allows for consensus participants to choose subjective
decision thresholds in order to obtain safety even in the case of a larger
proportion of Byzantine block proposers. Moreover, the liveness of the protocol
is shown if less than 1/3 of the block proposers crash.
  Based on the framework, we introduce Lisk-BFT, a Byzantine fault tolerant
consensus algorithm for the Lisk blockchain. Lisk-BFT integrates with the
existing block proposal, requires only two additional integers in block headers
and no additional messages. The protocol is simple and provides safety in the
case of static proposers if less than 1/3 of the block proposers are Byzantine.
For the case of dynamically changing proposers, we proof the safety of the
protocol assuming a bound on the number of Byzantine proposers and the number
of honest proposers that can change at one time. We further show the liveness
of the Lisk-BFT protocol for less than 1/3 crashing block proposers.",ad blocking
http://arxiv.org/abs/1604.04495v1,"Free content and services on the Web are often supported by ads. However,
with the proliferation of intrusive and privacy-invasive ads, a significant
proportion of users have started to use ad blockers. As existing ad blockers
are radical (they block all ads) and are not designed taking into account their
economic impact, ad-based economic model of the Web is in danger today. In this
paper, we target privacy-sensitive users and provide them with fine-grained
control over tracking. Our working assumption is that some categories of web
pages (for example, related to health, religion, etc.) are more
privacy-sensitive to users than others (education, science, etc.). Therefore,
our proposed approach consists in providing users with an option to specify the
categories of web pages that are privacy-sensitive to them and block trackers
present on such web pages only. As tracking is prevented by blocking network
connections of third-party domains, we avoid not only tracking but also
third-party ads. Since users will continue receiving ads on web pages belonging
to non-sensitive categories, our approach essentially provides a trade-off
between privacy and economy. To test the viability of our solution, we
implemented it as a Google Chrome extension, named MyTrackingChoices (available
on Chrome Web Store). Our real-world experiments with MyTrackingChoices show
that the economic impact of ad blocking exerted by privacy-sensitive users can
be significantly reduced.",ad blocking
http://arxiv.org/abs/1406.5039v1,"A deep-water approximation to the Stokes drift velocity profile is explored
as an alternative to the monochromatic profile. The alternative profile
investigated relies on the same two quantities required for the monochromatic
profile, viz the Stokes transport and the surface Stokes drift velocity.
Comparisons with parametric spectra and profiles under wave spectra from the
ERA-Interim reanalysis and buoy observations reveal much better agreement than
the monochromatic profile even for complex sea states. That the profile gives a
closer match and a more correct shear has implications for ocean circulation
models since the Coriolis-Stokes force depends on the magnitude and direction
of the Stokes drift profile and Langmuir turbulence parameterizations depend
sensitively on the shear of the profile. The alternative profile comes at no
added numerical cost compared to the monochromatic profile.",ad profiling
http://arxiv.org/abs/1407.0788v2,"Over the past decade, advertising has emerged as the primary source of
revenue for many web sites and apps. In this paper we report a
first-of-its-kind study that seeks to broadly understand the features,
mechanisms and dynamics of display advertising on the web - i.e., the Adscape.
Our study takes the perspective of users who are the targets of display ads
shown on web sites. We develop a scalable crawling capability that enables us
to gather the details of display ads including creatives and landing pages. Our
crawling strategy is focused on maximizing the number of unique ads harvested.
Of critical importance to our study is the recognition that a user's profile
(i.e. browser profile and cookies) can have a significant impact on which ads
are shown. We deploy our crawler over a variety of websites and profiles and
this yields over 175K distinct display ads.
  We find that while targeting is widely used, there remain many instances in
which delivered ads do not depend on user profile; further, ads vary more over
user profiles than over websites. We also assess the population of advertisers
seen and identify over 3.7K distinct entities from a variety of business
segments. Finally, we find that when targeting is used, the specific types of
ads delivered generally correspond with the details of user profiles, and also
on users' patterns of visit.",ad profiling
http://arxiv.org/abs/1601.02377v1,"User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.",ad profiling
http://arxiv.org/abs/1611.04175v1,"The domain of single crossing preference profiles is a widely studied domain
in social choice theory. It has been generalized to the domain of single
crossing preference profiles with respect to trees which inherits many
desirable properties from the single crossing domain, for example, transitivity
of majority relation, existence of polynomial time algorithms for finding
winners of Kemeny voting rule, etc. In this paper, we consider a further
generalization of the domain of single crossing profiles on trees to the domain
consisting of all preference profiles which can be extended to single crossing
preference profiles with respect to some tree by adding more preferences to it.
We call this domain the weakly single crossing domain on trees. We present a
polynomial time algorithm for recognizing weakly single crossing profiles on
trees. We then move on to develop a polynomial time algorithm with low query
complexity for eliciting weakly single crossing profiles on trees even when we
do not know any tree with respect to which the closure of the input profile is
single crossing and the preferences can be queried only sequentially; moreover,
the sequential order is also unknown. We complement the performance of our
preference elicitation algorithm by proving that our algorithm makes an optimal
number of queries up to constant factors when the number of preferences is
large compared to the number of candidates, even if the input profile is known
to be single crossing with respect to some given tree and the preferences can
be accessed randomly.",ad profiling
http://arxiv.org/abs/physics/0607219v2,"Evolutionally conserved quantity that specifies folding nuclei is pursued by
a case study for a small protein (PDB code: 1ten). First it is demonstrated
that the sequences of amino acids at folding nuclei are not conserved. Then 3D
(3-dimensional) information of the structure is considered and it is found that
a 3D hydrophobicity profile is essential to specify the folding nuclei and
evolutionally conserved. This profile is maintained by the interaction,
including entropic effect, among amino acids realized in the native state
structure. Experimentally observed phi-value is correlated to this 3D
hydrophobicity profile after taking into account the effect of the contact
distance in amino-acid sequence.",ad profiling
http://arxiv.org/abs/1402.4303v2,"Condorcet winning sets are a set-valued generalization of the well-known
concept of a Condorcet winner. As supersets of Condorcet winning sets are
always Condorcet winning sets themselves, an interesting property of preference
profiles is the size of the smallest Condorcet winning set they admit. This
smallest size is called the Condorcet dimension of a preference profile. Since
little is known about profiles that have a certain Condorcet dimension, we show
in this paper how the problem of finding a preference profile that has a given
Condorcet dimension can be encoded as a satisfiability problem and solved by a
SAT solver. Initial results include a minimal example of a preference profile
of Condorcet dimension 3, improving previously known examples both in terms of
the number of agents as well as alternatives. Due to the high complexity of
such problems it remains open whether a preference profile of Condorcet
dimension 4 exists.",ad profiling
http://arxiv.org/abs/1202.1691v1,"Quality of Service(QoS) in Mobile Ad Hoc Networks (MANETs) though a
challenge, becomes a necessity because of its applications in critical
scenarios. Providing QoS for users belonging to various profiles and playing
different roles, becomes the need of the hour. In this paper, we propose
proportional share scheduling and MAC protocol (PS2-MAC) model. It classifies
users based on their profile as High Profiled users (HP), Medium Profiled users
(MP) and Low profiled users (LP) and assigns proportional weights. Service
Differentiation for these three service classes is achieved through, rationed
dequeuing algorithm, variable inter frame space, proportionate prioritized
backoff timers and enhanced RTS/CTS control packets. Differentiated services is
simulated in ns2 and results show that 9.5% control overhead is reduced in our
proposed scheme than the existing scheme and results also justify that,
differentiated services have been achieved for the different profiles of users
with proportionate shares and thereby reducing starvation.",ad profiling
http://arxiv.org/abs/1803.00839v1,"Face recognition achieves exceptional success thanks to the emergence of deep
learning. However, many contemporary face recognition models still perform
relatively poor in processing profile faces compared to frontal faces. A key
reason is that the number of frontal and profile training faces are highly
imbalanced - there are extensively more frontal training samples compared to
profile ones. In addition, it is intrinsically hard to learn a deep
representation that is geometrically invariant to large pose variations. In
this study, we hypothesize that there is an inherent mapping between frontal
and profile faces, and consequently, their discrepancy in the deep
representation space can be bridged by an equivariant mapping. To exploit this
mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,
which is capable of adaptively adding residuals to the input deep
representation to transform a profile face representation to a canonical pose
that simplifies recognition. The DREAM block consistently enhances the
performance of profile face recognition for many strong deep networks,
including ResNet models, without deliberately augmenting training data of
profile faces. The block is easy to use, light-weight, and can be implemented
with a negligible computational overhead.",ad profiling
http://arxiv.org/abs/1502.06577v1,"Advertising, long the financial mainstay of the web ecosystem, has become
nearly ubiquitous in the world of mobile apps. While ad targeting on the web is
fairly well understood, mobile ad targeting is much less studied. In this
paper, we use empirical methods to collect a database of over 225,000 ads on 32
simulated devices hosting one of three distinct user profiles. We then analyze
how the ads are targeted by correlating ads to potential targeting profiles
using Bayes' rule and Pearson's chi squared test. This enables us to measure
the prevalence of different forms of targeting. We find that nearly all ads
show the effects of application- and time-based targeting, while we are able to
identify location-based targeting in 43% of the ads and user-based targeting in
39%.",ad profiling
http://arxiv.org/abs/1706.06944v3,"A profile describes a set of properties, e.g. a set of skills a person may
have, a set of skills required for a particular job, or a set of abilities a
football player may have with respect to a particular team strategy. Profile
matching aims to determine how well a given profile fits to a requested
profile. The approach taken in this article is grounded in a matching theory
that uses filters in lattices to represent profiles, and matching values in the
interval [0,1]: the higher the matching value the better is the fit. Such
lattices can be derived from knowledge bases exploiting description logics to
represent the knowledge about profiles. An interesting first question is, how
human expertise concerning the matching can be exploited to obtain most
accurate matchings. It will be shown that if a set of filters together with
matching values by some human expert is given, then under some mild
plausibility assumptions a matching measure can be determined such that the
computed matching values preserve the rankings given by the expert. A second
question concerns the efficient querying of databases of profile instances. For
matching queries that result in a ranked list of profile instances matching a
given one it will be shown how corresponding top-k queries can be evaluated on
grounds of pre-computed matching values, which in turn allows the maintenance
of the knowledge base to be decoupled from the maintenance of profile
instances. In addition, it will be shown how the matching queries can be
exploited for gap queries that determine how profile instances need to be
extended in order to improve in the rankings. Finally, the theory of matching
will be extended beyond the filters, which lead to a matching theory that
exploits fuzzy sets or probabilistic logic with maximum entropy semantics. It
will be shown that added fuzzy links can be captured by extending the
underlying lattice.",ad profiling
http://arxiv.org/abs/1810.07886v1,"Ad-hoc Social Networks have become popular to support novel applications
related to location-based mobile services that are of great importance to users
and businesses. Unlike traditional social services using a centralized server
to fetch location, ad-hoc social network services support infrastructure less
real-time social networking. It allows users to collaborate and share views
anytime anywhere. However, current ad-hoc social network applications are
either not available without rooting the mobile phones or don't filter the
nearby users based on common interests without a centralized server. This paper
presents an architecture and implementation of social networks on commercially
available mobile devices that allow broadcasting name and a limited number of
keywords representing users' interests without any connection in a nearby
region to facilitate matching of interests. The broadcasting region creates a
digital aura and is limited by WiFi region that is around 200 meters. The
application connects users to form a group based on their profile or interests
using peer-to-peer communication mode without using any centralized networking
or profile matching infrastructure. The peer-to-peer group can be used for
private communication when the network is not available.",ad profiling
http://arxiv.org/abs/cs/0411056v1,"This paper proposes a software architecture for dynamical service adaptation.
The services are constituted by reusable software components. The adaptation's
goal is to optimize the service function of their execution context. For a
first step, the context will take into account just the user needs but other
elements will be added. A particular feature in our proposition is the profiles
that are used not only to describe the context's elements but also the
components itself. An Adapter analyzes the compatibility between all these
profiles and detects the points where the profiles are not compatibles. The
same Adapter search and apply the possible adaptation solutions: component
customization, insertion, extraction or replacement.",ad profiling
http://arxiv.org/abs/1408.6491v2,"To partly address people's concerns over web tracking, Google has created the
Ad Settings webpage to provide information about and some choice over the
profiles Google creates on users. We present AdFisher, an automated tool that
explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher
can run browser-based experiments and analyze data using machine learning and
significance tests. Our tool uses a rigorous experimental design and
statistical analysis to ensure the statistical soundness of our results. We use
AdFisher to find that the Ad Settings was opaque about some features of a
user's profile, that it does provide some choice on ads, and that these choices
can lead to seemingly discriminatory ads. In particular, we found that visiting
webpages associated with substance abuse changed the ads shown but not the
settings page. We also found that setting the gender to female resulted in
getting fewer instances of an ad related to high paying jobs than setting it to
male. We cannot determine who caused these findings due to our limited
visibility into the ad ecosystem, which includes Google, advertisers, websites,
and users. Nevertheless, these results can form the starting point for deeper
investigations by either the companies themselves or by regulatory bodies.",ad profiling
http://arxiv.org/abs/1809.10948v1,"In Named Data Networking (NDN), there is a need for routing protocols to
populate Forwarding Information Base (FIB) tables so that the Interest messages
can be forwarded. To populate FIBs, clients and routers require some routing
information. One method to obtain this information is that network nodes
exchange routing information by each node advertising the available content
objects. Bloom Filter-based Routing approaches like BFR [1], use Bloom Filters
(BFs) to advertise all provided content objects, which consumes valuable
bandwidth and storage resources. This strategy is inefficient as clients
request only a small number of the provided content objects and they do not
need the content advertisement information for all provided content objects. In
this paper, we propose a novel routing algorithm for NDN called pull-based BFR
in which servers only advertise the demanded file names. We compare the
performance of pull-based BFR with original BFR and with a flooding-assisted
routing protocol. Our experimental evaluations show that pull-based BFR
outperforms original BFR in terms of communication overhead needed for content
advertisements, average roundtrip delay, memory resources needed for storing
content advertisements at clients and routers, and the impact of false positive
reports on routing. The comparisons also show that pull-based BFR outperforms
flooding-assisted routing in terms of average round-trip delay.",false advertising
http://arxiv.org/abs/1808.09218v4,"Targeted advertising is meant to improve the efficiency of matching
advertisers to their customers. However, targeted advertising can also be
abused by malicious advertisers to efficiently reach people susceptible to
false stories, stoke grievances, and incite social conflict. Since targeted ads
are not seen by non-targeted and non-vulnerable people, malicious ads are
likely to go unreported and their effects undetected. This work examines a
specific case of malicious advertising, exploring the extent to which political
ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
elections exploited Facebook's targeted advertising infrastructure to
efficiently target ads on divisive or polarizing topics (e.g., immigration,
race-based policing) at vulnerable sub-populations. In particular, we do the
following: (a) We conduct U.S. census-representative surveys to characterize
how users with different political ideologies report, approve, and perceive
truth in the content of the IRA ads. Our surveys show that many ads are
""divisive"": they elicit very different reactions from people belonging to
different socially salient groups. (b) We characterize how these divisive ads
are targeted to sub-populations that feel particularly aggrieved by the status
quo. Our findings support existing calls for greater transparency of content
and targeting of political ads. (c) We particularly focus on how the Facebook
ad API facilitates such targeting. We show how the enormous amount of personal
data Facebook aggregates about users and makes available to advertisers enables
such malicious targeting.",false advertising
http://arxiv.org/abs/1702.00340v1,"Locating the demanded content is one of the major challenges in
Information-Centric Networking (ICN). This process is known as content
discovery. To facilitate content discovery, in this paper we focus on Named
Data Networking (NDN) and propose a novel routing scheme for content discovery,
called Bloom Filter-based Routing (BFR), which is fully distributed, content
oriented, and topology agnostic at the intra-domain level. In BFR, origin
servers advertise their content objects using Bloom filters. We compare the
performance of the proposed BFR with flooding and shortest path content
discovery approaches. BFR outperforms its counterparts in terms of the average
round-trip delay, while it is shown to be very robust to false positive reports
from Bloom filters. Also, BFR is much more robust than shortest path routing to
topology changes. BFR strongly outperforms flooding and performs almost equal
with shortest path routing with respect to the normalized communication costs
for data retrieval and total communication overhead for forwarding Interests.
All the three approaches achieve similar mean hit distance. The signalling
overhead for content advertisement in BFR is much lower than the signalling
overhead for calculating shortest paths in the shortest path approach. Finally,
BFR requires small storage overhead for maintaining content advertisements.",false advertising
http://arxiv.org/abs/1305.4045v1,"In recent years, there has been rapid growth in mobile devices such as
smartphones, and a number of applications are developed specifically for the
smartphone market. In particular, there are many applications that are ``free''
to the user, but depend on advertisement services for their revenue. Such
applications include an advertisement module - a library provided by the
advertisement service - that can collect a user's sensitive information and
transmit it across the network. Users accept this business model, but in most
cases the applications do not require the user's acknowledgment in order to
transmit sensitive information. Therefore, such applications' behavior becomes
an invasion of privacy. In our analysis of 1,188 Android applications' network
traffic and permissions, 93% of the applications we analyzed connected to
multiple destinations when using the network. 61% required a permission
combination that included both access to sensitive information and use of
networking services. These applications have the potential to leak the user's
sensitive information. In an effort to enable users to control the transmission
of their private information, we propose a system which, using a novel
clustering method based on the HTTP packet destination and content distances,
generates signatures from the clustering result and uses them to detect
sensitive information leakage from Android applications. Our system does not
require an Android framework modification or any special privileges. Thus users
can easily introduce our system to their devices, and manage suspicious
applications' network behavior in a fine grained manner. Our system accurately
detected 94% of the sensitive information leakage from the applications
evaluated and produced only 5% false negative results, and less than 3% false
positive results.",false advertising
http://arxiv.org/abs/1903.00733v2,"Advertising is a primary means for revenue generation for millions of
websites and smartphone apps (publishers). Naturally, a fraction of publishers
abuse the ad-network to systematically defraud advertisers of their money.
Defenses have matured to overcome some forms of click fraud but are inadequate
against the threat of organic click fraud attacks. Malware detection systems
including honeypots fail to stop click fraud apps; ad-network filters are
better but measurement studies have reported that a third of the clicks
supplied by ad-networks are fake; collaborations between ad-networks and app
stores that bad-lists malicious apps works better still, but fails to prevent
criminals from writing fraudulent apps which they monetise until they get
banned and start over again. This work develops novel inference techniques that
can isolate click fraud attacks using their fundamental properties. In the {\em
mimicry defence}, we leverage the observation that organic click fraud involves
the re-use of legitimate clicks. Thus we can isolate fake-clicks by detecting
patterns of click-reuse within ad-network clickstreams with historical
behaviour serving as a baseline. Second, in {\em bait-click defence}. we
leverage the vantage point of an ad-network to inject a pattern of bait clicks
into the user's device, to trigger click fraud-apps that are gated on
user-behaviour. Our experiments show that the mimicry defence detects around
81\% of fake-clicks in stealthy (low rate) attacks with a false-positive rate
of 110110 per hundred thousand clicks. Bait-click defence enables further
improvements in detection rates of 95\% and reduction in false-positive rates
of between 0 and 30 clicks per million, a substantial improvement over current
approaches.",false advertising
http://arxiv.org/abs/1602.07128v2,"It is known that some network operators inject false content into users'
network traffic. Yet all previous works that investigate this practice focus on
edge ISPs (Internet Service Providers), namely, those that provide Internet
access to end users. Edge ISPs that inject false content affect their customers
only. However, in this work we show that not only edge ISPs may inject false
content, but also core network operators. These operators can potentially alter
the traffic of \emph{all} Internet users who visit predetermined websites. We
expose this practice by inspecting a large amount of traffic originating from
several networks. Our study is based on the observation that the forged traffic
is injected in an out-of-band manner: the network operators do not update the
network packets in-path, but rather send the forged packets \emph{without}
dropping the legitimate ones. This creates a race between the forged and the
legitimate packets as they arrive to the end user. This race can be identified
and analyzed. Our analysis shows that the main purpose of content injection is
to increase the network operators' revenue by inserting advertisements to
websites. Nonetheless, surprisingly, we have also observed numerous cases of
injected malicious content. We publish representative samples of the injections
to facilitate continued analysis of this practice by the security community.",false advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",false advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",false advertising
http://arxiv.org/abs/1209.2557v1,"A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.",false advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",false advertising
http://arxiv.org/abs/1508.01843v2,"Background: Twitter has become the ""wild-west"" of marketing and promotional
strategies for advertisement agencies. Electronic cigarettes have been heavily
marketed across Twitter feeds, offering discounts, ""kid-friendly"" flavors,
algorithmically generated false testimonials, and free samples. Methods:All
electronic cigarette keyword related tweets from a 10% sample of Twitter
spanning January 2012 through December 2014 (approximately 850,000 total
tweets) were identified and categorized as Automated or Organic by combining a
keyword classification and a machine trained Human Detection algorithm. A
sentiment analysis using Hedonometrics was performed on Organic tweets to
quantify the change in consumer sentiments over time. Commercialized tweets
were topically categorized with key phrasal pattern matching. Results:The
overwhelming majority (80%) of tweets were classified as automated or
promotional in nature. The majority of these tweets were coded as
commercialized (83.65% in 2013), up to 33% of which offered discounts or free
samples and appeared on over a billion twitter feeds as impressions. The
positivity of Organic (human) classified tweets has decreased over time (5.84
in 2013 to 5.77 in 2014) due to a relative increase in the negative words
ban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the
positive words like haha,good,cool. Automated tweets are more positive than
organic (6.17 versus 5.84) due to a relative increase in the marketing words
best,win,buy,sale,health,discount and a relative decrease in negative words
like bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter
and the clinical uncertainty of the long term health complications of
electronic cigarette consumption, the protection of public health warrants
scrutiny and potential regulation of social media marketing.",false advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",false advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",false advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",false advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",false advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",false advertising
http://arxiv.org/abs/1111.0387v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms,absence of centralized
monitoring points, and lack of clear lines of defense. Most of the routing
protocols for MANETs are thus vulnerable to various types of attacks. Ad hoc
on-demand distance vector routing (AODV) is a very popular routing algorithm.
However, it is vulnerable to the well-known black hole attack, where a
malicious node falsely advertises good paths to a destination node during the
route discovery process. This attack becomes more sever when a group of
malicious nodes cooperate each other. In this paper, a defense mechanism is
presented against a coordinated attack by multiple black hole nodes in a MANET.
The simulation carried out on the proposed scheme has produced results that
demonstrate the effectiveness of the mechanism in detection of the attack while
maintaining a reasonable level of throughput in the network.",false advertising
http://arxiv.org/abs/1302.4882v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms, absence of centralized
monitoring points, and lack of clear lines of defense. Protecting the network
layer of a MANET from malicious attacks is an important and challenging
security issue, since most of the routing protocols for MANETs are vulnerable
to various types of attacks. Ad hoc on-demand distance vector routing (AODV) is
a very popular routing algorithm. However, it is vulnerable to the well-known
black hole attack, where a malicious node falsely advertises good paths to a
destination node during the route discovery process but drops all packets in
the data forwarding phase. This attack becomes more severe when a group of
malicious nodes cooperate each other. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of the nodes.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead and computation overhead in the nodes.",false advertising
http://arxiv.org/abs/1709.10388v1,"The online ads trading platform plays a crucial role in connecting publishers
and advertisers and generates tremendous value in facilitating the convenience
of our lives. It has been evolving into a more and more complicated structure.
In this paper, we consider the problem of maximizing the revenue for the seller
side via utilizing proper reserve price for the auctions in a dynamical way.
  Predicting the optimal reserve price for each auction in the repeated auction
marketplaces is a non-trivial problem. However, we were able to come up with an
efficient method of improving the seller revenue by mainly focusing on
adjusting the reserve price for those high-value inventories. Previously, no
dedicated work has been performed from this perspective. Inspired by Paul and
Michael, our model first identifies the value of the inventory by predicting
the top bid price bucket using a cascade of classifiers. The cascade is
essential in significantly reducing the false positive rate of a single
classifier. Based on the output of the first step, we build another cluster of
classifiers to predict the price separations between the top two bids. We
showed that although the high-value auctions are only a small portion of all
the traffic, successfully identifying them and setting correct reserve price
would result in a significant revenue lift. Moreover, our optimization is
compatible with all other reserve price models in the system and does not
impact their performance. In other words, when combined with other models, the
enhancement on exchange revenue will be aggregated. Simulations on randomly
sampled Yahoo ads exchange (YAXR) data showed stable and expected lift after
applying our model.",false advertising
http://arxiv.org/abs/1807.07741v1,"Employers actively look for talents having not only specific hard skills but
also various soft skills. To analyze the soft skill demands on the job market,
it is important to be able to detect soft skill phrases from job advertisements
automatically. However, a naive matching of soft skill phrases can lead to
false positive matches when a soft skill phrase, such as friendly, is used to
describe a company, a team, or another entity, rather than a desired candidate.
  In this paper, we propose a phrase-matching-based approach which
differentiates between soft skill phrases referring to a candidate vs.
something else. The disambiguation is formulated as a binary text
classification problem where the prediction is made for the potential soft
skill based on the context where it occurs. To inform the model about the soft
skill for which the prediction is made, we develop several approaches,
including soft skill masking and soft skill tagging.
  We compare several neural network based approaches, including CNN, LSTM and
Hierarchical Attention Model. The proposed tagging-based input representation
using LSTM achieved the highest recall of 83.92% on the job dataset when fixing
a precision to 95%.",false advertising
http://arxiv.org/abs/1908.07087v2,"Given the reach of web platforms, bad actors have considerable incentives to
manipulate and defraud users at the expense of platform integrity. This has
spurred research in numerous suspicious behavior detection tasks, including
detection of sybil accounts, false information, and payment scams/fraud. In
this paper, we draw the insight that many such initiatives can be tackled in a
common framework by posing a detection task which seeks to find groups of
entities which share too many properties with one another across multiple
attributes (sybil accounts created at the same time and location, propaganda
spreaders broadcasting articles with the same rhetoric and with similar
reshares, etc.) Our work makes four core contributions: Firstly, we posit a
novel formulation of this task as a multi-view graph mining problem, in which
distinct views reflect distinct attribute similarities across entities, and
contextual similarity and attribute importance are respected. Secondly, we
propose a novel suspiciousness metric for scoring entity groups given the
abnormality of their synchronicity across multiple views, which obeys intuitive
desiderata that existing metrics do not. Finally, we propose the SliceNDice
algorithm which enables efficient extraction of highly suspicious entity
groups, and demonstrate its practicality in production, in terms of strong
detection performance and discoveries on Snapchat's large advertiser ecosystem
(89% precision and numerous discoveries of real fraud rings), marked
outperformance of baselines (over 97% precision/recall in simulated settings)
and linear scalability.",false advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",false advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",false advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",false advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",false advertising
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",illegal advertising
http://arxiv.org/abs/1910.05424v1,"Illegal fishing is prevalent throughout the world and heavily impacts the
health of our oceans, the sustainability and profitability of fisheries, and
even acts to destabilize geopolitical relations. To achieve the United Nations'
Sustainable Development Goal of ""Life Below Water"", our ability to detect and
predict illegal fishing must improve. Recent advances have been made through
the use of vessel location data, however, most analyses to date focus on
anomalous spatial behaviors of vessels one at a time. To improve predictions,
we develop a method inspired by complex systems theory to monitor the anomalous
multi-scale behavior of whole fleets as they respond to nearby illegal
activities. Specifically, we analyze changes in the multiscale geospatial
organization of fishing fleets operating on the Patagonia Shelf, an important
fishing region with chronic exposure to illegal fishing. We show that legally
operating (and visible) vessels respond anomalously to nearby illegal
activities (by vessels that are difficult to detect). Indeed, precursor
behaviors are identified, suggesting a path towards pre-empting illegal
activities. This approach offers a promising step towards a global system for
detecting, predicting and deterring illegal activities at sea in near
real-time. Doing so will be a big step forward to achieving sustainable life
underwater.",illegal advertising
http://arxiv.org/abs/1710.02546v1,"The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.",illegal advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",illegal advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",illegal advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",illegal advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",illegal advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",illegal advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",illegal advertising
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",illegal advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",illegal advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",illegal advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",illegal advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",illegal advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",illegal advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",illegal advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",illegal advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",illegal advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",illegal advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",illegal advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",illegal advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",illegal advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",illegal advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",illegal advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",illegal advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",illegal advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",illegal advertising
http://arxiv.org/abs/1802.08365v6,"Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.",misleading advertising
http://arxiv.org/abs/1908.10679v1,"Customers make a lot of reviews on online shopping websites every day, e.g.,
Amazon and Taobao. Reviews affect the buying decisions of customers, meanwhile,
attract lots of spammers aiming at misleading buyers. Xianyu, the largest
second-hand goods app in China, suffering from spam reviews. The anti-spam
system of Xianyu faces two major challenges: scalability of the data and
adversarial actions taken by spammers. In this paper, we present our technical
solutions to address these challenges. We propose a large-scale anti-spam
method based on graph convolutional networks (GCN) for detecting spam
advertisements at Xianyu, named GCN-based Anti-Spam (GAS) model. In this model,
a heterogeneous graph and a homogeneous graph are integrated to capture the
local context and global context of a comment. Offline experiments show that
the proposed method is superior to our baseline model in which the information
of reviews, features of users and items being reviewed are utilized.
Furthermore, we deploy our system to process million-scale data daily at
Xianyu. The online performance also demonstrates the effectiveness of the
proposed method.",misleading advertising
http://arxiv.org/abs/1901.00546v1,"Adversarial examples are delicately perturbed inputs, which aim to mislead
machine learning models towards incorrect outputs. While most of the existing
work focuses on generating adversarial perturbations in multi-class
classification problems, many real-world applications fall into the multi-label
setting in which one instance could be associated with more than one label. For
example, a spammer may generate adversarial spams with malicious advertising
while maintaining the other labels such as topic labels unchanged. To analyze
the vulnerability and robustness of multi-label learning models, we investigate
the generation of multi-label adversarial perturbations. This is a challenging
task due to the uncertain number of positive labels associated with one
instance, as well as the fact that multiple labels are usually not mutually
exclusive with each other. To bridge this gap, in this paper, we propose a
general attacking framework targeting on multi-label classification problem and
conduct a premier analysis on the perturbations for deep neural networks.
Leveraging the ranking relationships among labels, we further design a
ranking-based framework to attack multi-label ranking algorithms. We specify
the connection between the two proposed frameworks and separately design two
specific methods grounded on each of them to generate targeted multi-label
perturbations. Experiments on real-world multi-label image classification and
ranking problems demonstrate the effectiveness of our proposed frameworks and
provide insights of the vulnerability of multi-label deep learning models under
diverse targeted attacking strategies. Several interesting findings including
an unpolished defensive strategy, which could potentially enhance the
interpretability and robustness of multi-label deep learning models, are
further presented and discussed at the end.",misleading advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",misleading advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",misleading advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",misleading advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",misleading advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",misleading advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",misleading advertising
http://arxiv.org/abs/1811.03194v3,"Perceptual ad-blocking is a novel approach that detects online advertisements
based on their visual content. Compared to traditional filter lists, the use of
perceptual signals is believed to be less prone to an arms race with web
publishers and ad networks. We demonstrate that this may not be the case. We
describe attacks on multiple perceptual ad-blocking techniques, and unveil a
new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual
ad-blocking can also introduce new vulnerabilities that let an attacker bypass
web security boundaries and mount DDoS attacks.
  We first analyze the design space of perceptual ad-blockers and present a
unified architecture that incorporates prior academic and commercial work. We
then explore a variety of attacks on the ad-blocker's detection pipeline, that
enable publishers or ad networks to evade or detect ad-blocking, and at times
even abuse its high privilege level to bypass web security boundaries.
  On one hand, we show that perceptual ad-blocking must visually classify
rendered web content to escape an arms race centered on obfuscation of page
markup. On the other, we present a concrete set of attacks on visual
ad-blockers by constructing adversarial examples in a real web page context.
For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and
native web content that misleads perceptual ad-blocking with 100% success
rates. In one of our attacks, we demonstrate how a malicious user can upload
adversarial content, such as a perturbed image in a Facebook post, that fools
the ad-blocker into removing another users' non-ad content.
  Moving beyond the Web and visual domain, we also build adversarial examples
for AdblockRadio, an open source radio client that uses machine learning to
detects ads in raw audio streams.",misleading advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",misleading advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",misleading advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",misleading advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",misleading advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",misleading advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",misleading advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",misleading advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",misleading advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",misleading advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",misleading advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",misleading advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",misleading advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",misleading advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",misleading advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",misleading advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",misleading advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",misleading advertising
http://arxiv.org/abs/1507.08874v2,"While substantial effort has been devoted to understand fraudulent activity
in traditional online advertising (search and banner), more recent forms such
as video ads have received little attention. The understanding and
identification of fraudulent activity (i.e., fake views) in video ads for
advertisers, is complicated as they rely exclusively on the detection
mechanisms deployed by video hosting portals. In this context, the development
of independent tools able to monitor and audit the fidelity of these systems
are missing today and needed by both industry and regulators.
  In this paper we present a first set of tools to serve this purpose. Using
our tools, we evaluate the performance of the audit systems of five major
online video portals. Our results reveal that YouTube's detection system
significantly outperforms all the others. Despite this, a systematic evaluation
indicates that it may still be susceptible to simple attacks. Furthermore, we
find that YouTube penalizes its videos' public and monetized view counters
differently, the former being more aggressive. This means that views identified
as fake and discounted from the public view counter are still monetized. We
speculate that even though YouTube's policy puts in lots of effort to
compensate users after an attack is discovered, this practice places the burden
of the risk on the advertisers, who pay to get their ads displayed.",aggressive advertisement
http://arxiv.org/abs/1712.03086v1,"In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.",aggressive advertisement
http://arxiv.org/abs/1901.06244v1,"On October 14, 2015, Tesla Inc. an American electric car company, released
the initial version of the Autopilot system. This system promised to provide
semi-autonomous driving using the existing hardware already installed on Tesla
vehicles. On March 23rd, 2018, a Tesla vehicle ran into a divider at highway
speed, killing the driver. This occurred under the control of the Autopilot
system with no driver intervention. Critics argue that though Tesla gives
drivers warnings in its owner's manual, it is ultimately unethical to release a
system that is marketed as an Autopilot yet still makes grave mistakes that any
human driver would not make. Others defend Tesla by stating that their
advisories are suitable and that drivers should ultimately be at fault for any
mistakes of the Autopilot. This paper will scrutinize the ethical implications
of Tesla's choice to develop, market, and ship a beta product that requires
extensive testing. It will further analyze the implications of Tesla's
aggressive advertisement of the product under the name Autopilot along with
associated marketing materials. By applying the joint ACM/IEEE-CS Software
Engineering Code of Ethics, this paper will show that Tesla's choices and
actions during this event are inconsistent with the code and are unethical
since they are responsible for adequately testing and honestly marketing their
product.",aggressive advertisement
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",aggressive advertisement
http://arxiv.org/abs/1812.02978v1,"Users of Online Social Networks (OSNs) interact with each other more than
ever. In the context of a public discussion group, people receive, read, and
write comments in response to articles and postings. In the absence of access
control mechanisms, OSNs are a great environment for attackers to influence
others, from spreading phishing URLs, to posting fake news. Moreover, OSN user
behavior can be predicted by social science concepts which include conformity
and the bandwagon effect. In this paper, we show how social recommendation
systems affect the occurrence of malicious URLs on Facebook. We exploit
temporal features to build a prediction framework, having greater than 75%
accuracy, to predict whether the following group users' behavior will increase
or not. Included in this work, we demarcate classes of URLs, including those
malicious URLs classified as creating critical damage, as well as those of a
lesser nature which only inflict light damage such as aggressive commercial
advertisements and spam content. It is our hope that the data and analyses in
this paper provide a better understanding of OSN user reactions to different
categories of malicious URLs, thereby providing a way to mitigate the influence
of these malicious URL attacks.",aggressive advertisement
http://arxiv.org/abs/1903.07581v2,"In the recent political climate, the topic of news quality has drawn
attention both from the public and the academic communities. The growing
distrust of traditional news media makes it harder to find a common base of
accepted truth. In this work, we design and build MediaRank
(www.media-rank.com), a fully automated system to rank over 50,000 online news
sources around the world. MediaRank collects and analyzes one million news
webpages and two million related tweets everyday. We base our algorithmic
analysis on four properties journalists have established to be associated with
reporting quality: peer reputation, reporting bias / breadth, bottomline
financial pressure, and popularity.
  Our major contributions of this paper include: (i) Open, interpretable
quality rankings for over 50,000 of the world's major news sources. Our
rankings are validated against 35 published news rankings, including French,
German, Russian, and Spanish language sources. MediaRank scores correlate
positively with 34 of 35 of these expert rankings. (ii) New computational
methods for measuring influence and bottomline pressure. To the best of our
knowledge, we are the first to study the large-scale news reporting citation
graph in-depth. We also propose new ways to measure the aggressiveness of
advertisements and identify social bots, establishing a connection between both
types of bad behavior. (iii) Analyzing the effect of media source bias and
significance. We prove that news sources cite others despite different
political views in accord with quality measures. However, in four
English-speaking countries (US, UK, Canada, and Australia), the highest ranking
sources all disproportionately favor left-wing parties, even when the majority
of news sources exhibited conservative slants.",aggressive advertisement
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",aggressive advertisement
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",aggressive advertisement
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",aggressive advertisement
http://arxiv.org/abs/1604.06648v1,"The problem of aggression for Internet communities is rampant. Anonymous
forums usually called imageboards are notorious for their aggressive and
deviant behaviour even in comparison with other Internet communities. This
study is aimed at studying ways of automatic detection of verbal expression of
aggression for the most popular American (4chan.org) and Russian (2ch.hk)
imageboards. A set of 1,802,789 messages was used for this study. The machine
learning algorithm word2vec was applied to detect the state of aggression. A
decent result is obtained for English (88%), the results for Russian are yet to
be improved.",aggressive advertisement
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",aggressive advertisement
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",aggressive advertisement
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",aggressive advertisement
http://arxiv.org/abs/1208.3561v3,"We study pool-based active learning of half-spaces. We revisit the aggressive
approach for active learning in the realizable case, and show that it can be
made efficient and practical, while also having theoretical guarantees under
reasonable assumptions. We further show, both theoretically and experimentally,
that it can be preferable to mellow approaches. Our efficient aggressive active
learner of half-spaces has formal approximation guarantees that hold when the
pool is separable with a margin. While our analysis is focused on the
realizable setting, we show that a simple heuristic allows using the same
algorithm successfully for pools with low error as well. We further compare the
aggressive approach to the mellow approach, and prove that there are cases in
which the aggressive approach results in significantly better label complexity
compared to the mellow approach. We demonstrate experimentally that substantial
improvements in label complexity can be achieved using the aggressive approach,
for both realizable and low-error settings.",aggressive advertisement
http://arxiv.org/abs/1607.01076v1,"Prison facilities, mental correctional institutions, sports bars and places
of public protest are prone to sudden violence and conflicts. Surveillance
systems play an important role in mitigation of hostile behavior and
improvement of security by detecting such provocative and aggressive
activities. This research proposed using automatic aggressive behavior and
anger detection to improve the effectiveness of the surveillance systems. An
emotion and aggression aware component will make the surveillance system highly
responsive and capable of alerting the security guards in real time. This
research proposed facial expression, head, hand and body movement and speech
tracking for detecting anger and aggressive actions. Recognition was achieved
using support vector machines and rule based features. The multimodal affect
recognition precision rate for anger improved by 15.2% and recall rate improved
by 11.7% when behavioral rule based features were used in aggressive action
detection.",aggressive advertisement
http://arxiv.org/abs/1904.08770v1,"This paper attempt to study the effectiveness of text representation schemes
on two tasks namely: User Aggression and Fact Detection from the social media
contents. In User Aggression detection, The aim is to identify the level of
aggression from the contents generated in the Social media and written in the
English, Devanagari Hindi and Romanized Hindi. Aggression levels are
categorized into three predefined classes namely: `Non-aggressive`, `Overtly
Aggressive`, and `Covertly Aggressive`. During the disaster-related incident,
Social media like, Twitter is flooded with millions of posts. In such emergency
situations, identification of factual posts is important for organizations
involved in the relief operation. We anticipated this problem as a combination
of classification and Ranking problem. This paper presents a comparison of
various text representation scheme based on BoW techniques, distributed
word/sentence representation, transfer learning on classifiers. Weighted $F_1$
score is used as a primary evaluation metric. Results show that text
representation using BoW performs better than word embedding on machine
learning classifiers. While pre-trained Word embedding techniques perform
better on classifiers based on deep neural net. Recent transfer learning model
like ELMO, ULMFiT are fine-tuned for the Aggression classification task.
However, results are not at par with pre-trained word embedding model. Overall,
word embedding using fastText produce best weighted $F_1$-score than Word2Vec
and Glove. Results are further improved using pre-trained vector model.
Statistical significance tests are employed to ensure the significance of the
classification results. In the case of lexically different test Dataset, other
than training Dataset, deep neural models are more robust and perform
substantially better than machine learning classifiers.",aggressive advertisement
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",aggressive advertisement
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",aggressive advertisement
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",aggressive advertisement
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",aggressive advertisement
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",aggressive advertisement
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",aggressive advertisement
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",aggressive advertisement
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",aggressive advertisement
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",aggressive advertisement
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",aggressive advertisement
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",aggressive advertisement
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",native advertising
http://arxiv.org/abs/1512.01174v2,"Social media platforms are popular venues for fashion brand marketing and
advertising. With the introduction of native advertising, users don't have to
endure banner ads that hold very little saliency and are unattractive. Using
images and subtle text overlays, even in a world of ever-depreciating attention
span, brands can retain their audience and have a capacious creative potential.
While an assortment of marketing strategies are conjectured, the subtle
distinctions between various types of marketing strategies remain
under-explored. This paper presents a qualitative analysis on the influence of
social media platforms on different behaviors of fashion brand marketing. We
employ both linguistic and computer vision techniques while comparing and
contrasting strategic idiosyncrasies. We also analyze brand audience retention
and social engagement hence providing suggestions in adapting advertising and
marketing strategies over Twitter and Instagram.",native advertising
http://arxiv.org/abs/1606.07154v1,"In recent years online advertising has become increasingly ubiquitous and
effective. Advertisements shown to visitors fund sites and apps that publish
digital content, manage social networks, and operate e-mail services. Given
such large variety of internet resources, determining an appropriate type of
advertising for a given platform has become critical to financial success.
Native advertisements, namely ads that are similar in look and feel to content,
have had great success in news and social feeds. However, to date there has not
been a winning formula for ads in e-mail clients. In this paper we describe a
system that leverages user purchase history determined from e-mail receipts to
deliver highly personalized product ads to Yahoo Mail users. We propose to use
a novel neural language-based algorithm specifically tailored for delivering
effective product recommendations, which was evaluated against baselines that
included showing popular products and products predicted based on
co-occurrence. We conducted rigorous offline testing using a large-scale
product purchase data set, covering purchases of more than 29 million users
from 172 e-commerce websites. Ads in the form of product recommendations were
successfully tested on online traffic, where we observed a steady 9% lift in
click-through rates over other ad formats in mail, as well as comparable lift
in conversion rates. Following successful tests, the system was launched into
production during the holiday season of 2014.",native advertising
http://arxiv.org/abs/1804.09133v2,"Click through rate (CTR) prediction is very important for Native
advertisement but also hard as there is no direct query intent. In this paper
we propose a large-scale event embedding scheme to encode the each user
browsing event by training a Siamese network with weak supervision on the
users' consecutive events. The CTR prediction problem is modeled as a
supervised recurrent neural network, which naturally model the user history as
a sequence of events. Our proposed recurrent models utilizing pretrained event
embedding vectors and an attention layer to model the user history. Our
experiments demonstrate that our model significantly outperforms the baseline
and some variants.",native advertising
http://arxiv.org/abs/1609.03204v1,"We present a computational analysis of three language varieties: native,
advanced non-native, and translation. Our goal is to investigate the
similarities and differences between non-native language productions and
translations, contrasting both with native language. Using a collection of
computational methods we establish three main results: (1) the three types of
texts are easily distinguishable; (2) non-native language and translations are
closer to each other than each of them is to native language; and (3) some of
these characteristics depend on the source or native language, while others do
not, reflecting, perhaps, unified principles that similarly affect translations
and non-native language.",native advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",native advertising
http://arxiv.org/abs/1608.06759v1,"Companies developing and maintaining software-only products like web shops
aim for establishing persistent links to their software running in the field.
Monitoring data from real usage scenarios allows for a number of improvements
in the software life-cycle, such as quick identification and solution of
issues, and elicitation of requirements from previously unexpected usage. While
the processes of continuous integration, continuous deployment, and continuous
experimentation using sandboxing technologies are becoming well established in
said software-only products, adopting similar practices for the automotive
domain is more complex mainly due to real-time and safety constraints. In this
paper, we systematically evaluate sandboxed software deployment in the context
of a self-driving heavy vehicle that participated in the 2016 Grand Cooperative
Driving Challenge (GCDC) in The Netherlands. We measured the system's
scheduling precision after deploying applications in four different execution
environments. Our results indicate that there is no significant difference in
performance and overhead when sandboxed environments are used compared to
natively deployed software. Thus, recent trends in software architecting,
packaging, and maintenance using microservices encapsulated in sandboxes will
help to realize similar software and system engineering for cyber-physical
systems.",native advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",native advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",native advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",native advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",native advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",native advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",native advertising
http://arxiv.org/abs/1811.03194v3,"Perceptual ad-blocking is a novel approach that detects online advertisements
based on their visual content. Compared to traditional filter lists, the use of
perceptual signals is believed to be less prone to an arms race with web
publishers and ad networks. We demonstrate that this may not be the case. We
describe attacks on multiple perceptual ad-blocking techniques, and unveil a
new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual
ad-blocking can also introduce new vulnerabilities that let an attacker bypass
web security boundaries and mount DDoS attacks.
  We first analyze the design space of perceptual ad-blockers and present a
unified architecture that incorporates prior academic and commercial work. We
then explore a variety of attacks on the ad-blocker's detection pipeline, that
enable publishers or ad networks to evade or detect ad-blocking, and at times
even abuse its high privilege level to bypass web security boundaries.
  On one hand, we show that perceptual ad-blocking must visually classify
rendered web content to escape an arms race centered on obfuscation of page
markup. On the other, we present a concrete set of attacks on visual
ad-blockers by constructing adversarial examples in a real web page context.
For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and
native web content that misleads perceptual ad-blocking with 100% success
rates. In one of our attacks, we demonstrate how a malicious user can upload
adversarial content, such as a perturbed image in a Facebook post, that fools
the ad-blocker into removing another users' non-ad content.
  Moving beyond the Web and visual domain, we also build adversarial examples
for AdblockRadio, an open source radio client that uses machine learning to
detects ads in raw audio streams.",native advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",native advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",native advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",native advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",native advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",native advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",native advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",native advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",native advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",native advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",native advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",native advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",native advertising
http://arxiv.org/abs/1904.09407v1,"Self-imitating feedback is an effective and learner-friendly method for
non-native learners in Computer-Assisted Pronunciation Training. Acoustic
characteristics in native utterances are extracted and transplanted onto
learner's own speech input, and given back to the learner as a corrective
feedback. Previous works focused on speech conversion using prosodic
transplantation techniques based on PSOLA algorithm. Motivated by the visual
differences found in spectrograms of native and non-native speeches, we
investigated applying GAN to generate self-imitating feedback by utilizing
generator's ability through adversarial training. Because this mapping is
highly under-constrained, we also adopt cycle consistency loss to encourage the
output to preserve the global structure, which is shared by native and
non-native utterances. Trained on 97,200 spectrogram images of short utterances
produced by native and non-native speakers of Korean, the generator is able to
successfully transform the non-native spectrogram input to a spectrogram with
properties of self-imitating feedback. Furthermore, the transformed spectrogram
shows segmental corrections that cannot be obtained by prosodic
transplantation. Perceptual test comparing the self-imitating and correcting
abilities of our method with the baseline PSOLA method shows that the
generative approach with cycle consistency loss is promising.",native advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",sponsored advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",sponsored advertising
http://arxiv.org/abs/1907.12118v1,"Sponsored search has more than 20 years of history, and it has been proven to
be a successful business model for online advertising. Based on the
pay-per-click pricing model and the keyword targeting technology, the sponsored
system runs online auctions to determine the allocations and prices of search
advertisements. In the traditional setting, advertisers should manually create
lots of ad creatives and bid on some relevant keywords to target their
audience. Due to the huge amount of search traffic and a wide variety of ad
creations, the limits of manual optimizations from advertisers become the main
bottleneck for improving the efficiency of this market. Moreover, as many
emerging advertising forms and supplies are growing, it's crucial for sponsored
search platform to pay more attention to the ROI metrics of ads for getting the
marketing budgets of advertisers. In this paper, we present the AiAds system
developed at Baidu, which use machine learning techniques to build an automated
and intelligent advertising system. By designing and implementing the automated
bidding strategy, the intelligent targeting and the intelligent creation
models, the AiAds system can transform the manual optimizations into multiple
automated tasks and optimize these tasks in advanced methods. AiAds is a
brand-new architecture of sponsored search system which changes the bidding
language and allocation mechanism, breaks the limit of keyword targeting with
end-to-end ad retrieval framework and provides global optimization of ad
creation. This system can increase the advertiser's campaign performance, the
user experience and the revenue of the advertising platform simultaneously and
significantly. We present the overall architecture and modeling techniques for
each module of the system and share our lessons learned in solving several key
challenges.",sponsored advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",sponsored advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",sponsored advertising
http://arxiv.org/abs/0906.4764v2,"In this paper, we propose a bid optimizer for sponsored keyword search
auctions which leads to better retention of advertisers by yielding attractive
utilities to the advertisers without decreasing the revenue to the search
engine. The bid optimizer is positioned as a key value added tool the search
engine provides to the advertisers. The proposed bid optimizer algorithm
transforms the reported values of the advertisers for a keyword into a
correlated bid profile using many ideas from cooperative game theory. The
algorithm is based on a characteristic form game involving the search engine
and the advertisers. Ideas from Nash bargaining theory are used in formulating
the characteristic form game to provide for a fair share of surplus among the
players involved. The algorithm then computes the nucleolus of the
characteristic form game since we find that the nucleolus is an apt way of
allocating the gains of cooperation among the search engine and the
advertisers. The algorithm next transforms the nucleolus into a correlated bid
profile using a linear programming formulation. This bid profile is input to a
standard generalized second price mechanism (GSP) for determining the
allocation of sponsored slots and the prices to be be paid by the winners. The
correlated bid profile that we determine is a locally envy-free equilibrium and
also a correlated equilibrium of the underlying game. Through detailed
simulation experiments, we show that the proposed bid optimizer retains more
customers than a plain GSP mechanism and also yields better long-run utilities
to the search engine and the advertisers.",sponsored advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",sponsored advertising
http://arxiv.org/abs/1501.02785v5,"The growing demand for data has driven the Service Providers (SPs) to provide
differential treatment of traffic to generate additional revenue streams from
Content Providers (CPs). While SPs currently only provide best-effort services
to their CPs, it is plausible to envision a model in near future, where CPs are
willing to sponsor quality of service for their content in exchange of sharing
a portion of their profit with SPs. This quality sponsoring becomes invaluable
especially when the available resources are scarce such as in wireless
networks, and can be accommodated in a non-neutral network. In this paper, we
consider the problem of Quality-Sponsored Data (QSD) in a non-neutral network.
In our model, SPs allow CPs to sponsor a portion of their resources, and price
it appropriately to maximize their payoff. The payoff of the SP depends on the
monetary revenue and the satisfaction of end-users both for the non-sponsored
and sponsored content, while CPs generate revenue through advertisement. We
analyze the market dynamics and equilibria in two different frameworks, i.e.
sequential and bargaining game frameworks, and provide strategies for (i) SPs:
to determine if and how to price resources, and (ii) CPs: to determine if and
what quality to sponsor. The frameworks characterize different sets of
equilibrium strategies and market outcomes depending on the parameters of the
market.",sponsored advertising
http://arxiv.org/abs/1307.4980v7,"In sponsored search, advertisement (abbreviated ad) slots are usually sold by
a search engine to an advertiser through an auction mechanism in which
advertisers bid on keywords. In theory, auction mechanisms have many desirable
economic properties. However, keyword auctions have a number of limitations
including: the uncertainty in payment prices for advertisers; the volatility in
the search engine's revenue; and the weak loyalty between advertiser and search
engine. In this paper we propose a special ad option that alleviates these
problems. In our proposal, an advertiser can purchase an option from a search
engine in advance by paying an upfront fee, known as the option price. He then
has the right, but no obligation, to purchase among the pre-specified set of
keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks
in a specified period of time. The proposed option is closely related to a
special exotic option in finance that contains multiple underlying assets
(multi-keyword) and is also multi-exercisable (multi-click). This novel
structure has many benefits: advertisers can have reduced uncertainty in
advertising; the search engine can improve the advertisers' loyalty as well as
obtain a stable and increased expected revenue over time. Since the proposed ad
option can be implemented in conjunction with the existing keyword auctions,
the option price and corresponding fixed CPCs must be set such that there is no
arbitrage between the two markets. Option pricing methods are discussed and our
experimental results validate the development. Compared to keyword auctions, a
search engine can have an increased expected revenue by selling an ad option.",sponsored advertising
http://arxiv.org/abs/0706.1318v1,"We present an algorithm for constructing an optimal slate of sponsored search
advertisements which respects the ordering that is the outcome of a generalized
second price auction, but which must also accommodate complicating factors such
as overall budget constraints. The algorithm is easily fast enough to use on
the fly for typical problem sizes, or as a subroutine in an overall
optimization.",sponsored advertising
http://arxiv.org/abs/1806.05799v2,"As the largest e-commerce platform, Taobao helps advertisers reach billions
of search queries each day via sponsored search, which has also contributed
considerable revenue to the platform. An efficient bidding strategy to cater to
diverse advertiser demands while balancing platform revenue and consumer
experience is significant to a healthy and sustainable marketing ecosystem. In
this paper we propose \emph{Customer Intelligent Agent (CIA)}, a bidding
optimization framework which implements an impression-level bidding to reflect
advertisers' conversion willingness and budget control. In this way, CIA is
capable of fulfilling various e-commerce advertiser demands on different
levels, such as Gross Merchandise Volume optimization, style comparison etc.
Additionally, a replay based simulation system is designed to predict the
performance of different take-rate. CIA unifies the benefits of three parties
in the marketing ecosystem without changing the Generalized Second Price
mechanism. Our extensive offline simulations and large-scale online experiments
on \emph{Taobao Search Advertising (TSA)} platform verify the high
effectiveness of the CIA framework. Moreover, CIA has been deployed online as a
major bidding tool in TSA.",sponsored advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",sponsored advertising
http://arxiv.org/abs/1006.1019v2,"Sponsored search mechanisms have drawn much attention from both academic
community and industry in recent years since the seminal papers of [13] and
[14]. However, most of the existing literature concentrates on the mechanism
design and analysis within the scope of only one search engine in the market.
In this paper we propose a mathematical framework for modeling the interaction
of publishers, advertisers and end users in a competitive market. We first
consider the monopoly market model and provide optimal solutions for both ex
ante and ex post cases, which represents the long-term and short-term revenues
of search engines respectively. We then analyze the strategic behaviors of end
users and advertisers under duopoly and prove the existence of equilibrium for
both search engines to co-exist from ex-post perspective. To show the more
general ex ante results, we carry out extensive simulations under different
parameter settings. Our analysis and observation in this work can provide
useful insight in regulating the sponsored search market and protecting the
interests of advertisers and end users.",sponsored advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",sponsored advertising
http://arxiv.org/abs/1606.07189v1,"As one of the leading platforms for creative content, Tumblr offers
advertisers a unique way of creating brand identity. Advertisers can tell their
story through images, animation, text, music, video, and more, and promote that
content by sponsoring it to appear as an advertisement in the streams of Tumblr
users. In this paper we present a framework that enabled one of the key
targeted advertising components for Tumblr, specifically gender and interest
targeting. We describe the main challenges involved in development of the
framework, which include creating the ground truth for training gender
prediction models, as well as mapping Tumblr content to an interest taxonomy.
For purposes of inferring user interests we propose a novel semi-supervised
neural language model for categorization of Tumblr content (i.e., post tags and
post keywords). The model was trained on a large-scale data set consisting of
6.8 billion user posts, with very limited amount of categorized keywords, and
was shown to have superior performance over the bag-of-words model. We
successfully deployed gender and interest targeting capability in Yahoo
production systems, delivering inference for users that cover more than 90% of
daily activities at Tumblr. Online performance results indicate advantages of
the proposed approach, where we observed 20% lift in user engagement with
sponsored posts as compared to untargeted campaigns.",sponsored advertising
http://arxiv.org/abs/1902.10374v1,"Advertising (ad for short) keyword suggestion is important for sponsored
search to improve online advertising and increase search revenue. There are two
common challenges in this task. First, the keyword bidding problem: hot ad
keywords are very expensive for most of the advertisers because more
advertisers are bidding on more popular keywords, while unpopular keywords are
difficult to discover. As a result, most ads have few chances to be presented
to the users. Second, the inefficient ad impression issue: a large proportion
of search queries, which are unpopular yet relevant to many ad keywords, have
no ads presented on their search result pages. Existing retrieval-based or
matching-based methods either deteriorate the bidding competition or are unable
to suggest novel keywords to cover more queries, which leads to inefficient ad
impressions. To address the above issues, this work investigates to use
generative neural networks for keyword generation in sponsored search. Given a
purchased keyword (a word sequence) as input, our model can generate a set of
keywords that are not only relevant to the input but also satisfy the domain
constraint which enforces that the domain category of a generated keyword is as
expected. Furthermore, a reinforcement learning algorithm is proposed to
adaptively utilize domain-specific information in keyword generation. Offline
evaluation shows that the proposed model can generate keywords that are
diverse, novel, relevant to the source keyword, and accordant with the domain
constraint. Online evaluation shows that generative models can improve coverage
(COV), click-through rate (CTR), and revenue per mille (RPM) substantially in
sponsored search.",sponsored advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",sponsored advertising
http://arxiv.org/abs/0707.1057v2,"A mediator is a well-known construct in game theory, and is an entity that
plays on behalf of some of the agents who choose to use its services, while the
rest of the agents participate in the game directly. We initiate a game
theoretic study of sponsored search auctions, such as those used by Google and
Yahoo!, involving {\em incentive driven} mediators. We refer to such mediators
as {\em for-profit} mediators, so as to distinguish them from mediators
introduced in prior work, who have no monetary incentives, and are driven by
the altruistic goal of implementing certain desired outcomes. We show that in
our model, (i) players/advertisers can improve their payoffs by choosing to use
the services of the mediator, compared to directly participating in the
auction; (ii) the mediator can obtain monetary benefit by managing the
advertising burden of its group of advertisers; and (iii) the payoffs of the
mediator and the advertisers it plays for are compatible with the incentive
constraints from the advertisers who do dot use its services. A simple
intuition behind the above result comes from the observation that the mediator
has more information about and more control over the bid profile than any
individual advertiser, allowing her to reduce the payments made to the
auctioneer, while still maintaining incentive constraints. Further, our results
indicate that there are significant opportunities for diversification in the
internet economy and we should expect it to continue to develop richer
structure, with room for different types of agents to coexist.",sponsored advertising
http://arxiv.org/abs/1803.00259v1,"Bidding optimization is one of the most critical problems in online
advertising. Sponsored search (SS) auction, due to the randomness of user query
behavior and platform nature, usually adopts keyword-level bidding strategies.
In contrast, the display advertising (DA), as a relatively simpler scenario for
auction, has taken advantage of real-time bidding (RTB) to boost the
performance for advertisers. In this paper, we consider the RTB problem in
sponsored search auction, named SS-RTB. SS-RTB has a much more complex dynamic
environment, due to stochastic user query behavior and more complex bidding
policies based on multiple keywords of an ad. Most previous methods for DA
cannot be applied. We propose a reinforcement learning (RL) solution for
handling the complex dynamic environment. Although some RL methods have been
proposed for online advertising, they all fail to address the ""environment
changing"" problem: the state transition probabilities vary between two days.
Motivated by the observation that auction sequences of two days share similar
transition patterns at a proper aggregation level, we formulate a robust MDP
model at hour-aggregation level of the auction data and propose a
control-by-model framework for SS-RTB. Rather than generating bid prices
directly, we decide a bidding model for impressions of each hour and perform
real-time bidding accordingly. We also extend the method to handle the
multi-agent problem. We deployed the SS-RTB system in the e-commerce search
auction platform of Alibaba. Empirical experiments of offline evaluation and
online A/B test demonstrate the effectiveness of our method.",sponsored advertising
http://arxiv.org/abs/1808.04067v1,"With a sponsored content scheme in a wireless network, a sponsored content
service provider can pay to a network operator on behalf of the mobile
users/subscribers to lower down the network subscription fees at the reasonable
cost in terms of receiving some amount of advertisements. As such, content
providers, network operators and mobile users are all actively motivated to
participate in the sponsored content ecosystem. Meanwhile, in 5G cellular
networks, caching technique is employed to improve content service quality,
which stores potentially popular contents on edge networks nodes to serve
mobile users. In this work, we propose the joint sponsored and edge caching
content service market model. We investigate an interplay between the sponsored
content service provider and the edge caching content service provider under
the non-cooperative game framework. Furthermore, a three-stage Stackelberg game
is formulated to model the interactions among the network operator, content
service provider, and mobile users. Sub-game perfect equilibrium in each stage
is analyzed by backward induction. The existence of Stackelberg equilibrium is
validated by employing the bilevel optimization programming. Based on the game
properties, we propose a sub-gradient based iterative algorithm, which ensures
to converge to the Stackelberg equilibrium.",sponsored advertising
http://arxiv.org/abs/1210.4847v1,"We consider the budget optimization problem faced by an advertiser
participating in repeated sponsored search auctions, seeking to maximize the
number of clicks attained under that budget. We cast the budget optimization
problem as a Markov Decision Process (MDP) with censored observations, and
propose a learning algorithm based on the wellknown Kaplan-Meier or
product-limit estimator. We validate the performance of this algorithm by
comparing it to several others on a large set of search auction data from
Microsoft adCenter, demonstrating fast convergence to optimal performance.",sponsored advertising
http://arxiv.org/abs/1806.08211v1,"In online internet advertising, machine learning models are widely used to
compute the likelihood of a user engaging with product related advertisements.
However, the performance of traditional machine learning models is often
impacted due to variations in user and advertiser behavior. For example, search
engine traffic for florists usually tends to peak around Valentine's day,
Mother's day, etc. To overcome, this challenge, in this manuscript we propose
three models which are able to incorporate the effects arising due to
variations in product demand. The proposed models are a combination of product
demand features, specialized data sampling methodologies and ensemble
techniques. We demonstrate the performance of our proposed models on datasets
obtained from a real-world setting. Our results show that the proposed models
more accurately predict the outcome of users interactions with product related
advertisements while simultaneously being robust to fluctuations in user and
advertiser behaviors.",sponsored advertising
http://arxiv.org/abs/1112.6361v1,"In a sponsored search auction the advertisement slots on a search result page
are generally ordered by click-through rate. Bidders have a valuation, which is
usually assumed to be linear in the click-through rate, a budget constraint,
and receive at most one slot per search result page (round). We study
multi-round sponsored search auctions, where the different rounds are linked
through the budget constraints of the bidders and the valuation of a bidder for
all rounds is the sum of the valuations for the individual rounds. All
mechanisms published so far either study one-round sponsored search auctions or
the setting where every round has only one slot and all slots have the same
click-through rate, which is identical to a multi-item auction.
  This paper contains the following three results: (1) We give the first
mechanism for the multi-round sponsored search problem where different slots
have different click-through rates. Our mechanism is incentive compatible in
expectation, individually rational in expectation, Pareto optimal in
expectation, and also ex-post Pareto optimal for each realized outcome. (2)
Additionally we study the combinatorial setting, where each bidder is only
interested in a subset of the rounds. We give a deterministic, incentive
compatible, individually rational, and Pareto optimal mechanism for the setting
where all slots have the same click-through rate. (3) We present an
impossibility result for auctions where bidders have diminishing marginal
valuations. Specifically, we show that even for the multi-unit (one slot per
round) setting there is no incentive compatible, individually rational, and
Pareto optimal mechanism for private diminishing marginal valuations and public
budgets.",sponsored advertising
http://arxiv.org/abs/1405.2484v1,"Sponsored search auctions constitute one of the most successful applications
of microeconomic mechanisms. In mechanism design, auctions are usually designed
to incentivize advertisers to bid their truthful valuations and to assure both
the advertisers and the auctioneer a non-negative utility. Nonetheless, in
sponsored search auctions, the click-through-rates (CTRs) of the advertisers
are often unknown to the auctioneer and thus standard truthful mechanisms
cannot be directly applied and must be paired with an effective learning
algorithm for the estimation of the CTRs. This introduces the critical problem
of designing a learning mechanism able to estimate the CTRs at the same time as
implementing a truthful mechanism with a revenue loss as small as possible
compared to an optimal mechanism designed with the true CTRs. Previous work
showed that, when dominant-strategy truthfulness is adopted, in single-slot
auctions the problem can be solved using suitable exploration-exploitation
mechanisms able to achieve a per-step regret (over the auctioneer's revenue) of
order $O(T^{-1/3})$ (where T is the number of times the auction is repeated).
It is also known that, when truthfulness in expectation is adopted, a per-step
regret (over the social welfare) of order $O(T^{-1/2})$ can be obtained. In
this paper we extend the results known in the literature to the case of
multi-slot auctions. In this case, a model of the user is needed to
characterize how the advertisers' valuations change over the slots. We adopt
the cascade model that is the most famous model in the literature for sponsored
search auctions. We prove a number of novel upper bounds and lower bounds both
on the auctioneer's revenue loss and social welfare w.r.t. to the VCG auction
and we report numerical simulations investigating the accuracy of the bounds in
predicting the dependency of the regret on the auction parameters.",sponsored advertising
http://arxiv.org/abs/0805.0766v1,"Sponsored search involves running an auction among advertisers who bid in
order to have their ad shown next to search results for specific keywords.
Currently, the most popular auction for sponsored search is the ""Generalized
Second Price"" (GSP) auction in which advertisers are assigned to slots in the
decreasing order of their ""score,"" which is defined as the product of their bid
and click-through rate. In the past few years, there has been significant
research on the game-theoretic issues that arise in an advertiser's interaction
with the mechanism as well as possible redesigns of the mechanism, but this
ranking order has remained standard.
  From a search engine's perspective, the fundamental question is: what is the
best assignment of advertisers to slots? Here ""best"" could mean ""maximizing
user satisfaction,"" ""most efficient,"" ""revenue-maximizing,"" ""simplest to
interact with,"" or a combination of these. To answer this question we need to
understand the behavior of a search engine user when she sees the displayed
ads, since that defines the commodity the advertisers are bidding on, and its
value. Most prior work has assumed that the probability of a user clicking on
an ad is independent of the other ads shown on the page.
  We propose a simple Markovian user model that does not make this assumption.
We then present an algorithm to determine the most efficient assignment under
this model, which turns out to be different than that of GSP. A truthful
auction then follows from an application of the Vickrey-Clarke-Groves (VCG)
mechanism. Further, we show that our assignment has many of the desirable
properties of GSP that makes bidding intuitive. At the technical core of our
result are a number of insights about the structure of the optimal assignment.",sponsored advertising
http://arxiv.org/abs/1001.1414v2,"In pay-per click sponsored search auctions which are currently extensively
used by search engines, the auction for a keyword involves a certain number of
advertisers (say k) competing for available slots (say m) to display their ads.
This auction is typically conducted for a number of rounds (say T). There are
click probabilities mu_ij associated with each agent-slot pairs. The goal of
the search engine is to maximize social welfare of the advertisers, that is,
the sum of values of the advertisers. The search engine does not know the true
values advertisers have for a click to their respective ads and also does not
know the click probabilities mu_ij s. A key problem for the search engine
therefore is to learn these click probabilities during the T rounds of the
auction and also to ensure that the auction mechanism is truthful. Mechanisms
for addressing such learning and incentives issues have recently been
introduced and are aptly referred to as multi-armed-bandit (MAB) mechanisms.
When m = 1, characterizations for truthful MAB mechanisms are available in the
literature and it has been shown that the regret for such mechanisms will be
O(T^{2/3}). In this paper, we seek to derive a characterization in the
realistic but non-trivial general case when m > 1 and obtain several
interesting results.",sponsored advertising
http://arxiv.org/abs/1403.5768v1,"We consider the problem of designing optimal online-ad investment strategies
for a single advertiser, who invests at multiple sponsored search sites
simultaneously, with the objective of maximizing his average revenue subject to
the advertising budget constraint. A greedy online investment scheme is
developed to achieve an average revenue that can be pushed to within
$O(\epsilon)$ of the optimal, for any $\epsilon>0$, with a tradeoff that the
temporal budget violation is $O(1/\epsilon)$. Different from many existing
algorithms, our scheme allows the advertiser to \emph{asynchronously} update
his investments on each search engine site, hence applies to systems where the
timescales of action update intervals are heterogeneous for different sites. We
also quantify the impact of inaccurate estimation of the system dynamics and
show that the algorithm is robust against imperfect system knowledge.",sponsored advertising
http://arxiv.org/abs/1712.10110v5,"On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.",sponsored advertising
http://arxiv.org/abs/1807.11790v1,"Sponsored search in E-commerce platforms such as Amazon, Taobao and Tmall
provides sellers an effective way to reach potential buyers with most relevant
purpose. In this paper, we study the auction mechanism optimization problem in
sponsored search on Alibaba's mobile E-commerce platform. Besides generating
revenue, we are supposed to maintain an efficient marketplace with plenty of
quality users, guarantee a reasonable return on investment (ROI) for
advertisers, and meanwhile, facilitate a pleasant shopping experience for the
users. These requirements essentially pose a constrained optimization problem.
Directly optimizing over auction parameters yields a discontinuous, non-convex
problem that denies effective solutions. One of our major contribution is a
practical convex optimization formulation of the original problem. We devise a
novel re-parametrization of auction mechanism with discrete sets of
representative instances. To construct the optimization problem, we build an
auction simulation system which estimates the resulted business indicators of
the selected parameters by replaying the auctions recorded from real online
requests. We summarized the experiments on real search traffics to analyze the
effects of fidelity of auction simulation, the efficacy under various
constraint targets and the influence of regularization. The experiment results
show that with proper entropy regularization, we are able to maximize revenue
while constraining other business indicators within given ranges.",sponsored advertising
http://arxiv.org/abs/1810.08885v2,"Fraud has severely detrimental impacts on the business of social networks and
other online applications. A user can become a fake celebrity by purchasing
""zombie followers"" on Twitter. A merchant can boost his reputation through fake
reviews on Amazon. This phenomenon also conspicuously exists on Facebook, Yelp
and TripAdvisor, etc. In all the cases, fraudsters try to manipulate the
platform's ranking mechanism by faking interactions between the fake accounts
they control and the target customers.",fake followers
http://arxiv.org/abs/1806.07516v2,"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.",fake followers
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",fake followers
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",fake followers
http://arxiv.org/abs/1606.02409v2,"In the standard formulation of mechanism design, a key assumption is that the
designer has reliable information and technology to determine a prior
distribution on types of the agents. In the meanwhile, as pointed out by the
Wilson's Principle, a mechanism should reply as little as possible on the
accuracy of prior type distribution. In this paper, we put forward a model to
formalize and quantify this statement.
  In our model, each agent has a type distribution. In addition, the agent can
commit to a fake distribution and bids consistently and credibly with respect
to the fake distribution (i.e., plays Bayes equilibrium under the fake
distributions). We study the equilibria of the induced distribution-committing
games in several well-known mechanisms. Our results can be summarized as
follows: (1) the game induced by Myerson's auction under our model is
strategically equivalent to the first price auction under the standard model.
As a consequence, they are revenue-equivalent as well. (2) the second-price
auction yields weakly better revenue than several reserve-based and
virtual-value-based auctions, under our fake distribution model. These results
echo the recent literature on prior-independent mechanism design.",fake followers
http://arxiv.org/abs/1712.05999v1,"This article presents a preliminary approach towards characterizing political
fake news on Twitter through the analysis of their meta-data. In particular, we
focus on more than 1.5M tweets collected on the day of the election of Donald
Trump as 45th president of the United States of America. We use the meta-data
embedded within those tweets in order to look for differences between tweets
containing fake news and tweets not containing them. Specifically, we perform
our analysis only on tweets that went viral, by studying proxies for users'
exposure to the tweets, by characterizing accounts spreading fake news, and by
looking at their polarization. We found significant differences on the
distribution of followers, the number of URLs on tweets, and the verification
of the users.",fake followers
http://arxiv.org/abs/1605.07984v1,"This paper presents a method to validate the true patrons of a brand, group,
artist or any other entity on the social networking site Twitter. We analyze
the trend of total number of tweets, average retweets and total number of
followers for various nodes for different social and political backgrounds. We
argue that average retweets to follower ratio reveals the overall value of the
individual accounts and helps estimate the true to fake account ratio.",fake followers
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",fake followers
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake followers
http://arxiv.org/abs/1708.06233v1,"We model the spread of news as a social learning game on a network. Agents
can either endorse or oppose a claim made in a piece of news, which itself may
be either true or false. Agents base their decision on a private signal and
their neighbors' past actions. Given these inputs, agents follow strategies
derived via multi-agent deep reinforcement learning and receive utility from
acting in accordance with the veracity of claims. Our framework yields
strategies with agent utility close to a theoretical, Bayes optimal benchmark,
while remaining flexible to model re-specification. Optimized strategies allow
agents to correctly identify most false claims, when all agents receive
unbiased private signals. However, an adversary's attempt to spread fake news
by targeting a subset of agents with a biased private signal can be successful.
Even more so when the adversary has information about agents' network position
or private signal. When agents are aware of the presence of an adversary they
re-optimize their strategies in the training stage and the adversary's attack
is less effective. Hence, exposing agents to the possibility of fake news can
be an effective way to curtail the spread of fake news in social networks. Our
results also highlight that information about the users' private beliefs and
their social network structure can be extremely valuable to adversaries and
should be well protected.",fake followers
http://arxiv.org/abs/1802.00156v1,"When information flow fails, when Democrats and Republicans do not talk to
each other, when Israelis and Palestinians do not talk to each other, and when
North Koreans and South Koreans do not talk to each other, mis-perceptions,
biases and fake news arise. In this paper we present an in-depth study of
political polarization and social division using Twitter data and Monte Carlo
simulations. First, we study at the aggregate level people's inclination to
retweet within their own ideological circle. Introducing the concept of cocoon
ratio, we show that Donald Trump's followers are 2.56 more likely to retweet a
fellow Trump follower than to retweet a Hillary Clinton follower. Second, going
down to the individual level, we show that the tendency of retweeting
exclusively within one's ideological circle is stronger for women than for men
and that such tendency weakens as one's social capital grows. Third, we use a
one-dimensional Ising model to simulate how a society with high cocoon ratios
could end up becoming completely divided. We conclude with a discussion of our
findings with respect to fake news.",fake followers
http://arxiv.org/abs/1210.4517v1,"The proliferation of location-based social networks (LBSNs) has provided the
community with an abundant source of information that can be exploited and used
in many different ways. LBSNs offer a number of conveniences to its
participants, such as - but not limited to - a list of places in the vicinity
of a user, recommendations for an area never explored before provided by other
peers, tracking of friends, monetary rewards in the form of special deals from
the venues visited as well as a cheap way of advertisement for the latter.
However, service convenience and security have followed disjoint paths in LBSNs
and users can misuse the offered features. The major threat for the service
providers is that of fake check-ins. Users can easily manipulate the
localization module of the underlying application and declare their presence in
a counterfeit location. The incentives for these behaviors can be both earning
monetary as well as virtual rewards. Therefore, while fake check-ins driven
from the former motive can cause monetary losses, those aiming in virtual
rewards are also harmful. In particular, they can significantly degrade the
services offered from the LBSN providers (such as recommendations) or third
parties using these data (e.g., urban planners). In this paper, we propose and
analyze a honeypot venue-based solution, enhanced with a challenge-response
scheme, that flags users who are generating fake spatial information. We
believe that our work will stimulate further research on this important topic
and will provide new directions with regards to possible solutions.",fake followers
http://arxiv.org/abs/1711.09918v1,"Online social networking sites are experimenting with the following
crowd-powered procedure to reduce the spread of fake news and misinformation:
whenever a user is exposed to a story through her feed, she can flag the story
as misinformation and, if the story receives enough flags, it is sent to a
trusted third party for fact checking. If this party identifies the story as
misinformation, it is marked as disputed. However, given the uncertain number
of exposures, the high cost of fact checking, and the trade-off between flags
and exposures, the above mentioned procedure requires careful reasoning and
smart algorithms which, to the best of our knowledge, do not exist to date.
  In this paper, we first introduce a flexible representation of the above
procedure using the framework of marked temporal point processes. Then, we
develop a scalable online algorithm, Curb, to select which stories to send for
fact checking and when to do so to efficiently reduce the spread of
misinformation with provable guarantees. In doing so, we need to solve a novel
stochastic optimal control problem for stochastic differential equations with
jumps, which is of independent interest. Experiments on two real-world datasets
gathered from Twitter and Weibo show that our algorithm may be able to
effectively reduce the spread of fake news and misinformation.",fake followers
http://arxiv.org/abs/1809.04364v1,"This paper proposes a method for utilizing thermal features of the hand for
the purpose of presentation attack detection (PAD) that can be employed in a
hand biometrics system's pipeline. By envisaging two different operational
modes of our system, and by employing a DCNN-based classifiers fine-tuned with
a dataset of real and fake hand representations captured in both visible and
ther- mal spectrum, we were able to bring two important deliverables. First, a
PAD method operating in an open-set mode, capable of correctly discerning 100%
of fake thermal samples, achieving Attack Presentation Classification Error
Rate (APCER) and Bona-Fide Presentation Classification Error Rate (BPCER) equal
to 0%, which can be easily implemented into any existing system as a separate
component. Second, a hand biometrics system operating in a closed-set mode,
that has PAD built right into the recognition pipeline, and operating
simultaneously with the user-wise classification, achieving rank-1 recognition
accuracy of up to 99.75%. We also show that thermal images of the human hand,
in addition to liveness features they carry, can also improve classification
accuracy of a biometric system, when coupled with visible light images. To
follow the reproducibility guidelines and to stimulate further research in this
area, we share the trained model weights, source codes, and a newly created
dataset of fake hand representations with interested researchers.",fake followers
http://arxiv.org/abs/1811.06002v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip and GEM detectors due to appearance in
them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. We introduce a renewed method
that is a significant improvement of our previous two-stage approach based on
hit preprocessing using directed K-d tree search followed a deep neural
classifier. We combine these two stages in one by applying recurrent neural
network that simultaneously determines whether a set of points belongs to a
true track or not and predicts where to look for the next point of track on the
next coordinate plane of the detector. We show that proposed deep network is
more accurate, faster and does not require any special preprocessing stage.
Preliminary results of our approach for simulated events of the BM@N GEM
detector are presented.",fake followers
http://arxiv.org/abs/1812.03859v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip in GEM detectors due to the appearance
in them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. On the basis of our previous
two-stage approach based on hits preprocessing using directed K-d tree search
followed by a deep neural classifier we introduce here two new tracking
algorithms. Both algorithms combine those two stages in one while using
different types of deep neural nets. We show that both proposed deep networks
do not require any special preprocessing stage, are more accurate, faster and
can be easier parallelized. Preliminary results of our new approaches for
simulated events are presented.",fake followers
http://arxiv.org/abs/1907.03048v1,"Download fraud is a prevalent threat in mobile App markets, where fraudsters
manipulate the number of downloads of Apps via various cheating approaches.
Purchased fake downloads can mislead recommendation and search algorithms and
further lead to bad user experience in App markets. In this paper, we
investigate download fraud problem based on a company's App Market, which is
one of the most popular Android App markets. We release a honeypot App on the
App Market and purchase fake downloads from fraudster agents to track fraud
activities in the wild. Based on our interaction with the fraudsters, we
categorize download fraud activities into three types according to their
intentions: boosting front end downloads, optimizing App search ranking, and
enhancing user acquisition&retention rate. For the download fraud aimed at
optimizing App search ranking, we select, evaluate, and validate several
features in identifying fake downloads based on billions of download data. To
get a comprehensive understanding of download fraud, we further gather stances
of App marketers, fraudster agencies, and market operators on download fraud.
The followed analysis and suggestions shed light on the ways to mitigate
download fraud in App markets and other social platforms. To the best of our
knowledge, this is the first work that investigates the download fraud problem
in mobile App markets.",fake followers
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake followers
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake followers
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake followers
http://arxiv.org/abs/1610.07525v4,"In the past decade, network structures have penetrated nearly every aspect of
our lives. The detection of anomalous vertices in these networks has become
increasingly important, such as in exposing computer network intruders or
identifying fake online reviews. In this study, we present a novel unsupervised
two-layered meta-classifier that can detect irregular vertices in complex
networks solely by using features extracted from the network topology.
Following the reasoning that a vertex with many improbable links has a higher
likelihood of being anomalous,we employed our method on 10 networks of various
scales, from a network of several dozen students to online social networks with
millions of users. In every scenario, we were able to identify anomalous
vertices with lower false positive rates and higher AUCs compared to other
prevalent methods. Moreover, we demonstrated that the presented algorithm is
efficient both in revealing fake users and in disclosing the most influential
people in social networks.",fake followers
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake followers
http://arxiv.org/abs/0908.0809v1,"We use generating functional analysis to study minority-game type market
models with generalized strategy valuation updates that control the psychology
of agents' actions. The agents' choice between trend following and contrarian
trading, and their vigor in each, depends on the overall state of the market.
Even in `fake history' models, the theory now involves an effective overall bid
process (coupled to the effective agent process) which can exhibit profound
remanence effects and new phase transitions. For some models the bid process
can be solved directly, others require Maxwell-construction type
approximations.",fake followers
http://arxiv.org/abs/1012.3802v1,"This chapter presents a framework for detecting fake regions by using various
methods including watermarking technique and blind approaches. In particular,
we describe current categories on blind approaches which can be divided into
five: pixel-based techniques, format-based techniques, camera-based techniques,
physically-based techniques and geometric-based techniques. Then we take a
second look on the geometric-based techniques and further categorize them in
detail. In the following section, the state-of-the-art methods involved in the
geometric technique are elaborated.",fake followers
http://arxiv.org/abs/physics/0509090v2,"We study the effect of globalization on the Korean market, one of the
emerging markets. Some characteristics of the Korean market are different from
those of the mature market according to the latest market data, and this is due
to the influence of foreign markets or investors. We concentrate on the market
network structures over the past two decades with knowledge of the history of
the market, and determine the globalization effect and market integration as a
function of time.",influencer marketing
http://arxiv.org/abs/1505.02766v1,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.",influencer marketing
http://arxiv.org/abs/1511.00750v1,"The purchasing behaviour of consumers is often influenced by numerous
factors, including the visibility of the products and the influence of other
customers through their own purchases or their recommendations.
  Motivated by trial-offer and freemium markets and a number of online markets
for cultural products, leisure services, and retail, this paper studies the
dynamics of a marketplace ran by a single firm and which is visited by
heterogeneous consumers whose choice preferences can be modeled using a Mixed
Multinomial Logit. In this marketplace, consumers are influenced by past
purchases, the inherent appeal of the products, and the visibility of each
product. The resulting market generalizes recent models already verified in
cultural markets.
  We examine various marketing policies for this market and analyze their
long-term dynamics and the potential benefits of social influence. In
particular, we show that the heterogeneity of the customers complicates the
market significantly: Many of the optimality and computational properties of
the corresponding homogeneous market no longer hold. To remedy these
limitations, we explore a market segmentation strategy and quantify its
benefits. The theoretical results are complemented by Monte Carlo simulations
conducted on examples of interest.",influencer marketing
http://arxiv.org/abs/1505.02469v3,"Social influence is ubiquitous in cultural markets, from book recommendations
in Amazon, to song popularities in iTunes and the ranking of newspaper articles
in the online edition of the New York Times to mention only a few. Yet social
influence is often presented in a bad light, often because it supposedly
increases market unpredictability.
  Here we study a model of trial-offer markets, in which participants try
products and later decide whether to purchase. We consider a simple policy
which ranks the products by quality when presenting them to market
participants. We show that, in this setting, market efficiency always benefits
from social influence. Moreover, we prove that the market converges almost
surely to a monopoly for the product of highest quality, making the market both
predictable and asymptotically optimal. Computational experiments confirm that
the quality ranking policy identifies ""blockbusters"" in reasonable time,
outperforms other policies, and is highly predictable. These results indicate
that social influence does not necessarily increase market unpredicatibility.
The outcome really depends on how social influence is used.",influencer marketing
http://arxiv.org/abs/1009.0309v1,"In this paper, inspired by the work of Megiddo on the formation of
preferences and strategic analysis, we consider an early market model studied
in the field of economic theory, in which each trader's utility may be
influenced by the bundles of goods obtained by her social neighbors. The goal
of this paper is to understand and characterize the impact of social influence
on the complexity of computing and approximating market equilibria.
  We present complexity-theoretic and algorithmic results for approximating
market equilibria in this model with focus on two concrete influence models
based on the traditional linear utility functions. Recall that an Arrow-Debreu
market equilibrium in a conventional exchange market with linear utility
functions can be computed in polynomial time by convex programming. Our
complexity results show that even a bounded-degree, planar influence network
can significantly increase the difficulty of equilibrium computation even in
markets with only a constant number of goods. Our algorithmic results suggest
that finding an approximate equilibrium in markets with hierarchical influence
networks might be easier than that in markets with arbitrary neighborhood
structures. By demonstrating a simple market with a constant number of goods
and a bounded-degree, planar influence graph whose equilibrium is PPAD-hard to
approximate, we also provide a counterexample to a common belief, which we
refer to as the myth of a constant number of goods, that equilibria in markets
with a constant number of goods are easy to compute or easy to approximate.",influencer marketing
http://arxiv.org/abs/1508.07272v1,"Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.",influencer marketing
http://arxiv.org/abs/1907.05028v1,"The Viral Marketing is a relatively new form of marketing that exploits
social networks to promote a brand, a product, etc. The idea behind it is to
find a set of influencers on the network that can trigger a large cascade of
propagation and adoptions. In this paper, we will introduce an evidential
opinion-based influence maximization model for viral marketing. Besides, our
approach tackles three opinions based scenarios for viral marketing in the real
world. The first scenario concerns influencers who have a positive opinion
about the product. The second scenario deals with influencers who have a
positive opinion about the product and produce effects on users who also have a
positive opinion. The third scenario involves influence users who have a
positive opinion about the product and produce effects on the negative opinion
of other users concerning the product in question. Next, we proposed six
influence measures, two for each scenario. We also use an influence
maximization model that the set of detected influencers for each scenario.
Finally, we show the performance of the proposed model with each influence
measure through some experiments conducted on a generated dataset and a real
world dataset collected from Twitter.",influencer marketing
http://arxiv.org/abs/physics/0607071v1,"Financial markets are subject to long periods of polarized behavior, such as
bull-market or bear-market phases, in which the vast majority of market
participants seem to almost exclusively choose one action (between buying or
selling) over the other. From the point of view of conventional economic
theory, such events are thought to reflect the arrival of ``external news''
that justifies the observed behavior. However, empirical observations of the
events leading up to such market phases, as well events occurring during the
lifetime of such a phase, have often failed to find significant correlation
between news from outside the market and the behavior of the agents comprising
the market. In this paper, we explore the alternative hypothesis that the
occurrence of such market polarizations are due to interactions amongst the
agents in the market, and not due to any influence external to it. In
particular, we present a model where the market (i.e., the aggregate behavior
of all the agents) is observed to become polarized even though individual
agents regularly change their actions (buy or sell) on a time-scale much
shorter than that of the market polarization phase.",influencer marketing
http://arxiv.org/abs/1503.00823v1,"In a stock market, the price fluctuations are interactive, that is, one
listed company can influence others. In this paper, we seek to study the
influence relationships among listed companies by constructing a directed
network on the basis of Chinese stock market. This influence network shows
distinct topological properties, particularly, a few large companies that can
lead the tendency of stock market are recognized. Furthermore, by analyzing the
subnetworks of listed companies distributed in several significant economic
sectors, it is found that the influence relationships are totally different
from one economic sector to another, of which three types of connectivity as
well as hub-like listed companies are identified. In addition, the rankings of
listed companies obtained from the centrality metrics of influence network are
compared with that according to the assets, which gives inspiration to uncover
and understand the importance of listed companies in the stock market. These
empirical results are meaningful in providing these topological properties of
Chinese stock market and economic sectors as well as revealing the
interactively influence relationships among listed companies.",influencer marketing
http://arxiv.org/abs/1512.07251v4,"This paper considers trial-offer markets where consumer preferences are
modeled by a multinomial logit with social influence and position bias. The
social signal for a product is given by its current market share raised to
power r (or equivalently the number of purchases raised to the power of r). The
paper shows that, when r is strictly between 0 and 1, and a static position
assignment (e.g., a quality ranking) is used, the market converges to a unique
equilibrium where the market shares depend only on product quality, not their
initial appeals or the early dynamics. When r is greater than 1, the market
becomes unpredictable. In many cases, the market goes to a monopoly for some
product: Which product becomes a monopoly depends on the initial conditions of
the market. These theoretical results are complemented by an agent-based
simulation which indicates that convergence is fast when r is between 0 and 1,
and that the quality ranking dominates the well-known popularity ranking in
terms of market efficiency. These results shed a new light on the role of
social influence which is often blamed for unpredictability, inequalities, and
inefficiencies in markets. In contrast, this paper shows that, with a proper
social signal and position assignment for the products, the market becomes
predictable, and inequalities and inefficiencies can be controlled
appropriately.",influencer marketing
http://arxiv.org/abs/1604.01672v1,"In this paper, we investigate the effect of brand in market competition.
Specifically, we propose a variant Hotelling model where companies and
customers are represented by points in an Euclidean space, with axes being
product features. $N$ companies compete to maximize their own profits by
optimally choosing their prices, while each customer in the market, when
choosing sellers, considers the sum of product price, discrepancy between
product feature and his preference, and a company's brand name, which is
modeled by a function of its market area of the form $-\beta\cdot\text{(Market
Area)}^q$, where $\beta$ captures the brand influence and $q$ captures how
market share affects the brand. By varying the parameters $\beta$ and $q$, we
derive existence results of Nash equilibrium and equilibrium market prices and
shares. In particular, we prove that pure Nash equilibrium always exists when
$q=0$ for markets with either one and two dominating features, and it always
exists in a single dominating feature market when market affects brand name
linearly, i.e., $q=1$. Moreover, we show that at equilibrium, a company's price
is proportional to its market area over the competition intensity with its
neighbors, a result that quantitatively reconciles the common belief of a
company's pricing power. We also study an interesting ""wipe out"" phenomenon
that only appears when $q>0$, which is similar to the ""undercut"" phenomenon in
the Hotelling model, where companies may suddenly lose the entire market area
with a small price increment. Our results offer novel insight into market
pricing and positioning under competition with brand effect.",influencer marketing
http://arxiv.org/abs/1407.7015v1,"Prediction markets are often used as mechanisms to aggregate information
about a future event, for example, whether a candidate will win an election.
The event is typically assumed to be exogenous. In reality, participants may
influence the outcome, and therefore (1) running the prediction market could
change the incentives of participants in the process that creates the outcome
(for example, agents may want to change their vote in an election), and (2)
simple results such as the myopic incentive compatibility of proper scoring
rules no longer hold in the prediction market itself. We introduce a model of
games of this kind, where agents first trade in a prediction market and then
take an action that influences the market outcome. Our two-stage two-player
model, despite its simplicity, captures two aspects of real-world prediction
markets: (1) agents may directly influence the outcome, (2) some of the agents
instrumental in deciding the outcome may not take part in the prediction
market. We show that this game has two different types of perfect Bayesian
equilibria, which we term LPP and HPP, depending on the values of the belief
parameters: in the LPP domain, equilibrium prices reveal expected market
outcomes conditional on the participants' private information, whereas HPP
equilibria are collusive -- participants effectively coordinate in an
uninformative and untruthful way.",influencer marketing
http://arxiv.org/abs/cs/0109108v1,"This paper examines the effects of licensing conditions, in particular of
spectrum fees, on the pricing and diffusion of mobile communications services.
Seemingly exorbitant sums paid for 3G licenses in the UK, Germany in 2000 and
similarly high fees paid by U.S. carriers in the re-auctioning of PCS licenses
early in 2001 raised concerns as to the impacts of the market entry regime on
the mobile communications market.
  The evidence from the GSM and PCS markets reviewed in this paper suggests
that market entry fees do indeed influence the subsequent development of the
market. We discuss three potential transmission channels by which license fees
can influence the price and quantity of service sold in a wireless market: an
increase in average cost, an increase in incremental costs, and impacts of sunk
costs on the emerging market structure.
  From this conceptual debate, an empirical model is developed and tested using
cross-sectional data for the residential mobile voice market. We utilize a
structural equation approach, modeling the supply and demand relationships
subject to the constraint that supply equals demand. The results confirm the
existence of a positive effect of license fees on the cost of supply. However,
we also find that higher market concentration has a positive effect on the
overall supply in the market, perhaps supporting a Schumpeterian view that a
certain degree of market concentration facilitates efficiency.",influencer marketing
http://arxiv.org/abs/1601.05283v2,"The paper proposes a way to add marketing into the standard threshold model
of social networks. Within this framework, the paper studies logical properties
of the influence relation between sets of agents in social networks. Two
different forms of this relation are considered: one for promotional marketing
and the other for preventive marketing. In each case a sound and complete
logical system describing properties of the influence relation is proposed.
Both systems could be viewed as extensions of Armstrong's axioms of functional
dependency from the database theory.",influencer marketing
http://arxiv.org/abs/1408.1542v4,"Social influence has been shown to create significant unpredictability in
cultural markets, providing one potential explanation why experts routinely
fail at predicting commercial success of cultural products. To counteract the
difficulty of making accurate predictions, ""measure and react"" strategies have
been advocated but finding a concrete strategy that scales for very large
markets has remained elusive so far. Here we propose a ""measure and optimize""
strategy based on an optimization policy that uses product quality, appeal, and
social influence to maximize expected profits in the market at each decision
point. Our computational experiments show that our policy leverages social
influence to produce significant performance benefits for the market, while our
theoretical analysis proves that our policy outperforms in expectation any
policy not displaying social information. Our results contrast with earlier
work which focused on showing the unpredictability and inequalities created by
social influence. Not only do we show for the first time that dynamically
showing consumers positive social information under our policy increases the
expected performance of the seller in cultural markets. We also show that, in
reasonable settings, our policy does not introduce significant unpredictability
and identifies ""blockbusters"". Overall, these results shed new light on the
nature of social influence and how it can be leveraged for the benefits of the
market.",influencer marketing
http://arxiv.org/abs/1107.0028v1,"With the recent technological feasibility of electronic commerce over the
Internet, much attention has been given to the design of electronic markets for
various types of electronically-tradable goods. Such markets, however, will
normally need to function in some relationship with markets for other related
goods, usually those downstream or upstream in the supply chain. Thus, for
example, an electronic market for rubber tires for trucks will likely need to
be strongly influenced by the rubber market as well as by the truck market. In
this paper we design protocols for exchange of information between a sequence
of markets along a single supply chain. These protocols allow each of these
markets to function separately, while the information exchanged ensures
efficient global behavior across the supply chain. Each market that forms a
link in the supply chain operates as a double auction, where the bids on one
side of the double auction come from bidders in the corresponding segment of
the industry, and the bids on the other side are synthetically generated by the
protocol to express the combined information from all other links in the chain.
The double auctions in each of the markets can be of several types, and we
study several variants of incentive compatible double auctions, comparing them
in terms of their efficiency and of the market revenue.",influencer marketing
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detecting fake reviews
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detecting fake reviews
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detecting fake reviews
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detecting fake reviews
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",detecting fake reviews
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detecting fake reviews
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detecting fake reviews
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detecting fake reviews
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",detecting fake reviews
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",detecting fake reviews
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",detecting fake reviews
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",detecting fake reviews
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",detecting fake reviews
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",detecting fake reviews
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",detecting fake reviews
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detecting fake reviews
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",detecting fake reviews
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",detecting fake reviews
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detecting fake reviews
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detecting fake reviews
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",detecting fake reviews
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detecting fake reviews
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detecting fake reviews
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detecting fake reviews
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",detecting fake reviews
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detecting fake reviews
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",detecting fake reviews
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detecting fake reviews
http://arxiv.org/abs/1706.00884v1,"Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.",detecting fake reviews
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",detecting fake reviews
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake review detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake review detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake review detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake review detection
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake review detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake review detection
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",fake review detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake review detection
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",fake review detection
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",fake review detection
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",fake review detection
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",fake review detection
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",fake review detection
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake review detection
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",fake review detection
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake review detection
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",fake review detection
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",fake review detection
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake review detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake review detection
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",fake review detection
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake review detection
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake review detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake review detection
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake review detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake review detection
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",fake review detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake review detection
http://arxiv.org/abs/1706.00884v1,"Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.",fake review detection
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake review detection
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",tracking fake reviews
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",tracking fake reviews
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",tracking fake reviews
http://arxiv.org/abs/1512.05457v2,"How can web services that depend on user generated content discern fake
social engagement activities by spammers from legitimate ones? In this paper,
we focus on the social site of YouTube and the problem of identifying bad
actors posting inorganic contents and inflating the count of social engagement
metrics. We propose an effective method, Leas (Local Expansion at Scale), and
show how the fake engagement activities on YouTube can be tracked over time by
analyzing the temporal graph based on the engagement behavior pattern between
users and YouTube videos. With the domain knowledge of spammer seeds, we
formulate and tackle the problem in a semi-supervised manner --- with the
objective of searching for individuals that have similar pattern of behavior as
the known seeds --- based on a graph diffusion process via local spectral
subspace. We offer a fast, scalable MapReduce deployment adapted from the
localized spectral clustering algorithm. We demonstrate the effectiveness of
our deployment at Google by achieving an manual review accuracy of 98% on
YouTube Comments graph in practice. Comparing with the state-of-the-art
algorithm CopyCatch, Leas achieves 10 times faster running time. Leas is
actively in use at Google, searching for daily deceptive practices on YouTube's
engagement graph spanning over a billion users.",tracking fake reviews
http://arxiv.org/abs/0812.5036v1,"The expected performance of track reconstruction with LHC events using the
CMS silicon tracker is presented. Track finding and fitting is accomplished
with Kalman Filter techniques that achieve efficiencies above 99% on single
muons with pT>1 GeV/c. Difficulties arise in the context of standard LHC events
with a high density of charged particles, where the rate of fake combinatorial
tracks is very large for low pT tracks, and nuclear interactions in the tracker
material reduce the tracking efficiency for charged hadrons. Recent
improvements with the CMS track reconstruction now allow to efficiently
reconstruct charged tracks with pT down to few hundred MeV/c and as few as
three crossed layers, with a very small fake fraction, by making use of an
optimal rejection of fake tracks in conjunction with an iterative tracking
procedure.",tracking fake reviews
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",tracking fake reviews
http://arxiv.org/abs/1706.05924v2,"An important challenge in the process of tracking and detecting the
dissemination of misinformation is to understand the political gap between
people that engage with the so called ""fake news"". A possible factor
responsible for this gap is opinion polarization, which may prompt the general
public to classify content that they disagree or want to discredit as fake. In
this work, we study the relationship between political polarization and content
reported by Twitter users as related to ""fake news"". We investigate how
polarization may create distinct narratives on what misinformation actually is.
We perform our study based on two datasets collected from Twitter. The first
dataset contains tweets about US politics in general, from which we compute the
degree of polarization of each user towards the Republican and Democratic
Party. In the second dataset, we collect tweets and URLs that co-occurred with
""fake news"" related keywords and hashtags, such as #FakeNews and
#AlternativeFact, as well as reactions towards such tweets and URLs. We then
analyze the relationship between polarization and what is perceived as
misinformation, and whether users are designating information that they
disagree as fake. Our results show an increase in the polarization of users and
URLs associated with fake-news keywords and hashtags, when compared to
information not labeled as ""fake news"". We discuss the impact of our findings
on the challenges of tracking ""fake news"" in the ongoing battle against
misinformation.",tracking fake reviews
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",tracking fake reviews
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",tracking fake reviews
http://arxiv.org/abs/1810.08885v2,"Fraud has severely detrimental impacts on the business of social networks and
other online applications. A user can become a fake celebrity by purchasing
""zombie followers"" on Twitter. A merchant can boost his reputation through fake
reviews on Amazon. This phenomenon also conspicuously exists on Facebook, Yelp
and TripAdvisor, etc. In all the cases, fraudsters try to manipulate the
platform's ranking mechanism by faking interactions between the fake accounts
they control and the target customers.",tracking fake reviews
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",tracking fake reviews
http://arxiv.org/abs/1803.03443v3,"Social media can be a double-edged sword for society, either as a convenient
channel exchanging ideas or as an unexpected conduit circulating fake news
through a large population. While existing studies of fake news focus on
theoretical modeling of propagation or identification methods based on machine
learning, it is important to understand the realistic mechanisms between
theoretical models and black-box methods. Here we track large databases of fake
news and real news in both, Weibo in China and Twitter in Japan from different
culture, which include their complete traces of re-postings. We find in both
online social networks that fake news spreads distinctively from real news even
at early stages of propagation, e.g. five hours after the first re-postings.
Our finding demonstrates collective structural signals that help to understand
the different propagation evolution of fake news and real news. Different from
earlier studies, identifying the topological properties of the information
propagation at early stages may offer novel features for early detection of
fake news in social media.",tracking fake reviews
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",tracking fake reviews
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",tracking fake reviews
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",tracking fake reviews
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",tracking fake reviews
http://arxiv.org/abs/1811.06002v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip and GEM detectors due to appearance in
them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. We introduce a renewed method
that is a significant improvement of our previous two-stage approach based on
hit preprocessing using directed K-d tree search followed a deep neural
classifier. We combine these two stages in one by applying recurrent neural
network that simultaneously determines whether a set of points belongs to a
true track or not and predicts where to look for the next point of track on the
next coordinate plane of the detector. We show that proposed deep network is
more accurate, faster and does not require any special preprocessing stage.
Preliminary results of our approach for simulated events of the BM@N GEM
detector are presented.",tracking fake reviews
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",tracking fake reviews
http://arxiv.org/abs/1904.05386v1,"The rise of ubiquitous misinformation, disinformation, propaganda and
post-truth, often referred to as fake news, raises some concerns over the role
of Internet and social media in modern democratic societies. Due to its rapid
and widespread diffusion, online fake news have not only an individual or
societal cost (e.g., hamper the integrity of elections), but they can lead to
significant economic losses (e.g., affect stock market performance) or risks to
national security. Blockchain and other Distributed Ledger Technologies (DLTs)
guarantee the provenance and traceability of the data by providing a
transparent, immutable and verifiable record of transactions while creating a
peer-to-peer platform for exchanging, storing and securing information. This
article aims to explore the potential of DLTs and blockchain to combat fake
news, reviewing initiatives that are currently under development and
identifying their main current challenges. Moreover, some recommendations are
enumerated to guide future researchers on issues that will have to be tackled
to face fake news, as an integral part of strengthening the resilience against
cyber-threats of today's online media.",tracking fake reviews
http://arxiv.org/abs/1812.03859v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip in GEM detectors due to the appearance
in them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. On the basis of our previous
two-stage approach based on hits preprocessing using directed K-d tree search
followed by a deep neural classifier we introduce here two new tracking
algorithms. Both algorithms combine those two stages in one while using
different types of deep neural nets. We show that both proposed deep networks
do not require any special preprocessing stage, are more accurate, faster and
can be easier parallelized. Preliminary results of our new approaches for
simulated events are presented.",tracking fake reviews
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",tracking fake reviews
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",tracking fake reviews
http://arxiv.org/abs/1408.5536v1,"The hit combinatorial problem is a main challenge for track reconstruction
and triggering at high rate experiments. At hadron colliders the dominant
fraction of hits is due to low momentum tracks for which multiple scattering
(MS) effects dominate the hit resolution. MS is also the dominating source for
hit confusion and track uncertainties in low energy precision experiments. In
all such environments, where MS dominates, track reconstruction and fitting can
be largely simplified by using three-dimensional (3D) hit-triplets as provided
by pixel detectors. This simplification is possible since track uncertainties
are solely determined by MS if high precision spatial information is provided.
Fitting of hit-triplets is especially simple for tracking detectors in
solenoidal magnetic fields. The over-constrained 3D-triplet method provides a
complete set of track parameters and is robust against fake hit combinations.
The triplet method is ideally suited for pixel detectors where hits can be
treated as 3D-space points. With the advent of relatively cheap and
industrially available CMOS-sensors the construction of highly granular full
scale pixel tracking detectors seems to be possible also for experiments at LHC
or future high energy (hadron) colliders. In this paper tracking performance
studies for full-scale pixel detectors, including their optimisation for
3D-triplet tracking, are presented. The results obtained for different types of
tracker geometries and different reconstruction methods are compared. The
potential of reducing the number of tracking layers and -- along with that --
the material budget using this new tracking concept is discussed. The
possibility of using 3D-triplet tracking for triggering and fast online
reconstruction is highlighted.",tracking fake reviews
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",tracking fake reviews
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",tracking fake reviews
http://arxiv.org/abs/1512.09008v1,"We propose a novel fast track finding system capable of reconstructing four
dimensional particle trajectories in real time using precise space and time
information of the hits. Recent developments in silicon pixel detectors
achieved 150 ps time resolution and intense R&D is in progress to improve the
timing performance, aiming at 10 ps. The use of the precise space and time
information allows the suppression of background hits not compatible with the
time of passage of the particle and the determination of its time evolution.
The fast track finding device that we are proposing is based on a massively
parallel algorithm implemented in commercial field-programmable gate array
using a pipelined architecture. We describe the algorithm and its
implementation for a tracking system prototype based on 8 planes of silicon
sensors used as a case study. According to simulations the suppression of noise
hits is effective in reducing fake track combinations and improving real-time
track reconstruction in presence of background hits. The system provides
offline-like tracks with sub-microsecond latency and it is capable to determine
the time of the track with picosecond resolution assuming 10 ps resolution for
the hits.",tracking fake reviews
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",tracking fake reviews
http://arxiv.org/abs/1303.3630v1,"The ATLAS Inner Detector is responsible for particle tracking in ATLAS
experiment at CERN Large Hadron Collider (LHC) and comprises silicon and gas
based detectors. The combination of both silicon and gas based detectors
provides high precision impact parameter and momentum measurement of charged
particles, with high efficiency and small fake rate. The ID has been used to
exploit fully the physics potential of the LHC since the first proton-proton
collisions at 7 TeV were delivered in 2009. The performance of track and vertex
reconstruction is presented, as well as the operation aspects of the Inner
Detector and the data quality during the many months of data taking.",tracking fake reviews
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",tracking fake reviews
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",web beacon behavioral tracking
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",web beacon behavioral tracking
http://arxiv.org/abs/1709.10237v1,"Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.",web beacon behavioral tracking
http://arxiv.org/abs/1503.03388v1,"This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.",web beacon behavioral tracking
http://arxiv.org/abs/1706.05569v3,"In this paper, we develop a system for the low-cost indoor localization and
tracking problem using radio signal strength indicator, Inertial Measurement
Unit (IMU), and magnetometer sensors. We develop a novel and simplified
probabilistic IMU motion model as the proposal distribution of the sequential
Monte-Carlo technique to track the robot trajectory. Our algorithm can globally
localize and track a robot with a priori unknown location, given an informative
prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the
problem as an optimization problem that serves as the Back-end of the algorithm
mentioned above (Front-end). Thus, by simultaneously solving for the robot
trajectory and the map of BLE beacons, we recover a continuous and smooth
trajectory of the robot, corrected locations of the BLE beacons, and the
time-varying IMU bias. The evaluations achieved using hardware show that
through the proposed closed-loop system the localization performance can be
improved; furthermore, the system becomes robust to the error in the map of
beacons by feeding back the optimized map to the Front-end.",web beacon behavioral tracking
http://arxiv.org/abs/1812.01514v3,"Web tracking has been extensively studied over the last decade. To detect
tracking, previous studies and user tools rely on filter lists. However, it has
been shown that filter lists miss trackers. In this paper, we propose an
alternative method to detect trackers inspired by analyzing behavior of
invisible pixels. By crawling 84,658 webpages from 8,744 domains, we detect
that third-party invisible pixels are widely deployed: they are present on more
than 94.51% of domains and constitute 35.66% of all third-party images. We
propose a fine-grained behavioral classification of tracking based on the
analysis of invisible pixels. We use this classification to detect new
categories of tracking and uncover new collaborations between domains on the
full dataset of 4,216,454 third-party requests. We demonstrate that two popular
methods to detect tracking, based on EasyList&EasyPrivacy and on Disconnect
lists respectively miss 25.22% and 30.34% of the trackers that we detect.
Moreover, we find that if we combine all three lists 379,245 requests
originated from 8,744 domains still track users on 68.70% of websites.",web beacon behavioral tracking
http://arxiv.org/abs/1904.01725v1,"Web traffic is a valuable data source, typically used in the marketing space
to track brand awareness and advertising effectiveness. However, web traffic is
also a rich source of information for cybersecurity monitoring efforts. To
better understand the threat of malicious cyber actors, this study develops a
methodology to monitor and evaluate web activity using data archived from
Google Analytics. Google Analytics collects and aggregates web traffic,
including information about web visitors' location, date and time of visit,
visited webpages, and searched keywords. This study seeks to streamline
analysis of this data and uses rule-based anomaly detection and predictive
modeling to identify web traffic that deviates from normal patterns. Rather
than evaluating pieces of web traffic individually, the methodology seeks to
emulate real user behavior by creating a new unit of analysis: the user
session. User sessions group individual pieces of traffic from the same
location and date, which transforms the available information from single
point-in-time snapshots to dynamic sessions showing users' trajectory and
intent. The result is faster and better insight into large volumes of noisy web
traffic.",web beacon behavioral tracking
http://arxiv.org/abs/1901.06601v1,"Recent years have seen interest in device tracking and localization using
acoustic signals. State-of-the-art acoustic motion tracking systems however do
not achieve millimeter accuracy and require large separation between
microphones and speakers, and as a result, do not meet the requirements for
many VR/AR applications. Further, tracking multiple concurrent acoustic
transmissions from VR devices today requires sacrificing accuracy or frame
rate. We present MilliSonic, a novel system that pushes the limits of acoustic
based motion tracking. Our core contribution is a novel localization algorithm
that can provably achieve sub-millimeter 1D tracking accuracy in the presence
of multipath, while using only a single beacon with a small 4-microphone
array.Further, MilliSonic enables concurrent tracking of up to four smartphones
without reducing frame rate or accuracy. Our evaluation shows that MilliSonic
achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for
smartphones, which is 5x more accurate than state-of-the-art systems.
MilliSonic enables two previously infeasible interaction applications: a) 3D
tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D
tracking for the Google Cardboard VR system using a small microphone array.",web beacon behavioral tracking
http://arxiv.org/abs/1702.05116v2,"Cyclic pursuit frameworks, which are built upon pursuit interactions between
neighboring agents in a cycle graph, provide an efficient way to create useful
global behaviors in a collective of autonomous robots. Previous work had
considered cyclic pursuit with a constant bearing (CB) pursuit law, and
demonstrated the existence of circling equilibria for the corresponding
dynamics. In this work, we propose a beacon-referenced version of the CB
pursuit law, wherein a stationary beacon provides an additional reference for
the individual agents in a collective. When implemented in a cyclic framework,
we show that the resulting dynamics admit relative equilibria corresponding to
a circling orbit around the beacon, with the circling radius and the
distribution of agents along the orbit determined by parameters of the proposed
pursuit law. We also derive necessary conditions for stability of the circling
equilibria, which provides a guide for parameter selection. Finally, by
introducing a change of variables, we demonstrate the existence of a family of
invariant manifolds related to spiraling motions around the beacon which
preserve the ""pure shape"" of the collective, and study the reduced dynamics on
a representative manifold.",web beacon behavioral tracking
http://arxiv.org/abs/1506.05367v1,"We propose and investigate a compressive architecture for estimation and
tracking of sparse spatial channels in millimeter (mm) wave picocellular
networks. The base stations are equipped with antenna arrays with a large
number of elements (which can fit within compact form factors because of the
small carrier wavelength) and employ radio frequency (RF) beamforming, so that
standard least squares adaptation techniques (which require access to
individual antenna elements) are not applicable. We focus on the downlink, and
show that ""compressive beacons,"" transmitted using pseudorandom phase settings
at the base station array, and compressively processed using pseudorandom phase
settings at the mobile array, provide information sufficient for accurate
estimation of the two-dimensional (2D) spatial frequencies associated with the
directions of departure of the dominant rays from the base station, and the
associated complex gains. This compressive approach is compatible with coarse
phase-only control, and is based on a near-optimal sequential algorithm for
frequency estimation which can exploit the geometric continuity of the channel
across successive beaconing intervals to reduce the overhead to less than 1%
even for very large (32 x 32) arrays. Compressive beaconing is essentially
omnidirectional, and hence does not enjoy the SNR and spatial reuse benefits of
beamforming obtained during data transmission. We therefore discuss system
level design considerations for ensuring that the beacon SNR is sufficient for
accurate channel estimation, and that inter-cell beacon interference is
controlled by an appropriate reuse scheme.",web beacon behavioral tracking
http://arxiv.org/abs/1603.09533v1,"Despite current controversy over e-cigarettes as a smoking cessation aid, we
present early work based on a web survey (N=249) that shows that some
e-cigarette users (46.2%) want to quit altogether, and that behavioral feedback
that can be tracked can fulfill that purpose. Based on our survey findings, we
designed VapeTracker, an early prototype that can attach to any e-cigarette
device to track vaping activity. We discuss our future research on vaping
cessation, addressing how to improve our VapeTracker prototype, ambient
feedback mechanisms, and the future inclusion of behavior change models to
support quitting e-cigarettes.",web beacon behavioral tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",web beacon behavioral tracking
http://arxiv.org/abs/1902.04262v1,"In Interactive Information Retrieval (IIR) experiments the user's gaze motion
on web pages is often recorded with eye tracking. The data is used to analyze
gaze behavior or to identify Areas of Interest (AOI) the user has looked at. So
far, tools for analyzing eye tracking data have certain limitations in
supporting the analysis of gaze behavior in IIR experiments. Experiments often
consist of a huge number of different visited web pages. In existing analysis
tools the data can only be analyzed in videos or images and AOIs for every
single web page have to be specified by hand, in a very time consuming process.
In this work, we propose the reading protocol software which breaks eye
tracking data down to the textual level by considering the HTML structure of
the web pages. This has a lot of advantages for the analyst. First and
foremost, it can easily be identified on a large scale what has actually been
viewed and read on the stimuli pages by the subjects. Second, the web page
structure can be used to filter to AOIs. Third, gaze data of multiple users can
be presented on the same page, and fourth, fixation times on text can be
exported and further processed in other tools. We present the software, its
validation, and example use cases with data from three existing IIR
experiments.",web beacon behavioral tracking
http://arxiv.org/abs/1204.3141v1,"Location information of sensor nodes has become an essential part of many
applications in Wireless Sensor Networks (WSN). The importance of location
estimation and object tracking has made them the target of many security
attacks. Various methods have tried to provide location information with high
accuracy, while lots of them have neglected the fact that WSNs may be deployed
in hostile environments. In this paper, we address the problem of securely
tracking a Mobile Node (MN) which has been noticed very little previously. A
novel secure tracking algorithm is proposed based on Extended Kalman Filter
(EKF) that is capable of tracking a Mobile Node (MN) with high resolution in
the presence of compromised or colluding malicious beacon nodes. It filters out
and identifies the malicious beacon data in the process of tracking. The
proposed method considerably outperforms the previously proposed secure
algorithms in terms of either detection rate or MSE. The experimental data
based on different settings for the network has shown promising results.",web beacon behavioral tracking
http://arxiv.org/abs/1811.06193v1,"Tracking users' activities on the World Wide Web (WWW) allows researchers to
analyze each user's internet behavior as time passes and for the amount of time
spent on a particular domain. This analysis can be used in research design, as
researchers may access to their participant's behaviors while browsing the web.
Web search behavior has been a subject of interest because of its real-world
applications in marketing, digital advertisement, and identifying potential
threats online. In this paper, we present an image-processing based method to
extract domains which are visited by a participant over multiple browsers
during a lab session. This method could provide another way to collect users'
activities during an online session given that the session recorder collected
the data. The method can also be used to collect the textual content of
web-pages that an individual visits for later analysis",web beacon behavioral tracking
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",web beacon behavioral tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",web beacon behavioral tracking
http://arxiv.org/abs/1612.00766v3,"Several studies have been conducted on understanding third-party user
tracking on the web. However, web trackers can only track users on sites where
they are embedded by the publisher, thus obtaining a fragmented view of a
user's online footprint. In this work, we investigate a different form of user
tracking, where browser extensions are repurposed to capture the complete
online activities of a user and communicate the collected sensitive information
to a third-party domain. We conduct an empirical study of spying browser
extensions on the Chrome Web Store. First, we present an in-depth analysis of
the spying behavior of these extensions. We observe that these extensions steal
a variety of sensitive user information, such as the complete browsing history
(e.g., the sequence of web traversals), online social network (OSN) access
tokens, IP address, and user geolocation. Second, we investigate the potential
for automatically detecting spying extensions by applying machine learning
schemes. We show that using a Recurrent Neural Network (RNN), the sequences of
browser API calls can be a robust feature, outperforming hand-crafted features
(used in prior work on malicious extensions) to detect spying extensions. Our
RNN based detection scheme achieves a high precision (90.02%) and recall
(93.31%) in detecting spying extensions.",web beacon behavioral tracking
http://arxiv.org/abs/1307.1542v1,"The investigation of the browsing behavior of users provides useful
information to optimize web site design, web browser design, search engines
offerings, and online advertisement. This has been a topic of active research
since the Web started and a large body of work exists. However, new online
services as well as advances in Web and mobile technologies clearly changed the
meaning behind ""browsing the Web"" and require a fresh look at the problem and
research, specifically in respect to whether the used models are still
appropriate. Platforms such as YouTube, Netflix or last.fm have started to
replace the traditional media channels (cinema, television, radio) and media
distribution formats (CD, DVD, Blu-ray). Social networks (e.g., Facebook) and
platforms for browser games attracted whole new, particularly less tech-savvy
audiences. Furthermore, advances in mobile technologies and devices made
browsing ""on-the-move"" the norm and changed the user behavior as in the mobile
case browsing is often being influenced by the user's location and context in
the physical world. Commonly used datasets, such as web server access logs or
search engines transaction logs, are inherently not capable of capturing the
browsing behavior of users in all these facets. DOBBS (DERI Online Behavior
Study) is an effort to create such a dataset in a non-intrusive, completely
anonymous and privacy-preserving way. To this end, DOBBS provides a browser
add-on that users can install, which keeps track of their browsing behavior
(e.g., how much time they spent on the Web, how long they stay on a website,
how often they visit a website, how they use their browser, etc.). In this
paper, we outline the motivation behind DOBBS, describe the add-on and captured
data in detail, and present some first results to highlight the strengths of
DOBBS.",web beacon behavioral tracking
http://arxiv.org/abs/1003.5325v1,"We examine the properties of all HTTP requests generated by a thousand
undergraduates over a span of two months. Preserving user identity in the data
set allows us to discover novel properties of Web traffic that directly affect
models of hypertext navigation. We find that the popularity of Web sites -- the
number of users who contribute to their traffic -- lacks any intrinsic mean and
may be unbounded. Further, many aspects of the browsing behavior of individual
users can be approximated by log-normal distributions even though their
aggregate behavior is scale-free. Finally, we show that users' click streams
cannot be cleanly segmented into sessions using timeouts, affecting any attempt
to model hypertext navigation using statistics of individual sessions. We
propose a strictly logical definition of sessions based on browsing activity as
revealed by referrer URLs; a user may have several active sessions in their
click stream at any one time. We demonstrate that applying a timeout to these
logical sessions affects their statistics to a lesser extent than a purely
timeout-based mechanism.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",web beacon behavioral tracking
http://arxiv.org/abs/1708.05625v2,"Simultaneous Localization and Mapping (SLAM) systems use commodity
visible/near visible digital sensors coupled with processing units that detect,
recognize and track image points in a camera stream. These systems are cheap,
fast and make use of readily available camera technologies. However, SLAM
systems suffer from issues of drift as well as sensitivity to lighting
variation such as shadows and changing brightness. Beaconless SLAM systems will
continue to suffer from this inherent drift problem irrespective of the
improvements in on-board camera resolution, speed and inertial sensor
precision. To cancel out destructive forms of drift, relocalization algorithms
are used which use known detected landmarks together with loop closure
processes to continually readjust the current location and orientation
estimates to match ""known"" positions. However this is inherently problematic
because these landmarks themselves may have been recorded with errors and they
may also change under different illumination conditions. In this note we
describe a unique beacon light coding system which is robust to desynchronized
clock bit drift. The described beacons and codes are designed to be used in
industrial or consumer environments for full standalone 6dof tracking or as
known error free landmarks in a SLAM pipeline.",web beacon behavioral tracking
http://arxiv.org/abs/1904.12568v1,"Quality of Experience (QoE) typically involves conducting experiments in
which stimuli are presented to participants and their judgments as well as
behavioral data are collected. Nowadays, many experiments require software for
the presentation of stimuli and the data collection from participants. While
different software solutions exist, these are not tailored to conduct
experiments on QoE. Moreover, replicating experiments or repeating the same
experiment in different settings (e. g., laboratory vs. crowdsourcing) can
further increase the software complexity. TheFragebogen is an open-source,
versatile, extendable software framework for the implementation of
questionnaires - especially for research on QoE. Implemented questionnaires can
be presented with a state-of-the-art web browser to support a broad range of
devices while the use of a web server being optional. Out-of-the-box,
TheFragebogen provides graphical exact scales as well as free-hand input, the
ability to collect behavioral data, and playback multimedia content.",web beacon behavioral tracking
http://arxiv.org/abs/1703.05174v1,"The intelligent transportation systems (ITS) framework from European
Telecommunication Standards Institute (ETSI) imposes requirements on the
exchange of periodic safety messages between components of ITS such as
vehicles. In particular, it requires ETSI standardized Decentralized Congestion
Control (DCC) algorithm to regulate the beaconing activity of vehicles based on
wireless channel utilization. However, the DCC state that defines the beaconing
behavior under heavy channel congestion, i.e., the Restrictive state, has a
serious connectivity problem that safety beacons do not reach other vehicles in
safety-critical distances. In this paper, we demonstrate the problem through
analysis, simulation, and on-road measurements. We suggest that DCC change the
transmit power setting for the Restrictive state before a full-scale deployment
of the ETSI ITS framework starts, and we discuss its consequences in terms of
changes in communicability and channel utilization.",web beacon behavioral tracking
http://arxiv.org/abs/1507.03509v1,"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger.""",web beacon behavioral tracking
http://arxiv.org/abs/1802.01050v1,"WebAssembly (wasm) has recently emerged as a promisingly portable,
size-efficient, fast, and safe binary format for the web. As WebAssembly can
interact freely with JavaScript libraries, this gives rise to a potential for
undesirable behavior to occur. It is therefore important to be able to detect
when this might happen. A way to do this is through taint tracking, where we
follow the flow of information by applying taint labels to data. In this paper,
we describe TaintAssembly, a taint tracking engine for interpreted WebAssembly,
that we have created by modifying the V8 JavaScript engine. We implement basic
taint tracking functionality, taint in linear memory, and a probabilistic
variant of taint. We then benchmark our TaintAssembly engine by incorporating
it into a Chromium build and running it on custom test scripts and various real
world WebAssembly applications. We find that our modifications to the V8 engine
do not incur significant overhead with respect to vanilla V8's interpreted
WebAssembly, making TaintAssembly suitable for development and debugging.",web beacon behavioral tracking
http://arxiv.org/abs/1611.06417v1,"Nowadays, many web databases ""hidden"" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",web beacon behavioral tracking
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",web beacon
http://arxiv.org/abs/1507.03509v1,"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger.""",web beacon
http://arxiv.org/abs/cs/0201003v1,"Random beacons-information sources that broadcast a stream of random digits
unknown by anyone beforehand-are useful for various cryptographic purposes. But
such beacons can be easily and undetectably sabotaged, so that their output is
known beforehand by a dishonest party, who can use this information to defeat
the cryptographic protocols supposedly protected by the beacon. We explore a
strategy to reduce this hazard by combining the outputs from several
noninteracting (eg spacelike-separated) beacons by XORing them together to
produce a single digit stream which is more trustworthy than any individual
beacon, being random and unpredictable if at least one of the contributing
beacons is honest. If the contributing beacons are not spacelike separated, so
that a dishonest beacon can overhear and adapt to earlier outputs of other
beacons, the beacons' trustworthiness can still be enhanced to a lesser extent
by a time sharing strategy. We point out some disadvantages of alternative
trust amplification methods based on one-way hash functions.",web beacon
http://arxiv.org/abs/1605.07329v1,"Vehicular communication requires vehicles to self-organize through the
exchange of periodic beacons. Recent analysis on beaconing indicates that the
standards for beaconing restrict the desired performance of vehicular
applications. This situation can be attributed to the quality of the available
transmission medium, persistent change in the traffic situation and the
inability of standards to cope with application requirements. To this end, this
paper is motivated by the classifications and capability evaluations of
existing adaptive beaconing approaches. To begin with, we explore the anatomy
and the performance requirements of beaconing. Then, the beaconing design is
analyzed to introduce a design-based beaconing taxonomy. A survey of the
state-of-the-art is conducted with an emphasis on the salient features of the
beaconing approaches. We also evaluate the capabilities of beaconing approaches
using several key parameters. A comparison among beaconing approaches is
presented, which is based on the architectural and implementation
characteristics. The paper concludes by discussing open challenges in the
field.",web beacon
http://arxiv.org/abs/1505.05106v1,"We establish tight bounds for beacon-based coverage problems, and improve the
bounds for beacon-based routing problems in simple rectilinear polygons.
Specifically, we show that $\lfloor \frac{n}{6} \rfloor$ beacons are always
sufficient and sometimes necessary to cover a simple rectilinear polygon $P$
with $n$ vertices. We also prove tight bounds for the case where $P$ is
monotone, and we present an optimal linear-time algorithm that computes the
beacon-based kernel of $P$. For the routing problem, we show that $\lfloor
\frac{3n-4}{8} \rfloor - 1$ beacons are always sufficient, and $\lceil
\frac{n}{4}\rceil-1$ beacons are sometimes necessary to route between all pairs
of points in $P$.",web beacon
http://arxiv.org/abs/1712.07416v1,"A beacon is a point-like object which can be enabled to exert a magnetic pull
on other point-like objects in space. Those objects then move towards the
beacon in a greedy fashion until they are either stuck at an obstacle or reach
the beacon's location. Beacons placed inside polyhedra can be used to route
point-like objects from one location to another. A second use case is to cover
a polyhedron such that every point-like object at an arbitrary location in the
polyhedron can reach at least one of the beacons once the latter is activated.
  The notion of beacon-based routing and guarding was introduced by Biro et al.
[FWCG'11] in 2011 and covered in detail by Biro in his PhD thesis [SUNY-SB'13],
which focuses on the two-dimensional case.
  We extend Biro's result to three dimensions by considering beacon routing in
polyhedra. We show that $\lfloor\frac{m+1}{3}\rfloor$ beacons are always
sufficient and sometimes necessary to route between any pair of points in a
given polyhedron $P$, where $m$ is the number of tetrahedra in a tetrahedral
decomposition of $P$. This is one of the first results that show that beacon
routing is also possible in three dimensions.",web beacon
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",web beacon
http://arxiv.org/abs/1802.05735v1,"Traditionally, there have been few options for navigational aids for the
blind and visually impaired (BVI) in large indoor spaces. Some recent indoor
navigation systems allow users equipped with smartphones to interact with low
cost Bluetoothbased beacons deployed strategically within the indoor space of
interest to navigate their surroundings. A major challenge in deploying such
beacon-based navigation systems is the need to employ a time and
labor-expensive beacon planning process to identify potential beacon placement
locations and arrive at a topological structure representing the indoor space.
This work presents a technique called IBeaconMap for creating such topological
structures to use with beacon-based navigation that only needs the floor plans
of the indoor spaces of interest. IBeaconMap employs a combination of computer
vision and machine learning techniques to arrive at the required set of beacon
locations and a weighted connectivity graph (with directional orientations) for
subsequent navigational needs. Evaluations show IBeaconMap to be both fast and
reasonably accurate, potentially proving to be an essential tool to be utilized
before mass deployments of beacon-based indoor wayfinding systems of the
future.",web beacon
http://arxiv.org/abs/1503.08404v1,"Beacon node placement, node-to-node measurement, and target node positioning
are the three key steps for a localization process. However, compared with the
other two steps, beacon node placement still lacks a comprehensive, systematic
study in research literatures. To fill this gap, we address the Beacon Node
Placment (BNP) problem that deploys beacon nodes for minimal localization error
in this paper. BNP is difficult in that the localization error is determined by
a complicated combination of factors, i.e., the localization error differing
greatly under a different environment, with a different algorithm applied, or
with a different type of beacon node used. In view of the hardness of BNP, we
propose an approximate function to reduce time cost in localization error
calculation, and also prove its time complexity and error bound. By
approximation, a sub-optimal distribution of beacon nodes could be found within
acceptable time cost for placement. In the experiment, we test our method and
compare it with other node placement methods under various settings and
environments. The experimental results show feasibility and effectiveness of
our method in practice.",web beacon
http://arxiv.org/abs/1709.10237v1,"Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.",web beacon
http://arxiv.org/abs/0810.3966v3,"What would SETI Beacon transmitters be like if built by civilizations with a
variety of motivations, but who cared about cost? We studied in a companion
paper how, for fixed power density in the far field, we could build a
cost-optimum interstellar Beacon system. Here we consider, if someone like us
were to produce a Beacon, how should we look for it? High-power transmitters
might be built for wide variety of motives other than twoway communication;
Beacons built to be seen over thousands of light years are such. Altruistic
Beacon builders will have to contend with other altruistic causes, just as
humans do, so may select for economy of effort. Cost, spectral lines near 1 GHz
and interstellar scintillation favor radiating frequencies substantially above
the classic water hole. Therefore the transmission strategy for a distant,
cost-conscious Beacon will be a rapid scan of the galactic plane, to cover the
angular space. Such pulses will be infrequent events for the receiver. Such
Beacons built by distant advanced, wealthy societies will have very different
characteristics from what SETI researchers seek. Future searches should pay
special attention to areas along the galactic disk where SETI searches have
seen coherent signals that have not recurred on the limited listening time
intervals we have used. We will need to wait for recurring events that may
arrive in intermittent bursts. Several new SETI search strategies emerge from
these ideas. We propose a new test for SETI Beacons, based on the Life Plane
hypotheses.",web beacon
http://arxiv.org/abs/1803.05946v1,"The beacon model is a recent paradigm for guiding the trajectory of messages
or small robotic agents in complex environments. A beacon is a fixed point with
an attraction pull that can move points within a given polygon. Points move
greedily towards a beacon: if unobstructed, they move along a straight line to
the beacon, and otherwise they slide on the edges of the polygon. The Euclidean
distance from a moving point to a beacon is monotonically decreasing. A given
beacon attracts a point if the point eventually reaches the beacon.
  The problem of attracting all points within a polygon with a set of beacons
can be viewed as a variation of the art gallery problem. Unlike most
variations, the beacon attraction has the intriguing property of being
asymmetric, leading to separate definitions of attraction region and inverse
attraction region. The attraction region of a beacon is the set of points that
it attracts. It is connected and can be computed in linear time for simple
polygons. By contrast, it is known that the inverse attraction region of a
point---the set of beacon positions that attract it---could have $\Omega(n)$
disjoint connected components.
  In this paper, we prove that, in spite of this, the total complexity of the
inverse attraction region of a point in a simple polygon is linear, and present
a $O(n \log n)$ time algorithm to construct it. This improves upon the best
previous algorithm which required $O(n^3)$ time and $O(n^2)$ space. Furthermore
we prove a matching $\Omega(n\log n)$ lower bound for this task in the
algebraic computation tree model of computation, even if the polygon is
monotone.",web beacon
http://arxiv.org/abs/1504.07192v1,"This work presents a mobile sign-on scheme, which utilizes Bluetooth Low
Energy beacons for location awareness and Attribute-Based Encryption for
expressive, broadcast-style key exchange. Bluetooth Low Energy beacons
broadcast encrypted messages with encoded access policies. Within range of the
beacons, a user with appropriate attributes is able to decrypt the broadcast
message and obtain parameters that allow the user to perform a short or
simplified login. The effect is a ""traveling"" sign-on that accompanies the user
throughout different locations.",web beacon
http://arxiv.org/abs/0810.3964v2,"This paper considers galactic scale Beacons from the point of view of expense
to a builder on Earth. For fixed power density in the far field, what is the
cost-optimum interstellar Beacon system? Experience shows an optimum tradeoff,
depending on transmission frequency and on antenna size and power. This emerges
by minimizing the cost of producing a desired effective isotropic radiated
power, which in turn determines the maximum range of detectability of a
transmitted signal. We derive general relations for cost-optimal aperture and
power. For linear dependence of capital cost on transmitter power and antenna
area, minimum capital cost occurs when the cost is equally divided between
antenna gain and radiated power. For non-linear power law dependence a similar
simple division occurs. This is validated in cost data for many systems;
industry uses this cost optimum as a rule-of-thumb. Costs of pulsed
cost-efficient transmitters are estimated from these relations using current
cost parameters ($/W, $/m2) as a basis. Galactic-scale Beacons demand effective
isotropic radiated power >1017 W, emitted powers are >1 GW, with antenna areas
> km2. We show the scaling and give examples of such Beacons. Thrifty beacon
systems would be large and costly, have narrow searchlight beams and short
dwell times when the Beacon would be seen by an alien oberver at target areas
in the sky. They may revisit an area infrequently and will likely transmit at
higher microwave frequencies, ~10 GHz. The natural corridor to broadcast is
along the galactic spiral radius or along the spiral galactic arm we are in.
Our second paper argues that nearly all SETI searches to date had little chance
of seeing such Beacons.",web beacon
http://arxiv.org/abs/1407.6965v3,"Cooperative inter-vehicular applications rely on the exchange of broadcast
single-hop status messages among vehicles, called beacons. The aggregated load
on the wireless channel due to periodic beacons can prevent the transmission of
other types of messages, what is called channel congestion due to beaconing
activity. In this paper we approach the problem of controlling the beaconing
rate on each vehicle by modeling it as a Network Utility Maximization (NUM)
problem. This allows us to formally apply the notion of fairness of a beaconing
rate allocation in vehicular networks and to control the trade-off between
efficiency and fairness. The NUM methodology provides a rigorous framework to
design a broad family of simple and decentralized algorithms, with proved
convergence guarantees to a fair allocation solution. In this context, we focus
exclusively in beaconing rate control and propose the Fair Adaptive Beaconing
Rate for Intervehicular Communications (FABRIC) algorithm, which uses a
particular scaled gradient projection algorithm to solve the dual of the NUM
problem. The desired fairness notion in the allocation can be established with
an algorithm parameter. Simulation results validate our approach and show that
FABRIC converges to fair rate allocations in multi-hop and dynamic scenarios.",web beacon
http://arxiv.org/abs/1805.04548v1,"The DFINITY blockchain computer provides a secure, performant and flexible
consensus mechanism. At its core, DFINITY contains a decentralized randomness
beacon which acts as a verifiable random function (VRF) that produces a stream
of outputs over time. The novel technique behind the beacon relies on the
existence of a unique-deterministic, non-interactive, DKG-friendly threshold
signatures scheme. The only known examples of such a scheme are pairing-based
and derived from BLS.
  The DFINITY blockchain is layered on top of the DFINITY beacon and uses the
beacon as its source of randomness for leader selection and leader ranking. A
""weight"" is attributed to a chain based on the ranks of the leaders who propose
the blocks in the chain, and that weight is used to select between competing
chains. The DFINITY blockchain is layered on top of the DFINITY beacon and uses
the beacon as its source of randomness for leader selection and leader ranking
blockchain is further hardened by a notarization process which dramatically
improves the time to finality and eliminates the nothing-at-stake and selfish
mining attacks.
  DFINITY consensus algorithm is made to scale through continuous quorum
selections driven by the random beacon. In practice, DFINITY achieves block
times of a few seconds and transaction finality after only two confirmations.
The system gracefully handles temporary losses of network synchrony including
network splits, while it is provably secure under synchrony.",web beacon
http://arxiv.org/abs/1208.2403v1,"IEEE 802.15.4 standard is designed for low power and low data rate
applications with high reliability. It operates in beacon enable and non-beacon
enable modes. In this work, we analyze delay, throughput, load, and end-to-end
delay of nonbeacon enable mode. Analysis of these parameters are performed at
varying data rates. Evaluation of non beacon enabled mode is done in a 10 node
network. We limit our analysis to non beacon or unslotted version because, it
performs better than other. Protocol performance is examined by changing
different Medium Access Control (MAC) parameters. We consider a full size MAC
packet with payload size of 114 bytes. In this paper we show that maximum
throughput and lowest delay is achieved at highest data rate.",web beacon
http://arxiv.org/abs/1702.05116v2,"Cyclic pursuit frameworks, which are built upon pursuit interactions between
neighboring agents in a cycle graph, provide an efficient way to create useful
global behaviors in a collective of autonomous robots. Previous work had
considered cyclic pursuit with a constant bearing (CB) pursuit law, and
demonstrated the existence of circling equilibria for the corresponding
dynamics. In this work, we propose a beacon-referenced version of the CB
pursuit law, wherein a stationary beacon provides an additional reference for
the individual agents in a collective. When implemented in a cyclic framework,
we show that the resulting dynamics admit relative equilibria corresponding to
a circling orbit around the beacon, with the circling radius and the
distribution of agents along the orbit determined by parameters of the proposed
pursuit law. We also derive necessary conditions for stability of the circling
equilibria, which provides a guide for parameter selection. Finally, by
introducing a change of variables, we demonstrate the existence of a family of
invariant manifolds related to spiraling motions around the beacon which
preserve the ""pure shape"" of the collective, and study the reduced dynamics on
a representative manifold.",web beacon
http://arxiv.org/abs/1703.08612v2,"The ability of robots to estimate their location is crucial for a wide
variety of autonomous operations. In settings where GPS is unavailable,
measurements of transmissions from fixed beacons provide an effective means of
estimating a robot's location as it navigates. The accuracy of such a
beacon-based localization system depends both on how beacons are distributed in
the environment, and how the robot's location is inferred based on noisy and
potentially ambiguous measurements. We propose an approach for making these
design decisions automatically and without expert supervision, by explicitly
searching for the placement and inference strategies that, together, are
optimal for a given environment. Since this search is computationally
expensive, our approach encodes beacon placement as a differential neural layer
that interfaces with a neural network for inference. This formulation allows us
to employ standard techniques for training neural networks to carry out the
joint optimization. We evaluate this approach on a variety of environments and
settings, and find that it is able to discover designs that enable high
localization accuracy.",web beacon
http://arxiv.org/abs/1910.00507v1,"To address 5G challenges, IEEE 802.11 is currently developing new amendments
to the Wi-Fi standard, the most promising of which is 802.11ax. A key scenario
considered by the developers of this amendment is dense and overlapped networks
typically present in residential buildings, offices, airports, stadiums, and
other places of a modern city. Being crucial for Wi-Fi hotspots, the hidden
station problem becomes even more challenging for dense and overlapped
networks, where even access points (APs) can be hidden. In this case, user
stations can experience continuous collisions of beacons sent by different APs,
which can cause disassociation and break Internet access. In this paper, we
show that beacon collisions are rather typical for residential networks and may
lead to unexpected and irreproducible malfunction. We investigate how often
beacon collisions occur, and describe a number of mechanisms which can be used
to avoid beacon collisions in dense deployment. Specifically, we pay much
attention to those mechanisms which are currently under consideration of the
IEEE 802.11ax group.",web beacon
http://arxiv.org/abs/1503.03388v1,"This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.",web beacon
http://arxiv.org/abs/1212.2404v1,"Vehicular Ad-Hoc Networks (VANETs) are special forms of Mobile Ad-Hoc
Networks (MANETs) that allows vehicles to communicate together in the absence
of fixed infrastructure.In this type of network beaconing is the means used to
discover the nodes in its eighborhood.For routing protocol successful delivery
of beacons containing speed, direction and position of a car is extremely
important.Otherwise, routing information should not be modified/manipulated
during transmission without detection, in order to ensure the routing
information, messages must be signed and provided with a certificate to attest
valid network participants. In this work we present a beaconing protocol with
key exchange to prepare the generation of a signature to protect the routing
information protocol 'Greedy Perimeter Stateless Routing'.",web beacon
http://arxiv.org/abs/1703.04150v1,"Location sensing is a key enabling technology for Ubicomp to support
contextual interaction. However, the laboratories where calibrated testing of
location technologies is done are very different to the domestic situations
where `context' is a problematic social construct. This study reports
measurements of Bluetooth beacons, informed by laboratory studies, but done in
diverse domestic settings. The design of these surveys has been motivated by
the natural environment implied in the Bluetooth beacon standards - relating
the technical environment of the beacon to the function of spaces within the
home. This research method can be considered as a situated, `ethnographic'
technical response to the study of physical infrastructure that arises through
social processes. The results offer insights for the future design of `seamful'
approaches to indoor location sensing, and to the ways that context might be
constructed and interpreted in a seamful manner.",web beacon
http://arxiv.org/abs/1806.02325v1,"We consider a sensing application where the sensor nodes are wirelessly
powered by an energy beacon. We focus on the problem of jointly optimizing the
energy allocation of the energy beacon to different sensors and the data
transmission powers of the sensors in order to minimize the field
reconstruction error at the sink. In contrast to the standard ideal linear
energy harvesting (EH) model, we consider practical non-linear EH models. We
investigate this problem under two different frameworks: i) an optimization
approach where the energy beacon knows the utility function of the nodes,
channel state information and the energy harvesting characteristics of the
devices; hence optimal power allocation strategies can be designed using an
optimization problem and ii) a learning approach where the energy beacon
decides on its strategies adaptively with battery level information and
feedback on the utility function. Our results illustrate that deep
reinforcement learning approach can obtain the same error levels with the
optimization approach and provides a promising alternative to the optimization
framework.",web beacon
http://arxiv.org/abs/1812.02349v2,"An ultrasonic Positioning System (UPS) has outperformed RF-based systems in
terms of its accuracy for years. However, few of the developed solutions have
been deployed in practice to satisfy the localization demand of today's smart
devices, which lack ultrasonic sensors and were considered as being `deaf' to
ultrasound. A recent finding demonstrates that ultrasound may be audible to the
smart devices under certain conditions due to their microphone's nonlinearity.
Inspired by this insight, this work revisits the ultrasonic positioning
technique and builds a practical UPS, called UPS+ for ultrasound-incapable
smart devices. The core concept is to deploy two types of indoor beacon
devices, which will advertise ultrasonic beacons at two different ultrasonic
frequencies respectively. Their superimposed beacons are shifted to a
low-frequency by virtue of the nonlinearity effect at the receiver's
microphone. This underlying property functions as an implicit ultrasonic
downconverter without throwing harm to the hearing system of humans. We
demonstrate UPS+, a fully functional UPS prototype, with centimeter-level
localization accuracy using custom-made beacon hardware and well-designed
algorithms.",web beacon
http://arxiv.org/abs/1905.02709v1,"In this paper we introduce the hiring under uncertainty problem to model the
questions faced by hiring committees in large enterprises and universities
alike. Given a set of $n$ eligible candidates, the decision maker needs to
choose the sequence of candidates to make offers so as to hire the $k$ best
candidates. However, candidates may choose to reject an offer (for instance,
due to a competing offer) and the decision maker has a time limit by which all
positions must be filled. Given an estimate of the probabilities of acceptance
for each candidate, the hiring under uncertainty problem is to design a
strategy of making offers so that the total expected value of all candidates
hired by the time limit is maximized. We provide a 2-approximation algorithm
for the setting where offers must be made in sequence, an 8-approximation when
offers may be made in parallel, and a 10-approximation for the more general
stochastic knapsack setting with finite probes.",limited time offer
http://arxiv.org/abs/1704.04101v1,"In this paper, we study fractional order heat equation in higher space-time
dimensions and offer specific role of heat flows in various fractional
dimensions. We offer fractional solutions of the heat equations thus obtained,
and examine the associated implications in various limiting cases. We
anticipate perspective applications of fractional heat flow solutions in
physical systems.",limited time offer
http://arxiv.org/abs/physics/0701191v1,"Atom interferometers using Bose-Einstein condensate that is confined in a
waveguide and manipulated by optical pulses have been limited by their short
coherence times. We present a theoretical model that offers a physically simple
explanation for the loss of contrast and propose the method for increasing the
fringe contrast by recombining the atoms at a different time. A simple,
quantitatively accurate, analytical expression for the optimized recombination
time is presented and used to place limits on the physical parameters for which
the contrast may be recovered.",limited time offer
http://arxiv.org/abs/1305.7375v2,"The pseudo-spectral analytical time-domain (PSATD) particle-in-cell (PIC)
algorithm solves the vacuum Maxwell's equations exactly, has no Courant
time-step limit (as conventionally defined), and offers substantial flexibility
in plasma and particle beam simulations. It is, however, not free of the usual
numerical instabilities, including the numerical Cherenkov instability, when
applied to relativistic beam simulations. This paper derives and solves the
numerical dispersion relation for the PSATD algorithm and compares the results
with corresponding behavior of the more conventional pseudo-spectral
time-domain (PSTD) and finite difference time-domain (FDTD) algorithms. In
general, PSATD offers superior stability properties over a reasonable range of
time steps. More importantly, one version of the PSATD algorithm, when combined
with digital filtering, is almost completely free of the numerical Cherenkov
instability for time steps (scaled to the speed of light) comparable to or
smaller than the axial cell size.",limited time offer
http://arxiv.org/abs/1902.05192v1,"The previous work showed that the Y00 protocol could stay secure with the
eavesdropper's guessing probability on the secret keys being strictly less than
one under an unlimitedly long known-plaintext attack with quantum memory.
However, an assumption that at least a fast correlation attack is completely
disabled by irregular mapping. The present study shows that the Y00 protocol
can be information-theoretic secure under any quantum-computational
crypto-analyses if the Y00 system is well designed. The Y00 protocol directly
encrypts messages with short secret keys expanded into pseudo-random running
keys unlike One-Time Pad. However, it may offer information-theoretic security
beyond the Shannon limit of cryptography.",limited time offer
http://arxiv.org/abs/1904.07328v2,"Blended courses that mix in-person instruction with online platforms are
increasingly popular in secondary education. These tools record a rich amount
of data on students' study habits and social interactions. Prior research has
shown that these metrics are correlated with students' performance in face to
face classes. However, predictive models for blended courses are still limited
and have not yet succeeded at early prediction or cross-class predictions even
for repeated offerings of the same course.
  In this work, we use data from two offerings of two different undergraduate
courses to train and evaluate predictive models on student performance based
upon persistent student characteristics including study habits and social
interactions. We analyze the performance of these models on the same offering,
on different offerings of the same course, and across courses to see how well
they generalize. We also evaluate the models on different segments of the
courses to determine how early reliable predictions can be made. This work
tells us in part how much data is required to make robust predictions and how
cross-class data may be used, or not, to boost model performance. The results
of this study will help us better understand how similar the study habits,
social activities, and the teamwork styles are across semesters for students in
each performance category. These trained models also provide an avenue to
improve our existing support platforms to better support struggling students
early in the semester with the goal of providing timely intervention.",limited time offer
http://arxiv.org/abs/0708.0330v2,"Spin noise sets fundamental limits to the precision of measurements using
spin-polarized atomic vapors, such as performed with sensitive atomic
magnetometers. Spin squeezing offers the possibility to extend the measurement
precision beyond the standard quantum limit of uncorrelated atoms. Contrary to
the current understanding, we show that even in the presence of spin
relaxation, spin squeezing can lead to a significant reduction of spin noise,
and hence an increase in magnetometric sensitivity, for a long measurement
time. This is the case when correlated spin relaxation due to binary
alkali-atom collisions dominates independently acting decoherence processes.",limited time offer
http://arxiv.org/abs/1107.5752v2,"Overall, the two main contributions of this work include the application of
sentence simplification to association extraction as described above, and the
use of distributional semantics for concept extraction. The proposed work on
concept extraction amalgamates for the first time two diverse research areas
-distributional semantics and information extraction. This approach renders all
the advantages offered in other semi-supervised machine learning systems, and,
unlike other proposed semi-supervised approaches, it can be used on top of
different basic frameworks and algorithms.
http://gradworks.umi.com/34/49/3449837.html",limited time offer
http://arxiv.org/abs/1803.09843v1,"Pixel size in cameras and other refractive imaging devices is typically
limited by the free-space diffraction. However, a vast majority of
semiconductor-based detectors are based on materials with substantially high
refractive index. We demonstrate that diffractive optics can be used to take
advantage of this high refractive index to reduce effective pixel size of the
sensors below free-space diffraction limit. At the same time, diffractive
systems encode both amplitude and phase information about the incoming beam
into multiple pixels, offering the platform for noise-tolerant imaging with
dynamical refocusing. We explore the opportunities opened by high index
diffractive optics to reduce sensor size and increase signal-to-noise ratio of
imaging structures.",limited time offer
http://arxiv.org/abs/1307.3724v1,"This paper describes the limiting behavior of linear and decision feedback
equalizers (DFEs) in single/multiple antenna systems employing
real/complex-valued modulation alphabets. The wideband frequency selective
channel is modeled using a Rayleigh fading channel model with infinite number
of time domain channel taps. Using this model, we show that the considered
equalizers offer a fixed post signal-to-noise-ratio (post-SNR) at the equalizer
output that is close to the matched filter bound (MFB). General expressions for
the post-SNR are obtained for zero-forcing (ZF) based conventional receivers as
well as for the case of receivers employing widely linear (WL) processing.
Simulation is used to study the bit error rate (BER) performance of both MMSE
and ZF based receivers. Results show that the considered receivers
advantageously exploit the rich frequency selective channel to mitigate both
fading and inter-symbol-interference (ISI) while offering a performance
comparable to the MFB.",limited time offer
http://arxiv.org/abs/1310.5793v1,"Intelligent Transportation System in case of cities is controlling traffic
congestion and regulating the traffic flow. This paper presents three modules
that will help in managing city traffic issues and ultimately gives advanced
development in transportation system. First module, Congestion Detection and
Management will provide user real time information about congestion on the road
towards his destination, Second module, Intelligent Public Transport System
will provide user real time public transport information,i.e, local buses, and
the third module, Signal Synchronization will help in controlling congestion at
signals, with real time adjustments of signal timers according to the
congestion. All the information that user is getting about the traffic or
public transportation will be provided on users day to day device that is
mobile through Android application or SMS. Moreover, communication can also be
done via Website for Clients having internet access. And all these modules will
be fully automated without any human intervention at server side.",website timer
http://arxiv.org/abs/0909.1241v1,"Timer-based mechanisms are often used to help a given (sink) node select the
best helper node among many available nodes. Specifically, a node transmits a
packet when its timer expires, and the timer value is a monotone non-increasing
function of its local suitability metric. The best node is selected
successfully if no other node's timer expires within a 'vulnerability' window
after its timer expiry, and so long as the sink can hear the available nodes.
In this paper, we show that the optimal metric-to-timer mapping that (i)
maximizes the probability of success or (ii) minimizes the average selection
time subject to a minimum constraint on the probability of success, maps the
metric into a set of discrete timer values. We specify, in closed-form, the
optimal scheme as a function of the maximum selection duration, the
vulnerability window, and the number of nodes. An asymptotic characterization
of the optimal scheme turns out to be elegant and insightful. For any
probability distribution function of the metric, the optimal scheme is
scalable, distributed, and performs much better than the popular inverse metric
timer mapping. It even compares favorably with splitting-based selection, when
the latter's feedback overhead is accounted for.",website timer
http://arxiv.org/abs/1706.04252v1,"A great deal of effort has gone into trying to model social influence ---
including the spread of behavior, norms, and ideas --- on networks. Most models
of social influence tend to assume that individuals react to changes in the
states of their neighbors without any time delay, but this is often not true in
social contexts, where (for various reasons) different agents can have
different response times. To examine such situations, we introduce the idea of
a timer into threshold models of social influence. The presence of timers on
nodes delays the adoption --- i.e., change of state --- of each agent, which in
turn delays the adoptions of its neighbors. With a homogeneous-distributed
timer, in which all nodes exhibit the same amount of delay, adoption delays are
also homogeneous, so the adoption order of nodes remains the same. However,
heterogeneously-distributed timers can change the adoption order of nodes and
hence the ""adoption paths"" through which state changes spread in a network.
Using a threshold model of social contagions, we illustrate that heterogeneous
timers can either accelerate or decelerate the spread of adoptions compared to
an analogous situation with homogeneous timers, and we investigate the
relationship of such acceleration or deceleration with respect to timer
distribution and network structure. We derive an analytical approximation for
the temporal evolution of the fraction of adopters by modifying a pair
approximation of the Watts threshold model, and we find good agreement with
numerical computations. We also examine our new timer model on networks
constructed from empirical data.",website timer
http://arxiv.org/abs/0910.1217v1,"A feature of current membrane systems is the fact that objects and membranes
are persistent. However, this is not true in the real world. In fact, cells and
intracellular proteins have a well-defined lifetime. Inspired from these
biological facts, we define a model of systems of mobile membranes in which
each membrane and each object has a timer representing their lifetime. We show
that systems of mutual mobile membranes with and without timers have the same
computational power. An encoding of timed safe mobile ambients into systems of
mutual mobile membranes with timers offers a relationship between two
formalisms used in describing biological systems.",website timer
http://arxiv.org/abs/1710.09494v2,"Watchdog timers are devices that are commonly used to monitor the health of
safety-critical hardware and software systems. Their primary function is to
raise an alarm if the monitored systems fail to emit periodic ""heartbeats"" that
signal their well-being. In this paper we design and verify a molecular
watchdog timer for monitoring the health of programmed molecular nanosystems.
This raises new challenges because our molecular watchdog timer and the system
that it monitors both operate in the probabilistic environment of chemical
kinetics, where many failures are certain to occur and it is especially hard to
detect the absence of a signal.
  Our molecular watchdog timer is the result of an incremental design process
that uses goal-oriented requirements engineering, simulation, stochastic
analysis, and software verification tools. We demonstrate the molecular
watchdog's functionality by having it monitor a molecular oscillator. Both the
molecular watchdog timer and the oscillator are implemented as chemical
reaction networks, which are the current programming language of choice for
many molecular programming applications.",website timer
http://arxiv.org/abs/1201.5190v1,"Ubiquitously during experiments one encounters a situation where time lapse
between two events has to measured. For example during the oscillations of a
pendulum or a vibrating reed, the powering of a lamp and achieving of its full
intensity. The powering of a relay and the closure of its contacts etc.
Situations like these call for a time measuring device between two events.
Hence this article describes a general Bi-Event timer that can be used in a
physics lab for ubiquitous time lapse measurements during experiments. These
measurements in turn can be used to interpret other parameters like velocity,
acceleration etc. The timer described here is simple to build and accurate in
performance. The Bi-event occurence can be applied as a signal to the inputs of
the timer either on separate lines or along a single path in series as voltage
pulses.",website timer
http://arxiv.org/abs/0908.1437v1,"Experiments in mechanics can often be timed by the sounds they produce. In
such cases, digital audio recordings provide a simple way of measuring time
intervals with an accuracy comparable to that of photogate timers. We
illustrate this with an experiment in the physics of sports: to measure the
speed of a hard-kicked soccer ball.",website timer
http://arxiv.org/abs/1104.0064v1,"I detail applications of timer interrupts in a popular micro-controller
family to time critical applications in laser-cooling type experiments. I
demonstrate a low overhead 1-bit frequency locking scheme and a multichannel
experimental sequencer using the timer-counter intterrupts to achieve accurate
timing along with flexible interfaces. The general purpose nature of
micro-controllers can offer unique functionality compared with commercial
solutions due to the flexibility of a computer controlled interface without the
poor latencies associated with computer timing.",website timer
http://arxiv.org/abs/1108.1361v1,"In this article, we examine the Location Management costs in mobile
communication networks utilizing the timer-based method. From the study of the
probabilities that a mobile terminal changes a number of Location Areas between
two calls, we identify a threshold value of 0.7 for the Call-to-Mobility Ratio
(CMR) below which the application of the timer-based method is most
appropriate. We characterize the valley appearing in the evolution of the costs
with the timeout period, showing that the time interval required to reach 90%
of the stabilized costs grows with the mobility index, the paging cost per
Location Area and the movement dimension, in opposition to the behavior
presented by the time interval that achieves the minimum of the costs. The
results obtained for CMRs below the suggested 0.7 threshold show that the
valley appearing in the costs tends to disappear for CMRs within [0.001, 0.7]
in onedimensional movements and within [0.2, 0.7] in two-dimensional ones, and
when the normalized paging cost per Location Area is below 0.3.",website timer
http://arxiv.org/abs/1902.06040v1,"Due to a hard dependency between time steps, large-scale simulations of gas
using the Direct Simulation Monte Carlo (DSMC) method proceed at the pace of
the slowest processor. Scalability is therefore achievable only by ensuring
that the work done each time step is as evenly apportioned among the processors
as possible. Furthermore, as the simulated system evolves, the load shifts, and
thus this load-balancing typically needs to be performed multiple times over
the course of a simulation. Common methods generally use either crude
performance models or processor-level timers. We combine both to create a
timer-augmented cost function which both converges quickly and yields
well-balanced processor decompositions. When compared to a particle-based
performance model alone, our method achieves 2x speedup at steady-state on up
to 1024 processors for a test case consisting of a Mach 9 argon jet impacting a
solid wall.",website timer
http://arxiv.org/abs/1906.10860v2,"As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases.",website timer
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",website timer
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",website timer
http://arxiv.org/abs/1701.01654v2,"Washing machine is of great domestic necessity as it frees us from the burden
of washing our clothes and saves ample of our time. This paper will cover the
aspect of designing and developing of Fuzzy Logic based, Smart Washing Machine.
The regular washing machine (timer based) makes use of multi-turned timer based
start-stop mechanism which is mechanical as is prone to breakage. In addition
to its starting and stopping issues, the mechanical timers are not efficient
with respect of maintenance and electricity usage. Recent developments have
shown that merger of digital electronics in optimal functionality of this
machine is possible and nowadays in practice. A number of international
renowned companies have developed the machine with the introduction of smart
artificial intelligence. Such a machine makes use of sensors and smartly
calculates the amount of run-time (washing time) for the main machine motor.
Realtime calculations and processes are also catered in optimizing the run-time
of the machine. The obvious result is smart time management, better economy of
electricity and efficiency of work. This paper deals with the indigenization of
FLC (Fuzzy Logic Controller) based Washing Machine, which is capable of
automating the inputs and getting the desired output (wash-time).",website timer
http://arxiv.org/abs/1711.03941v3,"Caching algorithms are usually described by the eviction method and analyzed
using a metric of hit probability. Since contents have different importance
(e.g. popularity), the utility of a high hit probability, and the cost of
transmission can vary across contents. In this paper, we consider timer-based
(TTL) policies across a cache network, where contents have differentiated
timers over which we optimize. Each content is associated with a utility
measured in terms of the corresponding hit probability. We start our analysis
from a linear cache network: we propose a utility maximization problem where
the objective is to maximize the sum of utilities and a cost minimization
problem where the objective is to minimize the content transmission cost across
the network. These frameworks enable us to design online algorithms for cache
management, for which we prove achieving optimal performance. Informed by the
results of our analysis, we formulate a non-convex optimization problem for a
general cache network. We show that the duality gap is zero, hence we can
develop a distributed iterative primal-dual algorithm for content management in
the network. Numerical evaluations show that our algorithm significant
outperforms path replication with traditional caching algorithms over some
network topologies. Finally, we consider a direct application of our cache
network model to content distribution.",website timer
http://arxiv.org/abs/1902.10369v3,"We consider the task of measuring time with probabilistic threshold gates
implemented by bio-inspired spiking neurons. In the model of spiking neural
networks, network evolves in discrete rounds, where in each round, neurons fire
in pulses in response to a sufficiently high membrane potential. This potential
is induced by spikes from neighboring neurons that fired in the previous round,
which can have either an excitatory or inhibitory effect. We first consider a
deterministic implementation of a neural timer and show that $\Theta(\log t)$
(deterministic) threshold gates are both sufficient and necessary. This raised
the question of whether randomness can be leveraged to reduce the number of
neurons. We answer this question in the affirmative by considering neural
timers with spiking neurons where the neuron $y$ is required to fire for $t$
consecutive rounds with probability at least $1-\delta$, and should stop firing
after at most $2t$ rounds with probability $1-\delta$ for some input parameter
$\delta \in (0,1)$. Our key result is a construction of a neural timer with
$O(\log\log 1/\delta)$ spiking neurons. Interestingly, this construction uses
only one spiking neuron, while the remaining neurons can be deterministic
threshold gates. We complement this construction with a matching lower bound of
$\Omega(\min\{\log\log 1/\delta, \log t\})$ neurons. This provides the first
separation between deterministic and randomized constructions in the setting of
spiking neural networks. Finally, we demonstrate the usefulness of compressed
counting networks for synchronizing neural networks.",website timer
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",website timer
http://arxiv.org/abs/1811.00923v1,"Shared Web Hosting service enables hosting multitude of websites on a single
powerful server. It is a well-known solution as many people share the overall
cost of server maintenance and also, website owners do not need to deal with
administration issues is not necessary for website owners. In this paper, we
illustrate how shared web hosting service works and demonstrate the security
weaknesses rise due to the lack of proper isolation between different websites,
hosted on the same server. We exhibit two new server-side attacks against the
log file whose objectives are revealing information of other hosted websites
which are considered to be private and arranging other complex attacks. In the
absence of isolated log files among websites, an attacker controlling a website
can inspect and manipulate contents of the log file. These attacks enable an
attacker to disclose file and directory structure of other websites and launch
other sorts of attacks. Finally, we propose several countermeasures to secure
shared web hosting servers against the two attacks subsequent to the separation
of log files for each website.",website timer
http://arxiv.org/abs/1305.4018v1,"Many online video websites provide the shortcut links to facilitate the video
sharing to other websites especially to the online social networks (OSNs). Such
video sharing behavior greatly changes the interplays between the two types of
websites. For example, users in OSNs may watch and re-share videos shared by
their friends from online video websites, and this can also boost the
popularity of videos in online video websites and attract more people to watch
and share them. Characterizing these interplays can provide great insights for
understanding the relationships among online video websites, OSNs, ISPs and so
on. In this paper we conduct empirical experiments to study the interplays
between video sharing websites and OSNs using three totally different data
sources: online video websites, OSNs, and campus network traffic. We find that,
a) there are many factors that can affect the external sharing probability of
videos in online video websites. b) The popularity of a video itself in online
video websites can greatly impact on its popularity in OSNs. Videos in Renren,
Qzone (the top two most popular Chinese OSNs) usually attract more viewers than
in Sina and Tencent Weibo (the top two most popular Chinese microblogs), which
indicates the different natures of the two kinds of OSNs. c) The analysis based
on real traffic data illustrates that 10\% of video flows are related to OSNs,
and they account for 25\% of traffic generated by all videos.",website timer
http://arxiv.org/abs/cs/0701015v2,"We consider the problem of failure detection in dynamic networks such as
MANETs. Unreliable failure detectors are classical mechanisms which provide
information about process failures. However, most of current implementations
consider that the network is fully connected and that the initial number of
nodes of the system is known. This assumption is not applicable to dynamic
environments. Furthermore, such implementations are usually timer-based while
in dynamic networks there is no upper bound for communication delays since
nodes can move. This paper presents an asynchronous implementation of a failure
detector for unknown and mobile networks. Our approach does not rely on timers
and neither the composition nor the number of nodes in the system are known. We
prove that our algorithm can implement failure detectors of class <>S when
behavioral properties and connectivity conditions are satisfied by the
underlying system.",website timer
http://arxiv.org/abs/0705.3015v1,"Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.",website timer
http://arxiv.org/abs/0910.0316v1,"Rate based transport protocol determines the rate of data transmission
between the sender and receiver and then sends the data according to that rate.
To notify the rate to the sender, the receiver sends ACKplusRate packet based
on epoch timer expiry. In this paper, through detailed arguments and simulation
it is shown that the transmission of ACKplusRate packet based on epoch timer
expiry consumes more energy in network with low mobility. To overcome this
problem, a new technique called Dynamic Rate Feedback (DRF) is proposed. DRF
sends ACKplusRate whenever there is a change in rate of (plus or minus) 25
percent than the previous rate. Based on ns2 simulation DRF is compared with a
reliable transport protocol for ad hoc network (ATP)",website timer
http://arxiv.org/abs/1009.4992v1,"The main objective of this work is to design and construct a microcomputer
based system: to control electric appliances such as light, fan, heater,
washing machine, motor, TV, etc. The paper discusses two major approaches to
control home appliances. The first involves controlling home appliances using
timer option. The second approach is to control home appliances using voice
command. Moreover, it is also possible to control appliances using Graphical
User Interface. The parallel port is used to transfer data from computer to the
particular device to be controlled. An interface box is designed to connect the
high power loads to the parallel port. This system will play an important role
for the elderly and physically disable people to control their home appliances
in intuitive and flexible way. We have developed a system, which is able to
control eight electric appliances properly in these three modes.",website timer
http://arxiv.org/abs/0709.2618v2,"Wireless Sensor Networks research and demand are now in full expansion, since
people came to understand these are the key to a large number of issues in
industry, commerce, home automation, healthcare, agriculture and environment,
monitoring, public safety etc. One of the most challenging research problems in
sensor networks research is power awareness and power-saving techniques. In
this master's thesis, we have studied one particular power-saving technique,
i.e. frequency scaling. In particular, we analysed the close relationship
between clock frequencies in a microcontroller and several types of constraints
imposed on these frequencies, e.g. by other components of the microcontroller,
by protocol specifications, by external factors etc. Among these constraints,
we were especially interested in the ones imposed by the timer service and by
the serial ports' transmission rates. Our efforts resulted in a microcontroller
configuration management tool which aims at assisting application programmers
in choosing microcontroller configurations, in function of the particular needs
and constraints of their application.",website timer
http://arxiv.org/abs/1502.00050v1,"To circumvent the FLP impossibility result in a deterministic way several
protocols have been proposed on top of an asynchronous distributed system
enriched with additional assumptions. In the context of Byzantine failures for
systems where at most t processes may exhibit a Byzantine behavior, two
approaches have been investigated to solve the consensus problem.The first,
relies on the addition of synchrony, called Timer-Based, but the second is
based on the pattern of the messages that are exchanged, called Time-Free. This
paper shows that both types of assumptions are not antagonist and can be
combined to solve authenticated Byzantine consensus. This combined assumption
considers a correct process pi, called 2t-BW, and a set X of 2t processes such
that, eventually, for each query broadcasted by a correct process pj of X, pj
receives a response from pi 2 X among the (n- t) first responses to that query
or both links connecting pi and pj are timely. Based on this combination, a
simple hybrid authenticated Byzantine consensus protocol,benefiting from the
best of both worlds, is proposed. Whereas many hybrid protocols have been
designed for the consensus problem in the crash model, this is, to our
knowledge, the first hybrid deterministic solution to the Byzantine consensus
problem.",website timer
http://arxiv.org/abs/1908.07753v1,"City authorities need to analyze urban geospatial data to improve
transportation and infrastructure. Current tools do not address the exploratory
and interactive nature of these analyses and in many cases consult the raw data
to compute query results. While pre-aggregation and materializing intermediate
query results is common practice in many OLAP settings, it is rarely used to
speed up geospatial queries. We introduce GeoBlocks, a pre-aggregating,
query-driven storage layout for geospatial point data that can provide
approximate, yet precision-bounded aggregation results over arbitrary query
polygons. GeoBlocks adapt to the skew naturally present in query workloads to
improve query performance over time. In summary, GeoBlocks outperform
on-the-fly aggregation by up to several orders of magnitude, providing the
sub-second query latencies required for interactive analytics.",geoblocking
http://arxiv.org/abs/1805.03741v1,"We consider the 4-block $n$-fold integer programming (IP), in which the
constraint matrix consists of $n$ copies of small matrices $A$, $B$, $D$ and
one copy of $C$ in a specific block structure. We prove that, the
$\ell_{\infty}$-norm of the Graver basis elements of 4-block $n$-fold IP is
upper bounded by $O_{FPT}(n^{s_c})$ where $s_c$ is the number of rows of matrix
$C$ and $O_{FPT}$ hides a multiplicative factor that is only dependent on the
parameters of the small matrices $A,B,C,D$ (i.e., the number of rows and
columns, and the largest absolute value among the entries). This improves upon
the existing upper bound of $O_{FPT}(n^{2^{s_c}})$. We provide a matching lower
bounded of $\Omega(n^{s_c})$, which even holds for an arbitrary non-zero
integral element in the kernel space. We then consider a special case of
4-block $n$-fold in which $C$ is a zero matrix (called 3-block $n$-fold IP). We
show that, surprisingly, 3-block $n$-fold IP admits a Hilbert basis whose
$\ell_{\infty}$-norm is bounded by $O_{FPT}(1)$, despite the fact that the
$\ell_{\infty}$-norm of its Graver basis elements is still $\Omega(n)$.
Finally, we provide upper bounds on the $\ell_{\infty}$-norm of Graver basis
elements for 3-block $n$-fold IP. Based on these upper bounds, we establish
algorithms for 3-block $n$-fold IP and provide improved algorithms for 4-block
$n$-fold IP.",ip blocking
http://arxiv.org/abs/0812.2559v1,"Maximum Likelihood (ML) decoding is the optimal decoding algorithm for
arbitrary linear block codes and can be written as an Integer Programming (IP)
problem. Feldman et al. relaxed this IP problem and presented Linear
Programming (LP) based decoding algorithm for linear block codes. In this
paper, we propose a new IP formulation of the ML decoding problem and solve the
IP with generic methods. The formulation uses indicator variables to detect
violated parity checks. We derive Gomory cuts from our formulation and use them
in a separation algorithm to find ML codewords. We further propose an efficient
method of finding cuts induced by redundant parity checks (RPC). Under certain
circumstances we can guarantee that these RPC cuts are valid and cut off the
fractional optimal solutions of LP decoding. We demonstrate on two LDPC codes
and one BCH code that our separation algorithm performs significantly better
than LP decoding.",ip blocking
http://arxiv.org/abs/1205.4487v1,"The Network-on-chip (NoC) designs consisting of large pack of Intellectual
Property (IP) blocks (cores) on the same silicon die is becoming technically
possible nowadays. But, the communication between the IP Cores is the main
issue in recent years. This paper presents the design of a Code Division
Multiple Access (CDMA) based wrapper interconnect as a component of System on
programmable chip (SOPC) builder to communicate between IP cores. In the
proposal, only bus lines that carry address and data signals are CDMA coded.
CDMA technology has better data integrity, channel continuity, channel
isolation, and also mainly it reduces the no.of lines in the bus architecture
for transmitting the data from master to slave.",ip blocking
http://arxiv.org/abs/1901.01135v2,"We consider so called $2$-stage stochastic integer programs (IPs) and their
generalized form of multi-stage stochastic IPs. A $2$-stage stochastic IP is an
integer program of the form $\max \{ c^T x \mid Ax = b, l \leq x \leq u, x \in
\mathbb{Z}^{nt + s} \}$ where the constraint matrix $A \in \mathbb{Z}^{r \times
s}$ consists roughly of $n$ repetition of a block matrix $A$ on the vertical
line and $n$ repetitions of a matrix $B \in \mathbb{Z}^{r \times t}$ on the
diagonal. In this paper we improve upon an algorithmic result by Hemmecke and
Schultz form 2003 to solve $2$-stage stochastic IPs. The algorithm is based on
the Graver augmentation framework where our main contribution is to give an
explicit doubly exponential bound on the size of the augmenting steps. The
previous bound for the size of the augmenting steps relied on non-constructive
finiteness arguments from commutative algebra and therefore only an implicit
bound was known that depends on parameters $r,s,t$ and $\Delta$, where $\Delta$
is the largest entry of the constraint matrix. Our new improved bound however
is obtained by a novel theorem which argues about the intersection of paths in
a vector space. As a result of our new bound we obtain an algorithm to solve
$2$-stage stochastic IPs in time $poly(n,t) \cdot f(r,s,\Delta)$, where $f$ is
a doubly exponential function. To complement our result, we also prove a doubly
exponential lower bound for the size of the augmenting steps.",ip blocking
http://arxiv.org/abs/0811.3828v1,"How can we protect the network infrastructure from malicious traffic, such as
scanning, malicious code propagation, and distributed denial-of-service (DDoS)
attacks? One mechanism for blocking malicious traffic is filtering: access
control lists (ACLs) can selectively block traffic based on fields of the IP
header. Filters (ACLs) are already available in the routers today but are a
scarce resource because they are stored in the expensive ternary content
addressable memory (TCAM).
  In this paper, we develop, for the first time, a framework for studying
filter selection as a resource allocation problem. Within this framework, we
study five practical cases of source address/prefix filtering, which correspond
to different attack scenarios and operator's policies. We show that filter
selection optimization leads to novel variations of the multidimensional
knapsack problem and we design optimal, yet computationally efficient,
algorithms to solve them. We also evaluate our approach using data from
Dshield.org and demonstrate that it brings significant benefits in practice.
Our set of algorithms is a building block that can be immediately used by
operators and manufacturers to block malicious traffic in a cost-efficient way.",ip blocking
http://arxiv.org/abs/1809.09086v2,"Tor and I2P are well-known anonymity networks used by many individuals to
protect their online privacy and anonymity. Tor's centralized directory
services facilitate the understanding of the Tor network, as well as the
measurement and visualization of its structure through the Tor Metrics project.
In contrast, I2P does not rely on centralized directory servers, and thus
obtaining a complete view of the network is challenging. In this work, we
conduct an empirical study of the I2P network, in which we measure properties
including population, churn rate, router type, and the geographic distribution
of I2P peers. We find that there are currently around 32K active I2P peers in
the network on a daily basis. Of these peers, 14K are located behind NAT or
firewalls.
  Using the collected network data, we examine the blocking resistance of I2P
against a censor that wants to prevent access to I2P using address-based
blocking techniques. Despite the decentralized characteristics of I2P, we
discover that a censor can block more than 95% of peer IP addresses known by a
stable I2P client by operating only 10 routers in the network. This amounts to
severe network impairment: a blocking rate of more than 70% is enough to cause
significant latency in web browsing activities, while blocking more than 90% of
peer IP addresses can make the network unusable. Finally, we discuss the
security consequences of the network being blocked, and directions for
potential approaches to make I2P more resistant to blocking.",ip blocking
http://arxiv.org/abs/1107.5372v1,"Both IP lookup and packet classification in IP routers can be implemented by
some form of tree traversal. SRAM-based Pipelining can improve the throughput
dramatically. However, previous pipelining schemes result in unbalanced memory
allocation over the pipeline stages. This has been identified as a major
challenge for scalable pipelined solutions. This paper proposes a flexible
bidirectional linear pipeline architecture based on widely-used dual-port
SRAMs. A search tree is partitioned, and then mapped onto pipeline stages by a
bidirectional fine-grained mapping scheme. We introduce the notion of inversion
factor and several heuristics to invert subtrees for memory balancing. Due to
its linear structure, the architecture maintains packet input order, and
supports non-blocking route updates. Our experiments show that, the
architecture can achieve a perfectly balanced memory distribution over the
pipeline stages, for both trie-based IP lookup and tree-based multi-dimensional
packet classification. For IP lookup, it can store a full backbone routing
table with 154419 entries using 2MB of memory, and sustain a high throughput of
1.87 billion packets per second (GPPS), i.e. 0.6 Tbps for the minimum size (40
bytes) packets. The throughput can be improved further to be 2.4 Tbps, by
employing caching to exploit the Internet traffic locality.",ip blocking
http://arxiv.org/abs/1110.1753v1,"The challenging number is used for the detection of Spoofing attack. The IP
Spoofing is considered to be one of the potentially brutal attack which acts as
a tool for the DDoS attack which is considered to be a major threat among
security problems in today's internet. These kinds of attack are extremely
severe. They bring down business of company drastically. DDoS attack can easily
exhaust the computing and communication resources of its victim within a short
period of time. There are attacks exploiting some vulnerability or
implementation bug in the software implementation of a service to bring that
down and some attacks will use all the available resources at the target
machine. This deals on attacks that consume all the bandwidth available to the
victim machine. While concentrating on the bandwidth attack the TCP SYN flood
is the more prominent attack. TCP/IP protocol suite is the most widely used
protocol suite for data communication. The TCP SYN flood works by exhausting
the TCP connection queue of the host and thus denying legitimate connection
request. There are various methods used to detect and prevent this attack, one
of which is to block the packet based on SYN flag count from the same IP
address. This kind of prevention methods becomes unsuitable when the attackers
use the Spoofed IP address. The SYN spoofing becomes a major tool the TCP SYN
flooding. For the prevention of this kind of attacks, the TCP specific probing
is used in the proposed scheme where the client is requested challenging number
while sending the ACK in the three way hand shake. This is very useful to find
the Spoofed IP Packets/TCP SYN flood and preventing them.",ip blocking
http://arxiv.org/abs/1910.01519v1,"A massive threat to the modern and complex IC production chain is the use of
untrusted off-shore foundries which are able to infringe valuable hardware
design IP or to inject hardware Trojans causing severe loss of safety and
security. Similarly, market dominating SRAM-based FPGAs are vulnerable to both
attacks since the crucial gate-level netlist can be retrieved even in field for
the majority of deployed device series. In order to perform IP infringement or
Trojan injection, reverse engineering (parts of) the hardware design is
necessary to understand its internal workings. Even though IP protection and
obfuscation techniques exist to hinder both attacks, the security of most
techniques is doubtful since realistic capabilities of reverse engineering are
often neglected. The contribution of our work is twofold: first, we carefully
review an IP watermarking scheme tailored to FPGAs and improve its security by
using opaque predicates. In addition, we show novel reverse engineering
strategies on proposed opaque predicate implementations that again enables to
automatically detect and alter watermarks. Second, we demonstrate automatic
injection of hardware Trojans specifically tailored for third-party
cryptographic IP gate-level netlists. More precisely, we extend our
understanding of adversary's capabilities by presenting how block and stream
cipher implementations can be surreptitiously weakened.",ip blocking
http://arxiv.org/abs/1105.1967v1,"An algebra-logical repair method for FPGA functional logic blocks on the
basis of solving the coverage problem is proposed. It is focused on
implementation into Infrastructure IP for system-on-a chip and
system-in-package. A method is designed for providing the operability of FPGA
blocks and digital system as a whole. It enables to obtain exact and optimal
solution associated with the minimum number of spares needed to repair the FPGA
logic components with multiple faults.",ip blocking
http://arxiv.org/abs/1802.06289v1,"We consider integer programming problems $\max \{ c^T x : \mathcal{A} x = b,
l \leq x \leq u, x \in \mathbb{Z}^{nt}\}$ where $\mathcal{A}$ has a (recursive)
block-structure generalizing ""$n$-fold integer programs"" which recently
received considerable attention in the literature. An $n$-fold IP is an integer
program where $\mathcal{A}$ consists of $n$ repetitions of submatrices $A \in
\mathbb{Z}^{r \times t}$ on the top horizontal part and $n$ repetitions of a
matrix $B \in \mathbb{Z}^{s \times t}$ on the diagonal below the top part.
Instead of allowing only two types of block matrices, one for the horizontal
line and one for the diagonal, we generalize the $n$-fold setting to allow for
arbitrary matrices in every block. We show that such an integer program can be
solved in time $n^2 t^2 {\phi} \cdot (rs{\Delta})^{\mathcal{O}(rs^2+ sr^2)}$
(ignoring logarithmic factors). Here ${\Delta}$ is an upper bound on the
largest absolute value of an entry of $\mathcal{A}$ and ${\phi}$ is the largest
binary encoding length of a coefficient of $c$. This improves upon the
previously best algorithm of Hemmecke, Onn and Romanchuk that runs in time
$n^3t^3 {\phi} \cdot {\Delta}^{\mathcal{O}(t^2s)}$. In particular, our
algorithm is not exponential in the number $t$ of columns of $A$ and $B$.
  Our algorithm is based on a new upper bound on the $l_1$-norm of an element
of the ""Graver basis"" of an integer matrix and on a proximity bound between the
LP and IP optimal solutions tailored for IPs with block structure. These new
bounds rely on the ""Steinitz Lemma"".
  Furthermore, we extend our techniques to the recently introduced ""tree-fold
IPs"", where we again present a more efficient algorithm in a generalized
setting.",ip blocking
http://arxiv.org/abs/0710.4805v1,"The idea of design domain specific Mother Model of IP block family as a base
of modeling of system integration is presented here. A common reconfigurable
Mother Model for ten different standardized digital OFDM transmitters has been
developed. By means of a set of parameters, the mother model can be
reconfigured to any of the ten selected standards. So far the applicability of
the proposed reconfiguration and analog-digital co-modeling methods have been
proved by modeling the function of the digital parts of three, 802.11a, ADSL
and DRM, transmitters in an RF system simulator. The model is intended to be
used as signal source template in RF system simulations. The concept is not
restricted to signal sources, it can be applied to any IP block development.
The idea of the Mother Model will be applied in other design domains to prove
that in certain application areas, OFDM transceivers in this case, the design
process can progress simultaneously in different design domains - mixed signal,
system and RTL-architectural - without the need of high-level synthesis. Only
the Mother Models of three design domains are needed to be formally proved to
function as specified.",ip blocking
http://arxiv.org/abs/1112.4018v1,"Mobile IP is an open standard, defined by the Internet Engineering Task Force
(IETF) RFC 3220. By using Mobile IP, you can keep the same IP address, stay
connected, and maintain ongoing applications while roaming between IP networks.
Mobile IP is scalable for the Internet because it is based on IP - any media
that can support IP can support Mobile IP.",ip blocking
http://arxiv.org/abs/1203.1673v2,"A key challenge in censorship-resistant web browsing is being able to direct
legitimate users to redirection proxies while preventing censors, posing as
insiders, from discovering their addresses and blocking them. We propose a new
framework for censorship-resistant web browsing called {\it CensorSpoofer} that
addresses this challenge by exploiting the asymmetric nature of web browsing
traffic and making use of IP spoofing. CensorSpoofer de-couples the upstream
and downstream channels, using a low-bandwidth indirect channel for delivering
outbound requests (URLs) and a high-bandwidth direct channel for downloading
web content. The upstream channel hides the request contents using
steganographic encoding within email or instant messages, whereas the
downstream channel uses IP address spoofing so that the real address of the
proxies is not revealed either to legitimate users or censors. We built a
proof-of-concept prototype that uses encrypted VoIP for this downstream channel
and demonstrated the feasibility of using the CensorSpoofer framework in a
realistic environment.",ip blocking
http://arxiv.org/abs/1404.3465v2,"Mobile devices are in roles where the integrity and confidentiality of their
apps and data are of paramount importance. They usually contain a
System-on-Chip (SoC), which integrates microprocessors and peripheral
Intellectual Property (IP) connected by a Network-on-Chip (NoC). Malicious IP
or software could compromise critical data. Some types of attacks can be
blocked by controlling data transfers on the NoC using Memory Management Units
(MMUs) and other access control mechanisms. However, commodity processors do
not provide strong assurances regarding the correctness of such mechanisms, and
it is challenging to verify that all access control mechanisms in the system
are correctly configured. We propose a NoC Firewall (NoCF) that provides a
single locus of control and is amenable to formal analysis. We demonstrate an
initial analysis of its ability to resist malformed NoC commands, which we
believe is the first effort to detect vulnerabilities that arise from NoC
protocol violations perpetrated by erroneous or malicious IP.",ip blocking
http://arxiv.org/abs/1401.6370v1,"By using the dynamic reconfigurable transceiver in high speed interface
design, designer can solve critical technology problems such as ensuring signal
integrity conveniently, with lower error binary rate. In this paper, we
designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to
transparently extend the physical reach of the XGMII. The following points are
focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two
transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design
which is featured of dynamic reconfiguration performance, that is,
ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by
changing settings of the devices without power off. These two blocks can
accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium
Attachment), however, with higher efficiency and reliability. (2) 1+1
protection. In our design, two ALTGX IP cores are used to work in parallel,
which named XAUI0 and XAUI1. The former works as the main channel while the
latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1
will start to work to keep the business. (3) RTL (Register Transfer Level)
coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and
ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then
connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the
design was simulated on VCS simulator. The validated result indicates that the
packets are transferred efficiently. FPGA makes high-speed optical
communication system design simplified.",ip blocking
http://arxiv.org/abs/1707.09791v1,"A few years after standardization of the High Efficiency Video Coding (HEVC),
now the Joint Video Exploration Team (JVET) group is exploring post-HEVC video
compression technologies. In the intra prediction domain, this effort has
resulted in an algorithm with 67 internal modes, new filters and tools which
significantly improve HEVC. However, the improved algorithm still suffers from
the long distance prediction inaccuracy problem. In this paper, we propose an
In-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes
inner-block reconstructed pixels as references to reduce the distance from
predicted pixels. This is done by using the ILR signal for partially
reconstructing each pixel, right after its prediction and before its
block-level out-loop residual calculation. The ILR signal is decided in the
rate-distortion sense, by a brute-force search on a QP-dependent finite
codebook that is known to the decoder. Experiments show that the proposed
ILR-IP algorithm improves the existing method in the Joint Exploration Model
(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at
the decoder side.",ip blocking
http://arxiv.org/abs/0706.2824v1,"The re-use of pre-designed blocks is a well-known concept of the software
development. This technique has been applied to System-on-Chip (SoC) design
whose complexity and heterogeneity are growing. The re-use is made thanks to
high level components, called virtual components (IP), available in more or
less flexible forms. These components are dedicated blocks: digital signal
processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These
blocks rest on a model of fixed architecture with very few degrees of
personalization. This rigidity is particularly true for the communication
interface whose orders of acquisition and production of data, the temporal
behavior and protocols of exchanges are fixed. The successful integration of
such an IP requires that the designer (1) synchronizes the components (2)
converts the protocols between ""incompatible"" blocks (3) temporizes the data to
guarantee the temporal constraints and the order of the data. This phase
remains however very manual and source of errors. Our approach proposes a
formal modeling, based on an original Ressource Compatibility Graph. The
synthesis flow is based on a set of transformations of the initial graph to
lead to an interface architecture allowing the space-time adaptation of the
data exchanges between several components.",ip blocking
http://arxiv.org/abs/1904.04324v2,"Like many other user-generated content sites, Wikipedia blocks contributions
through open proxies like Tor, because of a perception among the Wikipedia
community that privacy enhancing tools are a source of vandalism, spam, and
abuse. While blocking editors that use these tools is done in an attempt to
stem abuse, collateral damage in the form of unrealized valuable contributions
from IP-anonymity-seeking editors is nearly invisible. Although Wikipedia has
taken steps to block contributions from Tor users since as early as 2005, we
demonstrate that these blocks have been imperfect and that thousands of
attempts to edit on Wikipedia through Tor have been successful. We draw upon
several data sources and analytical techniques to measure and describe the
history of Tor editing on Wikipedia over time and to compare contributions from
Tor users to those from other groups of Wikipedia users. Our analysis suggests
that Tor users who manage to slip through Wikipedia's ban contribute content
that is similar in quality to unregistered Wikipedia contributors and to the
initial contributions of registered users.",ip blocking
http://arxiv.org/abs/1802.09007v2,"In recent years, algorithmic breakthroughs in stringology, computational
social choice, scheduling, etc., were achieved by applying the theory of
so-called $n$-fold integer programming. An $n$-fold integer program (IP) has a
highly uniform block structured constraint matrix. Hemmecke, Onn, and Romanchuk
[Math. Programming, 2013] showed an algorithm with runtime $a^{O(rst + r^2s)}
n^3$, where $a$ is the largest coefficient, $r,s$, and $t$ are dimensions of
blocks of the constraint matrix and $n$ is the total dimension of the IP; thus,
an algorithm efficient if the blocks are of small size and with small
coefficients. The algorithm works by iteratively improving a feasible solution
with augmenting steps, and $n$-fold IPs have the special property that
augmenting steps are guaranteed to exist in a not-too-large neighborhood.
  We have implemented the algorithm and learned the following along the way.
The original algorithm is practically unusable, but we discover a series of
improvements which make its evaluation possible. Crucially, we observe that a
certain constant in the algorithm can be treated as a tuning parameter, which
yields an efficient heuristic (essentially searching in a
smaller-than-guaranteed neighborhood). Furthermore, the algorithm uses an
overly expensive strategy to find a ""best"" step, while finding only an
""approximatelly best"" step is much cheaper, yet sufficient for quick
convergence. Using this insight, we improve the asymptotic dependence on $n$
from $n^3$ to $n^2 \log n$.
  We show that decreasing the tuning parameter initially leads to an increased
number of iterations needed for convergence and eventually to getting stuck in
local optima, as expected. However, surprisingly small values of the parameter
already exhibit good behavior. Second, our new strategy for finding
""approximatelly best"" steps wildly outperforms the original construction.",ip blocking
http://arxiv.org/abs/1207.2683v2,"Open communication over the Internet poses a serious threat to countries with
repressive regimes, leading them to develop and deploy network-based censorship
mechanisms within their networks. Existing censorship circumvention systems
face different difficulties in providing unobservable communication with their
clients; this limits their availability and poses threats to their users. To
provide the required unobservability, several recent circumvention systems
suggest modifying Internet routers running outside the censored region to
intercept and redirect packets to censored destinations. However, these
approaches require modifications to ISP networks, and hence requires
cooperation from ISP operators and/or network equipment vendors, presenting a
substantial deployment challenge. In this report we propose a deployable and
unobservable censorship-resistant infrastructure, called FreeWave. FreeWave
works by modulating a client's Internet connections into acoustic signals that
are carried over VoIP connections. Such VoIP connections are targeted to a
server, FreeWave server, that extracts the tunneled traffic of clients and
proxies them to the uncensored Internet. The use of actual VoIP connections, as
opposed to traffic morphing, allows FreeWave to relay its VoIP connections
through oblivious VoIP nodes, hence keeping itself unblockable from censors
that perform IP address blocking. Also, the use of end-to-end encryption
prevents censors from identifying FreeWave's VoIP connections using packet
content filtering technologies, like deep-packet inspection. We prototype the
designed FreeWave system over the popular VoIP system of Skype. We show that
FreeWave is able to reliably achieve communication bandwidths that are
sufficient for web browsing, even when clients are far distanced from the
FreeWave server.",ip blocking
http://arxiv.org/abs/1412.5052v2,"The vulnerability of the Internet has been demonstrated by prominent IP
prefix hijacking events. Major outages such as the China Telecom incident in
2010 stimulate speculations about malicious intentions behind such anomalies.
Surprisingly, almost all discussions in the current literature assume that
hijacking incidents are enabled by the lack of security mechanisms in the
inter-domain routing protocol BGP. In this paper, we discuss an attacker model
that accounts for the hijacking of network ownership information stored in
Regional Internet Registry (RIR) databases. We show that such threats emerge
from abandoned Internet resources (e.g., IP address blocks, AS numbers). When
DNS names expire, attackers gain the opportunity to take resource ownership by
re-registering domain names that are referenced by corresponding RIR database
objects. We argue that this kind of attack is more attractive than conventional
hijacking, since the attacker can act in full anonymity on behalf of a victim.
Despite corresponding incidents have been observed in the past, current
detection techniques are not qualified to deal with these attacks. We show that
they are feasible with very little effort, and analyze the risk potential of
abandoned Internet resources for the European service region: our findings
reveal that currently 73 /24 IP prefixes and 7 ASes are vulnerable to be
stealthily abused. We discuss countermeasures and outline research directions
towards preventive solutions.",ip blocking
http://arxiv.org/abs/1205.4011v1,"We present practical poisoning and name-server block- ing attacks on standard
DNS resolvers, by off-path, spoofing adversaries. Our attacks exploit large DNS
responses that cause IP fragmentation; such long re- sponses are increasingly
common, mainly due to the use of DNSSEC. In common scenarios, where DNSSEC is
partially or incorrectly deployed, our poisoning attacks allow 'com- plete'
domain hijacking. When DNSSEC is fully de- ployed, attacker can force use of
fake name server; we show exploits of this allowing off-path traffic analy- sis
and covert channel. When using NSEC3 opt-out, attacker can also create fake
subdomains, circumvent- ing same origin restrictions. Our attacks circumvent
resolver-side defenses, e.g., port randomisation, IP ran- domisation and query
randomisation. The (new) name server (NS) blocking attacks force re- solver to
use specific name server. This attack allows Degradation of Service,
traffic-analysis and covert chan- nel, and also facilitates DNS poisoning. We
validated the attacks using standard resolver soft- ware and standard DNS name
servers and zones, e.g., org.",ip blocking
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",cloned firms scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",cloned firms scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",cloned firms scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",cloned firms scam
http://arxiv.org/abs/1110.3939v1,"In elections, a set of candidates ranked consecutively (though possibly in
different order) by all voters is called a clone set, and its members are
called clones. A clone structure is a family of all clone sets of a given
election. In this paper we study properties of clone structures. In particular,
we give an axiomatic characterization of clone structures, show their
hierarchical structure, and analyze clone structures in single-peaked and
single-crossing elections. We give a polynomial-time algorithm that finds a
minimal collection of clones that need to be collapsed for an election to
become single-peaked, and we show that this problem is NP-hard for
single-crossing elections.",cloned firms scam
http://arxiv.org/abs/1611.08005v1,"Background: Code cloning - copying and reusing pieces of source code - is a
common phenomenon in software development in practice. There have been several
empirical studies on the effects of cloning, but there are contradictory
results regarding the connection of cloning and faults. Objective: Our aim is
to clarify the relationship between code clones and faults. In particular, we
focus on inconsistent (or type-3) clones in this work. Method: We conducted a
case study with TWT GmbH where we detected the code clones in three Java
systems, set them into relation to information from issue tracking and version
control and interviewed three key developers. Results: Of the type-3 clones, 17
% contain faults. Developers modified most of the type-3 clones simultaneously
and thereby fixed half of the faults in type-3 clones consistently. Type-2
clones with faults all evolved to fixed type-3 clones. Clone length is only
weakly correlated with faultiness. Conclusion: There are indications that the
developers in two cases have been aware of clones. It might be a reason for the
weak relationship between type-3 clones and faults. Hence, it seems important
to keep developers aware of clones, potentially with new tool support. Future
studies need to investigate if the rate of faults in type-3 clones justifies
using them as cues in defect detection.",cloned firms scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",cloned firms scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",cloned firms scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",cloned firms scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",cloned firms scam
http://arxiv.org/abs/1903.04958v1,"In coal-fired power plants, it is critical to improve the operational
efficiency of boilers for sustainability. In this work, we formulate real-time
boiler control as an optimization problem that looks for the best distribution
of temperature in different zones and oxygen content from the flue to improve
the boiler's stability and energy efficiency. We employ an efficient algorithm
by integrating appropriate machine learning and optimization techniques. We
obtain a large dataset collected from a real boiler for more than two months
from our industry partner, and conduct extensive experiments to demonstrate the
effectiveness and efficiency of the proposed algorithm.",boiler room scam
http://arxiv.org/abs/1302.5942v1,"Low temperature heating panel systems offer distinctive advantages in terms
of thermal comfort and energy consumption, allowing work with low exergy
sources. The purpose of this paper is to compare floor, wall, ceiling, and
floor-ceiling panel heating systems in terms of energy, exergy and CO2
emissions. Simulation results for each of the analyzed panel system are given
by its energy (the consumption of gas for heating, electricity for pumps and
primary energy) and exergy consumption, the price of heating, and its carbon
dioxide emission. Then, the values of the air temperatures of rooms are
investigated and that of the surrounding walls and floors. It is found that the
floor-ceiling heating system has the lowest energy, exergy, CO2 emissions,
operating costs, and uses boiler of the lowest power. The worst system by all
these parameters is the classical ceiling heating",boiler room scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",boiler room scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",boiler room scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",boiler room scam
http://arxiv.org/abs/1303.2619v1,"Modern distributed systems use names everywhere. Lockservices such as Chubby
and ZooKeeper provide an effective mechanism for mapping from application names
to server instances, but proper usage of them requires a large amount of
error-prone boiler-plate code.
  Application programmers often try to write wrappers to abstract away this
logic, but it turns out there is a more general and easier way of handling the
issue. We show that by extending the existing name resolution capabilities of
RPC libraries, we can remove the need for such annoying boiler-plate code while
at the same time making our services more robust.",boiler room scam
http://arxiv.org/abs/1606.00827v1,"The article discusses a possibility of removing smog particles from a boiler
smoke. To do this, the boiler smoke is passed through a flow of gamma
radiation, formed by interaction of the microtron beam with a heavy target. The
energy of the microtron electrons twenty five megaelectronvolts, the beam
current one hundred microamperes. Smog particles are ionized with gamma
radiation and then sat down on the plates of the electrostatic filter. The
height of the filter plates is one m, the electric field between the plates one
kilovolt per centimeter. The smog particles on the plates should be removed
regularly to a specialized dust collector.",boiler room scam
http://arxiv.org/abs/1803.05362v1,"With the advancement of software engineering in recent years, the model
checking techniques are widely applied in various areas to do the verification
for the system model. However, it is difficult to apply the model checking to
verify requirements due to lacking the details of the design. Unlike other
model checking tools, LTSA provides the structure diagram, which can bridge the
gap between the requirements and the design. In this paper, we demonstrate the
abilities of LTSA shipped with the classic case study of the steam boiler
system. The structure diagram of LTSA can specify the interactions between the
controller and the steam boiler, which can be derived from UML requirements
model such as system sequence diagram of the steam boiler system. The start-up
design model of LTSA can be generated from the structure diagram. Furthermore,
we provide a variation law of the steam rate to avoid the issue of state space
explosion and show how explicitly and implicitly model the time that reflects
the difference between system modeling and the physical world. Finally, the
derived model is verified against the required properties. Our work
demonstrates the potential power of integrating UML with model checking tools
in requirement elicitation, system design, and verification.",boiler room scam
http://arxiv.org/abs/1910.05118v1,"Accurate prediction of mercury content emitted from fossil fueled power
stations is of utmost important for environmental pollution assessment and
hazard mitigation. In this paper, mercury content in the output gas of power
stations boilers was predicted using adaptive neuro fuzzy inference system
method integrated with particle swarm optimization. The input parameters of the
model include coal characteristics and the operational parameters of the
boilers. The dataset has been collected from a number of power plants and
employed to educate and examine the proposed model. To evaluate the performance
of the proposed ANFIS PSO model the statistical meter of MARE was implemented.
Furthermore, relative errors between acquired data and predicted values
presented, which confirm the accuracy of the model to deal nonlinearity and
representing the dependency of flue gas mercury content into the specifications
of coal and the boiler type.",boiler room scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",boiler room scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",boiler room scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",boiler room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",boiler room scam
http://arxiv.org/abs/1811.09406v1,"This paper proposes a detailed optimal scheduling model of an exemplar
multi-energy system comprising combined cycle power plants (CCPPs), battery
energy storage systems, renewable energy sources, boilers, thermal energy
storage systems,electric loads and thermal loads. The proposed model considers
the detailed start-up and shutdown power trajectories of the gas turbines,
steam turbines and boilers. Furthermore, a practical,multi-energy load
management scheme is proposed within the framework of the optimal scheduling
problem. The proposed load management scheme utilizes the flexibility offered
by system components such as flexible electrical pump loads, electrical
interruptible loads and a flexible thermal load to reduce the overall energy
cost of the system. The efficacy of the proposed model in reducing the energy
cost of the system is demonstrated in the context of a day-ahead scheduling
problem using four illustrative scenarios.",boiler room scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",boiler room scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",boiler room scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",boiler room scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",boiler room scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",boiler room scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",boiler room scam
http://arxiv.org/abs/1405.1512v1,"This set of theories presents an Isabelle/HOL+Isar formalisation of stream
processing components introduces in Focus, a framework for formal specification
and development of interactive systems. This is an extended and updated version
of the formalisation, which was elaborated within the methodology 'Focus on
Isabelle'. In addition, we also applied the formalisation on three case studies
that cover different application areas: process control (Steam Boiler System),
data transmission (FlexRay communication protocol), memory and processing
components (Automotive-Gateway System).",boiler room scam
http://arxiv.org/abs/1610.07884v1,"In this technical report we summarise the spatio-temporal features and
present the core operators of FocusST specification framework. We present the
general idea of these operators, using a Steam Boiler System example to
illustrate how the specification framework can be applied.
  FocusST was inspired by Focus, a framework for formal specification and
development of interactive systems. In contrast to Focus, FocusST is devoted to
specify and to analyse spatial (S) and timing (T) aspects of the systems, which
is also reflected in the name of the framework: the extension ST highlights the
spatio-temporal nature of the specifications.",boiler room scam
http://arxiv.org/abs/1704.08323v1,"This paper will cover several studies and design changes that will eventually
be implemented to the Fermi National Accelerator Laboratory (FNAL) magnetron
ion source. The topics include tungsten cathode insert, solenoid gas valves,
current controlled arc pulser, cesium boiler redesign, gas mixtures of hydrogen
and nitrogen, and duty factor reduction. The studies were performed on the FNAL
test stand, with the aim to improve source lifetime, stability, and reducing
the amount of tuning needed.",boiler room scam
http://arxiv.org/abs/1808.09418v1,"The results of research pipes diffusion hardening as an effective method for
increasing the durability and reliability of power equipment are presented. The
experience of commercial operation of pipes manufactured from diffusion
chromized and heat-hardened steel 14 MoV6-3; on the heating surfaces of the
supercritical pressure boilers are generalized. The possibility and
effectiveness of this method on the example of capacities of Trypil'ska CHP are
shown.",boiler room scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",boiler room scam
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",boiler room scam
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",boiler room scam
http://arxiv.org/abs/1210.6766v1,"We tackle the multi-party speech recovery problem through modeling the
acoustic of the reverberant chambers. Our approach exploits structured sparsity
models to perform room modeling and speech recovery. We propose a scheme for
characterizing the room acoustic from the unknown competing speech sources
relying on localization of the early images of the speakers by sparse
approximation of the spatial spectra of the virtual sources in a free-space
model. The images are then clustered exploiting the low-rank structure of the
spectro-temporal components belonging to each source. This enables us to
identify the early support of the room impulse response function and its unique
map to the room geometry. To further tackle the ambiguity of the reflection
ratios, we propose a novel formulation of the reverberation model and estimate
the absorption coefficients through a convex optimization exploiting joint
sparsity model formulated upon spatio-spectral sparsity of concurrent speech
representation. The acoustic parameters are then incorporated for separating
individual speech signals through either structured sparse recovery or inverse
filtering the acoustic channels. The experiments conducted on real data
recordings demonstrate the effectiveness of the proposed approach for
multi-party speech recovery and recognition.",recovery room scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",recovery room scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",recovery room scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",recovery room scam
http://arxiv.org/abs/1307.4894v1,"We study two cases of acoustic source localization in a reverberant room,
from a number of point-wise narrowband measurements. In the first case, the
room is perfectly known. We show that using a sparse recovery algorithm with a
dictionary of sources computed a priori requires measurements at multiple
frequencies. Furthermore, we study the choice of frequencies for these
measurements, and show that one should avoid the modal frequencies of the room.
In the second case, when the shape and the boundary conditions of the room are
unknown, we propose a model of the acoustical field based on the Vekua theory,
still allowing the localization of sources, at the cost of an increased number
of measurements. Numerical results are given, using simple adaptations of
standard sparse recovery methods.",recovery room scam
http://arxiv.org/abs/1704.02420v1,"We analyze the list-decodability, and related notions, of random linear
codes. This has been studied extensively before: there are many different
parameter regimes and many different variants. Previous works have used
complementary styles of arguments---which each work in their own parameter
regimes but not in others---and moreover have left some gaps in our
understanding of the list-decodability of random linear codes. In particular,
none of these arguments work well for list-recovery, a generalization of
list-decoding that has been useful in a variety of settings.
  In this work, we present a new approach, which works across parameter regimes
and further generalizes to list-recovery. Our main theorem can establish better
list-decoding and list-recovery results for low-rate random linear codes over
large fields; list-recovery of high-rate random linear codes; and it can
recover the rate bounds of Guruswami, Hastad, and Kopparty for constant-rate
random linear codes (although with large list sizes).",recovery room scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",recovery room scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",recovery room scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",recovery room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",recovery room scam
http://arxiv.org/abs/1612.05793v1,"This paper considers the problem of simultaneous 2-D room shape
reconstruction and self-localization without the requirement of any
pre-established infrastructure. A mobile device equipped with co-located
microphone and loudspeaker as well as internal motion sensors is used to emit
acoustic pulses and collect echoes reflected by the walls. Using only first
order echoes, room shape recovery and self-localization is feasible when
auxiliary information is obtained using motion sensors. In particular, it is
established that using echoes collected at three measurement locations and the
two distances between consecutive measurement points, unique localization and
mapping can be achieved provided that the three measurement points are not
collinear. Practical algorithms for room shape reconstruction and
self-localization in the presence of noise and higher order echoes are proposed
along with experimental results to demonstrate the effectiveness of the
proposed approach.",recovery room scam
http://arxiv.org/abs/1806.08294v1,"In this paper, we propose a novel procedure for 3D layout recovery of indoor
scenes from single 360 degrees panoramic images. With such images, all scene is
seen at once, allowing to recover closed geometries. Our method combines
strategically the accuracy provided by geometric reasoning (lines and vanishing
points) with the higher level of data abstraction and pattern recognition
achieved by deep learning techniques (edge and normal maps). Thus, we extract
structural corners from which we generate layout hypotheses of the room
assuming Manhattan world. The best layout model is selected, achieving good
performance on both simple rooms (box-type) and complex shaped rooms (with more
than four walls). Experiments of the proposed approach are conducted within two
public datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of
estimating layouts by combining geometry and deep learning and the
effectiveness of our proposal with respect to the state of the art.",recovery room scam
http://arxiv.org/abs/1411.7889v1,"Single particle 3D imaging with ultrashort X-ray laser pulses is based on
collecting and combining the information content of 2D scattering patterns of
an object at different orientations. Typical sample-delivery schemes leave
little or no room for controlling the orientations. As such, the orientation
associated with a given snapshot should be estimated after the experiment. Here
we present an open-source code for the most rigorous technique having been
reported in this context. Some practical issues along with proposed solutions
are also discussed.",recovery room scam
http://arxiv.org/abs/1604.00408v1,"Generation of photon pairs from compact, manufacturable and inexpensive
silicon (Si) photonic devices at room temperature may help develop practical
applications of quantum photonics. An important characteristic of photon-pair
generation is the two-photon joint spectral intensity (JSI), which describes
the frequency correlations of the photon pair. In particular, heralded
single-photon generation requires uncorrelated photons, rather than the highly
anti-correlated photons conventionally obtained under continuous-wave (CW)
pumping. Recent attempts to achieve such a factorizable JSI have used short
optical pulses from mode-locked lasers, which are much more expensive and
bigger table-top or rack-sized instruments compared to the Si microchip pair
generator, dominate the cost and inhibit the miniaturization of the source.
Here, we generate photon pairs from a Si microring resonator by using an
electronic step-recovery diode to drive an electro-optic modulator which carves
the pump light from a CW optical diode into pulses of the appropriate width,
thus potentially eliminating the need for optical mode-locked lasers.",recovery room scam
http://arxiv.org/abs/1605.07469v1,"Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In applications
such as source separation, the phase recovery for each extracted component is a
major issue since it often leads to audible artifacts. In this paper, we
present a methodology for evaluating various NMF-based source separation
techniques involving phase reconstruction. For each model considered, a
comparison between two approaches (blind separation without prior information
and oracle separation with supervised model learning) is performed, in order to
inquire about the room for improvement for the estimation methods. Experimental
results show that the High Resolution NMF (HRNMF) model is particularly
promising, because it is able to take phases and correlations over time into
account with a great expressive power.",recovery room scam
http://arxiv.org/abs/1711.06503v1,"Location fingerprinting locates devices based on pattern matching signal
observations to a pre-defined signal map. This paper introduces a technique to
enable fast signal map creation given a dedicated surveyor with a smartphone
and floorplan. Our technique (PFSurvey) uses accelerometer, gyroscope and
magnetometer data to estimate the surveyor's trajectory post-hoc using
Simultaneous Localisation and Mapping and particle filtering to incorporate a
building floorplan. We demonstrate conventional methods can fail to recover the
survey path robustly and determine the room unambiguously. To counter this we
use a novel loop closure detection method based on magnetic field signals and
propose to incorporate the magnetic loop closures and straight-line constraints
into the filtering process to ensure robust trajectory recovery. We show this
allows room ambiguities to be resolved.
  An entire building can be surveyed by the proposed system in minutes rather
than days. We evaluate in a large office space and compare to state-of-the-art
approaches. We achieve trajectories within 1.1 m of the ground truth 90% of the
time. Output signal maps well approximate those built from conventional,
laborious manual survey. We also demonstrate that the signal maps built by
PFSurvey provide similar or even better positioning performance than the manual
signal maps.",recovery room scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",recovery room scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",recovery room scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",recovery room scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",recovery room scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",recovery room scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",recovery room scam
http://arxiv.org/abs/physics/0703199v1,"The electromagnetic calorimeter of PANDA at the FAIR facility will rely on an
operation of lead tungstate (PWO) scintillation crystals at temperatures near
-25 deg.C to provide sufficient resolution for photons in the energy range from
8 GeV down to 10 MeV. Radiation hardness of PWO crystals was studied at the
IHEP (Protvino) irradiation facility in the temperature range from room
temperature down to -25 deg.C. These studies have indicated a significantly
different behaviour in the time evolution of the damaging processes well below
room temperature. Different signal loss levels at the same dose rate, but at
different temperatures were observed. The effect of a deep suppression of the
crystal recovery process at temperatures below
  0 deg.C has been seen.",recovery room scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",recovery room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",credit scam
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",credit scam
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",credit scam
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",credit scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",credit scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",credit scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",credit scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",credit scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",credit scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",credit scam
http://arxiv.org/abs/1408.3455v1,"Collaboration among researchers is an essential component of the modern
scientific enterprise, playing a particularly important role in
multidisciplinary research. However, we continue to wrestle with allocating
credit to the coauthors of publications with multiple authors, since the
relative contribution of each author is difficult to determine. At the same
time, the scientific community runs an informal field-dependent credit
allocation process that assigns credit in a collective fashion to each work.
Here we develop a credit allocation algorithm that captures the coauthors'
contribution to a publication as perceived by the scientific community,
reproducing the informal collective credit allocation of science. We validate
the method by identifying the authors of Nobel-winning papers that are credited
for the discovery, independent of their positions in the author list. The
method can also compare the relative impact of researchers working in the same
field, even if they did not publish together. The ability to accurately measure
the relative credit of researchers could affect many aspects of credit
allocation in science, potentially impacting hiring, funding, and promotion
decisions.",credit scam
http://arxiv.org/abs/1312.7740v1,"Today, with respect to the increasing growth of demand to get credit from the
customers of banks and finance and credit institutions, using an effective and
efficient method to decrease the risk of non-repayment of credit given is very
necessary. Assessment of customers' credit is one of the most important and the
most essential duties of banks and institutions, and if an error occurs in this
field, it would leads to the great losses for banks and institutions. Thus,
using the predicting computer systems has been significantly progressed in
recent decades. The data that are provided to the credit institutions' managers
help them to make a straight decision for giving the credit or not-giving it.
In this paper, we will assess the customer credit through a combined
classification using artificial neural networks, genetics algorithm and
Bayesian probabilities simultaneously, and the results obtained from three
methods mentioned above would be used to achieve an appropriate and final
result. We use the K_folds cross validation test in order to assess the method
and finally, we compare the proposed method with the methods such as
Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well
as GA+SVM where the genetics algorithm has been used to improve them.",credit scam
http://arxiv.org/abs/0812.0956v2,"EcoTRADE is a multi player network game of a virtual biodiversity credit
market. Each player controls the land use of a certain amount of parcels on a
virtual landscape. The biodiversity credits of a particular parcel depend on
neighboring parcels, which may be owned by other players. The game can be used
to study the strategies of players in experiments or classroom games and also
as a communication tool for stakeholders participating in credit markets that
include spatially interdependent credits.",credit scam
http://arxiv.org/abs/1901.04957v3,"In Time-Sensitive Networking (TSN), it is important to formally prove per
flow latency and backlog bounds. To this end, recent works apply network
calculus and obtain latency bounds from service curves. The latency component
of such service curves is directly derived from upper bounds on the values of
the credit counters used by the Credit-Based Shaper (CBS), an essential
building-block of TSN. In this paper, we derive and formally prove credit upper
bounds for CBS, which improve on existing bounds.",credit scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",credit scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",credit scam
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake credit
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake credit
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake credit
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake credit
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake credit
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake credit
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake credit
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake credit
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake credit
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake credit
http://arxiv.org/abs/1408.3455v1,"Collaboration among researchers is an essential component of the modern
scientific enterprise, playing a particularly important role in
multidisciplinary research. However, we continue to wrestle with allocating
credit to the coauthors of publications with multiple authors, since the
relative contribution of each author is difficult to determine. At the same
time, the scientific community runs an informal field-dependent credit
allocation process that assigns credit in a collective fashion to each work.
Here we develop a credit allocation algorithm that captures the coauthors'
contribution to a publication as perceived by the scientific community,
reproducing the informal collective credit allocation of science. We validate
the method by identifying the authors of Nobel-winning papers that are credited
for the discovery, independent of their positions in the author list. The
method can also compare the relative impact of researchers working in the same
field, even if they did not publish together. The ability to accurately measure
the relative credit of researchers could affect many aspects of credit
allocation in science, potentially impacting hiring, funding, and promotion
decisions.",fake credit
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake credit
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake credit
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake credit
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake credit
http://arxiv.org/abs/1811.00661v2,"In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.",fake credit
http://arxiv.org/abs/1312.7740v1,"Today, with respect to the increasing growth of demand to get credit from the
customers of banks and finance and credit institutions, using an effective and
efficient method to decrease the risk of non-repayment of credit given is very
necessary. Assessment of customers' credit is one of the most important and the
most essential duties of banks and institutions, and if an error occurs in this
field, it would leads to the great losses for banks and institutions. Thus,
using the predicting computer systems has been significantly progressed in
recent decades. The data that are provided to the credit institutions' managers
help them to make a straight decision for giving the credit or not-giving it.
In this paper, we will assess the customer credit through a combined
classification using artificial neural networks, genetics algorithm and
Bayesian probabilities simultaneously, and the results obtained from three
methods mentioned above would be used to achieve an appropriate and final
result. We use the K_folds cross validation test in order to assess the method
and finally, we compare the proposed method with the methods such as
Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well
as GA+SVM where the genetics algorithm has been used to improve them.",fake credit
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake credit
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake consumer
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake consumer
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake consumer
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake consumer
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake consumer
http://arxiv.org/abs/1910.03496v2,"The evolution of the information and communication technologies has
dramatically increased the number of people with access to the Internet, which
has changed the way the information is consumed. As a consequence of the above,
fake news have become one of the major concerns because its potential to
destabilize governments, which makes them a potential danger to modern society.
An example of this can be found in the US. electoral campaign, where the term
""fake news"" gained great notoriety due to the influence of the hoaxes in the
final result of these. In this work the feasibility of applying deep learning
techniques to discriminate fake news on the Internet using only their text is
studied. In order to accomplish that, three different neural network
architectures are proposed, one of them based on BERT, a modern language model
created by Google which achieves state-of-the-art results.",fake consumer
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake consumer
http://arxiv.org/abs/1902.07207v1,"Social networks offer a ready channel for fake and misleading news to spread
and exert influence. This paper examines the performance of different
reputation algorithms when applied to a large and statistically significant
portion of the news that are spread via Twitter. Our main result is that simple
crowdsourcing-based algorithms are able to identify a large portion of fake or
misleading news, while incurring only very low false positive rates for
mainstream websites. We believe that these algorithms can be used as the basis
of practical, large-scale systems for indicating to consumers which news sites
deserve careful scrutiny and skepticism.",fake consumer
http://arxiv.org/abs/1601.06372v1,"Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.",fake consumer
http://arxiv.org/abs/1908.02589v1,"Social technologies have made it possible to propagate disinformation and
manipulate the masses at an unprecedented scale. This is particularly alarming
from a security perspective, as humans have proven to be the weakest link when
protecting critical infrastructure in general, and the power grid in
particular. Here, we consider an attack in which an adversary attempts to
manipulate the behavior of energy consumers by sending fake discount
notifications encouraging them to shift their consumption into the peak-demand
period. We conduct surveys to assess the propensity of people to follow-through
on such notifications and forward them to their friends. This allows us to
model how the disinformation propagates through social networks. Finally, using
Greater London as a case study, we show that disinformation can indeed be used
to orchestrate an attack wherein unwitting consumers synchronize their
energy-usage patterns, resulting in blackouts on a city-scale. These findings
demonstrate that in an era when disinformation can be weaponized, system
vulnerabilities arise not only from the hardware and software of critical
infrastructure, but also from the behavior of the consumers.",fake consumer
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake consumer
http://arxiv.org/abs/1802.08066v3,"Social networks offer a ready channel for fake and misleading news to spread
and exert influence. This paper examines the performance of different
reputation algorithms when applied to a large and statistically significant
portion of the news that are spread via Twitter. Our main result is that simple
algorithms based on the identity of the users spreading the news, as well as
the words appearing in the titles and descriptions of the linked articles, are
able to identify a large portion of fake or misleading news, while incurring
only very low (<1%) false positive rates for mainstream websites. We believe
that these algorithms can be used as the basis of practical, large-scale
systems for indicating to consumers which news sites deserve careful scrutiny
and skepticism.",fake consumer
http://arxiv.org/abs/1806.04853v1,"Detecting fake users (also called Sybils) in online social networks is a
basic security research problem. State-of-the-art approaches rely on a large
amount of manually labeled users as a training set. These approaches suffer
from three key limitations: 1) it is time-consuming and costly to manually
label a large training set, 2) they cannot detect new Sybils in a timely
fashion, and 3) they are vulnerable to Sybil attacks that leverage information
of the training set. In this work, we propose SybilBlind, a structure-based
Sybil detection framework that does not rely on a manually labeled training
set. SybilBlind works under the same threat model as state-of-the-art
structure-based methods. We demonstrate the effectiveness of SybilBlind using
1) a social network with synthetic Sybils and 2) two Twitter datasets with real
Sybils. For instance, SybilBlind achieves an AUC of 0.98 on a Twitter dataset.",fake consumer
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake consumer
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake consumer
http://arxiv.org/abs/1811.00656v3,"In this work, we describe a new deep learning based method that can
effectively distinguish AI-generated fake videos (referred to as {\em DeepFake}
videos hereafter) from real videos. Our method is based on the observations
that current DeepFake algorithm can only generate images of limited
resolutions, which need to be further warped to match the original faces in the
source video. Such transforms leave distinctive artifacts in the resulting
DeepFake videos, and we show that they can be effectively captured by
convolutional neural networks (CNNs). Compared to previous methods which use a
large amount of real and DeepFake generated images to train CNN classifier, our
method does not need DeepFake generated images as negative training examples
since we target the artifacts in affine face warping as the distinctive feature
to distinguish real and fake images. The advantages of our method are two-fold:
(1) Such artifacts can be simulated directly using simple image processing
operations on a image to make it as negative example. Since training a DeepFake
model to generate negative examples is time-consuming and resource-demanding,
our method saves a plenty of time and resources in training data collection;
(2) Since such artifacts are general existed in DeepFake videos from different
sources, our method is more robust compared to others. Our method is evaluated
on two sets of DeepFake video datasets for its effectiveness in practice.",fake consumer
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake consumer
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake consumer
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake consumer
http://arxiv.org/abs/1202.2369v1,"Daily deals sites such as Groupon offer deeply discounted goods and services
to tens of millions of customers through geographically targeted daily e-mail
marketing campaigns. In our prior work we observed that a negative side effect
for merchants using Groupons is that, on average, their Yelp ratings decline
significantly. However, this previous work was essentially observational,
rather than explanatory. In this work, we rigorously consider and evaluate
various hypotheses about underlying consumer and merchant behavior in order to
understand this phenomenon, which we dub the Groupon effect. We use statistical
analysis and mathematical modeling, leveraging a dataset we collected spanning
tens of thousands of daily deals and over 7 million Yelp reviews. In
particular, we investigate hypotheses such as whether Groupon subscribers are
more critical than their peers, or whether some fraction of Groupon merchants
provide significantly worse service to customers using Groupons. We suggest an
additional novel hypothesis: reviews from Groupon subscribers are lower on
average because such reviews correspond to real, unbiased customers, while the
body of reviews on Yelp contain some fraction of reviews from biased or even
potentially fake sources. Although we focus on a specific question, our work
provides broad insights into both consumer and merchant behavior within the
daily deals marketplace.",fake consumer
http://arxiv.org/abs/1904.07569v1,"One main challenge in social media is to identify trustworthy information. If
we cannot recognize information as trustworthy, that information may become
useless or be lost. Opposite, we could consume wrong or fake information with
major consequences. How does a user handle the information provided before
consuming it? Are the comments on a post, the author or votes essential for
taking such a decision? Are these attributes considered together and which
attribute is more important? To answer these questions, we developed a trust
model to support knowledge sharing of user content in social media. This trust
model is based on the dimensions of stability, quality, and credibility. Each
dimension contains metrics (user role, user IQ, votes, etc.) that are important
to the user based on data analysis. We present in this paper, an evaluation of
the proposed trust model using conjoint analysis (CA) as an evaluation method.
The results obtained from 348 responses, validate the trust model. A trust
degree translator interprets the content as very trusted, trusted, untrusted,
and very untrusted based on the calculated value of trust. Furthermore, the
results show different importance for each dimension: stability 24%,
credibility 35% and quality 41%.",fake consumer
http://arxiv.org/abs/1803.01661v1,"Online portals include an increasing amount of user feedback in form of
ratings and reviews. Recent research highlighted the importance of this
feedback and confirmed that positive feedback improves product sales figures
and thus its success. However, online portals' operators act as central
authorities throughout the overall review process. In the worst case, operators
can exclude users from submitting reviews, modify existing reviews, and
introduce fake reviews by fictional consumers. This paper presents ReviewChain,
a decentralized review approach. Our approach avoids central authorities by
using blockchain technologies, decentralized apps and storage. Thereby, we
enable users to submit and retrieve untampered reviews. We highlight the
implementation challenges encountered when realizing our approach on the public
Ethereum blockchain. For each implementation challange, we discuss possible
design alternatives and their trade-offs regarding costs, security, and
trustworthiness. Finally, we analyze which design decision should be chosen to
support specific trade-offs and present resulting combinations of decentralized
blockchain technologies, also with conventional centralized technologies.",fake consumer
http://arxiv.org/abs/1906.11462v2,"With the recent advances in Reinforcement Learning (RL), there have been
tremendous interests in employing RL for recommender systems. However, directly
training and evaluating a new RL-based recommendation algorithm needs to
collect users' real-time feedback in the real system, which is time and efforts
consuming and could negatively impact on users' experiences. Thus, it calls for
a user simulator that can mimic real users' behaviors where we can pre-train
and evaluate new recommendation algorithms. Simulating users' behaviors in a
dynamic system faces immense challenges -- (i) the underlining item
distribution is complex, and (ii) historical logs for each user are limited. In
this paper, we develop a user simulator base on Generative Adversarial Network
(GAN). To be specific, the generator captures the underlining distribution of
users' historical logs and generates realistic logs that can be considered as
augmentations of real logs; while the discriminator not only distinguishes real
and fake logs but also predicts users' behaviors. The experimental results
based on real-world e-commerce data demonstrate the effectiveness of the
proposed simulator.",fake consumer
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake consumer
http://arxiv.org/abs/1904.00712v2,"The concept of `fake news' has been referenced and thrown around in news
reports so much in recent years that it has become a news topic in its own
right. At its core, it poses a chilling question -- what do we do if our
worldview is fundamentally wrong? Even if internally consistent, what if it
does not match the real world? Are our beliefs justified, or could we become
indoctrinated from living in a `bubble'? If the latter is true, how could we
even test the limits of said bubble from within its confines? We propose a new
method to augment the process of identifying fake news, by speeding up and
automating the more cumbersome and time-consuming tasks involved. Our
application, NewsCompare takes any list of target websites as input
(news-related in our use case, but otherwise not restricted), visits them in
parallel and retrieves any text content found within. Web pages are
subsequently compared to each other, and similarities are tentatively pointed
out. These results can be manually verified in order to determine which
websites tend to draw inspiration from one another. The data gathered on every
intermediate step can be queried and analyzed separately, and most notably we
already use the set of hyperlinks to and from the various websites we encounter
to paint a sort of `map' of that particular slice of the web. This map can then
be cross-referenced and further strengthen the conclusion that a particular
grouping of sites with strong links to each other, and posting similar content,
are likely to share the same allegiance. We run our application on the Romanian
news websites and we draw several interesting observations.",fake consumer
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake consumer
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake consumer
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake consumer
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake consumer detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake consumer detection
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake consumer detection
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake consumer detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake consumer detection
http://arxiv.org/abs/1910.03496v2,"The evolution of the information and communication technologies has
dramatically increased the number of people with access to the Internet, which
has changed the way the information is consumed. As a consequence of the above,
fake news have become one of the major concerns because its potential to
destabilize governments, which makes them a potential danger to modern society.
An example of this can be found in the US. electoral campaign, where the term
""fake news"" gained great notoriety due to the influence of the hoaxes in the
final result of these. In this work the feasibility of applying deep learning
techniques to discriminate fake news on the Internet using only their text is
studied. In order to accomplish that, three different neural network
architectures are proposed, one of them based on BERT, a modern language model
created by Google which achieves state-of-the-art results.",fake consumer detection
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake consumer detection
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake consumer detection
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake consumer detection
http://arxiv.org/abs/1806.04853v1,"Detecting fake users (also called Sybils) in online social networks is a
basic security research problem. State-of-the-art approaches rely on a large
amount of manually labeled users as a training set. These approaches suffer
from three key limitations: 1) it is time-consuming and costly to manually
label a large training set, 2) they cannot detect new Sybils in a timely
fashion, and 3) they are vulnerable to Sybil attacks that leverage information
of the training set. In this work, we propose SybilBlind, a structure-based
Sybil detection framework that does not rely on a manually labeled training
set. SybilBlind works under the same threat model as state-of-the-art
structure-based methods. We demonstrate the effectiveness of SybilBlind using
1) a social network with synthetic Sybils and 2) two Twitter datasets with real
Sybils. For instance, SybilBlind achieves an AUC of 0.98 on a Twitter dataset.",fake consumer detection
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake consumer detection
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake consumer detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake consumer detection
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake consumer detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake consumer detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake consumer detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake consumer detection
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake consumer detection
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake consumer detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake consumer detection
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake consumer detection
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake consumer detection
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",fake consumer detection
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake consumer detection
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake consumer detection
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake consumer detection
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",fake consumer detection
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",fake consumer detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake consumer detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake consumer detection
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",Identity theft
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",Identity theft
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",Identity theft
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",Identity theft
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",Identity theft
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",Identity theft
http://arxiv.org/abs/1206.2597v1,"The beauty of Information Technology (IT) is with its multifunction nature;
it is a support system, a networking system, a storage system, as well as an
information facilitator. Aided with their broad line of services, an IT system
aims to support or even drive organizations towards desired paths. Trends of IT
and information security awareness (ISA) in society today, particularly within
the business environment is quite interesting phenomenon. The overviews of the
role of IT in the modern world as well as the perception towards ISA are
initially introduced. A series of scope are outlined, and also further
examination on matter of IT and ISA in the business environment-emphasis on
revolution of business with ISA, security threats such as identity thefts,
hacking and web harassment, and the different mode of protections that are
applied in different business environments. Unfortunately, the advancement of
IT is not followed by the awareness of its security issues properly, especially
in the context of the business settings and functions. This research and review
is expected to influence the awareness of information security issues in
business processes.",Identity theft
http://arxiv.org/abs/1303.3764v3,"Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.",Identity theft
http://arxiv.org/abs/1304.6499v1,"Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.",Identity theft
http://arxiv.org/abs/1407.7146v3,"The use of TLS proxies to intercept encrypted traffic is controversial since
the same mechanism can be used for both benevolent purposes, such as protecting
against malware, and for malicious purposes, such as identity theft or
warrantless government surveillance. To understand the prevalence and uses of
these proxies, we build a TLS proxy measurement tool and deploy it via Google
AdWords campaigns. We generate 15.2 million certificate tests across two
large-scale measurement studies. We find that 1 in 250 TLS connections are
TLS-proxied. The majority of these proxies appear to be benevolent, however we
identify over 3,600 cases where eight malware products are using this
technology nefariously. We also find numerous instances of negligent,
duplicitous, and suspicious behavior, some of which degrade security for users
without their knowledge. Distinguishing these types of practices is challenging
in practice, indicating a need for transparency and user awareness.",Identity theft
http://arxiv.org/abs/1503.00454v1,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",Identity theft
http://arxiv.org/abs/1510.04921v4,"This paper reports the results of a survey of 1,976 individuals regarding
their opinions on TLS inspection, a controversial technique that can be used
for both benevolent and malicious purposes. Responses indicate that
participants hold nuanced opinions on security and privacy trade-offs, with
most recognizing legitimate uses for the practice, but also concerned about
threats from hackers or government surveillance. There is strong support for
notification and consent when a system is intercepting their encrypted traffic,
although this support varies depending on the situation. A significant concern
about malicious uses of TLS inspection is identity theft, and many would react
negatively and some would change their behavior if they discovered inspection
occurring without their knowledge. We also find that there are a small but
significant number of participants who are jaded by the current state of
affairs and have no expectation of privacy.",Identity theft
http://arxiv.org/abs/0812.4181v1,"Web Services are web-based applications made available for web users or
remote Web-based programs. In order to promote interoperability, they publish
their interfaces in the so-called WSDL file and allow remote call over the
network. Although Web Services can be used in different ways, the industry
standard is the Service Oriented Architecture Web Services that doesn't rely on
the implementation details. In this architecture, communication is performed
through XML-based messages called SOAP messages. However, those messages are
prone to attacks that can lead to code injection, unauthorized accesses,
identity theft, etc. This type of attacks, called XML Rewriting Attacks, are
all based on unauthorized, yet possible, modifications of SOAP messages. We
present in this paper an explanation of this kind of attack, review the
existing solutions, and show their limitations. We also propose some ideas to
secure SOAP messages, as well as implementation ideas.",Identity theft
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",Identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft  detection
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft  detection
http://arxiv.org/abs/1902.03296v1,"Energy theft constitutes an issue of great importance for electricity
operators. The attempt to detect and reduce non-technical losses is a
challenging task due to insufficient inspection methods. With the evolution of
advanced metering infrastructure (AMI) in smart grids, a more complicated
status quo in energy theft has emerged and many new technologies are being
adopted to solve the problem. In order to identify illegal residential
consumers, a computational method of analyzing and identifying electricity
consumption patterns of consumers based on data mining techniques has been
presented. Combining principal component analysis (PCA) with mean shift
algorithm for different power theft scenarios, we can now cope with the power
theft detection problem sufficiently. The overall research has shown
encouraging results in residential consumers power theft detection that will
help utilities to improve the reliability, security and operation of power
network.",Identity theft  detection
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft  detection
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft  detection
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",Identity theft  detection
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft  detection
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",Identity theft  detection
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",Identity theft  detection
http://arxiv.org/abs/1811.00925v1,"Botnets (networks of compromised computers) are often used for malicious
activities such as spam, click fraud, identity theft, phishing, and distributed
denial of service (DDoS) attacks. Most of previous researches have introduced
fully or partially signature-based botnet detection approaches. In this paper,
we propose a fully anomaly-based approach that requires no a priori knowledge
of bot signatures, botnet C&C protocols, and C&C server addresses. We start
from inherent characteristics of botnets. Bots connect to the C&C channel and
execute the received commands. Bots belonging to the same botnet receive the
same commands that causes them having similar netflows characteristics and
performing same attacks. Our method clusters bots with similar netflows and
attacks in different time windows and perform correlation to identify bot
infected hosts. We have developed a prototype system and evaluated it with
real-world traces including normal traffic and several real-world botnet
traces. The results show that our approach has high detection accuracy and low
false positive.",Identity theft  detection
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft  detection
http://arxiv.org/abs/1805.09591v1,"Electricity theft detection issue has drawn lots of attention during last
decades. Timely identification of the electricity theft in the power system is
crucial for the safety and availability of the system. Although sustainable
efforts have been made, the detection task remains challenging and falls short
of accuracy and efficiency, especially with the increase of the data size.
Recently, convolutional neural network-based methods have achieved better
performance in comparison with traditional methods, which employ handcrafted
features and shallow-architecture classifiers. In this paper, we present a
novel approach for automatic detection by using a multi-scale dense connected
convolution neural network (multi-scale DenseNet) in order to capture the
long-term and short-term periodic features within the sequential data. We
compare the proposed approaches with the classical algorithms, and the
experimental results demonstrate that the multiscale DenseNet approach can
significantly improve the accuracy of the detection. Moreover, our method is
scalable, enabling larger data processing while no handcrafted feature
engineering is needed.",Identity theft  detection
http://arxiv.org/abs/1708.05907v1,"Non-technical losses (NTL) in electric power grids arise through electricity
theft, broken electric meters or billing errors. They can harm the power
supplier as well as the whole economy of a country through losses of up to 40%
of the total power distribution. For NTL detection, researchers use artificial
intelligence to analyse data. This work is about improving the extraction of
more meaningful features from a data set. With these features, the prediction
quality will increase.",Identity theft  detection
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft  detection
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft  detection
http://arxiv.org/abs/1811.09024v1,"Phishing attacks are prevalent and humans are central to this online identity
theft attack, which aims to steal victims' sensitive and personal information
such as username, password, and online banking details. There are many
anti-phishing tools developed to thwart against phishing attacks. Since humans
are the weakest link in phishing, it is important to educate them to detect and
avoid phishing attacks. One can argue self-efficacy is one of the most
important determinants of individual's motivation in phishing threat avoidance
behavior, which has co-relation with knowledge. The proposed research endeavors
on the user's self-efficacy in order to enhance the individual's phishing
threat avoidance behavior through their motivation. Using social cognitive
theory, we explored that various knowledge attributes such as observational
(vicarious) knowledge, heuristic knowledge and structural knowledge contributes
immensely towards the individual's self-efficacy to enhance phishing threat
prevention behavior. A theoretical framework is then developed depicting the
mechanism that links knowledge attributes, self-efficacy, threat avoidance
motivation that leads to users' threat avoidance behavior. Finally, a gaming
prototype is designed incooperating the knowledge elements identified in this
research that aimed to enhance individual's self-efficacy in phishing threat
avoidance behavior.",Identity theft  detection
http://arxiv.org/abs/1703.03378v1,"The widespread use of smartphones gives rise to new security and privacy
concerns. Smartphone thefts account for the largest percentage of thefts in
recent crime statistics. Using a victim's smartphone, the attacker can launch
impersonation attacks, which threaten the security of the victim and other
users in the network. Our threat model includes the attacker taking over the
phone after the user has logged on with his password or pin. Our goal is to
design a mechanism for smartphones to better authenticate the current user,
continuously and implicitly, and raise alerts when necessary. In this paper, we
propose a multi-sensors-based system to achieve continuous and implicit
authentication for smartphone users. The system continuously learns the owner's
behavior patterns and environment characteristics, and then authenticates the
current user without interrupting user-smartphone interactions. Our method can
adaptively update a user's model considering the temporal change of user's
patterns. Experimental results show that our method is efficient, requiring
less than 10 seconds to train the model and 20 seconds to detect the abnormal
user, while achieving high accuracy (more than 90%). Also the combination of
more sensors provide better accuracy. Furthermore, our method enables adjusting
the security level by changing the sampling rate.",Identity theft  detection
http://arxiv.org/abs/1705.00242v1,"As item trading becomes more popular, users can change their game items or
money into real money more easily. At the same time, hackers turn their eyes on
stealing other users game items or money because it is much easier to earn
money than traditional gold-farming by running game bots. Game companies
provide various security measures to block account- theft attempts, but many
security measures on the user-side are disregarded by users because of lack of
usability. In this study, we propose a server-side account theft detection
system base on action sequence analysis to protect game users from malicious
hackers. We tested this system in the real Massively Multiplayer Online Role
Playing Game (MMORPG). By analyzing users full game play log, our system can
find the particular action sequences of hackers with high accuracy. Also, we
can trace where the victim accounts stolen money goes.",Identity theft  detection
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft  detection
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft  detection
http://arxiv.org/abs/1210.7678v1,"Today, Plagiarism has become a menace. Every journal editor or conference
organizers has to deal with this problem. Simply Copying or rephrasing of text
without giving due credit to the original author has become more common. This
is considered to be an Intellectual Property Theft. We are developing a
Plagiarism Detection Tool which would deal with this problem. In this paper we
discuss the common tools available to detect plagiarism and their short comings
and the advantages of our tool over these tools.",Identity theft  detection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft  detection
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft  detection
http://arxiv.org/abs/1904.04895v1,"A method for detecting electronic data theft from computer networks is
described, capable of recognizing patterns of remote exfiltration occurring
over days to weeks. Normal traffic flow data, in the form of a host's ingress
and egress bytes over time, is used to train an ensemble of one-class learners.
The detection ensemble is modular, with individual classifiers trained on
different traffic features thought to characterize malicious data transfers. We
select features that model the egress to ingress byte balance over time,
periodicity, short time-scale irregularity, and density of the traffic. The
features are most efficiently modeled in the frequency domain, which has the
added benefit that variable duration flows are transformed to a fixed-size
feature vector, and by sampling the frequency space appropriately,
long-duration flows can be tested. When trained on days- or weeks-worth of
traffic from individual hosts, our ensemble achieves a low false positive rate
(<2%) on a range of different system types. Simulated exfiltration samples with
a variety of different timing and data characteristics were generated and used
to test ensemble performance on different kinds of systems: when trained on a
client workstation's external traffic, the ensemble was generally successful at
detecting exfiltration that is not simultaneously ingress-heavy,
connection-sparse, and of short duration---a combination that is not optimal
for attackers seeking to transfer large amounts of data. Remote exfiltration is
more difficult to detect from egress-heavy systems, like web servers, with
normal traffic exhibiting timing characteristics similar to a wide range of
exfiltration types.",Identity theft  detection
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft  detection
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft  detection
http://arxiv.org/abs/1208.3205v1,"The main stretch in the paper is buffer overflow anomaly occurring in major
source codes, designed in various programming language. It describes the
various as to how to improve your code and increase its strength to withstand
security theft occurring at vulnerable areas in the code. The main language
used is JAVA, regarded as one of the most object oriented language still create
lot of error like stack overflow, illegal/inappropriate method overriding. I
used tools confined to JAVA to test as how weak points in the code can be
rectified before compiled. The byte code theft is difficult to be conquered, so
it's a better to get rid of it in the plain java code itself. The tools used in
the research are PMD(Programming mistake detector), it helps to detect line of
code that make pop out error in near future like defect in hashcode(memory
maps) overriding due to which the java code will not function correctly.
Another tool is FindBUGS which provide the tester of the code to analyze the
weak points in the code like infinite loop, unsynchronized wait, deadlock
situation, null referring and dereferencing. Another tool which provides the
base to above tools is JaCoCo code coverage analysis used to detect unreachable
part and unused conditions of the code which improves the space complexity and
helps in easy clarification of errors.
  Through this paper, we design an algorithm to prevent the loss of data. The
main audience is the white box tester who might leave out essential line of
code like, index variables, infinite loop, and inappropriate hashcode in the
major source program. This algorithm serves to reduce the damage in case of
buffer overflow",Identity theft  detection
http://arxiv.org/abs/1607.00872v2,"Electricity theft is a major problem around the world in both developed and
developing countries and may range up to 40% of the total electricity
distributed. More generally, electricity theft belongs to non-technical losses
(NTL), which are losses that occur during the distribution of electricity in
power grids. In this paper, we build features from the neighborhood of
customers. We first split the area in which the customers are located into
grids of different sizes. For each grid cell we then compute the proportion of
inspected customers and the proportion of NTL found among the inspected
customers. We then analyze the distributions of features generated and show why
they are useful to predict NTL. In addition, we compute features from the
consumption time series of customers. We also use master data features of
customers, such as their customer class and voltage of their connection. We
compute these features for a Big Data base of 31M meter readings, 700K
customers and 400K inspection results. We then use these features to train four
machine learning algorithms that are particularly suitable for Big Data sets
because of their parallelizable structure: logistic regression, k-nearest
neighbors, linear support vector machine and random forest. Using the
neighborhood features instead of only analyzing the time series has resulted in
appreciable results for Big Data sets for varying NTL proportions of 1%-90%.
This work can therefore be deployed to a wide range of different regions around
the world.",Identity theft  detection
http://arxiv.org/abs/1712.01397v1,"As an initial assessment, over 480,000 labeled virtual images of normal
highway driving were readily generated in Grand Theft Auto V's virtual
environment. Using these images, a CNN was trained to detect following distance
to cars/objects ahead, lane markings, and driving angle (angular heading
relative to lane centerline): all variables necessary for basic autonomous
driving. Encouraging results were obtained when tested on over 50,000 labeled
virtual images from substantially different GTA-V driving environments. This
initial assessment begins to define both the range and scope of the labeled
images needed for training as well as the range and scope of labeled images
needed for testing the definition of boundaries and limitations of trained
networks. It is the efficacy and flexibility of a ""GTA-V""-like virtual
environment that is expected to provide an efficient well-defined foundation
for the training and testing of Convolutional Neural Networks for safe driving.
Additionally, described is the Princeton Virtual Environment (PVE) for the
training, testing and enhancement of safe driving AI, which is being developed
using the video-game engine Unity. PVE is being developed to recreate rare but
critical corner cases that can be used in re-training and enhancing machine
learning models and understanding the limitations of current self driving
models. The Florida Tesla crash is being used as an initial reference.",Identity theft  detection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft monitor
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",Identity theft monitor
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft monitor
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft monitor
http://arxiv.org/abs/1209.5982v1,"As smartphones become more pervasive, they are increasingly targeted by
malware. At the same time, each new generation of smartphone features
increasingly powerful onboard sensor suites. A new strain of sensor malware has
been developing that leverages these sensors to steal information from the
physical environment (e.g., researchers have recently demonstrated how malware
can listen for spoken credit card numbers through the microphone, or feel
keystroke vibrations using the accelerometer). Yet the possibilities of what
malware can see through a camera have been understudied. This paper introduces
a novel visual malware called PlaceRaider, which allows remote attackers to
engage in remote reconnaissance and what we call virtual theft. Through
completely opportunistic use of the camera on the phone and other sensors,
PlaceRaider constructs rich, three dimensional models of indoor environments.
Remote burglars can thus download the physical space, study the environment
carefully, and steal virtual objects from the environment (such as financial
documents, information on computer monitors, and personally identifiable
information). Through two human subject studies we demonstrate the
effectiveness of using mobile devices as powerful surveillance and virtual
theft platforms, and we suggest several possible defenses against visual
malware.",Identity theft monitor
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft monitor
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft monitor
http://arxiv.org/abs/1307.3147v2,"The wide spread of mobiles as handheld devices leads to various innovative
applications that makes use of their ever increasing presence in our daily
life. One such application is location tracking and monitoring. This paper
proposes a prototype model for location tracking using Geographical Positioning
System (GPS) and Global System for Mobile Communication (GSM) technology. The
system displays the object moving path on the monitor and the same information
can also be communicated to the user cell phone, on demand of the user by
asking the specific information via SMS. This system is very useful for car
theft situations, for adolescent drivers being watched and monitored by
parents. The result shows that the object is being tracked with a minimal
tracking error.",Identity theft monitor
http://arxiv.org/abs/1904.11882v1,"In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.",Identity theft monitor
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft monitor
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft monitor
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft monitor
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",Identity theft monitor
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft monitor
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft monitor
http://arxiv.org/abs/1410.0519v1,"By increase of culture and knowledge of the people, request for visiting
museums has increased and made the management of these places more complex.
Valuable things in a museum or ancient place must be maintained well and also
it need to managing visitors. To maintain things we should prevent them from
theft, as well as environmental factors such as temperature, humidity, PH,
chemical factors and mechanical events should be monitored. And if the
conditions are damaging, appropriate alerts or reports to managers and experts
should be announced. Visitors should also be monitored, as well as visitors
need to be guided and getting information in the environment. By utilizing RFID
technology and short-distance network tools, technical solutions for more
efficient management and more effective retention in museums can be
implemented.",Identity theft monitor
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft monitor
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft monitor
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",Identity theft monitor
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",Identity theft monitor
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",Identity theft monitor
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",Identity theft monitor
http://arxiv.org/abs/1708.04278v1,"Data leakage and theft from databases is a dangerous threat to organizations.
Data Security and Data Privacy protection systems (DSDP) monitor data access
and usage to identify leakage or suspicious activities that should be
investigated. Because of the high velocity nature of database systems, such
systems audit only a portion of the vast number of transactions that take
place. Anomalies are investigated by a Security Officer (SO) in order to choose
the proper response. In this paper we investigate the effect of sampling
methods based on the risk the transaction poses and propose a new method for
""combined sampling"" for capturing a more varied sample.",Identity theft monitor
http://arxiv.org/abs/1908.10229v1,"Digital healthcare systems are very popular lately, as they provide a variety
of helpful means to monitor people's health state as well as to protect people
against an unexpected health situation. These systems contain a huge amount of
personal information in a form of electronic health records that are not
allowed to be disclosed to unauthorized users. Hence, health data and
information need to be protected against attacks and thefts. In this paper, we
propose a secure distributed architecture for healthcare data storage and
analysis. It uses a novel security model to rigorously control permissions of
accessing sensitive data in the system, as well as to protect the transmitted
data between distributed system servers and nodes. The model also satisfies the
NIST security requirements. Thorough experimental results show that the model
is very promising.",Identity theft monitor
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft monitor
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",Identity theft monitor
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft monitor
http://arxiv.org/abs/1811.02293v1,"3GPP Release 15, the first 5G standard, includes protection of user identity
privacy against IMSI catchers. These protection mechanisms are based on public
key encryption. Despite this protection, IMSI catching is still possible in LTE
networks which opens the possibility of a downgrade attack on user identity
privacy, where a fake LTE base station obtains the identity of a 5G user
equipment. We propose (i) to use an existing pseudonym-based solution to
protect user identity privacy of 5G user equipment against IMSI catchers in LTE
and (ii) to include a mechanism for updating LTE pseudonyms in the public key
encryption based 5G identity privacy procedure. The latter helps to recover
from a loss of synchronization of LTE pseudonyms. Using this mechanism,
pseudonyms in the user equipment and home network are automatically
synchronized when the user equipment connects to 5G. Our mechanisms utilize
existing LTE and 3GPP Release 15 messages and require modifications only in the
user equipment and home network in order to provide identity privacy.
Additionally, lawful interception requires minor patching in the serving
network.",identity protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",identity protection
http://arxiv.org/abs/1806.05943v1,"Anonymous Identity-Based Encryption can protect privacy of the receiver.
However, there are some situations that we need to recover the identity of the
receiver, for example a dispute occurs or the privacy mechanism is abused. In
this paper, we propose a new concept, referred to as Anonymous Identity-Based
Encryption with Identity Recovery(AIBEIR), which is an anonymous IBE with
identity recovery property. There is a party called the Identity Recovery
Manager(IRM) who has a secret key to recover the identity from the ciphertext
in our scheme. We construct it with an anonymous IBE and a special IBE which we
call it testable IBE. In order to ensure the semantic security in the case
where the identity recovery manager is an adversary, we define a stronger
semantic security model in which the adversary is given the secret key of the
identity recovery manager. To our knowledge, we propose the first AIBEIR scheme
and prove the security in our defined model.",identity protection
http://arxiv.org/abs/1710.03317v1,"Research use of sensitive information -- personally identifiable information
(PII), protected health information (PHI), commercial or proprietary data, and
the like -- is increasing as researchers' skill with ""big data"" matures. Duke
University's Protected Network is an environment with technical controls in
place that provide research groups with essential pieces of security measures
needed for studies using sensitive information. The environment uses
virtualization and authorization groups extensively to isolate data, provide
elasticity of resources, and flexibly meet a range of computational
requirements within tightly controlled network boundaries. Since its beginning
in 2011, the environment has supported about 200 research projects and groups
and has served as a foundation for specialized and protected IT infrastructures
in the social sciences, population studies, and medical research. This article
lays out key features of the development of the Protected Network and outlines
the IT infrastructure design and organizational features that Duke has used in
establishing this resource for researchers. It consists of four sections: 1.
Context, 2. Infrastructure, 3. Authentication and identity management, and 4.
The infrastructure as a ""platform.""",identity protection
http://arxiv.org/abs/1208.3192v1,"One of the most important issues in peer-to-peer networks is anonymity. The
major anonymity for peer-to-peer users concerned with the users' identities and
actions which can be revealed by any other members. There are many approaches
proposed to provide anonymous peer-to-peer communications. An intruder can get
information about the content of the data, the sender's and receiver's
identities. Anonymous approaches are designed with the following three goals:
to protect the identity of provider, to protect the identity of requester and
to protect the contents of transferred data between them. This article presents
a new peer-to-peer approach to achieve anonymity between a requester and a
provider in peer-to-peer networks with trusted servers called suppernode so
that the provider will not be able to identify the requester and no other peers
can identify the two communicating parties with certainty. This article shows
that the proposed algorithm improved reliability and has more security. This
algorithm, based on onion routing and randomization, protects transferring data
against traffic analysis attack. The ultimate goal of this anonymous
communications algorithm is to allow a requester to communicate with a provider
in such a manner that nobody can determine the requester's identity and the
content of transferred data.",identity protection
http://arxiv.org/abs/1701.00436v1,"Privacy has become a serious concern for modern Information Societies. The
sensitive nature of much of the data that are daily exchanged or released to
untrusted parties requires that responsible organizations undertake appropriate
privacy protection measures. Nowadays, much of these data are texts (e.g.,
emails, messages posted in social media, healthcare outcomes, etc.) that,
because of their unstructured and semantic nature, constitute a challenge for
automatic data protection methods. In fact, textual documents are usually
protected manually, in a process known as document redaction or sanitization.
To do so, human experts identify sensitive terms (i.e., terms that may reveal
identities and/or confidential information) and protect them accordingly (e.g.,
via removal or, preferably, generalization). To relieve experts from this
burdensome task, in a previous work we introduced the theoretical basis of
C-sanitization, an inherently semantic privacy model that provides the basis to
the development of automatic document redaction/sanitization algorithms and
offers clear and a priori privacy guarantees on data protection; even though
its potential benefits C-sanitization still presents some limitations when
applied to practice (mainly regarding flexibility, efficiency and accuracy). In
this paper, we propose a new more flexible model, named (C, g(C))-sanitization,
which enables an intuitive configuration of the trade-off between the desired
level of protection (i.e., controlled information disclosure) and the
preservation of the utility of the protected data (i.e., amount of semantics to
be preserved). Moreover, we also present a set of technical solutions and
algorithms that provide an efficient and scalable implementation of the model
and improve its practical accuracy, as we also illustrate through empirical
experiments.",identity protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",identity protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",identity protection
http://arxiv.org/abs/1312.7511v1,"In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.",identity protection
http://arxiv.org/abs/1401.0092v1,"In identity management system, commonly used biometric recognition system
needs attention towards issue of biometric template protection as far as more
reliable solution is concerned. In view of this biometric template protection
algorithm should satisfy security, discriminability and cancelability. As no
single template protection method is capable of satisfying the basic
requirements, a novel technique for face template generation and protection is
proposed. The novel approach is proposed to provide security and accuracy in
new user enrollment as well as authentication process. This novel technique
takes advantage of both the hybrid approach and the binary discriminant
analysis algorithm. This algorithm is designed on the basis of random
projection, binary discriminant analysis and fuzzy commitment scheme. Three
publicly available benchmark face databases are used for evaluation. The
proposed novel technique enhances the discriminability and recognition accuracy
by 80% in terms of matching score of the face images and provides high
security.",identity protection
http://arxiv.org/abs/cs/0701144v1,"Trusted Computing is a security base technology that will perhaps be
ubiquitous in a few years in personal computers and mobile devices alike.
Despite its neutrality with respect to applications, it has raised some privacy
concerns. We show that trusted computing can be applied for service access
control in a manner protecting users' privacy. We construct a ticket system --
a concept which is at the heart of Identity Management -- relying solely on the
capabilities of the trusted platform module and the standards specified by the
Trusted Computing Group. Two examples show how it can be used for pseudonymous
and protected service access.",identity protection
http://arxiv.org/abs/1506.00950v1,"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange system has been
introduced as a simple, very low cost and efficient classical physical
alternative to quantum key distribution systems. The ideal system uses only a
few electronic components - identical resistor pairs, switches and
interconnecting wires - to guarantee perfectly protected data transmission. We
show that a generalized KLJN system can provide unconditional security even if
it is used with significantly less limitations. The more universal conditions
ease practical realizations considerably and support more robust protection
against attacks. Our theoretical results are confirmed by numerical
simulations.",identity protection
http://arxiv.org/abs/1603.00182v1,"A marketplace is defined where the private data of suppliers (e.g.,
prosumers) are protected, so that neither their identity nor their level of
stock is made known to end customers, while they can sell their products at a
reduced price. A broker acts as an intermediary, which takes care of providing
the items missing to meet the customers' demand and allows end customers to
take advantages of reduced prices through the subscription of option contracts.
Formulas are provided for the option price under three different probability
models for the availability of items. Option pricing allows the broker to
partially transfer its risk on end customers.",identity protection
http://arxiv.org/abs/1907.12221v1,"We increasingly live in a world where there is a balance between the rights
to privacy and the requirements for consent, and the rights of society to
protect itself. Within this world, there is an ever-increasing requirement to
protect the identities involved within financial transactions, but this makes
things increasingly difficult for law enforcement agencies, especially in terms
of financial fraud and money laundering. This paper reviews the
state-of-the-art in terms of the methods of privacy that are being used within
cryptocurrency transactions, and in the challenges that law enforcement face.",identity protection
http://arxiv.org/abs/1506.06996v1,"Using communication services is a common part of everyday life in a personal
or business context. Communication services include Internet services like
voice services, chat service, and web 2.0 technologies (wikis, blogs, etc), but
other usage areas like home energy management and eMobility are will be
increasingly tackled. Such communication services typically authenticate
participants. For this identities of some kind are used to identify the
communication peer to the user of a service or to the service itself. Calling
line identification used in the Session Initiation Protocol (SIP) used for
Voice over IP (VoIP) is just one example. Authentication and identification of
eCar users for accounting during charging of the eCar is another example. Also,
further mechanisms rely on identities, e.g., whitelists defining allowed
communication peers. Trusted identities prevent identity spoofing, hence are a
basic building block for the protection of communication. However, providing
trusted identities in a practical way is still a difficult problem and too
often application specific identities are used, making identity handling a
hassle. Nowadays, many countries introduced electronic identity cards, e.g.,
the German ""Elektronischer Personalausweis"" (ePA). As many German citizens will
possess an ePA soon, it can be used as security token to provide trusted
identities. Especially new usage areas (like eMobility) should from the start
be based on the ubiquitous availability of trusted identities. This paper
describes how identity cards can be integrated within three domains: home
energy management, vehicle-2-grid communication, and SIP-based voice over IP
telephony. In all three domains, identity cards are used to reliably identify
users and authenticate participants. As an example for an electronic identity
card, this paper focuses on the German ePA.",identity protection
http://arxiv.org/abs/1207.0135v1,"In this work, we focus on protection against identity disclosure in the
publication of sparse multidimensional data. Existing multidimensional
anonymization techniquesa) protect the privacy of users either by altering the
set of quasi-identifiers of the original data (e.g., by generalization or
suppression) or by adding noise (e.g., using differential privacy) and/or (b)
assume a clear distinction between sensitive and non-sensitive information and
sever the possible linkage. In many real world applications the above
techniques are not applicable. For instance, consider web search query logs.
Suppressing or generalizing anonymization methods would remove the most
valuable information in the dataset: the original query terms. Additionally,
web search query logs contain millions of query terms which cannot be
categorized as sensitive or non-sensitive since a term may be sensitive for a
user and non-sensitive for another. Motivated by this observation, we propose
an anonymization technique termed disassociation that preserves the original
terms but hides the fact that two or more different terms appear in the same
record. We protect the users' privacy by disassociating record terms that
participate in identifying combinations. This way the adversary cannot
associate with high probability a record with a rare combination of terms. To
the best of our knowledge, our proposal is the first to employ such a technique
to provide protection against identity disclosure. We propose an anonymization
algorithm based on our approach and evaluate its performance on real and
synthetic datasets, comparing it against other state-of-the-art methods based
on generalization and differential privacy.",identity protection
http://arxiv.org/abs/1907.03710v1,"Data exfiltration attacks have led to huge data breaches. Recently, the
Equifax attack affected 147M users and a third-party library - Apache Struts -
was alleged to be responsible for it. These attacks often exploit the fact that
sensitive data are stored unencrypted in process memory and can be accessed by
any function executing within the same process, including untrusted third-party
library functions. This paper presents StackVault, a kernel-based system to
prevent sensitive stack-based data from being accessed in an unauthorized
manner by intra-process functions. Stack-based data includes data on stack as
well as data pointed to by pointer variables on stack. StackVault consists of
three components: (1) a set of programming APIs to allow users to specify which
data needs to be protected, (2) a kernel module which uses unforgeable function
identities to reliably carry out the sensitive data protection, and (3) an LLVM
compiler extension that enables transparent placement of stack protection
operations. The StackVault system automatically enforces stack protection
through spatial and temporal access monitoring and control over both sensitive
stack data and untrusted functions. We implemented StackVault and evaluated it
using a number of popular real-world applications, including gRPC. The results
show that StackVault is effective and efficient, incurring only up to 2.4%
runtime overhead.",identity protection
http://arxiv.org/abs/1806.04410v1,"A legally valid identification document allows impartial arbitration of the
identification of individuals. It protects individuals from a violation of
their dignity, justice, liberty and equality. It protects the nation from a
destruction of its republic, democratic, sovereign status. In order to test the
ability of an identification document to establish impartial identification of
individuals, it must be evaluated for its ability to establish identity,
undertake identification and build confidence to impartial, reliable and valid
identification. The processes of issuing, using and validating identification
documents alter the ability of the document to establish identity, undertake
identification and build confidence to impartial and valid identification.
These processes alter the ability of the document to serve as proof of
identity, proof of address, proof of being a resident, or even the proof of
existence of a person. We examine the ability of the UID number to serve as an
identification document with the ability to impartially arbitrate the
identification of individuals and serve as proof of identity, address, and
demonstrate existence of a person. We evaluate the implications of the
continued use UID system on our ability to undertake legally valid
identification ensure integrity of the identity and address databases across
the world.",identity protection
http://arxiv.org/abs/1406.7377v1,"Today, Online Social Networks such as Facebook, LinkedIn and Twitter are the
most popular platforms on the Internet, on which millions of users register to
share personal information with their friends. A large amount of data, social
links and statistics about users are collected by Online Social Networks
services and they create big digital mines of various statistical data. Leakage
of personal information is a significant concern for social network users.
Besides information propagation, some new attacks on Online Social Networks
such as Identity Clone attack (ICA) have been identified. ICA attempts to
create a fake online identity of a victim to fool their friends into believing
the authenticity of the fake identity to establish social links in order to
reap the private information of the victims friends which is not shared in
their public profiles. There are some identity validation services that perform
users identity validation, but they are passive services and they only protect
users who are informed on privacy concerns and online identity issues. This
paper starts with an explanation of two types of profile cloning attacks are
explained and a new approach for detecting clone identities is proposed by
defining profile similarity and strength of relationship measures. According to
similar attributes and strength of relationship among users which are computed
in detection steps, it will be decided which profile is clone and which one is
genuine by a predetermined threshold. Finally, the experimental results are
presented to demonstrate the effectiveness of the proposed approach.",identity protection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",monitoring identity theft
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",monitoring identity theft
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",monitoring identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",monitoring identity theft
http://arxiv.org/abs/1209.5982v1,"As smartphones become more pervasive, they are increasingly targeted by
malware. At the same time, each new generation of smartphone features
increasingly powerful onboard sensor suites. A new strain of sensor malware has
been developing that leverages these sensors to steal information from the
physical environment (e.g., researchers have recently demonstrated how malware
can listen for spoken credit card numbers through the microphone, or feel
keystroke vibrations using the accelerometer). Yet the possibilities of what
malware can see through a camera have been understudied. This paper introduces
a novel visual malware called PlaceRaider, which allows remote attackers to
engage in remote reconnaissance and what we call virtual theft. Through
completely opportunistic use of the camera on the phone and other sensors,
PlaceRaider constructs rich, three dimensional models of indoor environments.
Remote burglars can thus download the physical space, study the environment
carefully, and steal virtual objects from the environment (such as financial
documents, information on computer monitors, and personally identifiable
information). Through two human subject studies we demonstrate the
effectiveness of using mobile devices as powerful surveillance and virtual
theft platforms, and we suggest several possible defenses against visual
malware.",monitoring identity theft
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",monitoring identity theft
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",monitoring identity theft
http://arxiv.org/abs/1307.3147v2,"The wide spread of mobiles as handheld devices leads to various innovative
applications that makes use of their ever increasing presence in our daily
life. One such application is location tracking and monitoring. This paper
proposes a prototype model for location tracking using Geographical Positioning
System (GPS) and Global System for Mobile Communication (GSM) technology. The
system displays the object moving path on the monitor and the same information
can also be communicated to the user cell phone, on demand of the user by
asking the specific information via SMS. This system is very useful for car
theft situations, for adolescent drivers being watched and monitored by
parents. The result shows that the object is being tracked with a minimal
tracking error.",monitoring identity theft
http://arxiv.org/abs/1904.11882v1,"In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.",monitoring identity theft
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",monitoring identity theft
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",monitoring identity theft
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",monitoring identity theft
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",monitoring identity theft
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",monitoring identity theft
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",monitoring identity theft
http://arxiv.org/abs/1410.0519v1,"By increase of culture and knowledge of the people, request for visiting
museums has increased and made the management of these places more complex.
Valuable things in a museum or ancient place must be maintained well and also
it need to managing visitors. To maintain things we should prevent them from
theft, as well as environmental factors such as temperature, humidity, PH,
chemical factors and mechanical events should be monitored. And if the
conditions are damaging, appropriate alerts or reports to managers and experts
should be announced. Visitors should also be monitored, as well as visitors
need to be guided and getting information in the environment. By utilizing RFID
technology and short-distance network tools, technical solutions for more
efficient management and more effective retention in museums can be
implemented.",monitoring identity theft
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",monitoring identity theft
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",monitoring identity theft
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",monitoring identity theft
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",monitoring identity theft
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",monitoring identity theft
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",monitoring identity theft
http://arxiv.org/abs/1708.04278v1,"Data leakage and theft from databases is a dangerous threat to organizations.
Data Security and Data Privacy protection systems (DSDP) monitor data access
and usage to identify leakage or suspicious activities that should be
investigated. Because of the high velocity nature of database systems, such
systems audit only a portion of the vast number of transactions that take
place. Anomalies are investigated by a Security Officer (SO) in order to choose
the proper response. In this paper we investigate the effect of sampling
methods based on the risk the transaction poses and propose a new method for
""combined sampling"" for capturing a more varied sample.",monitoring identity theft
http://arxiv.org/abs/1908.10229v1,"Digital healthcare systems are very popular lately, as they provide a variety
of helpful means to monitor people's health state as well as to protect people
against an unexpected health situation. These systems contain a huge amount of
personal information in a form of electronic health records that are not
allowed to be disclosed to unauthorized users. Hence, health data and
information need to be protected against attacks and thefts. In this paper, we
propose a secure distributed architecture for healthcare data storage and
analysis. It uses a novel security model to rigorously control permissions of
accessing sensitive data in the system, as well as to protect the transmitted
data between distributed system servers and nodes. The model also satisfies the
NIST security requirements. Thorough experimental results show that the model
is very promising.",monitoring identity theft
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",monitoring identity theft
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",monitoring identity theft
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",monitoring identity theft
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing detection
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing detection
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing detection
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing detection
http://arxiv.org/abs/1104.0582v1,"Bag-of-words model is implemented and tried on 10-class visual concept
detection problem. The experimental results show that ""DURF+ERT+SVM""
outperforms ""SIFT+ERT+SVM"" both in detection performance and computation
efficiency. Besides, combining DURF and SIFT results in even better detection
performance. Real-time object detection using SIFT and RANSAC is also tried on
simple objects, e.g. drink can, and good result is achieved.",pishing detection
http://arxiv.org/abs/1311.1446v1,"Mobile ad-hoc networks are temporary wireless networks. Network resources are
abnormally consumed by intruders. Anomaly and signature based techniques are
used for intrusion detection. Classification techniques are used in anomaly
based techniques. Intrusion detection techniques are used for the network
attack detection process. Two types of intrusion detection systems are
available. They are anomaly detection and signature based detection model. The
anomaly detection model uses the historical transactions with attack labels.
The signature database is used in the signature based IDS schemes.
  The mobile ad-hoc networks are infrastructure less environment. The intrusion
detection applications are placed in a set of nodes under the mobile ad-hoc
network environment. The nodes are grouped into clusters. The leader nodes are
assigned for the clusters. The leader node is assigned for the intrusion
detection process. Leader nodes are used to initiate the intrusion detection
process. Resource sharing and lifetime management factors are considered in the
leader election process. The system optimizes the leader election and intrusion
detection process.
  The system is designed to handle leader election and intrusion detection
process. The clustering scheme is optimized with coverage and traffic level.
Cost and resource utilization is controlled under the clusters. Node mobility
is managed by the system.",pishing detection
http://arxiv.org/abs/1612.08242v1,"We introduce YOLO9000, a state-of-the-art, real-time object detection system
that can detect over 9000 object categories. First we propose various
improvements to the YOLO detection method, both novel and drawn from prior
work. The improved model, YOLOv2, is state-of-the-art on standard detection
tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At
40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like
Faster RCNN with ResNet and SSD while still running significantly faster.
Finally we propose a method to jointly train on object detection and
classification. Using this method we train YOLO9000 simultaneously on the COCO
detection dataset and the ImageNet classification dataset. Our joint training
allows YOLO9000 to predict detections for object classes that don't have
labelled detection data. We validate our approach on the ImageNet detection
task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite
only having detection data for 44 of the 200 classes. On the 156 classes not in
COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;
it predicts detections for more than 9000 different object categories. And it
still runs in real-time.",pishing detection
http://arxiv.org/abs/1810.02659v1,Research Proposal in Automated Fix Detection,pishing detection
http://arxiv.org/abs/1901.06585v1,"This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.",pishing detection
http://arxiv.org/abs/1702.04377v1,"Face detection is one of the challenging tasks in computer vision. Human face
detection plays an essential role in the first stage of face processing
applications such as face recognition, face tracking, image database
management, etc. In these applications, face objects often come from an
inconsequential part of images that contain variations, namely different
illumination, poses, and occlusion. These variations can decrease face
detection rate noticeably. Most existing face detection approaches are not
accurate, as they have not been able to resolve unstructured images due to
large appearance variations and can only detect human faces under one
particular variation. Existing frameworks of face detection need enhancements
to detect human faces under the stated variations to improve detection rate and
reduce detection time. In this study, an enhanced face detection framework is
proposed to improve detection rate based on skin color and provide a validation
process. A preliminary segmentation of the input images based on skin color can
significantly reduce search space and accelerate the process of human face
detection. The primary detection is based on Haar-like features and the
Adaboost algorithm. A validation process is introduced to reject non-face
objects, which might occur during the face detection process. The validation
process is based on two-stage Extended Local Binary Patterns. The experimental
results on the CMU-MIT and Caltech 10000 datasets over a wide range of facial
variations in different colors, positions, scales, and lighting conditions
indicated a successful face detection rate.",pishing detection
http://arxiv.org/abs/cs/0501001v1,"Distributed intrustion detection systems detect attacks on computer systems
by analyzing data aggregated from distributed sources. The distributed nature
of the data sources allows patterns in the data to be seen that might not be
detectable if each of the sources were examined individually. This paper
describes the various approaches that have been developed to share and analyze
data in such systems, and discusses some issues that must be addressed before
fully decentralized distributed intrusion detection systems can be made viable.",pishing detection
http://arxiv.org/abs/1710.03958v2,"Recent approaches for high accuracy detection and tracking of object
categories in video consist of complex multistage solutions that become more
cumbersome each year. In this paper we propose a ConvNet architecture that
jointly performs detection and tracking, solving the task in a simple and
effective way. Our contributions are threefold: (i) we set up a ConvNet
architecture for simultaneous detection and tracking, using a multi-task
objective for frame-based object detection and across-frame track regression;
(ii) we introduce correlation features that represent object co-occurrences
across time to aid the ConvNet during tracking; and (iii) we link the frame
level detections based on our across-frame tracklets to produce high accuracy
detections at the video level. Our ConvNet architecture for spatiotemporal
object detection is evaluated on the large-scale ImageNet VID dataset where it
achieves state-of-the-art results. Our approach provides better single model
performance than the winning method of the last ImageNet challenge while being
conceptually much simpler. Finally, we show that by increasing the temporal
stride we can dramatically increase the tracker speed.",pishing detection
http://arxiv.org/abs/1902.01031v1,"The main essence of this paper is to investigate the performance of RetinaNet
based object detectors on pedestrian detection. Pedestrian detection is an
important research topic as it provides a baseline for general object detection
and has a great number of practical applications like autonomous car, robotics
and Security camera. Though extensive research has made huge progress in
pedestrian detection, there are still many issues and open for more research
and improvement. Recent deep learning based methods have shown state-of-the-art
performance in computer vision tasks such as image classification, object
detection, and segmentation. Wider pedestrian detection challenge aims at
finding improve solutions for pedestrian detection problem. In this paper, We
propose a pedestrian detection system based on RetinaNet. Our solution has
scored 0.4061 mAP. The code is available at
https://github.com/miltonbd/ECCV_2018_pedestrian_detection_challenege.",pishing detection
http://arxiv.org/abs/1905.05055v2,"Object detection, as of one the most fundamental and challenging problems in
computer vision, has received great attention in recent years. Its development
in the past two decades can be regarded as an epitome of computer vision
history. If we think of today's object detection as a technical aesthetics
under the power of deep learning, then turning back the clock 20 years we would
witness the wisdom of cold weapon era. This paper extensively reviews 400+
papers of object detection in the light of its technical evolution, spanning
over a quarter-century's time (from the 1990s to 2019). A number of topics have
been covered in this paper, including the milestone detectors in history,
detection datasets, metrics, fundamental building blocks of the detection
system, speed up techniques, and the recent state of the art detection methods.
This paper also reviews some important detection applications, such as
pedestrian detection, face detection, text detection, etc, and makes an in-deep
analysis of their challenges as well as technical improvements in recent years.",pishing detection
http://arxiv.org/abs/1906.00093v1,"In this paper, we present a novel model to detect lane regions and extract
lane departure events (changes and incursions) from challenging,
lower-resolution videos recorded with mobile cameras. Our algorithm used a
Mask-RCNN based lane detection model as pre-processor. Recently, deep
learning-based models provide state-of-the-art technology for object detection
combined with segmentation. Among the several deep learning architectures,
convolutional neural networks (CNNs) outperformed other machine learning
models, especially for region proposal and object detection tasks. Recent
development in object detection has been driven by the success of region
proposal methods and region-based CNNs (R-CNNs). Our algorithm utilizes lane
segmentation mask for detection and Fix-lag Kalman filter for tracking, rather
than the usual approach of detecting lane lines from single video frames. The
algorithm permits detection of driver lane departures into left or right lanes
from continuous lane detections. Preliminary results show promise for robust
detection of lane departure events. The overall sensitivity for lane departure
events on our custom test dataset is 81.81%.",pishing detection
http://arxiv.org/abs/1503.03771v1,"This paper studies efficient means for dealing with intra-category diversity
in object detection. Strategies for occlusion and orientation handling are
explored by learning an ensemble of detection models from visual and
geometrical clusters of object instances. An AdaBoost detection scheme is
employed with pixel lookup features for fast detection. The analysis provides
insight into the design of a robust vehicle detection system, showing promise
in terms of detection performance and orientation estimation accuracy.",pishing detection
http://arxiv.org/abs/1611.00301v1,"We introduce a powerful recurrent neural network based method for novelty
detection to the application of detecting radio anomalies. This approach holds
promise in significantly increasing the ability of naive anomaly detection to
detect small anomalies in highly complex complexity multi-user radio bands. We
demonstrate the efficacy of this approach on a number of common real over the
air radio communications bands of interest and quantify detection performance
in terms of probability of detection an false alarm rates across a range of
interference to band power ratios and compare to baseline methods.",pishing detection
http://arxiv.org/abs/1812.06292v2,"Deep learning is an advanced model of traditional machine learning. This has
the capability to extract optimal feature representation from raw input
samples. This has been applied towards various use cases in cyber security such
as intrusion detection, malware classification, android malware detection, spam
and phishing detection and binary analysis. This paper outlines the survey of
all the works related to deep learning based solutions for various cyber
security use cases. Keywords: Deep learning, intrusion detection, malware
detection, Android malware detection, spam & phishing detection, traffic
analysis, binary analysis.",pishing detection
http://arxiv.org/abs/1909.12483v1,"This paper presents a method to detect reflection with 3D light detection and
ranging (Lidar) and uses it to map the back side of objects. This method uses
several approaches to analyze the point cloud, including intensity peak
detection, dual return detection, plane fitting, and finding the boundaries.
These approaches can classify the point cloud and detect the reflection in it.
By mirroring the reflection points on the detected window pane and adding
classification labels on the points, we can have improve the map quality in a
Simultaneous Localization and Mapping (SLAM) framework.",pishing detection
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing monitoring
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing monitoring
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing monitoring
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing monitoring
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",pishing monitoring
http://arxiv.org/abs/1802.03667v1,"Runtime monitoring is essential for the violation detection during the
underlying software system execution. In this paper, an investigation of the
monitoring activity of MAPE-K control loop is performed which aims at
exploring:(1) the architecture of the monitoring activity in terms of the
involved components and control and data flow between them; (2) the standard
interface of the monitoring component with other MAPE-K components; (3) the
adaptive monitoring and its importance to the monitoring overhead issue; and
(4) the monitoring mode and its relevance to some specific situations and
systems. This paper also presented a Java framework for the monitoring process
for self adaptive systems.",pishing monitoring
http://arxiv.org/abs/1906.00766v1,"Monitorability delineates what properties can be verified at runtime.
Although many monitorability definitions exist, few are defined explicitly in
terms of the guarantees provided by monitors, i.e., the computational entities
carrying out the verification. We view monitorability as a spectrum: the fewer
monitor guarantees that are required, the more properties become monitorable.
We present a monitorability hierarchy and provide operational and syntactic
characterisations for its levels. Existing monitorability definitions are
mapped into our hierarchy, providing a unified framework that makes the
operational assumptions and guarantees of each definition explicit. This
provides a rigorous foundation that can inform design choices and correctness
claims for runtime verification tools.",pishing monitoring
http://arxiv.org/abs/1902.05135v1,"Monitoring kernel object modification of virtual machine is widely used by
virtual-machine-introspection-based security monitors to protect virtual
machines in cloud computing, such as monitoring dentry objects to intercept
file operations, etc. However, most of the current virtual machine monitors,
such as KVM and Xen, only support page-level monitoring, because the Intel EPT
technology can only monitor page privilege. If the out-of-virtual-machine
security tools want to monitor some kernel objects, they need to intercept the
operation of the whole memory page. Since there are some other objects stored
in the monitored pages, the modification of them will also trigger the monitor.
Therefore, page-level memory monitor usually introduces overhead to related
kernel services of the target virtual machine. In this paper, we propose a
low-overhead kernel object monitoring approach to reduce the overhead caused by
page-level monitor. The core idea is to migrate the target kernel objects to a
protected memory area and then to monitor the corresponding new memory pages.
Since the new pages only contain the kernel objects to be monitored, other
kernel objects will not trigger our monitor. Therefore, our monitor will not
introduce runtime overhead to the related kernel service. The experimental
results show that our system can monitor target kernel objects effectively only
with very low overhead.",pishing monitoring
http://arxiv.org/abs/1106.1816v1,"Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.",pishing monitoring
http://arxiv.org/abs/1106.0235v1,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",pishing monitoring
http://arxiv.org/abs/1411.5213v1,"With our growing reliability on distributed networks, the security aspect of
such networks becomes of prime importance. In large scale distributed networks
it becomes cardinal to have an efficient and effective monitoring scheme. The
monitoring schemes supervise the node behaviour in the network and look out for
any discrepancy. Monitoring schemes comprise of monitoring components that work
together to help schemes in meeting various security requirement parameters for
the networks. These security parameters are breached via various attacks by
manipulation of monitoring components of particular monitoring schemes to
produce faulty results and thereby reducing efficiency of networks, reliability
and security. In this paper we have discussed these components of monitoring,
multiple monitoring schemes, their security parameters and various types of
attacks possible on these monitoring components by manipulating assumptions of
monitoring schemes.",pishing monitoring
http://arxiv.org/abs/1708.01476v1,"System monitoring is an established tool to measure the utilization and
health of HPC systems. Usually system monitoring infrastructures make no
connection to job information and do not utilize hardware performance
monitoring (HPM) data. To increase the efficient use of HPC systems automatic
and continuous performance monitoring of jobs is an essential component. It can
help to identify pathological cases, provides instant performance feedback to
the users, offers initial data to judge on the optimization potential of
applications and helps to build a statistical foundation about application
specific system usage. The LIKWID monitoring stack is a modular framework build
on top of the LIKWID tools library. It aims on enabling job specific
performance monitoring using HPM data, system metrics and application-level
data for small to medium sized commodity clusters. Moreover, it is designed to
integrate in existing monitoring infrastructures to speed up the change from
pure system monitoring to job-aware monitoring.",pishing monitoring
http://arxiv.org/abs/1902.05152v1,"We compare the succinctness of two monitoring systems for properties of
infinite traces, namely parallel and regular monitors. Although a parallel
monitor can be turned into an equivalent regular monitor, the cost of this
transformation is a double-exponential blowup in the syntactic size of the
monitors, and a triple-exponential blowup when the goal is a deterministic
monitor. We show that these bounds are tight and that they also hold for
translations between corresponding fragments of Hennessy-Milner logic with
recursion over infinite traces.",pishing monitoring
http://arxiv.org/abs/1711.03952v2,"Trust in publicly verifiable Certificate Transparency (CT) logs is reduced
through cryptography, gossip, auditing, and monitoring. The role of a monitor
is to observe each and every log entry, looking for suspicious certificates
that interest the entity running the monitor. While anyone can run a monitor,
it requires continuous operation and copies of the logs to be inspected. This
has lead to the emergence of monitoring-as-a-service: a trusted party runs the
monitor and provides registered subjects with selective certificate
notifications, e.g., ""notify me of all foo.com certificates"". We present a
CT/bis extension for verifiable light-weight monitoring that enables subjects
to verify the correctness of such notifications, reducing the trust that is
placed in these monitors. Our extension supports verifiable monitoring of
wild-card domains and piggybacks on CT's existing gossip-audit security model.",pishing monitoring
http://arxiv.org/abs/1806.06143v2,"We study selective monitors for labelled Markov chains. Monitors observe the
outputs that are generated by a Markov chain during its run, with the goal of
identifying runs as correct or faulty. A monitor is selective if it skips
observations in order to reduce monitoring overhead. We are interested in
monitors that minimize the expected number of observations. We establish an
undecidability result for selectively monitoring general Markov chains. On the
other hand, we show for non-hidden Markov chains (where any output identifies
the state the Markov chain is in) that simple optimal monitors exist and can be
computed efficiently, based on DFA language equivalence. These monitors do not
depend on the precise transition probabilities in the Markov chain. We report
on experiments where we compute these monitors for several open-source Java
projects.",pishing monitoring
http://arxiv.org/abs/1902.00435v1,"This paper establishes a comprehensive theory of runtime monitorability for
Hennessy-Milner logic with recursion, a very expressive variant of the modal
$\mu$-calculus. It investigates the monitorability of that logic with a
linear-time semantics and then compares the obtained results with ones that
were previously presented in the literature for a branching-time setting. Our
work establishes an expressiveness hierarchy of monitorable fragments of
Hennessy-Milner logic with recursion in a linear-time setting and exactly
identifies what kinds of guarantees can be given using runtime monitors for
each fragment in the hierarchy. Each fragment is shown to be complete, in the
sense that it can express all properties that can be monitored under the
corresponding guarantees. The study is carried out using a principled approach
to monitoring that connects the semantics of the logic and the operational
semantics of monitors. The proposed framework supports the automatic,
compositional synthesis of correct monitors from monitorable properties.",pishing monitoring
http://arxiv.org/abs/1301.3839v1,"Monitoring plan preconditions can allow for replanning when a precondition
fails, generally far in advance of the point in the plan where the precondition
is relevant. However, monitoring is generally costly, and some precondition
failures have a very small impact on plan quality. We formulate a model for
optimal precondition monitoring, using partially-observable Markov decisions
processes, and describe methods for solving this model efficitively, though
approximately. Specifically, we show that the single-precondition monitoring
problem is generally tractable, and the multiple-precondition monitoring
policies can be efficitively approximated using single-precondition soultions.",pishing monitoring
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",pishing monitoring
http://arxiv.org/abs/1611.10212v1,"We examine the determinization of monitors for HML with recursion. We
demonstrate that every monitor is equivalent to a deterministic one, which is
at most doubly exponential in size with respect to the original monitor. When
monitors are described as CCS-like processes, this doubly exponential bound is
optimal. When (deterministic) monitors are described as finite automata (as
their LTS), then they can be exponentially more succinct than their CCS process
form.",pishing monitoring
http://arxiv.org/abs/0711.0315v1,"Effective resource utilisation monitoring and highly granular yet adaptive
measurements are prerequisites for a more efficient Grid scheduler. We present
a suite of measurement applications able to monitor per-process resource
utilisation, and a customisable tool for emulating observed utilisation models.",pishing monitoring
http://arxiv.org/abs/1507.01020v1,"When a property needs to be checked against an unknown or very complex
system, classical exploration techniques like model-checking are not applicable
anymore. Sometimes a~monitor can be used, that checks a given property on the
underlying system at runtime. A monitor for a property $L$ is a deterministic
finite automaton $M_L$ that after each finite execution tells whether (1) every
possible extension of the execution is in $L$, or (2) every possible extension
is in the complement of $L$, or neither (1) nor (2) holds. Moreover, $L$ being
monitorable means that it is always possible that in some future the monitor
reaches (1) or (2). Classical examples for monitorable properties are safety
and cosafety properties. On the other hand, deterministic liveness properties
like ""infinitely many $a$'s"" are not monitorable. We discuss various monitor
constructions with a focus on deterministic omega-regular languages. We locate
a proper subclass of of deterministic omega-regular languages but also strictly
large than the subclass of languages which are deterministic and
codeterministic, and for this subclass there exists a canonical monitor which
also accepts the language itself.
  We also address the problem to decide monitorability in comparison with
deciding liveness. The state of the art is as follows. Given a B\""uchi
automaton, it is PSPACE-complete to decide liveness or monitorability. Given an
LTL formula, deciding liveness becomes EXPSPACE-complete, but the complexity to
decide monitorability remains open.",pishing monitoring
http://arxiv.org/abs/cs/0310025v1,"UFO is a new implementation of FORMAN, a declarative monitoring language, in
which rules are compiled into execution monitors that run on a virtual machine
supported by the Alamo monitor architecture.",pishing monitoring
http://arxiv.org/abs/0707.3490v1,"The author solves two problems: formation of object of econophysics, creation
of the general theory of financial-economic monitoring. In the first problem he
studied two fundamental tasks: a choice of conceptual model and creation of
axiomatic base. It is accepted, that the conceptual model of econophysics is a
concrete definition of entropy conceptual model. Financial and economic
monitoring is considered as monitoring of flows on entropy manifold of phase
space - on a Diffusion field.",pishing monitoring
http://arxiv.org/abs/0709.1333v2,"Shintake monitor is a nanometer-scale electron beam size monitor. It probes a
electron beam by an interference fringe pattern formed by split laser beams.
Minimum measurable beam size by this method is less than 1/10 of laser
wavelength. In ATF2, Shintake monitor will be used for the IP beam size monitor
to measure 37 nm (design) beam size. Development status of the Shintake
monitor, including fringe phase monitoring and stabilization, gamma detector
and collimators, is described. In addition, we discuss the beam size
measurement by Shintake monitor in ILC.",pishing monitoring
http://arxiv.org/abs/1502.06904v1,"In this short paper we consider the problem of monitoring physical activity
in the smart house. The authors suggested a simple device that allows medical
staff and relatives to monitor the activity for older adults living alone. This
sensor monitors the switching-on of electrical devices. The fact of switching
is seen as confirmation of physical activity. It is confirmed by SMS
notifications to observers.",pishing monitoring
http://arxiv.org/abs/1701.07484v1,"Our machines, products, utilities, and environments have long been monitored
by embedded software systems. Our professional, commercial, social and personal
lives are also subject to monitoring as they are mediated by software systems.
Data on nearly everything now exists, waiting to be collected and analysed for
all sorts of reasons. Given the rising tide of data we pose the questions: What
is monitoring? Do diverse and disparate monitoring systems have anything in
common? We attempt answer these questions by proposing an abstract conceptual
framework for studying monitoring. We argue that it captures a structure common
to many different monitoring practices, and that from it detailed formal models
can be derived, customised to applications. The framework formalises the idea
that monitoring is a process that observes the behaviour of people and objects
in a context. The entities and their behaviours are represented by abstract
data types and the observable attributes by logics. Since monitoring usually
has a specific purpose, we extend the framework with protocols for detecting
attributes or events that require interventions and, possibly, a change in
behaviour. Our theory is illustrated by a case study from criminal justice,
that of electronic tagging.",pishing monitoring
http://arxiv.org/abs/1511.06975v1,"Considering the level of competition prevailing in Business-to-Consumer (B2C)
E-Commerce domain and the huge investments required to attract new customers,
firms are now giving more focus to reduce their customer churn rate. Churn rate
is the ratio of customers who part away with the firm in a specific time
period. One of the best mechanism to retain current customers is to identify
any potential churn and respond fast to prevent it. Detecting early signs of a
potential churn, recognizing what the customer is looking for by the movement
and automating personalized win back campaigns are essential to sustain
business in this era of competition. E-Commerce firms normally possess large
volume of data pertaining to their existing customers like transaction history,
search history, periodicity of purchases, etc. Data mining techniques can be
applied to analyse customer behaviour and to predict the potential customer
attrition so that special marketing strategies can be adopted to retain them.
This paper proposes an integrated model that can predict customer churn and
also recommend personalized win back actions.",detecting e-commerce
http://arxiv.org/abs/1308.3559v1,"Man in the middle attacks are a significant threat to modern e-commerce and
online communications, even when such transactions are protected by TLS. We
intend to show that it is possible to detect man-in-the-middle attacks on SSL
and TLS by detecting timing differences between a standard SSL session and an
attack we created.",detecting e-commerce
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",detecting e-commerce
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",detecting e-commerce
http://arxiv.org/abs/1907.01284v1,"Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.",detecting e-commerce
http://arxiv.org/abs/1507.07382v1,"Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence.",detecting e-commerce
http://arxiv.org/abs/1512.04122v1,"The emergence of mobile platforms with increased storage and computing
capabilities and the pervasive use of these platforms for sensitive
applications such as online banking, e-commerce and the storage of sensitive
information on these mobile devices have led to increasing danger associated
with malware targeted at these devices. Detecting such malware presents
inimitable challenges as signature-based detection techniques available today
are becoming inefficient in detecting new and unknown malware. In this
research, a machine learning approach for the detection of malware on Android
platforms is presented. The detection system monitors and extracts features
from the applications while in execution and uses them to perform in-device
detection using a trained K-Nearest Neighbour classifier. Results shows high
performance in the detection rate of the classifier with accuracy of 93.75%,
low error rate of 6.25% and low false positive rate with ability of detecting
real Android malware.",detecting e-commerce
http://arxiv.org/abs/1510.05544v2,"Given a network with attributed edges, how can we identify anomalous
behavior? Networks with edge attributes are commonplace in the real world. For
example, edges in e-commerce networks often indicate how users rated products
and services in terms of number of stars, and edges in online social and
phonecall networks contain temporal information about when friendships were
formed and when users communicated with each other -- in such cases, edge
attributes capture information about how the adjacent nodes interact with other
entities in the network. In this paper, we aim to utilize exactly this
information to discern suspicious from typical node behavior. Our work has a
number of notable contributions, including (a) formulation: while most other
graph-based anomaly detection works use structural graph connectivity or node
information, we focus on the new problem of leveraging edge information, (b)
methodology: we introduce EdgeCentric, an intuitive and scalable
compression-based approach for detecting edge-attributed graph anomalies, and
(c) practicality: we show that EdgeCentric successfully spots numerous such
anomalies in several large, edge-attributed real-world graphs, including the
Flipkart e-commerce graph with over 3 million product reviews between 1.1
million users and 545 thousand products, where it achieved 0.87 precision over
the top 100 results.",detecting e-commerce
http://arxiv.org/abs/1905.02234v2,"In e-commerce, product content, especially product images have a significant
influence on a customer's journey from product discovery to evaluation and
finally, purchase decision. Since many e-commerce retailers sell items from
other third-party marketplace sellers besides their own, the content published
by both internal and external content creators needs to be monitored and
enriched, wherever possible. Despite guidelines and warnings, product listings
that contain offensive and non-compliant images continue to enter catalogs.
Offensive and non-compliant content can include a wide range of objects, logos,
and banners conveying violent, sexually explicit, racist, or promotional
messages. Such images can severely damage the customer experience, lead to
legal issues, and erode the company brand. In this paper, we present a computer
vision driven offensive and non-compliant image detection system for extremely
large image datasets. This paper delves into the unique challenges of applying
deep learning to real-world product image data from retail world. We
demonstrate how we resolve a number of technical challenges such as lack of
training data, severe class imbalance, fine-grained class definitions etc.
using a number of practical yet unique technical strategies. Our system
combines state-of-the-art image classification and object detection techniques
with budgeted crowdsourcing to develop a solution customized for a massive,
diverse, and constantly evolving product catalog.",detecting e-commerce
http://arxiv.org/abs/1811.04374v1,"We present an empirical study of applying deep Convolutional Neural Networks
(CNN) to the task of fashion and apparel image classification to improve
meta-data enrichment of e-commerce applications. Five different CNN
architectures were analyzed using clean and pre-trained models. The models were
evaluated in three different tasks person detection, product and gender
classification, on two small and large scale datasets.",detecting e-commerce
http://arxiv.org/abs/1909.10562v1,"Alibaba has China's largest e-commerce platform. To support its diverse
businesses, Alibaba has its own large-scale data centers providing the
computing foundation for a wide variety of software applications. Among these
applications, deep learning (DL) has been playing an important role in
delivering services like image recognition, objection detection, text
recognition, recommendation, and language processing. To build more efficient
data centers that deliver higher performance for these DL applications, it is
important to understand their computational needs and use that information to
guide the design of future computing infrastructure. An effective way to
achieve this is through benchmarks that can fully represent Alibaba's DL
applications.",detecting e-commerce
http://arxiv.org/abs/1202.1761v1,"Denial of Service (DoS) is a security threat which compromises the
confidentiality of information stored in Local Area Networks (LANs) due to
unauthorized access by spoofed IP addresses. SYN Flooding is a type of DoS
which is harmful to network as the flooding of packets may delay other users
from accessing the server and in severe cases, the server may need to be shut
down, wasting valuable resources, especially in critical real-time services
such as in e-commerce and the medical field. The objective of this paper is to
review the state-of-the art of detection mechanisms for SYN flooding. The
detection schemes for SYN Flooding attacks have been classified broadly into
three categories - detection schemes based on the router data structure,
detection schemes based on statistical analysis of the packet flow and
detection schemes based on artificial intelligence. The advantages and
disadvantages for various detection schemes under each category have been
critically examined. The performance measures of the categories have also been
compared.",detecting e-commerce
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",detecting e-commerce
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",detecting e-commerce
http://arxiv.org/abs/1411.5319v2,"In this work, we propose and address a new computer vision task, which we
call fashion item detection, where the aim is to detect various fashion items a
person in the image is wearing or carrying. The types of fashion items we
consider in this work include hat, glasses, bag, pants, shoes and so on. The
detection of fashion items can be an important first step of various e-commerce
applications for fashion industry. Our method is based on state-of-the-art
object detection method pipeline which combines object proposal methods with a
Deep Convolutional Neural Network. Since the locations of fashion items are in
strong correlation with the locations of body joints positions, we incorporate
contextual information from body poses in order to improve the detection
performance. Through the experiments, we demonstrate the effectiveness of the
proposed method.",detecting e-commerce
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",detecting e-commerce
http://arxiv.org/abs/1904.12574v2,"Learning product representations that reflect complementary relationship
plays a central role in modern recommender system for e-commerce platforms. A
notable challenge is that unlike many simple relationships such as similarity,
complementariness is often detected from customer purchase activities, which
are highly sparse and noisy. Also, standard usage of representation learning
emphasizes on only one set of embedding, which is problematic for modelling the
asymmetric property of complementariness. We propose using context-aware
multi-tasking learning with dual product embedding to solve the above
challenges. We encode contextual knowledge into product representation by
multi-task learning, in order to alleviate the sparsity issue. By explicitly
modelling with user bias terms, we take care of the noise induced by
customer-specific preferences. Furthermore, we adopt the dual embedding
framework to capture the intrinsic properties of complementariness and provide
geometric interpretation motivated by the classic separating hyperplane theory.
Finally, we propose a Bayesian network structure that unifies all the
components, which also concludes several popular models as special cases. The
proposed method compares favourably to state-of-art representation learning and
recommendation algorithms for e-commerce, in downstream classification and
recommendation tasks. We also develop an implementation that scales efficiently
to a dataset with millions of items and customers.",detecting e-commerce
http://arxiv.org/abs/1506.04584v1,"Collaborative filtering recommender systems (CFRSs) are the key components of
successful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks
since its openness. However, since attack size is far smaller than that of
genuine users, conventional supervised learning based detection methods could
be too ""dull"" to handle such imbalanced classification. In this paper, we
improve detection performance from following two aspects. First, we extract
well-designed features from user profiles based on the statistical properties
of the diverse attack models, making hard classification task becomes easier to
perform. Then, refer to the general idea of re-scale Boosting (RBoosting) and
AdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost
(RAdaBoost) as our detection method based on extracted features. RAdaBoost is
comparable to the optimal Boosting-type algorithm and can effectively improve
the performance in some hard scenarios. Finally, a series of experiments on the
MovieLens-100K data set are conducted to demonstrate the outperformance of
RAdaBoost comparing with some classical techniques such as SVM, kNN and
AdaBoost.",detecting e-commerce
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",detecting e-commerce
http://arxiv.org/abs/1002.3333v1,"In this paper, we describe an effective framework for adapting electronic
commerce or e-commerce services in developing countries like Bangladesh. The
internet has opened up a new horizon for commerce, namely electronic commerce
(e-commerce). It entails the use of the internet in the marketing,
identification, payment and delivery of goods and services. At present internet
facilities are available in Bangladesh. Slowly, but steadily these facilities
are holding a strong position in every aspects of our life. E-commerce is one
of those sectors which need more attention if we want to be a part of global
business. Bangladesh is far-far away to adapt the main stream of e-commerce
application. Though government is shouting to take the challenges of
e-commerce, but they do not take the right step, that is why e-commerce dose
not make any real contribution in our socio-economic life. Here we propose a
model which may develop the e-commerce infrastructure of Bangladesh.",detecting e-commerce
http://arxiv.org/abs/1807.04923v1,"Millions of people use online e-commerce platforms to search and buy
products. Identifying attributes in a query is a critical component in
connecting users to relevant items. However, in many cases, the queries have
multiple attributes, and some of them will be in conflict with each other. For
example, the query ""maroon 5 dvds"" has two candidate attributes, the color
""maroon"" or the band ""maroon 5"", where only one of the attributes can be
present. In this paper, we address the problem of resolving conflicting
attributes in e-commerce queries. A challenge in this problem is that knowledge
bases like Wikipedia that are used to understand web queries are not focused on
the e-commerce domain. E-commerce search engines, however, have access to the
catalog which contains detailed information about the items and its attributes.
We propose a framework that constructs knowledge graphs from catalog to resolve
conflicting attributes in e-commerce queries. Our experiments on real-world
queries on e-commerce platforms demonstrate that resolving conflicting
attributes by leveraging catalog information significantly improves attribute
identification, and also gives out more relevant search results.",detecting e-commerce
http://arxiv.org/abs/1602.07662v1,"Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.",detecting e-commerce
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",detecting e-commerce
http://arxiv.org/abs/1804.03836v3,"Anomaly Detection has several important applications. In this paper, our
focus is on detecting anomalies in seller-reviewer data using tensor
decomposition. While tensor-decomposition is mostly unsupervised, we formulate
Bayesian semi-supervised tensor decomposition to take advantage of sparse
labeled data. In addition, we use Polya-Gamma data augmentation for the
semi-supervised Bayesian tensor decomposition. Finally, we show that the
P\'olya-Gamma formulation simplifies calculation of the Fisher information
matrix for partial natural gradient learning. Our experimental results show
that our semi-supervised approach outperforms state of the art unsupervised
baselines. And that the partial natural gradient learning outperforms
stochastic gradient learning and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detecting e-commerce
http://arxiv.org/abs/1905.06112v1,"Recently, Vietnamese Natural Language Processing has been researched by
experts in academic and business. However, the existing papers have been
focused only on information classification or extraction from documents.
Nowadays, with quickly development of the e-commerce websites, forums and
social networks, the products, people, organizations or wonders are targeted of
comments or reviews of the network communities. Many people often use that
reviews to make their decision on something. Whereas, there are many people or
organizations use the reviews to mislead readers. Therefore, it is so necessary
to detect those bad behaviors in reviews. In this paper, we research this
problem and propose an appropriate method for detecting Vietnamese reviews
being spam or non-spam. The accuracy of our method is up to 90%.",detecting e-commerce
http://arxiv.org/abs/1905.06246v2,"Product reviews and ratings on e-commerce websites provide customers with
detailed insights about various aspects of the product such as quality,
usefulness, etc. Since they influence customers' buying decisions, product
reviews have become a fertile ground for abuse by sellers (colluding with
reviewers) to promote their own products or to tarnish the reputation of
competitor's products. In this paper, our focus is on detecting such abusive
entities (both sellers and reviewers) by applying tensor decomposition on the
product reviews data. While tensor decomposition is mostly unsupervised, we
formulate our problem as a semi-supervised binary multi-target tensor
decomposition, to take advantage of currently known abusive entities. We
empirically show that our multi-target semi-supervised model achieves higher
precision and recall in detecting abusive entities as compared to unsupervised
techniques. Finally, we show that our proposed stochastic partial natural
gradient inference for our model empirically achieves faster convergence than
stochastic gradient and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",detecting e-commerce
http://arxiv.org/abs/1506.05752v3,"Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to ""shilling"" attacks or ""profile injection"" attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
""shilling"" attacks or ""profile injection"" attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN.",detecting e-commerce
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",e-commerce fraud
http://arxiv.org/abs/1709.01213v4,"Although mobile ad frauds have been widespread, state-of-the-art approaches
in the literature have mainly focused on detecting the so-called static
placement frauds, where only a single UI state is involved and can be
identified based on static information such as the size or location of ad
views. Other types of fraud exist that involve multiple UI states and are
performed dynamically while users interact with the app. Such dynamic
interaction frauds, although now widely spread in apps, have not yet been
explored nor addressed in the literature. In this work, we investigate a wide
range of mobile ad frauds to provide a comprehensive taxonomy to the research
community. We then propose, FraudDroid, a novel hybrid approach to detect ad
frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI
state transition graphs and collects their associated runtime network traffics,
which are then leveraged to check against a set of heuristic-based rules for
identifying ad fraudulent behaviours. We show empirically that FraudDroid
detects ad frauds with a high precision (93%) and recall (92%). Experimental
results further show that FraudDroid is capable of detecting ad frauds across
the spectrum of fraud types. By analysing 12,000 ad-supported Android apps,
FraudDroid identified 335 cases of fraud associated with 20 ad networks that
are further confirmed to be true positive results and are shared with our
fellow researchers to promote advanced ad fraud detection",e-commerce fraud
http://arxiv.org/abs/1909.02398v1,"Automated fraud behaviors detection on electronic payment platforms is a
tough problem. Fraud users often exploit the vulnerability of payment platforms
and the carelessness of users to defraud money, steal passwords, do money
laundering, etc, which causes enormous losses to digital payment platforms and
users. There are many challenges for fraud detection in practice. Traditional
fraud detection methods require a large-scale manually labeled dataset, which
is hard to obtain in reality. Manually labeled data cost tremendous human
efforts. Besides, the continuous and rapid evolution of fraud users makes it
hard to find new fraud patterns based on existing detection rules. In our work,
we propose a real-world data oriented detection paradigm which can detect fraud
users and upgrade its detection ability automatically. Based on the new
paradigm, we design a novel fraud detection model, FraudJudger, to analyze
users behaviors on digital payment platforms and detect fraud users with fewer
labeled data in training. FraudJudger can learn the latent representations of
users from unlabeled data with the help of Adversarial Autoencoder (AAE).
Furthermore, FraudJudger can find new fraud patterns from unknown users by
cluster analysis. Our experiment is based on a real-world electronic payment
dataset. Comparing with other well-known fraud detection methods, FraudJudger
can achieve better detection performance with only 10% labeled data.",e-commerce fraud
http://arxiv.org/abs/1309.3944v1,"With an upsurge in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection (FAFD) has become an
emerging topic of great importance for academic, research and industries. The
failure of internal auditing system of the organization in identifying the
accounting frauds has lead to use of specialized procedures to detect financial
accounting fraud, collective known as forensic accounting. Data mining
techniques are providing great aid in financial accounting fraud detection,
since dealing with the large data volumes and complexities of financial data
are big challenges for forensic accounting. This paper presents a comprehensive
review of the literature on the application of data mining techniques for the
detection of financial accounting fraud and proposes a framework for data
mining techniques based accounting fraud detection. The systematic and
comprehensive literature review of the data mining techniques applicable to
financial accounting fraud detection may provide a foundation to future
research in this field. The findings of this review show that data mining
techniques like logistic models, neural networks, Bayesian belief network, and
decision trees have been applied most extensively to provide primary solutions
to the problems inherent in the detection and classification of fraudulent
data.",e-commerce fraud
http://arxiv.org/abs/1907.03048v1,"Download fraud is a prevalent threat in mobile App markets, where fraudsters
manipulate the number of downloads of Apps via various cheating approaches.
Purchased fake downloads can mislead recommendation and search algorithms and
further lead to bad user experience in App markets. In this paper, we
investigate download fraud problem based on a company's App Market, which is
one of the most popular Android App markets. We release a honeypot App on the
App Market and purchase fake downloads from fraudster agents to track fraud
activities in the wild. Based on our interaction with the fraudsters, we
categorize download fraud activities into three types according to their
intentions: boosting front end downloads, optimizing App search ranking, and
enhancing user acquisition&retention rate. For the download fraud aimed at
optimizing App search ranking, we select, evaluate, and validate several
features in identifying fake downloads based on billions of download data. To
get a comprehensive understanding of download fraud, we further gather stances
of App marketers, fraudster agencies, and market operators on download fraud.
The followed analysis and suggestions shed light on the ways to mitigate
download fraud in App markets and other social platforms. To the best of our
knowledge, this is the first work that investigates the download fraud problem
in mobile App markets.",e-commerce fraud
http://arxiv.org/abs/1601.01228v1,"Financial fraud detection is an important problem with a number of design
aspects to consider. Issues such as algorithm selection and performance
analysis will affect the perceived ability of proposed solutions, so for
auditors and re-searchers to be able to sufficiently detect financial fraud it
is necessary that these issues be thoroughly explored. In this paper we will
revisit the key performance metrics used for financial fraud detection with a
focus on credit card fraud, critiquing the prevailing ideas and offering our
own understandings. There are many different performance metrics that have been
employed in prior financial fraud detection research. We will analyse several
of the popular metrics and compare their effectiveness at measuring the ability
of detection mechanisms. We further investigated the performance of a range of
computational intelligence techniques when applied to this problem domain, and
explored the efficacy of several binary classification methods.",e-commerce fraud
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",e-commerce fraud
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",e-commerce fraud
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",e-commerce fraud
http://arxiv.org/abs/0803.4058v3,"Typical arguments against scientific misconduct generally fail to support
current policies on research fraud: they may not prove wrong what is usually
considered research misconduct and they tend to make wrong things that are not
normally seen as scientific fraud, in particular honest errors. I also point
out that sanctions are not consistent with the reasons why scientific fraud is
supposed to be wrong either. Moreover honestly seeking truth should not be
contrived as a moral rule -- it is instead a necessary condition for work to
qualify as scientific.
  Keywords: cheating; ethics; fabrication; falsification; integrity;
plagiarism; research fraud; scientific misconduct.",e-commerce fraud
http://arxiv.org/abs/1510.07167v1,"Financial statement fraud detection is an important problem with a number of
design aspects to consider. Issues such as (i) problem representation, (ii)
feature selection, and (iii) choice of performance metrics all influence the
perceived performance of detection algorithms. Efficient implementation of
financial fraud detection methods relies on a clear understanding of these
issues. In this paper we present an analysis of the three key experimental
issues associated with financial statement fraud detection, critiquing the
prevailing ideas and providing new understandings.",e-commerce fraud
http://arxiv.org/abs/1808.07288v1,"Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.",e-commerce fraud
http://arxiv.org/abs/0910.2048v1,"This brief paper outlines how spreadsheets were used as one of the vehicles
for John Rusnak's fraud and the revenue control lessons this case gives us.",e-commerce fraud
http://arxiv.org/abs/1611.06439v1,"Credit card plays a very important rule in today's economy. It becomes an
unavoidable part of household, business and global activities. Although using
credit cards provides enormous benefits when used carefully and
responsibly,significant credit and financial damages may be caused by
fraudulent activities. Many techniques have been proposed to confront the
growth in credit card fraud. However, all of these techniques have the same
goal of avoiding the credit card fraud; each one has its own drawbacks,
advantages and characteristics. In this paper, after investigating difficulties
of credit card fraud detection, we seek to review the state of the art in
credit card fraud detection techniques, data sets and evaluation criteria.The
advantages and disadvantages of fraud detection methods are enumerated and
compared.Furthermore, a classification of mentioned techniques into two main
fraud detection approaches, namely, misuses (supervised) and anomaly detection
(unsupervised) is presented. Again, a classification of techniques is proposed
based on capability to process the numerical and categorical data sets.
Different data sets used in literature are then described and grouped into real
and synthesized data and the effective and common attributes are extracted for
further usage.Moreover, evaluation employed criterions in literature are
collected and discussed.Consequently, open issues for credit card fraud
detection are explained as guidelines for new researchers.",e-commerce fraud
http://arxiv.org/abs/1304.6501v1,"Occupational fraud affects many companies worldwide causing them economic
loss and liability issues towards their customers and other involved entities.
Detecting internal fraud in a company requires significant effort and,
unfortunately cannot be entirely prevented. The internal auditors have to
process a huge amount of data produced by diverse systems, which are in most
cases in textual form, with little automated support. In this paper, we exploit
the advantages of information visualization and present a system that aims to
detect occupational fraud in systems which involve a pair of entities (e.g., an
employee and a client) and periodic activity. The main visualization is based
on a spiral system on which the events are drawn appropriately according to
their time-stamp. Suspicious events are considered those which appear along the
same radius or on close radii of the spiral. Before producing the
visualization, the system ranks both involved entities according to the
specifications of the internal auditor and generates a video file of the
activity such that events with strong evidence of fraud appear first in the
video. The system is also equipped with several different visualizations and
mechanisms in order to meet the requirements of an internal fraud detection
system.",e-commerce fraud
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online law enforcement
http://arxiv.org/abs/1612.05030v1,"Synchronous programming is a paradigm of choice for the design of
safety-critical reactive systems. Runtime enforcement is a technique to ensure
that the output of a black-box system satisfies some desired properties. This
paper deals with the problem of runtime enforcement in the context of
synchronous programs. We propose a framework where an enforcer monitors both
the inputs and the outputs of a synchronous program and (minimally) edits
erroneous inputs/outputs in order to guarantee that a given property holds. We
define enforceability conditions, develop an online enforcement algorithm, and
prove its correctness. We also report on an implementation of the algorithm on
top of the KIELER framework for the SCCharts synchronous language. Experimental
results show that enforcement has minimal execution time overhead, which
decreases proportionally with larger benchmarks.",online law enforcement
http://arxiv.org/abs/1701.01911v2,"Exemplar-based face sketch synthesis plays an important role in both digital
entertainment and law enforcement. It generally consists of two parts: neighbor
selection and reconstruction weight representation. The most time-consuming or
main computation complexity for exemplar-based face sketch synthesis methods
lies in the neighbor selection process. State-of-the-art face sketch synthesis
methods perform neighbor selection online in a data-driven manner by $K$
nearest neighbor ($K$-NN) searching. Actually, the online search increases the
time consuming for synthesis. Moreover, since these methods need to traverse
the whole training dataset for neighbor selection, the computational complexity
increases with the scale of the training database and hence these methods have
limited scalability. In this paper, we proposed a simple but effective offline
random sampling in place of online $K$-NN search to improve the synthesis
efficiency. Extensive experiments on public face sketch databases demonstrate
the superiority of the proposed method in comparison to state-of-the-art
methods, in terms of both synthesis quality and time consumption. The proposed
method could be extended to other heterogeneous face image transformation
problems such as face hallucination. We release the source codes of our
proposed methods and the evaluation metrics for future study online:
http://www.ihitworld.com/RSLCR.html.",online law enforcement
http://arxiv.org/abs/1509.06659v3,"Human trafficking is a challenging law enforcement problem, and a large
amount of such activity manifests itself on various online forums. Given the
large, heterogeneous and noisy structure of this data, building models to
predict instances of trafficking is an even more convolved a task. In this
paper we propose and entity resolution pipeline using a notion of proxy labels,
in order to extract clusters from this data with prior history of human
trafficking activity. We apply this pipeline to 5M records from backpage.com
and report on the performance of this approach, challenges in terms of
scalability, and some significant domain specific characteristics of our
resolved entities.",online law enforcement
http://arxiv.org/abs/1902.06961v1,"Cybercrime investigators face numerous challenges when policing online
crimes. Firstly, the methods and processes they use when dealing with
traditional crimes do not necessarily apply in the cyber-world. Additionally,
cyber criminals are usually technologically-aware and constantly adapting and
developing new tools that allow them to stay ahead of law enforcement
investigations. In order to provide adequate support for cybercrime
investigators, there needs to be a better understanding of the challenges they
face at both technical and socio-technical levels. In this paper, we
investigate this problem through an analysis of current practices and workflows
of investigators. We use interviews with experts from government and private
sectors who investigate cybercrimes as our main data gathering process. From an
analysis of the collected data, we identify several outstanding challenges
faced by investigators. These pertain to practical, technical, and social
issues such as systems availability, usability, and in computer-supported
collaborative work. Importantly, we use our findings to highlight research
areas where user-centric workflows and tools are desirable. We also define a
set of recommendations that can aid in providing a better foundation for future
research in the field and allow more effective combating of cybercrimes.",online law enforcement
http://arxiv.org/abs/1404.1295v1,"The study of criminal networks using traces from heterogeneous communication
media is acquiring increasing importance in nowadays society. The usage of
communication media such as phone calls and online social networks leaves
digital traces in the form of metadata that can be used for this type of
analysis. The goal of this work is twofold: first we provide a theoretical
framework for the problem of detecting and characterizing criminal
organizations in networks reconstructed from phone call records. Then, we
introduce an expert system to support law enforcement agencies in the task of
unveiling the underlying structure of criminal networks hidden in communication
data. This platform allows for statistical network analysis, community
detection and visual exploration of mobile phone network data. It allows
forensic investigators to deeply understand hierarchies within criminal
organizations, discovering members who play central role and provide connection
among sub-groups. Our work concludes illustrating the adoption of our
computational framework for a real-word criminal investigation.",online law enforcement
http://arxiv.org/abs/1603.07823v1,"Face sketch synthesis has wide applications ranging from digital
entertainments to law enforcements. Objective image quality assessment scores
and face recognition accuracy are two mainly used tools to evaluate the
synthesis performance. In this paper, we proposed a synthesized face sketch
recognition framework based on full-reference image quality assessment metrics.
Synthesized sketches generated from four state-of-the-art methods are utilized
to test the performance of the proposed recognition framework. For the image
quality assessment metrics, we employed the classical structured similarity
index metric and other three prevalent metrics: visual information fidelity,
feature similarity index metric and gradient magnitude similarity deviation.
Extensive experiments compared with baseline methods illustrate the
effectiveness of the proposed synthesized face sketch recognition framework.
Data and implementation code in this paper are available online at
www.ihitworld.com/WNN/IQA_Sketch.zip.",online law enforcement
http://arxiv.org/abs/1712.03086v1,"In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.",online law enforcement
http://arxiv.org/abs/1801.07207v1,"Security incidents such as targeted distributed denial of service (DDoS)
attacks on power grids and hacking of factory industrial control systems (ICS)
are on the increase. This paper unpacks where emerging security risks lie for
the industrial internet of things, drawing on both technical and regulatory
perspectives. Legal changes are being ushered by the European Union (EU)
Network and Information Security (NIS) Directive 2016 and the General Data
Protection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use
the case study of the emergent smart energy supply chain to frame, scope out
and consolidate the breadth of security concerns at play, and the regulatory
responses. We argue the industrial IoT brings four security concerns to the
fore, namely: appreciating the shift from offline to online infrastructure;
managing temporal dimensions of security; addressing the implementation gap for
best practice; and engaging with infrastructural complexity. Our goal is to
surface risks and foster dialogue to avoid the emergence of an Internet of
Insecure Industrial Things",online law enforcement
http://arxiv.org/abs/1810.03965v1,"We present an algorithm for realtime anomaly detection in low to medium
density crowd videos using trajectory-level behavior learning. Our formulation
combines online tracking algorithms from computer vision, non-linear pedestrian
motion models from crowd simulation, and Bayesian learning techniques to
automatically compute the trajectory-level pedestrian behaviors for each agent
in the video. These learned behaviors are used to segment the trajectories and
motions of different pedestrians or agents and detect anomalies. We demonstrate
the interactive performance on the PETS ARENA dataset as well as indoor and
outdoor crowd video benchmarks consisting of tens of human agents. We also
discuss the implications of recent public policy and law enforcement issues
relating to surveillance and our research.",online law enforcement
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",online law enforcement
http://arxiv.org/abs/1909.00912v2,"Neural networks can emulate non-linear physical systems with high accuracy,
yet they may produce physically-inconsistent results when violating fundamental
constraints. In this letter, we introduce a systematic way of enforcing
analytic constraints in neural networks via constraints in the architecture or
the loss function. Applied to the modeling of convective processes for climate
modeling, architectural constraints can enforce conservation laws to within
machine precision without degrading performance. Furthermore, enforcing
constraints can reduce the error of variables closely related to the
constraints.",online law enforcement
http://arxiv.org/abs/1801.04565v1,"Data retrieval systems such as online search engines and online social
networks must comply with the privacy policies of personal and selectively
shared data items, regulatory policies regarding data retention and censorship,
and the provider's own policies regarding data use. Enforcing these policies is
difficult and error-prone. Systematic techniques to enforce policies are either
limited to type-based policies that apply uniformly to all data of the same
type, or incur significant runtime overhead.
  This paper presents Shai, the first system that systematically enforces
data-specific policies with near-zero overhead in the common case. Shai's key
idea is to push as many policy checks as possible to an offline, ahead-of-time
analysis phase, often relying on predicted values of runtime parameters such as
the state of access control lists or connected users' attributes. Runtime
interception is used sparingly, only to verify these predictions and to make
any remaining policy checks. Our prototype implementation relies on efficient,
modern OS primitives for sandboxing and isolation. We present the design of
Shai and quantify its overheads on an experimental data indexing and search
pipeline based on the popular search engine Apache Lucene.",online law enforcement
http://arxiv.org/abs/1302.3946v1,"We consider the classical online scheduling problem P||C_{max} in which jobs
are released over list and provide a nearly optimal online algorithm. More
precisely, an online algorithm whose competitive ratio is at most (1+\epsilon)
times that of an optimal online algorithm could be achieved in polynomial time,
where m, the number of machines, is a part of the input. It substantially
improves upon the previous results by almost closing the gap between the
currently best known lower bound of 1.88 (Rudin, Ph.D thesis, 2001) and the
best known upper bound of 1.92 (Fleischer, Wahl, Journal of Scheduling, 2000).
It has been known by folklore that an online problem could be viewed as a game
between an adversary and the online player. Our approach extensively explores
such a structure and builds up a completely new framework to show that, for the
online over list scheduling problem, given any \epsilon>0, there exists a
uniform threshold K which is polynomial in m such that if the competitive ratio
of an online algorithm is \rho<=2, then there exists a list of at most K jobs
to enforce the online algorithm to achieve a competitive ratio of at least
\rho-O(\epsilon). Our approach is substantially different from that of Gunther
et al. (Gunther et al., SODA 2013), in which an approximation scheme for online
over time scheduling problems is given, where the number of machines is fixed.
Our method could also be extended to several related online over list
scheduling models.",online law enforcement
http://arxiv.org/abs/1705.04480v1,"While online services emerge in all areas of life, the voting procedure in
many democracies remains paper-based as the security of current online voting
technology is highly disputed. We address the issue of trustworthy online
voting protocols and recall therefore their security concepts with its trust
assumptions. Inspired by the Bitcoin protocol, the prospects of distributed
online voting protocols are analysed. No trusted authority is assumed to ensure
ballot secrecy. Further, the integrity of the voting is enforced by all voters
themselves and without a weakest link, the protocol becomes more robust. We
introduce a taxonomy of notions of distribution in online voting protocols that
we apply on selected online voting protocols. Accordingly, blockchain-based
protocols seem to be promising for online voting due to their similarity with
paper-based protocols.",online law enforcement
http://arxiv.org/abs/1909.09754v1,"This work proposes an approach for latent dynamics learning that exactly
enforces physical conservation laws. The method comprises two steps. First, we
compute a low-dimensional embedding of the high-dimensional dynamical-system
state using deep convolutional autoencoders. This defines a low-dimensional
nonlinear manifold on which the state is subsequently enforced to evolve.
Second, we define a latent dynamics model that associates with a constrained
optimization problem. Specifically, the objective function is defined as the
sum of squares of conservation-law violations over control volumes in a
finite-volume discretization of the problem; nonlinear equality constraints
explicitly enforce conservation over prescribed subdomains of the problem. The
resulting dynamics model-which can be considered as a projection-based
reduced-order model-ensures that the time-evolution of the latent state exactly
satisfies conservation laws over the prescribed subdomains. In contrast to
existing methods for latent dynamics learning, this is the only method that
both employs a nonlinear embedding and computes dynamics for the latent state
that guarantee the satisfaction of prescribed physical properties. Numerical
experiments on a benchmark advection problem illustrate the method's ability to
significantly reduce the dimensionality while enforcing physical conservation.",online law enforcement
http://arxiv.org/abs/1609.07602v1,"With an exponentially increasing usage of cloud services, the need for
forensic investigations of virtual space is equally in constantly increasing
demand, which includes as a very first approach, the gaining of access to it as
well as the data stored. This is an aspect that faces a number of challenges,
stemming not only from the technical difficulties and peculiarities, but
equally covers the interaction with an emerging line of businesses offering
cloud storage and services. Beyond the forensic aspects, it also covers to an
ever increasing amount the non-forensic considerations, such as the
availability of logs and archives, legal and data protection considerations
from a global perspective and the clashes in between, as well as the ever
competing interests between law enforcement to seize evidence which is
non-physical, and businesses who need to be able to continue to operate and
provide their hosted services, even if law enforcement seek to collect
evidence. The trend post-Snowden has been unequivocally towards default
encryption, and driven by market leaders such as Apple, motivated to a large
extent by the perceived demands for privacy of the consumer. The central
question to be explored in this paper is to what extent this trend towards
default encryption will have a negative impact on law enforcement
investigations and possibilities, and will at the end attempt to provide a
solution, which takes into account the needs of both law enforcement, but also
of the service providers. It is hoped that the recommendations from this paper
will be able to have an impact in the ability for law enforcement to continue
with their investigations in an efficient manner, whilst also safeguarding the
ability for business to thrive and continue to develop and offer new and
innovative solutions, which do not put law enforcement at risk.",online law enforcement
http://arxiv.org/abs/1401.5178v1,"The economics of an internet crime has newly developed into a field of
controlling black money. This economic approach not only provides estimated
technique of analyzing internet crimes but also gives details to analyzers of
system dependability and divergence. This paper will highlight on the subject
of online crime, which has formed its industry since. It all started from
amateur hackers who cracked websites and wrote malicious software in pursuit of
fun or achieving limited objectives to professional hacking. In the past days,
electronic fraud was main objective but now it has been changed into electronic
hacking. This study focuses the issue through an economic analysis of available
web forum to deals in malware and private information. The findings of this
survey research provide considerable in-depth sight into the functions of
malware economy spinning around computer impositions and compromise. In this
regard, the survey research paper may benefit particularly computer security
officials, the law enforcement agencies, and in general prospective anyone
involved in better understanding cybercrime from the offender standpoint.",online law enforcement
http://arxiv.org/abs/1403.6315v2,"The spread of rumors through social media and online social networks can not
only disrupt the daily lives of citizens but also result in loss of life and
property. A rumor spreads when individuals, who are unable decide the
authenticity of the information, mistake the rumor as genuine information and
pass it on to their acquaintances. We propose a solution where a set of
individuals (based on their degree) in the social network are trained and
provided resources to help them distinguish a rumor from genuine information.
By formulating an optimization problem we calculate the optimum set of
individuals, who must undergo training, and the quality of training that
minimizes the expected training cost and ensures an upper bound on the size of
the rumor outbreak. Our primary contribution is that although the optimization
problem turns out to be non convex, we show that the problem is equivalent to
solving a set of linear programs. This result also allows us to solve the
problem of minimizing the size of rumor outbreak for a given cost budget. The
optimum solution displays an interesting pattern which can be implemented as a
heuristic. These results can prove to be very useful for social planners and
law enforcement agencies for preventing dangerous rumors and misinformation
epidemics.",online law enforcement
http://arxiv.org/abs/1712.00846v1,"Web-based human trafficking activity has increased in recent years but it
remains sparsely dispersed among escort advertisements and difficult to
identify due to its often-latent nature. The use of intelligent systems to
detect trafficking can thus have a direct impact on investigative resource
allocation and decision-making, and, more broadly, help curb a widespread
social problem. Trafficking detection involves assigning a normalized score to
a set of escort advertisements crawled from the Web -- a higher score indicates
a greater risk of trafficking-related (involuntary) activities. In this paper,
we define and study the problem of trafficking detection and present a
trafficking detection pipeline architecture developed over three years of
research within the DARPA Memex program. Drawing on multi-institutional data,
systems, and experiences collected during this time, we also conduct post hoc
bias analyses and present a bias mitigation plan. Our findings show that, while
automatic trafficking detection is an important application of AI for social
good, it also provides cautionary lessons for deploying predictive machine
learning algorithms without appropriate de-biasing. This ultimately led to
integration of an interpretable solution into a search system that contains
over 100 million advertisements and is used by over 200 law enforcement
agencies to investigate leads.",online law enforcement
http://arxiv.org/abs/1504.01093v1,"We consider dynamic pricing schemes in online settings where selfish agents
generate online events. Previous work on online mechanisms has dealt almost
entirely with the goal of maximizing social welfare or revenue in an auction
settings. This paper deals with quite general settings and minimizing social
costs. We show that appropriately computed posted prices allow one to achieve
essentially the same performance as the best online algorithm. This holds in a
wide variety of settings. Unlike online algorithms that learn about the event,
and then make enforceable decisions, prices are posted without knowing the
future events or even the current event, and are thus inherently dominant
strategy incentive compatible.
  In particular we show that one can give efficient posted price mechanisms for
metrical task systems, some instances of the $k$-server problem, and metrical
matching problems. We give both deterministic and randomized algorithms. Such
posted price mechanisms decrease the social cost dramatically over selfish
behavior where no decision incurs a charge. One alluring application of this is
reducing the social cost of free parking exponentially.",online law enforcement
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",online law enforcement
http://arxiv.org/abs/1703.10764v1,"Global optimization algorithms have shown impressive performance in
data-association based multi-object tracking, but handling online data remains
a difficult hurdle to overcome. In this paper, we present a hybrid data
association framework with a min-cost multi-commodity network flow for robust
online multi-object tracking. We build local target-specific models interleaved
with global optimization of the optimal data association over multiple video
frames. More specifically, in the min-cost multi-commodity network flow, the
target-specific similarities are online learned to enforce the local
consistency for reducing the complexity of the global data association.
Meanwhile, the global data association taking multiple video frames into
account alleviates irrecoverable errors caused by the local data association
between adjacent frames. To ensure the efficiency of online tracking, we give
an efficient near-optimal solution to the proposed min-cost multi-commodity
flow problem, and provide the empirical proof of its sub-optimality. The
comprehensive experiments on real data demonstrate the superior tracking
performance of our approach in various challenging situations.",online law enforcement
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",online shopping fraud
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",online shopping fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping fraud
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping fraud
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping fraud
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",online shopping fraud
http://arxiv.org/abs/1906.06977v1,"Machine learning and data mining techniques have been used extensively in
order to detect credit card frauds. However purchase behaviour and fraudster
strategies may change over time. This phenomenon is named dataset shift or
concept drift in the domain of fraud detection. In this paper, we present a
method to quantify day-by-day the dataset shift in our face-to-face credit card
transactions dataset (card holder located in the shop) . In practice, we
classify the days against each other and measure the efficiency of the
classification. The more efficient the classification, the more different the
buying behaviour between two days, and vice versa. Therefore, we obtain a
distance matrix characterizing the dataset shift. After an agglomerative
clustering of the distance matrix, we observe that the dataset shift pattern
matches the calendar events for this time period (holidays, week-ends, etc). We
then incorporate this dataset shift knowledge in the credit card fraud
detection task as a new feature. This leads to a small improvement of the
detection.",online shopping fraud
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",online shopping fraud
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping fraud
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",online shopping fraud
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping fraud
http://arxiv.org/abs/0801.2700v1,"Labels and tags are accompanying us in almost each moment of our life and
everywhere we are going, in the form of electronic keys or money, or simply as
labels on products we are buying in shops and markets. The label diffusion,
rapidly increasing for logistic reasons in the actual global market, carries
huge amount of information but it is demanding security and anti-fraud systems.
The first crucial point, for the consumer and producer safety, is to ensure the
authenticity of the labelled products with systems against counterfeiting and
piracy. Recent anti-fraud techniques are based on a sophisticated use of
physical effects, from holograms till magnetic resonance or tunnel transitions
between atomic sublevels. In this paper we will discuss labels and anti-fraud
technologies as a new and very promising research field for applied physics.",online shopping fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",online shopping fraud
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",online shopping fraud
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",online shopping fraud
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping fraud
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",online shopping fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",online shopping fraud
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",online shopping fraud
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",online shopping fraud
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",online shopping fraud
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping fraud
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping fraud
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",online shopping fraud
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping law enforcement
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping law enforcement
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping law enforcement
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping law enforcement
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online shopping law enforcement
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping law enforcement
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping law enforcement
http://arxiv.org/abs/1807.05381v1,"Physical retailers, who once led the way in tracking with loyalty cards and
`reverse appends', now lag behind online competitors. Yet we might be seeing
these tables turn, as many increasingly deploy technologies ranging from simple
sensors to advanced emotion detection systems, even enabling them to tailor
prices and shopping experiences on a per-customer basis. Here, we examine these
in-store tracking technologies in the retail context, and evaluate them from
both technical and regulatory standpoints. We first introduce the relevant
technologies in context, before considering privacy impacts, the current
remedies individuals might seek through technology and the law, and those
remedies' limitations. To illustrate challenging tensions in this space we
consider the feasibility of technical and legal approaches to both a) the
recent `Go' store concept from Amazon which requires fine-grained, multi-modal
tracking to function as a shop, and b) current challenges in opting in or out
of increasingly pervasive passive Wi-Fi tracking. The `Go' store presents
significant challenges with its legality in Europe significantly unclear and
unilateral, technical measures to avoid biometric tracking likely ineffective.
In the case of MAC addresses, we see a difficult-to-reconcile clash between
privacy-as-confidentiality and privacy-as-control, and suggest a technical
framework which might help balance the two. Significant challenges exist when
seeking to balance personalisation with privacy, and researchers must work
together, including across the boundaries of preferred privacy definitions, to
come up with solutions that draw on both technology and the legal frameworks to
provide effective and proportionate protection. Retailers, simultaneously, must
ensure that their tracking is not just legal, but worthy of the trust of
concerned data subjects.",online shopping law enforcement
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping law enforcement
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online shopping law enforcement
http://arxiv.org/abs/1505.07922v1,"We address the problem of cross-domain image retrieval, considering the
following practical application: given a user photo depicting a clothing image,
our goal is to retrieve the same or attribute-similar clothing items from
online shopping stores. This is a challenging problem due to the large
discrepancy between online shopping images, usually taken in ideal
lighting/pose/background conditions, and user photos captured in uncontrolled
conditions. To address this problem, we propose a Dual Attribute-aware Ranking
Network (DARN) for retrieval feature learning. More specifically, DARN consists
of two sub-networks, one for each domain, whose retrieval feature
representations are driven by semantic attribute learning. We show that this
attribute-guided learning is a key factor for retrieval accuracy improvement.
In addition, to further align with the nature of the retrieval problem, we
impose a triplet visual similarity constraint for learning to rank across the
two sub-networks. Another contribution of our work is a large-scale dataset
which makes the network learning feasible. We exploit customer review websites
to crawl a large set of online shopping images and corresponding offline user
photos with fine-grained clothing attributes, i.e., around 450,000 online
shopping images and about 90,000 exact offline counterpart images of those
online ones. All these images are collected from real-world consumer websites
reflecting the diversity of the data modality, which makes this dataset unique
and rare in the academic community. We extensively evaluate the retrieval
performance of networks in different configurations. The top-20 retrieval
accuracy is doubled when using the proposed DARN other than the current popular
solution using pre-trained CNN features only (0.570 vs. 0.268).",online shopping law enforcement
http://arxiv.org/abs/1811.02385v1,"The ability to correctly classify and retrieve apparel images has a variety
of applications important to e-commerce, online advertising and internet
search. In this work, we propose a robust framework for fine-grained apparel
classification, in-shop and cross-domain retrieval which eliminates the
requirement of rich annotations like bounding boxes and human-joints or
clothing landmarks, and training of bounding box/ key-landmark detector for the
same. Factors such as subtle appearance differences, variations in human poses,
different shooting angles, apparel deformations, and self-occlusion add to the
challenges in classification and retrieval of apparel items. Cross-domain
retrieval is even harder due to the presence of large variation between online
shopping images, usually taken in ideal lighting, pose, positive angle and
clean background as compared with street photos captured by users in
complicated conditions with poor lighting and cluttered scenes. Our framework
uses compact bilinear CNN with tensor sketch algorithm to generate embeddings
that capture local pairwise feature interactions in a translationally invariant
manner. For apparel classification, we pass the feature embeddings through a
softmax classifier, while, the in-shop and cross-domain retrieval pipelines use
a triplet-loss based optimization approach, such that squared Euclidean
distance between embeddings measures the dissimilarity between the images.
Unlike previous works that relied on bounding box, key clothing landmarks or
human joint detectors to assist the final deep classifier, proposed framework
can be trained directly on the provided category labels or generated triplets
for triplet loss optimization. Lastly, Experimental results on the DeepFashion
fine-grained categorization, and in-shop and consumer-to-shop retrieval
datasets provide a comparative analysis with previous work performed in the
domain.",online shopping law enforcement
http://arxiv.org/abs/1901.04140v1,"In the current field of computer vision, automatically generating texts from
given images has been a fully worked technique. Up till now, most works of this
area focus on image content describing, namely image-captioning. However, rare
researches focus on generating product review texts, which is ubiquitous in the
online shopping malls and is crucial for online shopping selection and
evaluation. Different from content describing, review texts include more
subjective information of customers, which may bring difference to the results.
Therefore, we aimed at a new field concerning generating review text from
customers based on images together with the ratings of online shopping
products, which appear as non-image attributes. We made several adjustments to
the existing image-captioning model to fit our task, in which we should also
take non-image features into consideration. We also did experiments based on
our model and get effective primary results.",online shopping law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online shopping law enforcement
http://arxiv.org/abs/1812.07143v1,"Shopping is difficult for people with motor impairments. This includes online
shopping. Proprietary software can emulate mouse and keyboard via head
tracking. However, such a solution is not common for smartphones. Unlike
desktop and laptop computers, they are also much easier to carry indoors and
outdoors.To address this, we implement and open source button that is sensitive
to head movements tracked from the front camera of iPhone X. This allows
developers to integrate in eCommerce applications easily without requiring
specialized knowledge. Other applications include gaming and use in hands-free
situations such as during cooking, auto-repair. We built a sample online
shopping application that allows users to easily browse between items from
various categories and take relevant action just by head movements. We present
results of user studies on this sample application and also include sensitivity
studies based on two independent tests performed at 3 different distances to
the screen.",online shopping law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online shopping law enforcement
http://arxiv.org/abs/1612.01603v1,"In this paper, we propose a SaaS service which prevents shoplifting using
image analysis and ERP. In Japan, total damage of shoplifting reaches 450
billion yen and more than 1000 small shops gave up their businesses because of
shoplifting. Based on recent cloud technology and data analysis technology, we
propose a shoplifting prevention service with image analysis of security camera
and ERP data check for small shops. We evaluated stream analysis of security
camera movie using online machine learining framework Jubatus.",online shopping law enforcement
http://arxiv.org/abs/1703.07371v1,"Today, huge amount of data is available on the web. Now there is a need to
convert that data in knowledge which can be useful for different purposes. This
paper depicts the use of data mining process, OLAP with the combination of
multi agent system to find the knowledge from data in cloud computing. For
this, I am also trying to explain one case study of online shopping of one
Bakery Shop. May be we can increase the sale of items by using the model, which
I am trying to represent.",online shopping law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online shopping law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online shopping law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online shopping law enforcement
http://arxiv.org/abs/1804.05287v2,"In recent years, both online retail and video hosting service are
exponentially growing. In this paper, we explore a new cross-domain task,
Video2Shop, targeting for matching clothes appeared in videos to the exact same
items in online shops. A novel deep neural network, called AsymNet, is proposed
to explore this problem. For the image side, well-established methods are used
to detect and extract features for clothing patches with arbitrary sizes. For
the video side, deep visual features are extracted from detected object regions
in each frame, and further fed into a Long Short-Term Memory (LSTM) framework
for sequence modeling, which captures the temporal dynamics in videos. To
conduct exact matching between videos and online shopping images, LSTM hidden
states, representing the video, and image features, which represent static
object images, are jointly modeled under the similarity network with
reconfigurable deep tree structure. Moreover, an approximate training method is
proposed to achieve the efficiency when training. Extensive experiments
conducted on a large cross-domain dataset have demonstrated the effectiveness
and efficiency of the proposed AsymNet, which outperforms the state-of-the-art
methods.",online shopping law enforcement
http://arxiv.org/abs/1610.05562v1,"An important goal of online comparison shopping services is to ""convert"" a
viewer from general product category pages (for example product groups such as
""smartphones"" or ""air-conditioners"") to detailed product pages and ultimately
to order pages. Comparison shopping websites provide a familiar web interface
as well as a chance for consumers to purchase items at competitive prices. In
return for providing access to a large market of potential consumers, the
comparison shopping service usually receives financial compensation for product
clicks and orders. This study looked at 2.5 million product listing visits at
price.com.hk to determine whether a modification in the way prices are
displayed on general category pages resulted in more ""conversions"" to product
detail pages. We found a statistically significant improvement over-all as a
result of the new price display resulting in 3.6% more product clicks over all
categories. Additional analysis showed that the effect is heterogeneous among
different categories, and in a few cases there may be some categories
negatively affected by the display modification.",online shopping law enforcement
http://arxiv.org/abs/1806.11423v1,"While shopping for fashion products, customers usually prefer to try-out
products to examine fit, material, overall look and feel. Due to lack of try
out options during online shopping, it becomes pivotal to provide customers
with as much of this information as possible to enhance their shopping
experience. Also it becomes essential to provide same experience for new
customers. Our work here focuses on providing a production ready size
recommendation system for shoes and address the challenge of providing
recommendation for users with no previous purchases on the platform. In our
work, we present a probabilistic approach based on user co-purchase data
facilitated by generating a brand-brand relationship graph. Specifically we
address two challenges that are commonly faced while implementing such
solution. 1. Sparse signals for less popular or new products in the system 2.
Extending the solution for new users. Further we compare and contrast this
approach with our previous work and show significant improvement both in
recommendation precision and coverage.",online shopping law enforcement
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam
http://arxiv.org/abs/1604.03627v1,"Social botnets have become an important phenomenon on social media. There are
many ways in which social bots can disrupt or influence online discourse, such
as, spam hashtags, scam twitter users, and astroturfing. In this paper we
considered one specific social botnet in Twitter to understand how it grows
over time, how the content of tweets by the social botnet differ from regular
users in the same dataset, and lastly, how the social botnet may have
influenced the relevant discussions. Our analysis is based on a qualitative
coding for approximately 3000 tweets in Arabic and English from the Syrian
social bot that was active for 35 weeks on Twitter before it was shutdown. We
find that the growth, behavior and content of this particular botnet did not
specifically align with common conceptions of botnets. Further we identify
interesting aspects of the botnet that distinguish it from regular users.",scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam monitoring
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam monitoring
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam monitoring
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam monitoring
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam monitoring
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam monitoring
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam monitoring
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam monitoring
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam monitoring
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam monitoring
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam monitoring
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam monitoring
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam monitoring
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam monitoring
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam monitoring
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam monitoring
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam monitoring
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam monitoring
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam monitoring
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam monitoring
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",scam monitoring
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam monitoring
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam monitoring
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam monitoring
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam monitoring
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam monitoring
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam monitoring
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam detection
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam detection
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam detection
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam detection
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam detection
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam detection
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam detection
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam detection
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam detection
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam detection
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam detection
http://arxiv.org/abs/1808.06362v1,"Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.",scam detection
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",scam detection
http://arxiv.org/abs/1701.07179v3,"Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.",scam detection
http://arxiv.org/abs/1501.00802v1,"Online Social Networks (OSNs) witness a rise in user activity whenever an
event takes place. Malicious entities exploit this spur in user-engagement
levels to spread malicious content that compromises system reputation and
degrades user experience. It also generates revenue from advertisements,
clicks, etc. for the malicious entities. Facebook, the world's biggest social
network, is no exception and has recently been reported to face much abuse
through scams and other type of malicious content, especially during news
making events. Recent studies have reported that spammers earn $200 million
just by posting malicious links on Facebook. In this paper, we characterize
malicious content posted on Facebook during 17 events, and discover that
existing efforts to counter malicious content by Facebook are not able to stop
all malicious content from entering the social graph. Our findings revealed
that malicious entities tend to post content through web and third party
applications while legitimate entities prefer mobile platforms to post content.
In addition, we discovered a substantial amount of malicious content generated
by Facebook pages. Through our observations, we propose an extensive feature
set based on entity profile, textual content, metadata, and URL features to
identify malicious content on Facebook in real time and at zero-hour. This
feature set was used to train multiple machine learning models and achieved an
accuracy of 86.9%. The intent is to catch malicious content that is currently
evading Facebook's detection techniques. Our machine learning model was able to
detect more than double the number of malicious posts as compared to existing
malicious content detection techniques. Finally, we built a real world solution
in the form of a REST based API and a browser plug-in to identify malicious
Facebook posts in real time.",scam detection
http://arxiv.org/abs/1405.1511v1,"Existence of spam URLs over emails and Online Social Media (OSM) has become a
growing phenomenon. To counter the dissemination issues associated with long
complex URLs in emails and character limit imposed on various OSM (like
Twitter), the concept of URL shortening gained a lot of traction. URL
shorteners take as input a long URL and give a short URL with the same landing
page in return. With its immense popularity over time, it has become a prime
target for the attackers giving them an advantage to conceal malicious content.
Bitly, a leading service in this domain is being exploited heavily to carry out
phishing attacks, work from home scams, pornographic content propagation, etc.
This imposes additional performance pressure on Bitly and other URL shorteners
to be able to detect and take a timely action against the illegitimate content.
In this study, we analyzed a dataset marked as suspicious by Bitly in the month
of October 2013 to highlight some ground issues in their spam detection
mechanism. In addition, we identified some short URL based features and coupled
them with two domain specific features to classify a Bitly URL as malicious /
benign and achieved a maximum accuracy of 86.41%. To the best of our knowledge,
this is the first large scale study to highlight the issues with Bitly's spam
detection policies and proposing a suitable countermeasure.",scam detection
http://arxiv.org/abs/1908.07087v2,"Given the reach of web platforms, bad actors have considerable incentives to
manipulate and defraud users at the expense of platform integrity. This has
spurred research in numerous suspicious behavior detection tasks, including
detection of sybil accounts, false information, and payment scams/fraud. In
this paper, we draw the insight that many such initiatives can be tackled in a
common framework by posing a detection task which seeks to find groups of
entities which share too many properties with one another across multiple
attributes (sybil accounts created at the same time and location, propaganda
spreaders broadcasting articles with the same rhetoric and with similar
reshares, etc.) Our work makes four core contributions: Firstly, we posit a
novel formulation of this task as a multi-view graph mining problem, in which
distinct views reflect distinct attribute similarities across entities, and
contextual similarity and attribute importance are respected. Secondly, we
propose a novel suspiciousness metric for scoring entity groups given the
abnormality of their synchronicity across multiple views, which obeys intuitive
desiderata that existing metrics do not. Finally, we propose the SliceNDice
algorithm which enables efficient extraction of highly suspicious entity
groups, and demonstrate its practicality in production, in terms of strong
detection performance and discoveries on Snapchat's large advertiser ecosystem
(89% precision and numerous discoveries of real fraud rings), marked
outperformance of baselines (over 97% precision/recall in simulated settings)
and linear scalability.",scam detection
http://arxiv.org/abs/1804.00451v1,"Cybercriminals abuse Online Social Networks (OSNs) to lure victims into a
variety of spam. Among different spam types, a less explored area is OSN abuse
that leverages the telephony channel to defraud users. Phone numbers are
advertized via OSNs, and users are tricked into calling these numbers. To
expand the reach of such scam / spam campaigns, phone numbers are advertised
across multiple platforms like Facebook, Twitter, GooglePlus, Flickr, and
YouTube. In this paper, we present the first data-driven characterization of
cross-platform campaigns that use multiple OSN platforms to reach their victims
and use phone numbers for monetization.
  We collect 23M posts containing 1.8M unique phone numbers from Twitter,
Facebook, GooglePlus, Youtube, and Flickr over a period of six months.
Clustering these posts helps us identify 202 campaigns operating across the
globe with Indonesia, United States, India, and United Arab Emirates being the
most prominent originators. We find that even though Indonesian campaigns
generate highest volume (3.2M posts), only 1.6% of the accounts propagating
Indonesian campaigns have been suspended so far. By examining campaigns running
across multiple OSNs, we discover that Twitter detects and suspends 93% more
accounts than Facebook. Therefore, sharing intelligence about abuse-related
user accounts across OSNs can aid in spam detection. According to our dataset,
around 35K victims and 8.8M USD could have been saved if intelligence was
shared across the OSNs. By analyzing phone number based spam campaigns running
on OSNs, we highlight the unexplored variety of phone-based attacks surfacing
on OSNs.",scam detection
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",scam detection
http://arxiv.org/abs/1301.6899v1,"With the advent of online social media, phishers have started using social
networks like Twitter, Facebook, and Foursquare to spread phishing scams.
Twitter is an immensely popular micro-blogging network where people post short
messages of 140 characters called tweets. It has over 100 million active users
who post about 200 million tweets everyday. Phishers have started using Twitter
as a medium to spread phishing because of this vast information dissemination.
Further, it is difficult to detect phishing on Twitter unlike emails because of
the quick spread of phishing links in the network, short size of the content,
and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects
phishing on Twitter in realtime. We use Twitter specific features along with
URL features to detect whether a tweet posted with a URL is phishing or not.
Some of the Twitter specific features we use are tweet content and its
characteristics like length, hashtags, and mentions. Other Twitter features
used are the characteristics of the Twitter user posting the tweet such as age
of the account, number of tweets, and the follower-followee ratio. These
Twitter specific features coupled with URL based features prove to be a strong
mechanism to detect phishing tweets. We use machine learning classification
techniques and detect phishing tweets with an accuracy of 92.52%. We have
deployed our system for end-users by providing an easy to use Chrome browser
extension which works in realtime and classifies a tweet as phishing or safe.
We show that we are able to detect phishing tweets at zero hour with high
accuracy which is much faster than public blacklists and as well as Twitter's
own defense mechanism to detect malicious content. To the best of our
knowledge, this is the first realtime, comprehensive and usable system to
detect phishing on Twitter.",scam detection
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",scam detection
http://arxiv.org/abs/1901.02819v2,"We introduce Bug-Injector, a system that automatically creates benchmarks for
customized evaluation of static analysis tools. We share a benchmark generated
using Bug-Injector and illustrate its efficacy by using it to evaluate the
recall of two leading open-source static analysis tools: Clang Static Analyzer
and Infer.
  Bug-Injector works by inserting bugs based on bug templates into real-world
host programs. It runs tests on the host program to collect dynamic traces,
searches the traces for a point where the state satisfies the preconditions for
some bug template, then modifies the host program to inject a bug based on that
template. Injected bugs are used as test cases in a static analysis tool
evaluation benchmark. Every test case is accompanied by a program input that
exercises the injected bug. We have identified a broad range of requirements
and desiderata for bug benchmarks; our approach generates on-demand test
benchmarks that meet these requirements. It also allows us to create customized
benchmarks suitable for evaluating tools for a specific use case (e.g., a given
codebase and set of bug types).
  Our experimental evaluation demonstrates the suitability of our generated
benchmark for evaluating static bug-detection tools and for comparing the
performance of different tools.",scam detection
http://arxiv.org/abs/1910.00508v1,"We present an empirical and large-scale analysis of malware samples captured
from two different enterprises from 2017 to early 2018. Particularly, we
perform threat vector, social-engineering, vulnerability and time-series
analysis on our dataset. Unlike existing malware studies, our analysis is
specifically focused on the recent enterprise malware samples. First of all,
based on our analysis on the combined datasets of two enterprises, our results
confirm the general consensus that AV-only solutions are not enough for
real-time defenses in enterprise settings because on average 40% of the malware
samples, when first appeared, are not detected by most AVs on VirusTotal or not
uploaded to VT at all (i.e., never seen in the wild yet). Moreover, our
analysis also shows that enterprise users transfer documents more than
executables and other types of files. Therefore, attackers embed malicious
codes into documents to download and install the actual malicious payload
instead of sending malicious payload directly or using vulnerability exploits.
Moreover, we also found that financial matters (e.g., purchase orders and
invoices) are still the most common subject seen in Business Email Compromise
(BEC) scams that aim to trick employees. Finally, based on our analysis on the
timestamps of captured malware samples, we found that 93% of the malware
samples were delivered on weekdays. Our further analysis also showed that while
the malware samples that require user interaction such as macro-based malware
samples have been captured during the working hours of the employees, the
massive malware attacks are triggered during the off-times of the employees to
be able to silently spread over the networks.",scam detection
http://arxiv.org/abs/1910.06277v1,"Malicious websites are responsible for a majority of the cyber-attacks and
scams today. Malicious URLs are delivered to unsuspecting users via email, text
messages, pop-ups or advertisements. Clicking on or crawling such URLs can
result in compromised email accounts, launching of phishing campaigns, download
of malware, spyware and ransomware, as well as severe monetary losses. A
machine learning based ensemble classification approach is proposed to detect
malicious URLs in emails, which can be extended to other methods of delivery of
malicious URLs. The approach uses static lexical features extracted from the
URL string, with the assumption that these features are notably different for
malicious and benign URLs. The use of such static features is safer and faster
since it does not involve crawling the URLs or blacklist lookups which tend to
introduce a significant amount of latency in producing verdicts. The goal of
the classification was to achieve high sensitivity i.e. detect as many
malicious URLs as possible. URL strings tend to be very unstructured and noisy.
Hence, bagging algorithms were found to be a good fit for the task since they
average out multiple learners trained on different parts of the training data,
thus reducing variance. The classification model was tested on five different
testing sets and produced an average False Negative Rate (FNR) of 0.1%, average
accuracy of 92% and average AUC of 0.98. The model is presently being used in
the FireEye Advanced URL Detection Engine (used to detect malicious URLs in
emails), to generate fast real-time verdicts on URLs. The malicious URL
detections from the engine have gone up by 22% since the deployment of the
model into the engine workflow. The results obtained show noteworthy evidence
that a purely lexical approach can be used to detect malicious URLs.",scam detection
http://arxiv.org/abs/1209.2557v1,"A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.",scam detection
http://arxiv.org/abs/1406.3687v1,"Existence of spam URLs over emails and Online Social Media (OSM) has become a
massive e-crime. To counter the dissemination of long complex URLs in emails
and character limit imposed on various OSM (like Twitter), the concept of URL
shortening has gained a lot of traction. URL shorteners take as input a long
URL and output a short URL with the same landing page (as in the long URL) in
return. With their immense popularity over time, URL shorteners have become a
prime target for the attackers giving them an advantage to conceal malicious
content. Bitly, a leading service among all shortening services is being
exploited heavily to carry out phishing attacks, work-from-home scams,
pornographic content propagation, etc. This imposes additional performance
pressure on Bitly and other URL shorteners to be able to detect and take a
timely action against the illegitimate content. In this study, we analyzed a
dataset of 763,160 short URLs marked suspicious by Bitly in the month of
October 2013. Our results reveal that Bitly is not using its claimed spam
detection services very effectively. We also show how a suspicious Bitly
account goes unnoticed despite of a prolonged recurrent illegitimate activity.
Bitly displays a warning page on identification of suspicious links, but we
observed this approach to be weak in controlling the overall propagation of
spam. We also identified some short URL based features and coupled them with
two domain specific features to classify a Bitly URL as malicious or benign and
achieved an accuracy of 86.41%. The feature set identified can be generalized
to other URL shortening services as well. To the best of our knowledge, this is
the first large scale study to highlight the issues with the implementation of
Bitly's spam detection policies and proposing suitable countermeasures.",scam detection
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam detection
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam detection
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam detection
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam monitor
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam monitor
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam monitor
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam monitor
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam monitor
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam monitor
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam monitor
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam monitor
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam monitor
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam monitor
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam monitor
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam monitor
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam monitor
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam monitor
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam monitor
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam monitor
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam monitor
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam monitor
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam monitor
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam monitor
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",scam monitor
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam monitor
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam monitor
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam monitor
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam monitor
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam monitor
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam monitor
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detect fake advise
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detect fake advise
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detect fake advise
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detect fake advise
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detect fake advise
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detect fake advise
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detect fake advise
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detect fake advise
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detect fake advise
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detect fake advise
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",detect fake advise
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detect fake advise
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",detect fake advise
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",detect fake advise
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",detect fake advise
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",detect fake advise
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",detect fake advise
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",detect fake advise
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",detect fake advise
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",detect fake advise
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detect fake advise
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detect fake advise
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",detect fake advise
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detect fake advise
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",detect fake advise
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",detect fake advise
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detect fake advise
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",detect fake advise
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",detect fake advise
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",detect fake advise
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detect fake advise
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detect fake advise
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detect fake advise
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detect fake advise
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detect fake advise
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detect fake advise
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detect fake advise
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detect fake advise
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detect fake advise
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detect fake advise
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",detect fake advise
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detect fake advise
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",detect fake advise
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",detect fake advise
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",detect fake advise
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",detect fake advise
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",detect fake advise
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",detect fake advise
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",detect fake advise
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",detect fake advise
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detect fake advise
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detect fake advise
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",detect fake advise
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detect fake advise
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",detect fake advise
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",detect fake advise
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detect fake advise
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",detect fake advise
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",detect fake advise
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",detect fake advise
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",ai consumer fraud online
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",ai consumer fraud online
http://arxiv.org/abs/1906.04272v3,"Given the magnitude of online auction transactions, it is difficult to
safeguard consumers from dishonest sellers, such as shill bidders. To date, the
application of Machine Learning Techniques (MLTs) to auction fraud has been
limited, unlike their applications for combatting other types of fraud. Shill
Bidding (SB) is a severe auction fraud, which is driven by modern-day
technologies and clever scammers. The difficulty of identifying the behavior of
sophisticated fraudsters and the unavailability of training datasets hinder the
research on SB detection. In this study, we developed a high-quality SB
dataset. To do so, first, we crawled and preprocessed a large number of
commercial auctions and bidders' history as well. We thoroughly preprocessed
both datasets to make them usable for the computation of the SB metrics.
Nevertheless, this operation requires a deep understanding of the behavior of
auctions and bidders. Second, we introduced two new SB pattern s and
implemented other existing SB patterns. Finally, we removed outliers to improve
the quality of training SB data.",ai consumer fraud online
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",ai consumer fraud online
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",ai consumer fraud online
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",ai consumer fraud online
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",ai consumer fraud online
http://arxiv.org/abs/1908.10678v1,"Alzheimer's Disease (AD) is the most common type of dementia, comprising
60-80% of cases. There were an estimated 5.8 million Americans living with
Alzheimer's dementia in 2019, and this number will almost double every 20
years. The total lifetime cost of care for someone with dementia is estimated
to be $350,174 in 2018, 70% of which is associated with family-provided care.
Most family caregivers face emotional, financial and physical difficulties. As
a medium to relieve this burden, online communities in social media websites
such as Twitter, Reddit, and Yahoo! Answers provide potential venues for
caregivers to search relevant questions and answers, or post questions and seek
answers from other members. However, there are often a limited number of
relevant questions and responses to search from, and posted questions are
rarely answered immediately. Due to recent advancement in Artificial
Intelligence (AI), particularly Natural Language Processing (NLP), we propose
to utilize AI to automatically generate answers to AD-related consumer
questions posted by caregivers and evaluate how good AI is at answering those
questions. To the best of our knowledge, this is the first study in the
literature applying and evaluating AI models designed to automatically answer
consumer questions related to AD.",ai consumer fraud online
http://arxiv.org/abs/1906.10418v1,"The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process",ai consumer fraud online
http://arxiv.org/abs/1309.7262v1,"Fake websites have emerged as a major source of online fraud, accounting for
billions of dollars of loss by Internet users. We explore the process by which
salient design elements could increase the use of protective tools, thus
reducing the success rate of fake websites. Using the protection motivation
theory, we conceptualize a model to investigate how salient design elements of
detection tools could influence user perceptions of the tools, efficacy in
dealing with threats, and use of such tools. The research method was a
controlled lab experiment with a novel and extensive experimental design and
protocol. We found that trust in the detector is the pivotal coping mechanism
in dealing with security threats and is a major conduit for transforming
salient design elements into increased use. We also found that design elements
have profound and unexpected impacts on self-efficacy. The significant
theoretical and empirical implications of findings are discussed.",ai consumer fraud online
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",ai consumer fraud online
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",ai consumer fraud online
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",ai consumer fraud online
http://arxiv.org/abs/1910.03033v1,"Two elements have been essential to AI's recent boom: (1) deep neural nets
and the theory and practice behind them; and (2) cloud computing with its
abundant labeled data and large computing resources.
  Abundant labeled data is available for key domains such as images, speech,
natural language processing, and recommendation engines. However, there are
many other domains where such data is not available, or access to it is highly
restricted for privacy reasons, as with health and financial data. Even when
abundant data is available, it is often not labeled. Doing such labeling is
labor-intensive and non-scalable.
  As a result, to the best of our knowledge, key domains still lack labeled
data or have at most toy data; or the synthetic data must have access to real
data from which it can mimic new data. This paper outlines work to generate
realistic synthetic data for an important domain: credit card transactions.
  Some challenges: there are many patterns and correlations in real purchases.
There are millions of merchants and innumerable locations. Those merchants
offer a wide variety of goods. Who shops where and when? How much do people
pay? What is a realistic fraudulent transaction?
  We use a mixture of technical approaches and domain knowledge including
mechanics of credit card processing, a broad set of consumer domains:
electronics, clothing, hair styling, etc. Connecting everything is a virtual
world. This paper outlines some of our key techniques and provides evidence
that the data generated is indeed realistic.
  Beyond the scope of this paper: (1) use of our data to develop and train
models to predict fraud; (2) coupling models and the synthetic dataset to
assess performance in designing accelerators such as GPUs and TPUs.",ai consumer fraud online
http://arxiv.org/abs/1510.07165v1,"Financial fraud is an issue with far reaching consequences in the finance
industry, government, corporate sectors, and for ordinary consumers. Increasing
dependence on new technologies such as cloud and mobile computing in recent
years has compounded the problem. Traditional methods of detection involve
extensive use of auditing, where a trained individual manually observes reports
or transactions in an attempt to discover fraudulent behaviour. This method is
not only time consuming, expensive and inaccurate, but in the age of big data
it is also impractical. Not surprisingly, financial institutions have turned to
automated processes using statistical and computational methods. This paper
presents a comprehensive investigation on financial fraud detection practices
using such data mining methods, with a particular focus on computational
intelligence-based techniques. Classification of the practices based on key
aspects such as detection algorithm used, fraud type investigated, and success
rate have been covered. Issues and challenges associated with the current
practices and potential future direction of research have also been identified.",ai consumer fraud online
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",ai consumer fraud online
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",ai consumer fraud online
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",ai consumer fraud online
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",ai consumer fraud online
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",ai consumer fraud online
http://arxiv.org/abs/1611.02260v1,"Food fraud has been an area of great concern due to its risk to public
health, reduction of food quality or nutritional value and for its economic
consequences. For this reason, it's been object of regulation in many countries
(e.g. [1], [2]). One type of food that has been frequently object of fraud
through the addition of water or an aqueous solution is bovine meat. The
traditional methods used to detect this kind of fraud are expensive,
time-consuming and depend on physicochemical analysis that require complex
laboratory techniques, specific for each added substance. In this paper, based
on digital images of histological cuts of adulterated and not-adulterated
(normal) bovine meat, we evaluate the of digital image analysis methods to
identify the aforementioned kind of fraud, with focus on the Local Binary
Pattern (LBP) algorithm.",ai consumer fraud online
http://arxiv.org/abs/1805.09741v2,"The Automobile Insurance Fraud is one of the main challenges for insurance
companies. This form of fraud is performed either opportunistic or professional
occurring through group cooperation that leads to greater financial losses,
while most presented methods thus far are unsuited for flagging these groups.
The article has put forward a new approach for identification, representation,
and analysis of organized fraudulent groups in automobile insurance through
focusing on structural aspects of networks, and cycles in particular, that
demonstrate the occurrence of potential fraud. Suspicious groups have been
detected by applying cycle detection algorithms (using both DFS, BFS trees),
afterward, the probability of being fraudulent for suspicious components were
investigated to reveal fraudulent groups with the maximum likelihood, and their
reviews were prioritized. The actual data of Iran Insurance Company is used for
evaluating the provided approach. As a result, the detection of cycles is not
only more efficient, accurate, but also less time-consuming in comparison with
previous methods for finding such groups.",ai consumer fraud online
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",ai consumer fraud online
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",ai consumer fraud online
http://arxiv.org/abs/1607.04451v4,"Emerging trends in smartphones, online maps, social media, and the resulting
geo-located data, provide opportunities to collect traces of people's
socio-economical activities in a much more granular and direct fashion,
triggering a revolution in empirical research. These vast mobile data offer new
perspectives and approaches for measurements of economic dynamics and are
broadening the research fields of social science and economics. In this paper,
we explore the potential of using mobile big data for measuring economic
activities of China. Firstly, We build indices for gauging employment and
consumer trends based on billions of geo-positioning data. Secondly, we advance
the estimation of store offline foot traffic via location search data derived
from Baidu Maps, which is then applied to predict revenues of Apple in China
and detect box-office fraud accurately. Thirdly, we construct consumption
indicators to track the trends of various industries in service sector, which
are verified by several existing indicators. To the best of our knowledge, we
are the first to measure the second largest economy by mining such
unprecedentedly large scale and fine granular spatial-temporal data. Our
research provides new approaches and insights on measuring economic activities.",ai consumer fraud online
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",ai consumer fraud online
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",ai consumer fraud online
http://arxiv.org/abs/1803.01798v2,"Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.",ai consumer fraud online
http://arxiv.org/abs/1809.04683v2,"Many online platforms have deployed anti-fraud systems to detect and prevent
fraudulent activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, which maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe the user suspended time instead of the
fraudulent activity time in the training data, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.",ai consumer fraud online
http://arxiv.org/abs/1910.04133v1,"The massive growth of the Internet of Things (IoT) as a network of
interconnected entities [18], brings up new challenges in terms of privacy and
security requirements to the traditional software engineering domain [4]. To
protect the individuals' privacy, the FTC's Fair Information Practice
Principles (FIPPs) [6] proposes to companies to give notice to the consumer
about their data practices, provide them with choices and give them means to
have control over their own data.. Using privacy policy is the most common way
for this type of notices. However, privacy policies are not generally effective
due to two main reasons: first, privacy policies are long and full of legal
jargon which are not understandable by a normal user; second, it is not
guaranteed that an IoT device behave as it is explained in its privacy policy.
In this technical report, we propose and discuss our methodologies to analyze
privacy policies. By the help of this analysis, we reduce the length of a
privacy policy and make it organized based on privacy practices to improve
understanding level for the user. We also come up with a method to find the
inconsistencies between IoT devices and their privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1903.06068v2,"In this report, we present an approach to enhance informed consent for the
processing of personal data. The approach relies on a privacy policy language
used to express, compare and analyze privacy policies. We describe a tool that
automatically reports the privacy risks associated with a given privacy policy
in order to enhance data subjects' awareness and to allow them to make more
informed choices. The risk analysis of privacy policies is illustrated with an
IoT example.",analyzing privacy policies
http://arxiv.org/abs/1809.02236v1,"In this paper, we demonstrate the effectiveness of using the theory of
contextual integrity (CI) to annotate and evaluate privacy policy statements.
We perform a case study using CI annotations to compare Facebook's privacy
policy before and after the Cambridge Analytica scandal. The updated Facebook
privacy policy provides additional details about what information is being
transferred, from whom, by whom, to whom, and under what conditions. However,
some privacy statements prescribe an incomprehensibly large number of
information flows by including many CI parameters in single statements. Other
statements result in incomplete information flows due to the use of vague terms
or omitting contextual parameters altogether. We then demonstrate that
crowdsourcing can effectively produce CI annotations of privacy policies at
scale. We test the CI annotation task on 48 excerpts of privacy policies from
17 companies with 141 crowdworkers. The resulting high precision annotations
indicate that crowdsourcing could be used to produce a large corpus of
annotated privacy policies for future research.",analyzing privacy policies
http://arxiv.org/abs/1906.12038v1,"With the arrival of the European Union's General Data Protection Regulation
(GDPR), several companies are making significant changes to their systems to
achieve compliance. The changes range from modifying privacy policies to
redesigning systems which process personal data. This work analyzes the privacy
policies of large-scaled cloud services which seek to be GDPR compliant. The
privacy policy is the main medium of information dissemination between the data
controller and the users. We show that many services that claim compliance
today do not have clear and concise privacy policies. We identify several
points in the privacy policies which potentially indicate non-compliance; we
term these GDPR vulnerabilities. We identify GDPR vulnerabilities in ten cloud
services. Based on our analysis, we propose seven best practices for crafting
GDPR privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1908.06814v1,"Privacy policies are the main way to obtain information related to personal
data collection and processing.Originally, privacy policies were presented as
textual documents. However, the unsuitability of this format for the needs of
today's society gave birth to others means of expression. In this report, we
systematically study the different means of expression of privacy policies. In
doing so, we have identified three main categories, which we call dimensions,
i.e., natural language, graphical and machine-readable privacy policies. Each
of these dimensions focus on the particular needs of the communities they come
from, i.e., law experts, organizations and privacy advocates, and academics,
respectively. We then analyze the benefits and limitations of each dimension,
and explain why solutions based on a single dimension do not cover the needs of
other communities. Finally, we propose a new approach to expressing privacy
policies which brings together the benefits of each dimension as an attempt to
overcome their limitations.",analyzing privacy policies
http://arxiv.org/abs/1805.01187v1,"A dominant regulatory model for web privacy is ""notice and choice"". In this
model, users are notified of data collection and provided with options to
control it. To examine the efficacy of this approach, this study presents the
first large-scale audit of disclosure of third-party data collection in website
privacy policies. Data flows on one million websites are analyzed and over
200,000 websites' privacy policies are audited to determine if users are
notified of the names of the companies which collect their data. Policies from
25 prominent third-party data collectors are also examined to provide deeper
insights into the totality of the policy environment. Policies are additionally
audited to determine if the choice expressed by the ""Do Not Track"" browser
setting is respected.
  Third-party data collection is wide-spread, but fewer than 15% of attributed
data flows are disclosed. The third-parties most likely to be disclosed are
those with consumer services users may be aware of, those without consumer
services are less likely to be mentioned. Policies are difficult to understand
and the average time requirement to read both a given site{\guillemotright}s
policy and the associated third-party policies exceeds 84 minutes. Only 7% of
first-party site policies mention the Do Not Track signal, and the majority of
such mentions are to specify that the signal is ignored. Among third-party
policies examined, none offer unqualified support for the Do Not Track signal.
Findings indicate that current implementations of ""notice and choice"" fail to
provide notice or respect choice.",analyzing privacy policies
http://arxiv.org/abs/1810.11153v4,"A deterministic privacy metric using non-stochastic information theory is
developed. Particularly, minimax information is used to construct a measure of
information leakage, which is inversely proportional to the measure of privacy.
Anyone can submit a query to a trusted agent with access to a non-stochastic
uncertain private dataset. Optimal deterministic privacy-preserving policies
for responding to the submitted query are computed by maximizing the measure of
privacy subject to a constraint on the worst-case quality of the response
(i.e., the worst-case difference between the response by the agent and the
output of the query computed on the private dataset). The optimal
privacy-preserving policy is proved to be a piecewise constant function in the
form of a quantization operator applied on the output of the submitted query.
The measure of privacy is also used to analyze the performance of $k$-anonymity
methodology (a popular deterministic mechanism for privacy-preserving release
of datasets using suppression and generalization techniques), proving that it
is in fact not privacy-preserving.",analyzing privacy policies
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",analyzing privacy policies
http://arxiv.org/abs/cs/0001011v1,"A variety of tools have been introduced recently that are designed to help
people protect their privacy on the Internet. These tools perform many
different functions in-cluding encrypting and/or anonymizing communications,
preventing the use of persistent identifiers such as cookies, automatically
fetching and analyzing web site privacy policies, and displaying
privacy-related information to users. This paper discusses the set of privacy
tools that aim specifically at facilitating notice and choice about Web site
data practices. While these tools may also have components that perform other
functions such as encryption, or they may be able to work in conjunction with
other privacy tools, the primary pur-pose of these tools is to help make users
aware of web site privacy practices and to make it easier for users to make
informed choices about when to provide data to web sites. Examples of such
tools include the Platform for Privacy Preferences (P3P) and various
infomediary services.",analyzing privacy policies
http://arxiv.org/abs/1902.00174v1,"Many reinforcement learning applications involve the use of data that is
sensitive, such as medical records of patients or financial information.
However, most current reinforcement learning methods can leak information
contained within the (possibly sensitive) data on which they are trained. To
address this problem, we present the first differentially private approach for
off-policy evaluation. We provide a theoretical analysis of the
privacy-preserving properties of our algorithm and analyze its utility (speed
of convergence). After describing some results of this theoretical analysis, we
show empirically that our method outperforms previous methods (which are
restricted to the on-policy setting).",analyzing privacy policies
http://arxiv.org/abs/1608.04671v2,"Privacy analysis is critical but also a time-consuming and tedious task. We
present a formalization which eases designing and auditing high-level privacy
properties of software architectures. It is incorporated into a larger policy
analysis and verification framework and enables the assessment of commonly
accepted data protection goals of privacy. The formalization is based on static
taint analysis and makes flow and processing of privacy-critical data explicit,
globally as well as on the level of individual data subjects. Formally, we show
equivalence to traditional label-based information flow security and prove
overall soundness of our tool with Isabelle/HOL. We demonstrate applicability
in two real-world case studies, thereby uncovering previously unknown
violations of privacy constraints in the analyzed software architectures.",analyzing privacy policies
http://arxiv.org/abs/1710.08306v1,"Mobile phones provide an excellent opportunity for building context-aware
applications. In particular, location-based services are important
context-aware services that are more and more used for enforcing security
policies, for supporting indoor room navigation, and for providing personalized
assistance. However, a major problem still remains unaddressed---the lack of
solutions that work across buildings while not using additional infrastructure
and also accounting for privacy and reliability needs. In this paper, a
privacy-preserving, multi-modal, cross-building, collaborative localization
platform is proposed based on Wi-Fi RSSI (existing infrastructure), Cellular
RSSI, sound and light levels, that enables room-level localization as main
application (though sub room level granularity is possible). The privacy is
inherently built into the solution based on onion routing, and
perturbation/randomization techniques, and exploits the idea of weighted
collaboration to increase the reliability as well as to limit the effect of
noisy devices (due to sensor noise/privacy). The proposed solution has been
analyzed in terms of privacy, accuracy, optimum parameters, and other overheads
on location data collected at multiple indoor and outdoor locations using an
Android app.",analyzing privacy policies
http://arxiv.org/abs/1802.02561v2,"Privacy policies are the primary channel through which companies inform users
about their data collection and sharing practices. These policies are often
long and difficult to comprehend. Short notices based on information extracted
from privacy policies have been shown to be useful but face a significant
scalability hurdle, given the number of policies and their evolution over time.
Companies, users, researchers, and regulators still lack usable and scalable
tools to cope with the breadth and depth of privacy policies. To address these
hurdles, we propose an automated framework for privacy policy analysis
(Polisis). It enables scalable, dynamic, and multi-dimensional queries on
natural language privacy policies. At the core of Polisis is a privacy-centric
language model, built with 130K privacy policies, and a novel hierarchy of
neural-network classifiers that accounts for both high-level aspects and
fine-grained details of privacy practices. We demonstrate Polisis' modularity
and utility with two applications supporting structured and free-form querying.
The structured querying application is the automated assignment of privacy
icons from privacy policies. With Polisis, we can achieve an accuracy of 88.4%
on this task. The second application, PriBot, is the first freeform
question-answering system for privacy policies. We show that PriBot can produce
a correct answer among its top-3 results for 82% of the test questions. Using
an MTurk user study with 700 participants, we show that at least one of
PriBot's top-3 answers is relevant to users for 89% of the test questions.",analyzing privacy policies
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",analyzing privacy policies
http://arxiv.org/abs/1512.00201v1,"For security and privacy management and enforcement purposes, various policy
languages have been presented. We give an overview on 27 security and privacy
policy languages and present a categorization framework for policy languages.
We show how the current policy languages are represented in the framework and
summarize our interpretation. We show up identified gaps and motivate for the
adoption of policy languages for the specification of privacy-utility trade-off
policies.",analyzing privacy policies
http://arxiv.org/abs/1806.00114v1,"We examine the problem of target tracking whilst simultaneously preserving
the target's privacy as epitomized by the robotic panda tracking scenario,
which O'Kane introduced at the 2008 Workshop on the Algorithmic Foundations of
Robotics in order to elegantly illustrate the utility of ignorance. The present
paper reconsiders his formulation and the tracking strategy he proposed, along
with its completeness. We explore how the capabilities of the robot and panda
affect the feasibility of tracking with a privacy stipulation, uncovering
intrinsic limits, no matter the strategy employed. This paper begins with a
one-dimensional setting and, putting the trivially infeasible problems aside,
analyzes the strategy space as a function of problem parameters. We show that
it is not possible to actively track the target as well as protect its privacy
for every nontrivial pair of tracking and privacy stipulations. Secondly,
feasibility can be sensitive, in several cases, to the information available to
the robot initially. Quite naturally in the one-dimensional model, one may
quantify sensing power by the number of perceptual (or output) classes
available to the robot. The robot's power to achieve privacy-preserving
tracking is bounded, converging asymptotically with increasing sensing power.
We analyze the entire space of possible tracking problems, characterizing every
instance as either achievable, constructively by giving a policy where one
exists (some of which depend on the initial information), or proving the
instance impossible. Finally, to relate some of the impossibility results in
one dimension to their higher-dimensional counterparts, including the planar
panda tracking problem studied by O'Kane, we establish a connection between
tracking dimensionality and the sensing power of a one-dimensional robot.",analyzing privacy policies
http://arxiv.org/abs/1809.08396v3,"The EU General Data Protection Regulation (GDPR) is one of the most demanding
and comprehensive privacy regulations of all time. A year after it went into
effect, we study its impact on the landscape of privacy policies online. We
conduct the first longitudinal, in-depth, and at-scale assessment of privacy
policies before and after the GDPR. We gauge the complete consumption cycle of
these policies, from the first user impressions until the compliance
assessment. We create a diverse corpus of two sets of 6,278 unique
English-language privacy policies from inside and outside the EU, covering
their pre-GDPR and the post-GDPR versions. The results of our tests and
analyses suggest that the GDPR has been a catalyst for a major overhaul of the
privacy policies inside and outside the EU. This overhaul of the policies,
manifesting in extensive textual changes, especially for the EU-based websites,
comes at mixed benefits to the users. While the privacy policies have become
considerably longer, our user study with 470 participants on Amazon MTurk
indicates a significant improvement in the visual representation of privacy
policies from the users' perspective for the EU websites. We further develop a
new workflow for the automated assessment of requirements in privacy policies.
Using this workflow, we show that privacy policies cover more data practices
and are more consistent with seven compliance requirements post the GDPR. We
also assess how transparent the organizations are with their privacy practices
by performing specificity analysis. In this analysis, we find evidence for
positive changes triggered by the GDPR, with the specificity level improving on
average. Still, we find the landscape of privacy policies to be in a
transitional phase; many policies still do not meet several key GDPR
requirements or their improved coverage comes with reduced specificity.",analyzing privacy policies
http://arxiv.org/abs/1704.01218v1,"More data is currently being collected and shared by software applications
than ever before. In many cases, the user is asked if either all or none of
their data can be shared. We hypothesize that in some cases, users would like
to share data in more complex ways. In order to implement the sharing of data
using more complicated privacy preferences, complex data sharing policies must
be used. These complex sharing policies require more space to store than a
simple ""all or nothing"" approach to data sharing. In this paper, we present a
new probabilistic data structure, called the Min Mask Sketch, to efficiently
store these complex data sharing policies. We describe an implementation for
the Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility
of using a probabilistic data structure for storing complex data sharing
policies.",analyzing privacy policies
http://arxiv.org/abs/1910.03622v1,"Mobile applications (apps) have become deeply personal, constantly demanding
access to privacy-sensitive information in exchange for more personalized user
experiences. Such privacy-invading practices have generated major
multidimensional and unconventional privacy concerns among app users. To
address these concerns, the research on mobile app privacy has experienced
rapid growth over the past decade. In general, this line of research is aimed
at systematically exposing the privacy practices of apps and proposing
solutions to protect the privacy of mobile app users. In this survey paper, we
conduct a systematic mapping study of 54 Software Engineering (SE) primary
studies on mobile app privacy. Our objectives are to a) explore trends in SE
app privacy research, b) categorize existing evidence, and c) identify
potential directions for future research. Our results show that existing
literature can be divided into four main categories: privacy policy,
requirements, user perspective, and leak detection. Furthermore, our survey
reveals an imbalance between these categories; majority of existing research
focuses on proposing tools for detecting privacy leaks, with less studies
targeting privacy requirements and policy and even less on user perspective.
Finally, our survey exposes several gaps in existing research and suggests
areas for improvement.",analyzing privacy policies
http://arxiv.org/abs/1710.06494v2,"In this paper we propose a formal framework for studying privacy in
information systems. The proposal follows a two-axes schema where the first
axis considers privacy as a taxonomy of rights and the second axis involves the
ways an information system stores and manipulates information. We develop a
correspondence between the above schema and an associated model of computation.
In particular, we propose the \Pcalc, a calculus based on the $\pi$-calculus
with groups extended with constructs for reasoning about private data. The
privacy requirements of an information system are captured via a privacy policy
language. The correspondence between the privacy model and the \Pcalc semantics
is established using a type system for the calculus and a satisfiability
definition between types and privacy policies. We deploy a type preservation
theorem to show that a system respects a policy and it is safe if the typing of
the system satisfies the policy. We illustrate our methodology via analysis of
two use cases: a privacy-aware scheme for electronic traffic pricing and a
privacy-preserving technique for speed-limit enforcement.",analyzing privacy policies
http://arxiv.org/abs/1905.00111v1,"Smart meters enable improvements in electricity distribution system
efficiency at some cost in customer privacy. Users with home batteries can
mitigate this privacy loss by applying charging policies that mask their
underlying energy use. A battery charging policy is proposed and shown to
provide universal privacy guarantees subject to a constraint on energy cost.
The guarantee bounds our strategy's maximal information leakage from the user
to the utility provider under general stochastic models of user energy
consumption. The policy construction adapts coding strategies for
non-probabilistic permuting channels to this privacy problem.",analyzing privacy policies
http://arxiv.org/abs/1307.6980v1,"Privacy-aware processing of personal data on the web of services requires
managing a number of issues arising both from the technical and the legal
domain. Several approaches have been proposed to matching privacy requirements
(on the clients side) and privacy guarantees (on the service provider side).
Still, the assurance of effective data protection (when possible) relies on
substantial human effort and exposes organizations to significant
(non-)compliance risks. In this paper we put forward the idea that a privacy
certification scheme producing and managing machine-readable artifacts in the
form of privacy certificates can play an important role towards the solution of
this problem. Digital privacy certificates represent the reasons why a privacy
property holds for a service and describe the privacy measures supporting it.
Also, privacy certificates can be used to automatically select services whose
certificates match the client policies (privacy requirements).
  Our proposal relies on an evolution of the conceptual model developed in the
Assert4Soa project and on a certificate format specifically tailored to
represent privacy properties. To validate our approach, we present a worked-out
instance showing how privacy property Retention-based unlinkability can be
certified for a banking financial service.",analyzing privacy policies
http://arxiv.org/abs/1705.06805v1,"The increasing popularity of specialized Internet-connected devices and
appliances, dubbed the Internet-of-Things (IoT), promises both new conveniences
and new privacy concerns. Unlike traditional web browsers, many IoT devices
have always-on sensors that constantly monitor fine-grained details of users'
physical environments and influence the devices' network communications.
Passive network observers, such as Internet service providers, could
potentially analyze IoT network traffic to infer sensitive details about users.
Here, we examine four IoT smart home devices (a Sense sleep monitor, a Nest Cam
Indoor security camera, a WeMo switch, and an Amazon Echo) and find that their
network traffic rates can reveal potentially sensitive user interactions even
when the traffic is encrypted. These results indicate that a technological
solution is needed to protect IoT device owner privacy, and that IoT-specific
concerns must be considered in the ongoing policy debate around ISP data
collection and usage.",analyzing privacy policies
http://arxiv.org/abs/1001.4459v1,"The Privacy Coach is an application running on a mobile phone that supports
customers in making privacy decisions when confronted with RFID tags. The
approach we take to increase customer privacy is a radical departure from the
mainstream research efforts that focus on implementing privacy enhancing
technologies on the RFID tags themselves. Instead the Privacy Coach functions
as a mediator between customer privacy preferences and corporate privacy
policies, trying to find a match between the two, and informing the user of the
outcome. In this paper we report on the architecture of the Privacy Coach, and
show how it enables users to make informed privacy decisions in a user-friendly
manner. We also spend considerable time to discuss lessons learnt and to
describe future plans to further improve on the Privacy Coach concept.",analyzing privacy policies
http://arxiv.org/abs/1808.06219v2,"Website privacy policies represent the single most important source of
information for users to gauge how their personal data are collected, used and
shared by companies. However, privacy policies are often vague and people
struggle to understand the content. Their opaqueness poses a significant
challenge to both users and policy regulators. In this paper, we seek to
identify vague content in privacy policies. We construct the first corpus of
human-annotated vague words and sentences and present empirical studies on
automatic vagueness detection. In particular, we investigate context-aware and
context-agnostic models for predicting vague words, and explore
auxiliary-classifier generative adversarial networks for characterizing
sentence vagueness. Our experimental results demonstrate the effectiveness of
proposed approaches. Finally, we provide suggestions for resolving vagueness
and improving the usability of privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1710.03890v1,"End user privacy is a critical concern for all organizations that collect,
process and store user data as a part of their business. Privacy concerned
users, regulatory bodies and privacy experts continuously demand organizations
provide users with privacy protection. Current research lacks an understanding
of organizational characteristics that affect an organization's motivation
towards user privacy. This has resulted in a ""one solution fits all"" approach,
which is incapable of providing sustainable solutions for organizational issues
related to user privacy. In this work, we have empirically investigated 40
diverse organizations on their motivations and approaches towards user privacy.
Resources such as newspaper articles, privacy policies and internal privacy
reports that display information about organizational motivations and
approaches towards user privacy were used in the study. We could observe
organizations to have two primary motivations to provide end users with privacy
as voluntary driven inherent motivation, and risk driven compliance motivation.
Building up on these findings we developed a taxonomy of organizational privacy
approaches and further explored the taxonomy through limited exclusive
interviews. With his work, we encourage authorities and scholars to understand
organizational characteristics that define an organization's approach towards
privacy, in order to effectively communicate regulations that enforce and
encourage organizations to consider privacy within their business practices.",analyzing privacy policies
http://arxiv.org/abs/1710.05363v1,"With the advent of numerous online content providers, utilities and
applications, each with their own specific version of privacy policies and its
associated overhead, it is becoming increasingly difficult for concerned users
to manage and track the confidential information that they share with the
providers. Users consent to providers to gather and share their Personally
Identifiable Information (PII). We have developed a novel framework to
automatically track details about how a users' PII data is stored, used and
shared by the provider. We have integrated our Data Privacy ontology with the
properties of blockchain, to develop an automated access control and audit
mechanism that enforces users' data privacy policies when sharing their data
across third parties. We have also validated this framework by implementing a
working system LinkShare. In this paper, we describe our framework on detail
along with the LinkShare system. Our approach can be adopted by Big Data users
to automatically apply their privacy policy on data operations and track the
flow of that data across various stakeholders.",analyzing privacy policies
http://arxiv.org/abs/1312.3913v5,"Privacy definitions provide ways for trading-off the privacy of individuals
in a statistical database for the utility of downstream analysis of the data.
In this paper, we present Blowfish, a class of privacy definitions inspired by
the Pufferfish framework, that provides a rich interface for this trade-off. In
particular, we allow data publishers to extend differential privacy using a
policy, which specifies (a) secrets, or information that must be kept secret,
and (b) constraints that may be known about the data. While the secret
specification allows increased utility by lessening protection for certain
individual properties, the constraint specification provides added protection
against an adversary who knows correlations in the data (arising from
constraints). We formalize policies and present novel algorithms that can
handle general specifications of sensitive information and certain count
constraints. We show that there are reasonable policies under which our privacy
mechanisms for k-means clustering, histograms and range queries introduce
significantly lesser noise than their differentially private counterparts. We
quantify the privacy-utility trade-offs for various policies analytically and
empirically on real datasets.",analyzing privacy policies
http://arxiv.org/abs/1404.3722v3,"The problem of designing error optimal differentially private algorithms is
well studied. Recent work applying differential privacy to real world settings
have used variants of differential privacy that appropriately modify the notion
of neighboring databases. The problem of designing error optimal algorithms for
such variants of differential privacy is open. In this paper, we show a novel
transformational equivalence result that can turn the problem of query
answering under differential privacy with a modified notion of neighbors to one
of query answering under standard differential privacy, for a large class of
neighbor definitions.
  We utilize the Blowfish privacy framework that generalizes differential
privacy. Blowfish uses a {\em policy graph} to instantiate different notions of
neighboring databases. We show that the error incurred when answering a
workload $\mathbf{W}$ on a database $\mathbf{x}$ under a Blowfish policy graph
$G$ is identical to the error required to answer a transformed workload
$f_G(\mathbf{W})$ on database $g_G(\mathbf{x})$ under standard differential
privacy, where $f_G$ and $g_G$ are linear transformations based on $G$. Using
this result, we develop error efficient algorithms for releasing histograms and
multidimensional range queries under different Blowfish policies. We believe
the tools we develop will be useful for finding mechanisms to answer many other
classes of queries with low error under other policy graphs.",analyzing privacy policies
http://arxiv.org/abs/1309.6204v2,"In an undirected social graph, a friendship link involves two users and the
friendship is visible in both the users' friend lists. Such a dual visibility
of the friendship may raise privacy threats. This is because both users can
separately control the visibility of a friendship link to other users and their
privacy policies for the link may not be consistent. Even if one of them
conceals the link from a third user, the third user may find such a friendship
link from another user's friend list. In addition, as most users allow their
friends to see their friend lists in most social network systems, an adversary
can exploit the inconsistent policies to launch privacy attacks to identify and
infer many of a targeted user's friends. In this paper, we propose, analyze and
evaluate such an attack which is called Friendship Identification and Inference
(FII) attack. In a FII attack scenario, we assume that an adversary can only
see his friend list and the friend lists of his friends who do not hide the
friend lists from him. Then, a FII attack contains two attack steps: 1) friend
identification and 2) friend inference. In the friend identification step, the
adversary tries to identify a target's friends based on his friend list and
those of his friends. In the friend inference step, the adversary attempts to
infer the target's friends by using the proposed random walk with restart
approach. We present experimental results using three real social network
datasets and show that FII attacks are generally efficient and effective when
adversaries and targets are friends or 2-distant neighbors. We also
comprehensively analyze the attack results in order to find what values of
parameters and network features could promote FII attacks. Currently, most
popular social network systems with an undirected friendship graph, such as
Facebook, LinkedIn and Foursquare, are susceptible to FII attacks.",analyzing privacy policies
http://arxiv.org/abs/1807.07468v1,"A text mining approach is proposed based on latent Dirichlet allocation (LDA)
to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints.
The proposed approach aims to extract latent topics in the CFPB complaint
narratives, and explores their associated trends over time. The time trends
will then be used to evaluate the effectiveness of the CFPB regulations and
expectations on financial institutions in creating a consumer oriented culture
that treats consumers fairly and prioritizes consumer protection in their
decision making processes. The proposed approach can be easily operationalized
as a decision support system to automate detection of emerging topics in
consumer complaints. Hence, the technology-human partnership between the
proposed approach and the CFPB team could certainly improve consumer
protections from unfair, deceptive or abusive practices in the financial
markets by providing more efficient and effective investigations of consumer
complaint narratives.",consumer protection
http://arxiv.org/abs/1705.06809v1,"The growing market for smart home IoT devices promises new conveniences for
consumers while presenting novel challenges for preserving privacy within the
home. Specifically, Internet service providers or neighborhood WiFi
eavesdroppers can measure Internet traffic rates from smart home devices and
infer consumers' private in-home behaviors. Here we propose four strategies
that device manufacturers and third parties can take to protect consumers from
side-channel traffic rate privacy threats: 1) blocking traffic, 2) concealing
DNS, 3) tunneling traffic, and 4) shaping and injecting traffic. We hope that
these strategies, and the implementation nuances we discuss, will provide a
foundation for the future development of privacy-sensitive smart homes.",consumer protection
http://arxiv.org/abs/1907.11717v1,"The benefits of the ubiquitous caching in ICN are profound, such features
make ICN promising for content distribution, but it also introduces a challenge
to content protection against the unauthorized access. The protection of a
content against unauthorized access requires consumer authentication and
involves the conventional end-to-end encryption. However, in
information-centric networking (ICN), such end-to-end encryption makes the
content caching ineffective since encrypted contents stored in a cache are
useless for any consumers except those who know the encryption key. For
effective caching of encrypted contents in ICN, we propose a secure
distribution of protected content (SDPC) scheme, which ensures that only
authenticated consumers can access the content. SDPC is lightweight and allows
consumers to verify the originality of the published content by using a
symmetric key encryption. SDPC also provides protection against privacy
leakage. The security of SDPC was proved with the BAN logic and Scyther tool
verification, and simulation results show that SDPC can reduce the content
download delay.",consumer protection
http://arxiv.org/abs/1703.00518v1,"Consumer protection agencies are charged with safeguarding the public from
hazardous products, but the thousands of products under their jurisdiction make
it challenging to identify and respond to consumer complaints quickly. From the
consumer's perspective, online reviews can provide evidence of product defects,
but manually sifting through hundreds of reviews is not always feasible. In
this paper, we propose a system to mine Amazon.com reviews to identify products
that may pose safety or health hazards. Since labeled data for this task are
scarce, our approach combines positive unlabeled learning with domain
adaptation to train a classifier from consumer complaints submitted to the U.S.
Consumer Product Safety Commission. On a validation set of manually annotated
Amazon product reviews, we find that our approach results in an absolute F1
score improvement of 8% over the best competing baseline. Furthermore, we apply
the classifier to Amazon reviews of known recalled products; the classifier
identifies reviews reporting safety hazards prior to the recall date for 45% of
the products. This suggests that the system may be able to provide an early
warning system to alert consumers to hazardous products before an official
recall is announced.",consumer protection
http://arxiv.org/abs/1405.3342v1,"In the event that a bacteriological or chemical toxin is intro- duced to a
water distribution network, a large population of consumers may become exposed
to the contaminant. A contamination event may be poorly predictable dynamic
process due to the interactions of consumers and utility managers during an
event. Consumers that become aware of a threat may select protective actions
that change their water demands from typical demand patterns, and new hydraulic
conditions can arise that differ from conditions that are predicted when
demands are considered as exogenous inputs. Consequently, the movement of the
contaminant plume in the pipe network may shift from its expected trajectory. A
sociotechnical model is developed here to integrate agent-based models of
consumers with an engineering water distribution system model and capture the
dynamics between consumer behaviors and the water distribution system for
predicting contaminant transport and public exposure. Consumers are simulated
as agents with behaviors defined for water use activities, mobility,
word-of-mouth communication, and demand reduction, based on a set of rules
representing an agents autonomy and reaction to health impacts, the
environment, and the actions of other agents. As consumers decrease their water
use, the demand exerted on the water distribution system is updated; as the
flow directions and volumes shift in response, the location of the contaminant
plume is updated and the amount of contaminant consumed by each agent is
calculated. The framework is tested through simulating realistic contamination
scenarios for a virtual city and water distribution system.",consumer protection
http://arxiv.org/abs/1808.03289v1,"The secure distribution of protected content requires consumer authentication
and involves the conventional method of end-to-end encryption. However, in
information-centric networking (ICN) the end-to-end encryption makes the
content caching ineffective since encrypted content stored in a cache is
useless for any consumer except those who know the encryption key. For
effective caching of encrypted content in ICN, we propose a novel scheme,
called the Secure Distribution of Protected Content (SDPC). SDPC ensures that
only authenticated consumers can access the content. The SDPC is a lightweight
authentication and key distribution protocol; it allows consumer nodes to
verify the originality of the published article by using a symmetric key
encryption. The security of the SDPC was proved with BAN logic and Scyther tool
verification.",consumer protection
http://arxiv.org/abs/1711.07220v1,"The AN.ON-Next project aims to integrate privacy-enhancing technologies into
the internet's infrastructure and establish them in the consumer mass market.
  The technologies in focus include a basis protection at internet service
provider level, an improved overlay network-based protection and a concept for
privacy protection in the emerging 5G mobile network. A crucial success factor
will be the viable adjustment and development of standards, business models and
pricing strategies for those new technologies.",consumer protection
http://arxiv.org/abs/1806.08274v1,"Pre-configured cycle (p-Cycle) method has been studied in literature
extensively for optical network protection. A large p-cycle has high capacity
efficiency and can protect a large number of nodes against the single link
failure scenarios. All the links protected by such a p-cycle lose protection
when the p-cycle is consumed to restore traffic after a failure. As the
probability of multiple link failure is high for a large network, it also means
that with higher probability, on the second failure, protection may not be
there for the failed link. Thus, if the number of links protected by a p-cycle
is large, it makes the network unprotected with high probability on the advent
of the second failure. In this paper, we study the impact zone due to a first
link failure in the various configurations of the p-cycles. The study gives
insight into how to choose the p-cycle configuration to reduce the impact zone
while using minimum spare capacity. We propose few methods and compare them to
show how the impact zone analysis can be used to improve the fault tolerance in
an optical network.",consumer protection
http://arxiv.org/abs/cs/9908012v1,"E-business, information serving, and ubiquitous computing will create heavy
request traffic from strangers or even incognitos. Such requests must be
managed automatically. Two ways of doing this are well known: giving every
incognito consumer the same treatment, and rendering service in return for
money. However, different behavior will be often wanted, e.g., for a university
library with different access policies for undergraduates, graduate students,
faculty, alumni, citizens of the same state, and everyone else.
  For a data or process server contacted by client machines on behalf of users
not previously known, we show how to provide reliable automatic access
administration conforming to service agreements. Implementations scale well
from very small collections of consumers and producers to immense client/server
networks. Servers can deliver information, effect state changes, and control
external equipment.
  Consumer privacy is easily addressed by the same protocol. We support
consumer privacy, but allow servers to deny their resources to incognitos. A
protocol variant even protects against statistical attacks by consortia of
service organizations.
  One e-commerce application would put the consumer's tokens on a smart card
whose readers are in vending kiosks. In e-business we can simplify supply chain
administration. Our method can also be used in sensitive networks without
introducing new security loopholes.",consumer protection
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",consumer protection
http://arxiv.org/abs/1805.02722v1,"Data encryption is the primary method of protecting the privacy of consumer
device Internet communications from network observers. The ability to
automatically detect unencrypted data in network traffic is therefore an
essential tool for auditing Internet-connected devices. Existing methods
identify network packets containing cleartext but cannot differentiate packets
containing encrypted data from packets containing compressed unencrypted data,
which can be easily recovered by reversing the compression algorithm. This
makes it difficult for consumer protection advocates to identify devices that
risk user privacy by sending sensitive data in a compressed unencrypted format.
Here, we present the first technique to automatically distinguish encrypted
from compressed unencrypted network transmissions on a per-packet basis. We
apply three machine learning models and achieve a maximum 66.9% accuracy with a
convolutional neural network trained on raw packet data. This result is a
baseline for this previously unstudied machine learning problem, which we hope
will motivate further attention and accuracy improvements. To facilitate
continuing research on this topic, we have made our training and test datasets
available to the public.",consumer protection
http://arxiv.org/abs/1902.08712v1,"Resource allocation is the process of optimizing the rare resources. In the
area of security, how to allocate limited resources to protect a massive number
of targets is especially challenging. This paper addresses this resource
allocation issue by constructing a game theoretic model. A defender and an
attacker are players and the interaction is formulated as a trade-off between
protecting targets and consuming resources. The action cost which is a
necessary role of consuming resource, is considered in the proposed model.
Additionally, a bounded rational behavior model (Quantal Response, QR), which
simulates a human attacker of the adversarial nature, is introduced to improve
the proposed model. To validate the proposed model, we compare the different
utility functions and resource allocation strategies. The comparison results
suggest that the proposed resource allocation strategy performs better than
others in the perspective of utility and resource effectiveness.",consumer protection
http://arxiv.org/abs/1612.05120v4,"The roll-out of smart meters in electricity networks introduces risks for
consumer privacy due to increased measurement frequency and granularity.
Through various Non-Intrusive Load Monitoring techniques, consumer behavior may
be inferred from their metering data. In this paper, we propose an energy
management method that reduces energy cost and protects privacy through the
minimization of information leakage. The method is based on a Model Predictive
Controller that utilizes energy storage and local generation, and that predicts
the effects of its actions on the statistics of the actual energy consumption
of a consumer and that seen by the grid. Computationally, the method requires
solving a Mixed-Integer Quadratic Program of manageable size whenever new meter
readings are available. We simulate the controller on generated residential
load profiles with different privacy costs in a two-tier time-of-use energy
pricing environment. Results show that information leakage is effectively
reduced at the expense of increased energy cost. The results also show that
with the proposed controller the consumer load profile seen by the grid
resembles a mixture between that obtained with Non-Intrusive Load Leveling and
Lazy Stepping.",consumer protection
http://arxiv.org/abs/1708.02629v1,"In the post-genomic era, large-scale personal DNA sequences are produced and
collected for genetic medical diagnoses and new drug discovery, which, however,
simultaneously poses serious challenges to the protection of personal genomic
privacy. Existing genomic privacy-protection methods are either time-consuming
or with low accuracy. To tackle these problems, this paper proposes a sequence
similarity-based obfuscation method, namely IterMegaBLAST, for fast and
reliable protection of personal genomic privacy. Specifically, given a randomly
selected sequence from a dataset of DNA sequences, we first use MegaBLAST to
find its most similar sequence from the dataset. These two aligned sequences
form a cluster, for which an obfuscated sequence was generated via a DNA
generalization lattice scheme. These procedures are iteratively performed until
all of the sequences in the dataset are clustered and their obfuscated
sequences are generated. Experimental results on two benchmark datasets
demonstrate that under the same degree of anonymity, IterMegaBLAST
significantly outperforms existing state-of-the-art approaches in terms of both
utility accuracy and time complexity.",consumer protection
http://arxiv.org/abs/1310.1551v1,"Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.",consumer protection
http://arxiv.org/abs/1709.09614v1,"Power grids are undergoing major changes due to rapid growth in renewable
energy resources and improvements in battery technology. While these changes
enhance sustainability and efficiency, they also create significant management
challenges as the complexity of power systems increases. To tackle these
challenges, decentralized Internet-of-Things (IoT) solutions are emerging,
which arrange local communities into transactive microgrids. Within a
transactive microgrid, ""prosumers"" (i.e., consumers with energy generation and
storage capabilities) can trade energy with each other, thereby smoothing the
load on the main grid using local supply. It is hard, however, to provide
security, safety, and privacy in a decentralized and transactive energy system.
On the one hand, prosumers' personal information must be protected from their
trade partners and the system operator. On the other hand, the system must be
protected from careless or malicious trading, which could destabilize the
entire grid. This paper describes Privacy-preserving Energy Transactions
(PETra), which is a secure and safe solution for transactive microgrids that
enables consumers to trade energy without sacrificing their privacy. PETra
builds on distributed ledgers, such as blockchains, and provides anonymity for
communication, bidding, and trading.",consumer protection
http://arxiv.org/abs/1803.10099v1,"Ad targeting is getting more powerful with introduction of new tools, such as
Custom Audiences, behavioral targeting, and Audience Insights. Although this is
beneficial for businesses as it enables people to receive more relevant
advertising, the power of the tools has downsides. In this paper, we focus on
three downsides: privacy violations, microtargeting (i.e., the ability to reach
a specific individual or individuals without their explicit knowledge that they
are the only ones an ad reaches) and ease of reaching marginalized groups.
Using Facebook's ad system as a case study, we demonstrate the feasibility of
such downsides. We then discuss Facebook's response to our responsible
disclosures of the findings and call for additional policy, science, and
engineering work to protect consumers in the rapidly evolving ecosystem of ad
targeting.",consumer protection
http://arxiv.org/abs/1908.02589v1,"Social technologies have made it possible to propagate disinformation and
manipulate the masses at an unprecedented scale. This is particularly alarming
from a security perspective, as humans have proven to be the weakest link when
protecting critical infrastructure in general, and the power grid in
particular. Here, we consider an attack in which an adversary attempts to
manipulate the behavior of energy consumers by sending fake discount
notifications encouraging them to shift their consumption into the peak-demand
period. We conduct surveys to assess the propensity of people to follow-through
on such notifications and forward them to their friends. This allows us to
model how the disinformation propagates through social networks. Finally, using
Greater London as a case study, we show that disinformation can indeed be used
to orchestrate an attack wherein unwitting consumers synchronize their
energy-usage patterns, resulting in blackouts on a city-scale. These findings
demonstrate that in an era when disinformation can be weaponized, system
vulnerabilities arise not only from the hardware and software of critical
infrastructure, but also from the behavior of the consumers.",consumer protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",consumer protection
http://arxiv.org/abs/1601.06372v1,"Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.",consumer protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",consumer protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",consumer protection
http://arxiv.org/abs/1901.03603v1,"Billions of users rely on the security of the Android platform to protect
phones, tablets, and many different types of consumer electronics. While
Android's permission model is well studied, the enforcement of the protection
policy has received relatively little attention. Much of this enforcement is
spread across system services, taking the form of hard-coded checks within
their implementations. In this paper, we propose Authorization Check Miner
(ACMiner), a framework for evaluating the correctness of Android's access
control enforcement through consistency analysis of authorization checks.
ACMiner combines program and text analysis techniques to generate a rich set of
authorization checks, mines the corresponding protection policy for each
service entry point, and uses association rule mining at a service granularity
to identify inconsistencies that may correspond to vulnerabilities. We used
ACMiner to study the AOSP version of Android 7.1.1 to identify 28
vulnerabilities relating to missing authorization checks. In doing so, we
demonstrate ACMiner's ability to help domain experts process thousands of
authorization checks scattered across millions of lines of code.",consumer protection
http://arxiv.org/abs/0712.2587v1,"The code that combines channel estimation and error protection has received
general attention recently, and has been considered a promising methodology to
compensate multi-path fading effect. It has been shown by simulations that such
code design can considerably improve the system performance over the
conventional design with separate channel estimation and error protection
modules under the same code rate. Nevertheless, the major obstacle that
prevents from the practice of the codes is that the existing codes are mostly
searched by computers, and hence exhibit no good structure for efficient
decoding. Hence, the time-consuming exhaustive search becomes the only decoding
choice, and the decoding complexity increases dramatically with the codeword
length. In this paper, by optimizing the signal-tonoise ratio, we found a
systematic construction for the codes for combined channel estimation and error
protection, and confirmed its equivalence in performance to the
computer-searched codes by simulations. Moreover, the structural codes that we
construct by rules can now be maximum-likelihoodly decodable in terms of a
newly derived recursive metric for use of the priority-first search decoding
algorithm. Thus,the decoding complexity reduces significantly when compared
with that of the exhaustive decoder. The extension code design for fast-fading
channels is also presented. Simulations conclude that our constructed extension
code is robust in performance even if the coherent period is shorter than the
codeword length.",consumer protection
http://arxiv.org/abs/cs/0611102v1,"We present a method to secure the complete path between a server and the
local human user at a network node. This is useful for scenarios like internet
banking, electronic signatures, or online voting. Protection of input
authenticity and output integrity and authenticity is accomplished by a
combination of traditional and novel technologies, e.g., SSL, ActiveX, and
DirectX. Our approach does not require administrative privileges to deploy and
is hence suitable for consumer applications. Results are based on the
implementation of a proof-of-concept application for the Windows platform.",consumer protection
http://arxiv.org/abs/1004.4732v2,"In this paper, we calculate energy required to copy one bit of useful
information in the presence of thermal noise. For this purpose, we consider a
quantum system capable of storing one bit of classical information, which is
initially in a mixed state corresponding to temperature T. We calculate how
many of these systems must be used to store useful information and control bits
protecting the content against transmission errors. Finally, we analyze how
adding these extra bits changes the total energy consumed during the copying.",consumer protection
http://arxiv.org/abs/1201.0949v1,"Phishing (password + fishing) is a form of cyber crime based on social
engineering and site spoofing techniques. The name of 'phishing' is a conscious
misspelling of the word 'fishing' and involves stealing confidential data from
a user's computer and subsequently using the data to steal the user's money. In
this paper, we study, discuss and propose the phishing attack stages and types,
technologies for detection of phishing web pages, and conclude our paper with
some important recommendations for preventing phishing for both consumer and
company.",consumer protection
http://arxiv.org/abs/1607.06377v1,"Advanced Metering Infrastructure (AMI) have rapidly become a topic of
international interest as governments have sponsored their deployment for the
purposes of utility service reliability and efficiency, e.g., water and
electricity conservation. Two problems plague such deployments. First is the
protection of consumer privacy. Second is the problem of huge amounts of data
from such deployments. A new architecture is proposed to address these problems
through the use of Aggregators, which incorporate temporary data buffering and
the modularization of utility grid analysis. These Aggregators are used to
deliver anonymized summary data to the central utility while preserving billing
and automated connection services.",consumer protection
http://arxiv.org/abs/1006.2718v1,"RESTful services on the Web expose information through retrievable resource
representations that represent self-describing descriptions of resources, and
through the way how these resources are interlinked through the hyperlinks that
can be found in those representations. This basic design of RESTful services
means that for extracting the most useful information from a service, it is
necessary to understand a service's representations, which means both the
semantics in terms of describing a resource, and also its semantics in terms of
describing its linkage with other resources. Based on the Resource Linking
Language (ReLL), this paper describes a framework for how RESTful services can
be described, and how these descriptions can then be used to harvest
information from these services. Building on this framework, a layered model of
RESTful service semantics allows to represent a service's information in
RDF/OWL. Because REST is based on the linkage between resources, the same model
can be used for aggregating and interlinking multiple services for extracting
RDF data from sets of RESTful services.",terms of service
http://arxiv.org/abs/1303.5926v1,"Service discovery is one of the key problems that has been widely researched
in the area of Service Oriented Architecture (SOA) based systems. Service
category learning is a technique for efficiently facilitating service
discovery. Most approaches for service category learning are based on suitable
similarity distance measures using thresholds. Threshold selection is
essentially difficult and often leads to unsatisfactory accuracy. In this
paper, we have proposed a self-organizing based clustering algorithm called
Semantic Taxonomical Clustering (STC) for taxonomically organizing services
with self-organizing information and knowledge. We have tested the STC
algorithm on both randomly generated data and the standard OWL-S TC dataset. We
have observed promising results both in terms of classification accuracy and
runtime performance compared to existing approaches.",terms of service
http://arxiv.org/abs/1605.02432v1,"In the service landscape, the issues of service selection, negotiation of
Service Level Agreements (SLA), and SLA-compliance monitoring have typically
been used in separate and disparate ways, which affect the quality of the
services that consumers obtain from their providers. In this work, we propose a
broker-based framework to deal with these concerns in an integrated manner for
Software as a Service (SaaS) provisioning. The SaaS Broker selects a suitable
SaaS provider on behalf of the service consumer by using a utility-driven
selection algorithm that ranks the QoS offerings of potential SaaS providers.
Then, it negotiates the SLA terms with that provider based on the quality
requirements of the service consumer. The monitoring infrastructure observes
SLA-compliance during service delivery by using measurements obtained from
third-party monitoring services. We also define a utility-based bargaining
decision model that allows the service consumer to express her sensitivity for
each of the negotiated quality attributes and to evaluate the SaaS provider
offer in each round of negotiation. A use-case with few quality attributes and
their respective utility functions illustrates the approach.",terms of service
http://arxiv.org/abs/1111.5733v1,"The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.",terms of service
http://arxiv.org/abs/1604.07642v1,"Service-Oriented Computing is a paradigm that uses services as building
blocks for building distributed applications. The primary motivation for
orchestrating services in the cloud used to be distributed business processes,
which drove the standardization of the Business Process Execution Language
(BPEL) and its central notion that a service is a business process. In recent
years, there has been a transition towards other motivations for orchestrating
services in the cloud, {\em e.g.}, XaaS, RMAD. Although it is theoretically
possible to make all of those services into WSDL/SOAP services, it would be too
complicated and costly for industry adoption. Therefore, the central notion
that a service is a business process is too restrictive. Instead, we view a
service as a technology neutral, loosely coupled, location transparent
procedure. With these ideas in mind, we introduce a new approach to services
orchestration: Ozy, a general orchestration container. We define this new
approach in terms of existing technology, and we show that the Ozy container
relaxes many traditional constraints and allows for simpler, more feature-rich
applications.",terms of service
http://arxiv.org/abs/1903.04709v1,"An edge computing environment features multiple edge servers and multiple
service clients. In this environment, mobile service providers can offload
client-side computation tasks from service clients' devices onto edge servers
to reduce service latency and power consumption experienced by the clients. A
critical issue that has yet to be properly addressed is how to allocate edge
computing resources to achieve two optimization objectives: 1) minimize the
service cost measured by the service latency and the power consumption
experienced by service clients; and 2) maximize the service capacity measured
by the number of service clients that can offload their computation tasks in
the long term. This paper formulates this long-term problem as a stochastic
optimization problem and solves it with an online algorithm based on Lyapunov
optimization. This NP-hard problem is decomposed into three sub-problems, which
are then solved with a suite of techniques. The experimental results show that
our approach significantly outperforms two baseline approaches.",terms of service
http://arxiv.org/abs/1710.01476v2,"An increasing number of technology enterprises are adopting cloud-native
architectures to offer their web-based products, by moving away from
privately-owned data-centers and relying exclusively on cloud service
providers. As a result, cloud vendors have lately increased, along with the
estimated annual revenue they share. However, in the process of selecting a
provider's cloud service over the competition, we observe a lack of universal
common ground in terms of terminology, functionality of services and billing
models. This is an important gap especially under the new reality of the
industry where each cloud provider has moved towards his own service taxonomy,
while the number of specialized services has grown exponentially. This work
discusses cloud services offered by four dominant, in terms of their current
market share, cloud vendors. We provide a taxonomy of their services and
sub-services that designates major service families namely computing, storage,
databases, analytics, data pipelines, machine learning, and networking. The aim
of such clustering is to indicate similarities, common design approaches and
functional differences of the offered services. The outcomes are essential both
for individual researchers, and bigger enterprises in their attempt to identify
the set of cloud services that will utterly meet their needs without
compromises. While we acknowledge the fact that this is a dynamic industry,
where new services arise constantly, and old ones experience important updates,
this study paints a solid image of the current offerings and gives prominence
to the directions that cloud service providers are following.",terms of service
http://arxiv.org/abs/1111.5493v2,"Collaboration models and tools aim at improving the efficiency and
effectiveness of human interactions. Although social relations among
collaborators have been identified as having a strong influence on
collaboration, they are still insufficiently taken into account in current
collaboration models and tools. In this paper, the concept of service protocols
is proposed as a model for human interactions supporting social requirements,
i.e., sets of constraints on the relations among interacting humans. Service
protocols have been proposed as an answer to the need for models for human
interactions in which not only the potential sequences of activities are
specified-as in process models-but also the constraints on the relations among
collaborators. Service protocols are based on two main ideas: first, service
protocols are rooted in the service-oriented architecture (SOA): each service
protocol contains a service-oriented summary which provides a representation of
the activities of an associated process model in SOA terms. Second, a
class-based graph-referred to as a service network schema-restricts the set of
potential service elements that may participate in the service protocol by
defining constraints on nodes and constraints on arcs, i.e., social
requirements. Another major contribution to the modelling of human interactions
is a unified approach organized around the concept of service, understood in a
broad sense with services being not only Web services, but also provided by
humans.",terms of service
http://arxiv.org/abs/1504.02052v1,"Exchange of services and resources in, or over, networks is attracting
nowadays renewed interest. However, despite the broad applicability and the
extensive study of such models, e.g., in the context of P2P networks, many
fundamental questions regarding their properties and efficiency remain
unanswered. We consider such a service exchange model and analyze the users'
interactions under three different approaches. First, we study a centrally
designed service allocation policy that yields the fair total service each user
should receive based on the service it others to the others. Accordingly, we
consider a competitive market where each user determines selfishly its
allocation policy so as to maximize the service it receives in return, and a
coalitional game model where users are allowed to coordinate their policies. We
prove that there is a unique equilibrium exchange allocation for both game
theoretic formulations, which also coincides with the central fair service
allocation. Furthermore, we characterize its properties in terms of the
coalitions that emerge and the equilibrium allocations, and analyze its
dependency on the underlying network graph. That servicing policy is the
natural reference point to the various mechanisms that are currently proposed
to incentivize user participation and improve the efficiency of such networked
service (or, resource) exchange markets.",terms of service
http://arxiv.org/abs/cs/0212051v1,"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.",terms of service
http://arxiv.org/abs/1305.6011v2,"Information system evolved as the evolution of information technology. The
current state of information technology, placed the internet as a main
resources of computing. Cloud technology as the backbone of internet has been
utilized as a powerful computing resources. Therefore, cloud introduced new
term of service oriented technology, popular with ""as a service"" kind of name.
In this paper, the service oriented paradigm will be used to address future
trend of information system. Thus, this paper try to introduce the term
""information system as a service"", holistic view of infrastructure as a
service, platform as a service, software as a service, and data as a service.",terms of service
http://arxiv.org/abs/1106.1523v1,"Interactive query expansion can assist users during their query formulation
process. We conducted a user study with over 4,000 unique visitors and four
different design approaches for a search term suggestion service. As a basis
for our evaluation we have implemented services which use three different
vocabularies: (1) user search terms, (2) terms from a terminology service and
(3) thesaurus terms. Additionally, we have created a new combined service which
utilizes thesaurus term and terms from a domain-specific search term
re-commender. Our results show that the thesaurus-based method clearly is used
more often compared to the other single-method implementations. We interpret
this as a strong indicator that term suggestion mechanisms should be
domain-specific to be close to the user terminology. Our novel combined
approach which interconnects a thesaurus service with additional statistical
relations out-performed all other implementations. All our observations show
that domain-specific vocabulary can support the user in finding alternative
concepts and formulating queries.",terms of service
http://arxiv.org/abs/1907.13293v1,"Blockchain is an innovative distributed ledger technology which has attracted
a wide range of interests for building the next generation of applications to
address lack-of-trust issues in business. Blockchain as a service (BaaS) is a
promising solution to improve the productivity of blockchain application
development. However, existing BaaS deployment solutions are mostly
vendor-locked: they are either bound to a cloud provider or a blockchain
platform. In addition to deployment, design and implementation of
blockchain-based applications is a hard task requiring deep expertise.
Therefore, this paper presents a unified blockchain as a service platform
(uBaaS) to support both design and deployment of blockchain-based applications.
The services in uBaaS include deployment as a service, design pattern as a
service and auxiliary services. In uBaaS, deployment as a service is platform
agnostic, which can avoid lock-in to specific cloud platforms, while design
pattern as a service applies design patterns for data management and smart
contract design to address the scalability and security issues of blockchain.
The proposed solutions are evaluated using a real-world quality tracing use
case in terms of feasibility and scalability.",terms of service
http://arxiv.org/abs/1501.04298v1,"As the number of Web services with the same or similar functions increases
steadily on the Internet, nowadays more and more service consumers pay great
attention to the non-functional properties of Web services, also known as
quality of service (QoS), when finding and selecting appropriate Web services.
For most of the QoS-aware Web service recommendation systems, the list of
recommended Web services is generally obtained based on a rating-oriented
prediction approach, aiming at predicting the potential ratings that an active
user may assign to the unrated services as accurately as possible. However, in
some application scenarios, high accuracy of rating prediction may not
necessarily lead to a satisfactory recommendation result. In this paper, we
propose a ranking-oriented hybrid approach by combining the item-based
collaborative filtering and latent factor models to address the problem of Web
services ranking. In particular, the similarity between two Web services is
measured in terms of the correlation coefficient between their rankings instead
of between the traditional QoS ratings. Besides, we also improve the measure
NDCG (Normalized Discounted Cumulative Gain) for evaluating the accuracy of the
top K recommendations returned in ranked order. Comprehensive experiments on
the QoS data set composed of real-world Web services are conducted to test our
approach, and the experimental results demonstrate that our approach
outperforms other competing approaches.",terms of service
http://arxiv.org/abs/1611.05380v2,"The emerging marketplace for online free services in which service providers
earn revenue from using consumer data in direct and indirect ways has lead to
significant privacy concerns. This leads to the following question: can the
online marketplace sustain multiple service providers (SPs) that offer
privacy-differentiated free services? This paper studies the problem of market
segmentation for the free online services market by augmenting the classical
Hotelling model for market segmentation analysis to include the fact that for
the free services market, a consumer values service not in monetized terms but
by its quality of service (QoS) and that the differentiator of services is not
product price but the privacy risk advertised by a SP. Building upon the
Hotelling model, this paper presents a parametrized model for SP profit and
consumer valuation of service for both the two- and multi-SP problems to show
that: (i) when consumers place a high value on privacy, it leads to a lower use
of private data by SPs (i.e., their advertised privacy risk reduces), and thus,
SPs compete on the QoS; (ii) SPs that are capable of differentiating on
services that do not directly target consumers gain larger market share; and
(iii) a higher valuation of privacy by consumers forces SPs with smaller
untargeted revenue to offer lower privacy risk to attract more consumers. The
work also illustrates the market segmentation problem for more than two SPs and
highlights the instability of such markets.",terms of service
http://arxiv.org/abs/1608.08799v2,"Efficient service composition in real time while providing necessary Quality
of Service (QoS) guarantees has been a challenging research problem with ever
growing complexity. Several heuristic based approaches with diverse proposals
for taming the scale and complexity of web service composition, have been
proposed in literature. In this paper, we present a new approach for efficient
service composition based on abstraction refinement. Instead of considering
individual services during composition, we propose several abstractions to form
service groups and the composition is done on these abstract services.
Abstraction reduces the search space significantly and thereby can be done
reasonably fast. While this can expedite solution construction to a great
extent, this also entails a possibility that it may fail to generate any
solution satisfying the QoS constraints, though the individual services
construct a valid solution. Hence, we propose to refine an abstraction to
generate the composite solution with desired QoS values. A QoS satisfying
solution, if one exists, can be constructed with multiple iterations of
abstraction refinement. While in the worst case, this approach may end up
exploring the complete composition graph constructed on individual services, on
an average, the solution can be achieved on the abstract graph. The abstraction
refinement techniques give a significant speed-up compared to the traditional
composition techniques. Experimental results on real benchmarks show the
efficiency of our proposed mechanism in terms of time and the number of
services considered for composition.",terms of service
http://arxiv.org/abs/1308.5397v1,"Traffic shaping is a mechanism used by Internet Service Providers (ISPs) to
limit subscribers' traffic based on their service contracts. This paper
investigates the current implementation of traffic shaping based on the token
bucket filter (TBF), discusses its advantages and disadvantages, and proposes a
cooperative TBF that can improve subscribers' quality of service (QoS)/quality
of experience (QoE) without compromising business aspects of the service
contract model by proportionally allocating excess bandwidth from inactive
subscribers to active ones based on the long-term bandwidths per their service
contracts.",terms of service
http://arxiv.org/abs/1409.7233v1,"In this paper we propose I/O state transition diagrams for service
description In contrast to other techniques like for example Statecharts we
allow to model non atomic services by sequences of transitions This is
especially important in a distributed system where concurrent service
invocation cannot be prohibited We give a mathematical model of object
behaviour based on concurrent and sequential messages Then we give a precise
semantics of the service descriptions in terms of the mathematical model.",terms of service
http://arxiv.org/abs/1708.01412v1,"Background: Cloud Computing is increasingly booming in industry with many
competing providers and services. Accordingly, evaluation of commercial Cloud
services is necessary. However, the existing evaluation studies are relatively
chaotic. There exists tremendous confusion and gap between practices and theory
about Cloud services evaluation. Aim: To facilitate relieving the
aforementioned chaos, this work aims to synthesize the existing evaluation
implementations to outline the state-of-the-practice and also identify research
opportunities in Cloud services evaluation. Method: Based on a conceptual
evaluation model comprising six steps, the Systematic Literature Review (SLR)
method was employed to collect relevant evidence to investigate the Cloud
services evaluation step by step. Results: This SLR identified 82 relevant
evaluation studies. The overall data collected from these studies essentially
represent the current practical landscape of implementing Cloud services
evaluation, and in turn can be reused to facilitate future evaluation work.
Conclusions: Evaluation of commercial Cloud services has become a world-wide
research topic. Some of the findings of this SLR identify several research gaps
in the area of Cloud services evaluation (e.g., the Elasticity and Security
evaluation of commercial Cloud services could be a long-term challenge), while
some other findings suggest the trend of applying commercial Cloud services
(e.g., compared with PaaS, IaaS seems more suitable for customers and is
particularly important in industry). This SLR study itself also confirms some
previous experiences and reveals new Evidence-Based Software Engineering (EBSE)
lessons.",terms of service
http://arxiv.org/abs/1904.05864v1,"Although network functions virtualization and software-defined networking
offer many dynamic features such as flexibility, scalability, and
programmability for easy provisioning of services at a lesser cost and time
through service function chaining, it introduces new challenges in terms of
reliability, availability, and latency of services. Particularly,
softwarization of network and service functions (e.g., virtualization, anything
as a service, dynamic virtual chaining, and routing) impose high possibility of
network failures due to software issues than hardware. In this letter, we
propose a novel solution called eRESERV to enhance the reliability of service
chains in 5G while meeting the service level agreements.",terms of service
http://arxiv.org/abs/1905.09771v1,"Network slicing is increasingly used to partition network infrastructure
between different mobile services. Precise service-wise mobile traffic
forecasting becomes essential in this context, as mobile operators seek to
pre-allocate resources to each slice in advance, to meet the distinct
requirements of individual services. This paper attacks the problem of
multi-service mobile traffic forecasting using a sequence-to-sequence (S2S)
learning paradigm and convolutional long short-term memories (ConvLSTMs). The
proposed architecture is designed so as to effectively extract complex
spatiotemporal features of mobile network traffic and predict with high
accuracy the future demands for individual services at city scale. We conduct
experiments on a mobile traffic dataset collected in a large European
metropolis, demonstrating that the proposed S2S-ConvLSTM can forecast the
mobile traffic volume produced by tens of different services in advance of up
to one hour, by just using measurements taken during the past hour. In
particular, our solution achieves mean absolute errors (MAE) at antenna level
that are below 13KBps, outperforming other deep learning approaches by up to
31.2%.",terms of service
http://arxiv.org/abs/1003.5440v1,"The wide-band code division multiple access (WCDMA) based 3G and beyond
cellular mobile wireless networks are expected to provide a diverse range of
multimedia services to mobile users with guaranteed quality of service (QoS).
To serve diverse quality of service requirements of these networks it
necessitates new radio resource management strategies for effective utilization
of network resources with coding schemes. Call admission control (CAC) is a
significant component in wireless networks to guarantee quality of service
requirements and also to enhance the network resilience. In this paper capacity
enhancement for WCDMA network with convolutional coding scheme is discussed and
compared with block code and without coding scheme to achieve a better balance
between resource utilization and quality of service provisioning. The model of
this network is valid for the real-time (RT) and non-real-time (NRT) services
having different data rate. Simulation results demonstrate the effectiveness of
the network using convolutional code in terms of capacity enhancement and QoS
of the voice and video services.",terms of service
http://arxiv.org/abs/1205.5960v1,"The semantic e-government is a new application field accompanying the
development of semantic web where the ontologies have become a fertile field of
investigation. This is due firstly to both the complexity and the size of
e-government systems and secondly to the importance of the issues. However,
permitting easy and personalized access to e-government services has become, at
this juncture, an arduous and not spontaneous process. Indeed, the provided
e-gov services to the user represent a critical contact point between
administrations and users. The encountered problems in the e-gov services
retrieving process are: the absence of an integrated one-stop government, the
difficulty of localizing the services' sources, the lack of mastery of search
terms and the deficiency of multilingualism of the online services. In order to
solve these problems, to facilitate access to e-gov services and to satisfy the
needs of potential users, we propose an original approach to this issue. This
approach incorporates a semantic layer as a crucial element in the retrieving
process. It consists in implementing a personalized search system that
integrates ontology of the e-gov domain in this process.",terms of service
http://arxiv.org/abs/1512.07685v1,"We propose the use of structured natural language (English) in specifying
service choreographies, focusing on the what rather than the how of the
required coordination of participant services in realising a business
application scenario. The declarative approach we propose uses the OMG standard
Semantics of Business Vocabulary and Rules (SBVR) as a modelling language. The
service choreography approach has been proposed for describing the global
orderings of the invocations on interfaces of participant services. We
therefore extend SBVR with a notion of time which can capture the coordination
of the participant services, in terms of the observable message exchanges
between them. The extension is done using existing modelling constructs in
SBVR, and hence respects the standard specification. The idea is that users -
domain specialists rather than implementation specialists - can verify the
requested service composition by directly reading the structured English used
by SBVR. At the same time, the SBVR model can be represented in formal logic so
it can be parsed and executed by a machine.",terms of service
http://arxiv.org/abs/1306.4063v1,"Quality of Service (QoS) has gained more importance with the increase in
usage and adoption of web services. In recent years, various tools and
techniques developed for measurement and evaluation of QoS of web services.
There are commercial as well as open-source tools available today which are
being used for monitoring and testing QoS for web services. These tools
facilitate in QoS measurement and analysis and are helpful in evaluation of
service performance in real-time network. In this paper, we describe three
popular open-source tools and compare them in terms of features, usability,
performance, and software requirements. Results of the comparison will help in
adoption and usage of these tools, and also promote development and usage of
open-source web service testing tools.",terms of service
http://arxiv.org/abs/1502.02840v1,"In this paper we present a theoretical analysis of graph-based service
composition in terms of its dependency with service discovery. Driven by this
analysis we define a composition framework by means of integration with
fine-grained I/O service discovery that enables the generation of a graph-based
composition which contains the set of services that are semantically relevant
for an input-output request. The proposed framework also includes an optimal
composition search algorithm to extract the best composition from the graph
minimising the length and the number of services, and different graph
optimisations to improve the scalability of the system. A practical
implementation used for the empirical analysis is also provided. This analysis
proves the scalability and flexibility of our proposal and provides insights on
how integrated composition systems can be designed in order to achieve good
performance in real scenarios for the Web.",terms of service
http://arxiv.org/abs/1612.05416v2,"The ""Smart City"" (SC) concept revolves around the idea of embodying
cutting-edge ICT solutions in the very fabric of future cities, in order to
offer new and better services to citizens while lowering the city management
costs, both in monetary, social, and environmental terms. In this framework,
communication technologies are perceived as subservient to the SC services,
providing the means to collect and process the data needed to make the services
function. In this paper, we propose a new vision in which technology and SC
services are designed to take advantage of each other in a symbiotic manner.
According to this new paradigm, which we call ""SymbioCity"", SC services can
indeed be exploited to improve the performance of the same communication
systems that provide them with data. Suggestive examples of this symbiotic
ecosystem are discussed in the paper. The dissertation is then substantiated in
a proof-of-concept case study, where we show how the traffic monitoring service
provided by the London Smart City initiative can be used to predict the density
of users in a certain zone and optimize the cellular service in that area.",terms of service
http://arxiv.org/abs/1707.01064v2,"High transmission rate and secure communication have been identified as the
key targets that need to be effectively addressed by fifth generation (5G)
wireless systems. In this context, the concept of physical-layer security
becomes attractive, as it can establish perfect security using only the
characteristics of wireless medium. Nonetheless, to further increase the
spectral efficiency, an emerging concept, termed physical-layer service
integration (PHY-SI), has been recognized as an effective means. Its basic idea
is to combine multiple coexisting services, i.e., multicast/broadcast service
and confidential service, into one integral service for one-time transmission
at the transmitter side. This article first provides a tutorial on typical
PHY-SI models. Furthermore, we propose some state-of-the-art solutions to
improve the overall performance of PHY-SI in certain important communication
scenarios. In particular, we highlight the extension of several concepts
borrowed from conventional single-service communications, such as artificial
noise (AN), eigenmode transmission etc., to the scenario of PHY-SI. These
techniques are shown to be effective in the design of reliable and robust
PHY-SI schemes. Finally, several potential research directions are identified
for future work.",terms of service
http://arxiv.org/abs/1710.06190v1,"In their $1996$ paper Anantharam and Verd\'u showed that feedback does not
increase the capacity of a queue when the service time is exponentially
distributed. Whether this conclusion holds for general service times has
remained an open question which this paper addresses.
  Two main results are established for both the discrete-time and the
continuous-time models. First, a sufficient condition on the service
distribution for feedback to increase capacity under FIFO service policy.
Underlying this condition is a notion of weak feedback wherein instead of the
queue departure times the transmitter is informed about the instants when
packets start to be served. Second, a condition in terms of output entropy rate
under which feedback does not increase capacity. This condition is general in
that it depends on the output entropy rate of the queue but explicitly depends
neither on the queue policy nor on the service time distribution. This
condition is satisfied, for instance, by queues with LCFS service policies and
bounded service times.",terms of service
http://arxiv.org/abs/1406.5354v1,"With the rapid development of high-speed railway (HSR), how to provide the
passengers with multimedia services has attracted increasing attention. A key
issue is to develop an effective scheduling algorithm for multiple services
with different quality of service (QoS) requirements. In this paper, we
investigate the downlink service scheduling problem in HSR network taking
account of end-to-end deadline constraints and successfully packet delivery
ratio requirements. Firstly, by exploiting the deterministic high-speed train
trajectory, we present a time-distance mapping in order to obtain the highly
dynamic link capacity effectively. Next, a novel service model is developed for
deadline constrained services with delivery ratio requirements, which enables
us to turn the delivery ratio requirement into a single queue stability
problem. Based on the Lyapunov drift, the optimal scheduling problem is
formulated and the corresponding scheduling service algorithm is proposed by
stochastic network optimization approach. Simulation results show that the
proposed algorithm outperforms the conventional schemes in terms of QoS
requirements.",terms of service
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",unfair terms
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",unfair terms
http://arxiv.org/abs/1705.08804v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1706.09838v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1002.4833v1,"The number of users using wireless Local Area Network is increasing
exponentially and their behavior is changing day after day. Nowadays, users of
wireless LAN are using huge amount of bandwidth because of the explosive growth
of some services and applications such as video sharing. This situation imposes
massive pressure on the wireless LAN performance especially in term of fairness
among wireless stations. The limited resources are not distributed fairly in
saturated conditions. The most important resource is the access point buffer
space. This importance is a result of access point being the bottleneck between
two different types of networks. These two types are wired network with
relatively huge bandwidth and wireless network with much smaller bandwidth.
Also the unfairness problem is keep getting worse because of the greedy nature
Transmission Control Protocol (TCP). In this paper, we conduct a comprehensive
study on wireless LAN dynamics and proposed a new mathematical model that
describes the performance and effects of its behavior. We validate the proposed
model by using the simulation technique. The proposed model was able to produce
very good approximation in most of the cases. It also gave us a great insight
into the effective variables in the wireless LAN behavior and what are the
dimensions of the unfairness problem.",unfair terms
http://arxiv.org/abs/1607.07021v1,"We consider single-hop topologies with saturated transmitting nodes, using
IEEE~802.11 DCF for medium access. However, unlike the conventional WiFi, we
study systems where one or more of the protocol parameters are different from
the standard, and/or where the propagation delays among the nodes are not
negligible compared to the duration of a backoff slot. We observe that for
several classes of protocol parameters, and for large propagation delays, such
systems exhibit a certain performance anomaly known as short term unfairness,
which may lead to severe performance degradation. The standard fixed point
analysis technique (and its simple extensions) do not predict the system
behavior well in such cases; a mean field model based asymptotic approach also
is not adequate to predict the performance for networks of practical sizes in
such cases. We provide a detailed stochastic model that accurately captures the
system evolution. Since an exact analysis of this model is computationally
intractable, we develop a novel approximate, but accurate, analysis that uses a
parsimonious state representation for computational tractability. Apart from
providing insights into the system behavior, the analytical method is also able
to quantify the extent of short term unfairness in the system, and can
therefore be used for tuning the protocol parameters to achieve desired
throughput and fairness objectives.",unfair terms
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",unfair terms
http://arxiv.org/abs/0806.1093v1,"We present the station-based unfair access problem among the uplink and the
downlink stations in the IEEE 802.11e infrastructure Basic Service Set (BSS)
when the default settings of the Enhanced Distributed Channel Access (EDCA)
parameters are used. We discuss how the transport layer protocol
characteristics alleviate the unfairness problem. We design a simple,
practical, and standard-compliant framework to be employed at the Access Point
(AP) for fair and efficient access provisioning. A dynamic measurement-based
EDCA parameter adaptation block lies in the core of this framework. The
proposed framework is unique in the sense that it considers the characteristic
differences of Transmission Control Protocol (TCP) and User Datagram Protocol
(UDP) flows and the coexistence of stations with varying bandwidth or
Quality-of-Service (QoS) requirements. Via simulations, we show that our
solution provides short- and long-term fair access for all stations in the
uplink and downlink employing TCP and UDP flows with non-uniform packet rates
in a wired-wireless heterogeneous network. In the meantime, the QoS
requirements of coexisting real-time flows are also maintained.",unfair terms
http://arxiv.org/abs/1510.01125v1,"-Performance of Vehicular Adhoc Networks (VANETs) in high node density
situation has long been a major field of studies. Particular attention has been
paid to the frequent exchange of Cooperative Awareness Messages (CAMs) on which
many road safety applications rely. In the present paper, se focus on the
European Telecommunications Standard Institute (ETSI) Decentralized Congestion
Control (DCC) mechanism, particularly on the evaluation of its facility layers
component when applied in the context of dense networks. For this purpose, a
set of simulations has been conducted over several scenarios, considering rural
highway and urban mobility in order to investigate unfairness and oscillation
issues, and analyze the triggering factors. The experimental results show that
the latest technical specification of the ETSI DCC presents a significant
enhancement in terms of fairness. In contrast, the stability criterion leaves
room for improvement as channel load measurement presents (i) considerable
fluctuations when only the facility layer control is applied and (i.i) severe
state oscillation when different DCC control methods are combined.",unfair terms
http://arxiv.org/abs/1509.03815v1,"In this paper, we revisit two fundamental results of the self-stabilizing
literature about silent BFS spanning tree constructions: the Dolev et al
algorithm and the Huang and Chen's algorithm. More precisely, we propose in the
composite atomicity model three straightforward adaptations inspired from those
algorithms. We then present a deep study of these three algorithms. Our results
are related to both correctness (convergence and closure, assuming a
distributed unfair daemon) and complexity (analysis of the stabilization time
in terms of rounds and steps).",unfair terms
http://arxiv.org/abs/0806.1089v1,"When the stations in an IEEE 802.11 infrastructure Basic Service Set (BSS)
employ Transmission Control Protocol (TCP) in the transport layer, this
exacerbates per-flow unfair access which is a direct result of uplink/downlink
bandwidth asymmetry in the BSS. We propose a novel and simple analytical model
to approximately calculate the per-flow TCP congestion window limit that
provides fair and efficient TCP access in a heterogeneous wired-wireless
scenario. The proposed analysis is unique in that it considers the effects of
varying number of uplink and downlink TCP flows, differing Round Trip Times
(RTTs) among TCP connections, and the use of delayed TCP Acknowledgment (ACK)
mechanism. Motivated by the findings of this analysis, we design a link layer
access control block to be employed only at the Access Point (AP) in order to
resolve the unfair access problem. The novel and simple idea of the proposed
link layer access control block is employing a congestion control and filtering
algorithm on TCP ACK packets of uplink flows, thereby prioritizing the access
of TCP data packets of downlink flows at the AP. Via simulations, we show that
short- and long-term fair access can be provisioned with the introduction of
the proposed link layer access control block to the protocol stack of the AP
while improving channel utilization and access delay.",unfair terms
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",unfair terms
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",unfair terms
http://arxiv.org/abs/1903.01209v2,"Most existing notions of algorithmic fairness are one-shot: they ensure some
form of allocative equality at the time of decision making, but do not account
for the adverse impact of the algorithmic decisions today on the long-term
welfare and prosperity of certain segments of the population. We take a broader
perspective on algorithmic fairness. We propose an effort-based measure of
fairness and present a data-driven framework for characterizing the long-term
impact of algorithmic policies on reshaping the underlying population.
Motivated by the psychological literature on \emph{social learning} and the
economic literature on equality of opportunity, we propose a micro-scale model
of how individuals may respond to decision-making algorithms. We employ
existing measures of segregation from sociology and economics to quantify the
resulting macro-scale population-level change. Importantly, we observe that
different models may shift the group-conditional distribution of qualifications
in different directions. Our findings raise a number of important questions
regarding the formalization of fairness for decision-making models.",unfair terms
http://arxiv.org/abs/1511.06035v7,"Applications running in modern multithreaded environments are sometimes
\emph{over-threaded}. The excess threads do not improve performance, and in
fact may act to degrade performance via \emph{scalability collapse}. Often,
such software also has highly contended locks. We opportunistically leverage
the existence of such locks by modifying the lock admission policy so as to
intentionally limit the number of threads circulating over the lock in a given
period. Specifically, if there are more threads circulating than are necessary
to keep the lock saturated, our approach will selectively cull and passivate
some of those threads. We borrow the concept of \emph{swapping} from the field
of memory management and intentionally impose \emph{concurrency restriction}
(CR) if a lock is oversubscribed. In the worst case CR does no harm, but it
often yields performance benefits. The resultant admission order is unfair over
the short term but we explicitly provide long-term fairness by periodically
shifting threads between the set of passivated threads and those actively
circulating. Our approach is palliative, but often effective.",unfair terms
http://arxiv.org/abs/1905.11260v3,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called r-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and learning algorithm used as a black-box. We provide a
fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves O(log(T)) r-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1907.10516v1,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called $r$-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and the learning algorithm used as a black-box. We provide
a fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves $O(\ln T)$ $r$-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1403.4357v1,"High speed railways (HSRs) have been deployed widely all over the world in
recent years. Different from traditional cellular communication, its high
mobility makes it essential to implement power allocation along the time. In
the HSR case, the transmission rate depends greatly on the distance between the
base station (BS) and the train. As a result, the train receives a time varying
data rate service when passing by a BS. It is clear that the most efficient
power allocation will spend all the power when the train is nearest from the
BS, which will cause great unfairness along the time. On the other hand, the
channel inversion allocation achieves the best fairness in terms of constant
rate transmission. However, its power efficiency is much lower. Therefore, the
power efficiency and the fairness along time are two incompatible objects. For
the HSR cellular system considered in this paper, a trade-off between the two
is achieved by proposing a temporal proportional fair power allocation scheme.
Besides, near optimal closed form solution and one algorithm finding the
$\epsilon$-optimal allocation are presented.",unfair terms
http://arxiv.org/abs/1805.12572v3,"We compare and contrast fourteen measures that have been proposed for the
purpose of quantifying partisan gerrymandering. We consider measures that,
rather than examining the shapes of districts, utilize only the partisan vote
distribution among districts. The measures considered are two versions of
partisan bias; the efficiency gap and several of its variants; the mean-median
difference and the equal vote weight standard; the declination and one variant;
and the lopsided-means test. Our primary means of evaluating these measures is
a suite of hypothetical elections we classify from the start as fair or unfair.
We conclude that the declination is the most successful measure in terms of
avoiding false positives and false negatives on the elections considered. We
include in an appendix the most extreme outliers for each measure among
historical congressional and state legislative elections.",unfair terms
http://arxiv.org/abs/1806.09936v1,"Black box systems for automated decision making, often based on machine
learning over (big) data, map a user's features into a class or a score without
exposing the reasons why. This is problematic not only for lack of
transparency, but also for possible biases hidden in the algorithms, due to
human prejudices and collection artifacts hidden in the training data, which
may lead to unfair or wrong decisions. We introduce the local-to-global
framework for black box explanation, a novel approach with promising early
results, which paves the road for a wide spectrum of future developments along
three dimensions: (i) the language for expressing explanations in terms of
highly expressive logic-based rules, with a statistical and causal
interpretation; (ii) the inference of local explanations aimed at revealing the
logic of the decision adopted for a specific instance by querying and auditing
the black box in the vicinity of the target instance; (iii), the bottom-up
generalization of the many local explanations into simple global ones, with
algorithms that optimize the quality and comprehensibility of explanations.",unfair terms
http://arxiv.org/abs/1905.12535v1,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",unfair terms
http://arxiv.org/abs/1909.01825v1,"This document is an Internet Appendix of paper entitled ""Sequential
Bargaining Based Incentive Mechanism for Collaborative Internet Access"". It
includes information about LTE signal metrics, results of idle state
experiments, and linear regression assumptions of the models presented in the
related paper.",Internet accessibility
http://arxiv.org/abs/cs/0412119v1,"The rapid growth of the internet in general and of bandwidth capacity at
internet clients in particular poses increasing computation and bandwidth
demands on internet servers. Internet access technologies like ADSL [DSL],
Cable Modem and Wireless modem allow internet clients to access the internet
with orders of magnitude more bandwidth than using traditional modems. We
present CDTP a distributed transfer protocol that allows clients to cooperate
and therefore remove the strain from the internet server thus achieving much
better performance than traditional transfer protocols (e.g. FTP [FTP]). The
CDTP server and client tools are presented also as well as results of
experiments. Finally a bandwidth measurement technique is presented. CDTP tools
use this technique to differentiate between slow and fast clients.",Internet accessibility
http://arxiv.org/abs/cs/0609149v1,"In this article, we first provide a taxonomy of dynamic spectrum access. We
then focus on opportunistic spectrum access, the overlay approach under the
hierarchical access model of dynamic spectrum access. we aim to provide an
overview of challenges and recent developments in both technological and
regulatory aspects of opportunistic spectrum access.",Internet accessibility
http://arxiv.org/abs/1406.2516v1,"Unlike telephone operators, which pay termination fees to reach the users of
another network, Internet Content Providers (CPs) do not pay the Internet
Service Providers (ISPs) of users they reach. While the consequent cross
subsidization to CPs has nurtured content innovations at the edge of the
Internet, it reduces the investment incentives for the access ISPs to expand
capacity. As potential charges for terminating CPs' traffic are criticized
under the net neutrality debate, we propose to allow CPs to voluntarily
subsidize the usagebased fees induced by their content traffic for end-users.
We model the regulated subsidization competition among CPs under a neutral
network and show how deregulation of subsidization could increase an access
ISP's utilization and revenue, strengthening its investment incentives.
Although the competition might harm certain CPs, we find that the main cause
comes from high access prices rather than the existence of subsidization. Our
results suggest that subsidization competition will increase the
competitiveness and welfare of the Internet content market; however, regulators
might need to regulate access prices if the access ISP market is not
competitive enough. We envision that subsidization competition could become a
viable model for the future Internet.",Internet accessibility
http://arxiv.org/abs/1604.08243v1,"Recent years have witnessed several initiatives on enabling Internet access
to the next three billion people. Access to the Internet necessarily translates
to access to its services. This means that the goal of providing Internet
access requires ac- cess to its critical service infrastructure, which are
currently hosted in the cloud. However, recent works have pointed out that the
current cloud centric nature of the Internet is a fundamental barrier for
Internet access in rural/remote areas as well as in developing regions. It is
important to explore (low cost) solutions such as micro cloud infrastructures
that can provide services at the edge of the network (potentially on demand),
right near the users. In this paper, we present Cloudrone- a preliminary idea
of deploying a lightweight micro cloud infrastructure in the sky using
indigenously built low cost drones, single board computers and lightweight
Operating System virtualization technologies. Our paper lays out the
preliminary ideas on such a system that can be instantaneously deployed on
demand. We describe an initial design of the Cloudrone and provide a
preliminary evaluation of the proposed system mainly focussed on the
scalability issues of supporting multiple services and users.",Internet accessibility
http://arxiv.org/abs/1907.04570v1,"End-users and governments force network operators to deploy faster Internet
access services everywhere. Access technologies such as FTTx, VDSL2, DOCSIS3.0
can provide such services in cities. However, it is not cost-effective for
network operators to deploy them in less densely populated regions. The
recently proposed Hybrid Access Networks allow to boost xDSL networks by using
the available capacity in existing LTE networks. We first present the three
architectures defined by the Broadband Forum for such Hybrid Access Networks.
Then we describe our experience with the implementation and the deployment of
Multipath TCP-based Hybrid Access Networks.",Internet accessibility
http://arxiv.org/abs/1603.07431v1,"Decades of experience have shown that there is no single one-size-fits-all
solution that can be used to provision Internet globally and that invariably
there are tradeoffs in the design of Internet. Despite the best efforts of
networking researchers and practitioners, an ideal Internet experience is
inaccessible to an overwhelming majority of people the world over, mainly due
to the lack of cost efficient ways of provisioning high-performance global
Internet. In this paper, we argue that instead of an exclusive focus on a
utopian goal of universally accessible ""ideal networking"" (in which we have
high throughput and quality of service as well as low latency and congestion),
we should consider providing ""approximate networking"" through the adoption of
context-appropriate tradeoffs. Approximate networking can be used to implement
a pragmatic tiered global access to the Internet for all (GAIA) system in which
different users the world over have different context-appropriate (but still
contextually functional) Internet experience.",Internet accessibility
http://arxiv.org/abs/cs/0109113v1,"This focused study on state-level policy and access patterns contributes to a
fuller understanding of how these invisible barriers work to structure access
and define rural communities. Combining both quantitative and qualitative data,
this study examines the role of geo-policy barriers in one of the largest and
most rural states in the nation.
  Expanded Area Service policies are state policies wherein phone customers can
expand their local calling area. Because useful Internet access requires a
flat-price connection, EAS policies can play a crucial role in connecting
citizens to one another. EAS policies (including Texas') tend to vary along
five dimensions (community of interest, customer scope, directionality, pricing
mechanism and policy scope). EAS policies that rely on regulated market
boundaries for definition can generate gross inequities in rural Internet
access. Interviews with Internet Service Providers in a case study of 25 rural
communities reveals that LATA and exchange boundaries, along with
geographically restricted infrastructure investments, curtail service provision
in remote areas. A statistical analysis of 1300 telephone exchanges, including
208 rural telephone exchanges in Texas reveals that the farther a community
lies from a metropolitan area the less likely they are to have reliable
Internet access",Internet accessibility
http://arxiv.org/abs/1001.4191v1,"Broadband communications consists of the technologies and equipment required
to deliver packet-based digital voice, video, and data services to end users.
Broadband affords end users high-speed, always-on access to the Internet while
affording service providers the ability to offer value-added services to
increase revenues. Due to the growth of the Internet, there has been tremendous
buildout of high-speed, inter-city communications links that connect population
centers and Internet service providers (ISPs) points of presence (PoPs) around
the world. This build out of the backbone infrastructure or core network has
occurred primarily via optical transport technology. Broadband access
technologies are being deployed to address the bandwidth bottleneck for the
""last mile,"" the connection of homes and small businesses to this
infrastructure. One important aspect of broadband access to the home is that it
allows people to telecommute effectively by providing a similar environment as
when they are physically present in their office: simultaneous telephone and
computer access, high-speed Internet and intranet access for e-mail, file
sharing, and access to corporate servers.",Internet accessibility
http://arxiv.org/abs/cs/0701198v1,"We consider the RIPE WHOIS Internet data as characterized by the Cooperative
Association for Internet Data Analysis (CAIDA), and show that the Tempered
Preferential Attachment model [1] provides an excellent fit to this data.
  [1] D'Souza, Borgs, Chayes, Berger and Kleinberg, to appear PNAS USA, 2007.",Internet accessibility
http://arxiv.org/abs/1610.01065v1,"Cheating is a real problem in the Internet of Things. The fundamental
question that needs to be answered is how we can trust the validity of the data
being generated in the first place. The problem, however, isn't inherent in
whether or not to embrace the idea of an open platform and open-source
software, but to establish a methodology to verify the trustworthiness and
control any access. This paper focuses on building an access control model and
system based on trust computing. This is a new field of access control
techniques which includes Access Control, Trust Computing, Internet of Things,
network attacks, and cheating technologies. Nevertheless, the target access
control systems can be very complex to manage. This paper presents an overview
of the existing work on trust computing, access control models and systems in
IoT. It not only summarizes the latest research progress, but also provides an
understanding of the limitations and open issues of the existing work. It is
expected to provide useful guidelines for future research.",Internet accessibility
http://arxiv.org/abs/cs/0109059v1,"The Internet is changing rapidly the way people around the world communicate,
learn, and work. Yet the tremendous benefits of the Internet are not shared
equally by all. One way to close the gap of the ""digital divide"" is to ensure
Internet access to all schools from an early age. While both the USA and EU
have embraced the promotion of Internet access to schools, the two have decided
to finance it differently. This paper shows that the main costs of Internet
access to schools are not communications-related (telecommunications and
Internet services) but rather non-communications-related (hardware, educational
training, software). This paper goes on to discuss whether the identified costs
should be financed in any way by the universal service obligations funded by
the telecommunications industry/sector/consumers (sector specific) or a general
governmental budget (educational budget).",Internet accessibility
http://arxiv.org/abs/1610.04459v1,"Uniform and affordable Internet is emerging as one of the fundamental civil
rights in developing countries. However in India, the connectivity is far from
uniform across the regions, where the disparity is evident in the
infrastructure, the cost of access and telecommunication services to provide
Internet facilities among different economic classes. In spite of having a
large mobile user base, the mobile Internet are still remarkably slower in some
of the developing countries. Especially in India, it falls below 50% even in
comparison with the performance of its developing counterparts!
  This essay presents a study of connectivity and performance trends based on
an exploratory analysis of mobile Internet measurement data from India. In
order to assess the state of mobile networks and its readiness in adopting the
different mobile standards (2G, 3G, and 4G) for commercial use, we discuss the
spread, penetration, interoperability and the congestion trends. Based on our
analysis, we argue that the network operators have taken negligible measures to
scale the mobile Internet. Affordable Internet is definitely for everyone. But,
the affordability of the Internet in terms of cost does not necessarily imply
the rightful access to Internet services.
  Chota recharge is possibly leading us to chota (shrunken) Internet!",Internet accessibility
http://arxiv.org/abs/cs/0109049v1,"Many people expect the Internet to change American politics, most likely in
the direction of increasing direct citizen participation and forcing government
officials to respond more quickly to voter concerns. A recent California
initiative with these objectives would authorize use of encrypted digital
signatures over the Internet to qualify candidates, initiatives, and other
ballot measures. Proponents of Internet signature gathering say it will
significantly lower the cost of qualifying initiatives and thereby reduce the
influence of organized, well-financed interest groups. They also believe it
will increase both public participation in the political process and public
understanding about specific measures. However, opponents question whether
Internet security is adequate to prevent widespread abuse and argue that the
measure would create disadvantages for those who lack access to the Internet.
Beyond issues of security, cost, and access lie larger questions about the
effects of Internet signature gathering on direct democracy. Would it encourage
greater and more informed public participation in the political process? Or
would it flood voters with ballot measures and generally worsen current
problems with the initiative process itself? Because we lack good data on these
questions, answers to them today are largely conjectural. We can be fairly
sure, however, that Internet petition signing, like Internet voting, will have
unintended consequences.",Internet accessibility
http://arxiv.org/abs/1007.0126v1,"In this paper, we propose a cognitive radio based Internet access framework
for disaster response network deployment in challenged environments. The
proposed architectural framework is designed to help the existent but partially
damaged networks to restore their connectivity and to connect them to the
global Internet. This architectural framework provides the basis to develop
algorithms and protocols for the future cognitive radio network deployments in
challenged environments.",Internet accessibility
http://arxiv.org/abs/1807.06077v1,"Arpanet, Internet, Internet of Services, Internet of Things, Internet of
Skills. What next? We conjecture that in 15-20 years from now we will have the
Internet of Neurons, a new Internet paradigm in which humans will be able to
connect bi-directionally to the net using only their brain. The Internet of
Neurons will provide new, tremendous opportunities thanks to constant access to
unlimited information. It will empower all those outside of the technical
industry, actually it will empower all human beings, to access and use
technological products and services as everybody will be able to connect, even
without possessing a laptop, a tablet or a smartphone. The Internet of Neurons
will thus ultimately complete the currently still immature democratization of
knowledge and technology. But it will also bring along several enormous
challenges, especially concerning security (as well as privacy and trust).
  In this paper we speculate on the worldwide deployment of the Internet of
Neurons by 2038 and brainstorm about its disruptive impact, discussing the main
technological (and neurological) breakthroughs required to enable it, the new
opportunities it provides and the security challenges it raises. We also
elaborate on the novel system models, threat models and security properties
that are required to reason about privacy, security and trust in the Internet
of Neurons.",Internet accessibility
http://arxiv.org/abs/cs/0109064v1,"This paper explores commonalities between the creation of the Rural
Electrification Administration and the similar dilemma of providing an
affordable infrastructure for high-speed Internet access in places where profit
incentives do not exist. In the case of the R.E.A., the necessity for an
aggressive federal initiative to wire rural America, where the market for
electricity had failed, is revisited as the missing incentives are identified
and explored. We then examine the incentive-poor similarities between rural
electrification and rural high-speed Internet access through how consumers
currently and prospectively gain access to broadband Internet service. The
regulatory environment created by the Telecommunications Act of 1996 and the
Federal Communications Commission is considered. Although the FCC is required
(Section 254.b.3) to take regulatory measures to ensure comparable and
affordable access to the Internet for all Americans, the historical
similarities and comparative analysis of rural electrification and high-speed
Internet access suggests the goal of universal service is unlikely to be met in
the near future. Regulatory disincentives to build such networks are present,
driven in part by market realities and in part by competitive restrictions in
the Telecommunications Act of 1996. Finally, we pose the question of whether a
federal effort equivalent to the R.E.A. is needed to ensure that residents of
sparsely populated areas, like their predecessors in the 1930s, are not
comparatively disadvantaged in the first decades of the 21st century. The paper
concludes with a proposal to accelerate the deployment of broadband
infrastructure in rural America.",Internet accessibility
http://arxiv.org/abs/1802.04410v1,"This paper investigates a critical access control issue in the Internet of
Things (IoT). In particular, we propose a smart contract-based framework, which
consists of multiple access control contracts (ACCs), one judge contract (JC)
and one register contract (RC), to achieve distributed and trustworthy access
control for IoT systems. Each ACC provides one access control method for a
subject-object pair, and implements both static access right validation based
on predefined policies and dynamic access right validation by checking the
behavior of the subject. The JC implements a misbehavior-judging method to
facilitate the dynamic validation of the ACCs by receiving misbehavior reports
from the ACCs, judging the misbehavior and returning the corresponding penalty.
The RC registers the information of the access control and misbehavior-judging
methods as well as their smart contracts, and also provides functions (e.g.,
register, update and delete) to manage these methods. To demonstrate the
application of the framework, we provide a case study in an IoT system with one
desktop computer, one laptop and two Raspberry Pi single-board computers, where
the ACCs, JC and RC are implemented based on the Ethereum smart contract
platform to achieve the access control.",Internet accessibility
http://arxiv.org/abs/1603.09537v1,"Internet has shown itself to be a catalyst for economic growth and social
equity but its potency is thwarted by the fact that the Internet is off limits
for the vast majority of human beings. Mobile phones---the fastest growing
technology in the world that now reaches around 80\% of humanity---can enable
universal Internet access if it can resolve coverage problems that have
historically plagued previous cellular architectures (2G, 3G, and 4G). These
conventional architectures have not been able to sustain universal service
provisioning since these architectures depend on having enough users per cell
for their economic viability and thus are not well suited to rural areas (which
are by definition sparsely populated). The new generation of mobile cellular
technology (5G), currently in a formative phase and expected to be finalized
around 2020, is aimed at orders of magnitude performance enhancement. 5G offers
a clean slate to network designers and can be molded into an architecture also
amenable to universal Internet provisioning. Keeping in mind the great social
benefits of democratizing Internet and connectivity, we believe that the time
is ripe for emphasizing universal Internet provisioning as an important goal on
the 5G research agenda. In this paper, we investigate the opportunities and
challenges in utilizing 5G for global access to the Internet for all (GAIA). We
have also identified the major technical issues involved in a 5G-based GAIA
solution and have set up a future research agenda by defining open research
problems.",Internet accessibility
http://arxiv.org/abs/cs/0110016v1,"Advanced services require more reliable bandwidth than currently provided by
the Internet Protocol, even with the reliability enhancements provided by TCP.
More reliable bandwidth will be provided through QoS (quality of service), as
currently discussed widely. Yet QoS has some implications beyond providing
ubiquitous access to advance Internet service, which are of interest from a
policy perspective. In particular, what are the implications for price of
Internet services? Further, how will these changes impact demand and universal
service for the Internet. This paper explores the relationship between
certainty of bandwidth and certainty of price for Internet services over a
statistically shared network and finds that these are mutually exclusive goals.",Internet accessibility
http://arxiv.org/abs/1005.4028v1,"Internet Banking System refers to systems that enable bank customers to
access accounts and general information on bank products and services through a
personal computer or other intelligent device. Internet banking products and
services can include detailed account information for corporate customers as
well as account summery and transfer money. Ultimately, the products and
services obtained through Internet Banking may mirror products and services
offered through other bank delivery channels. In this paper, Internet Banking
System Prototype has been proposed in order to illustrate the services which is
provided by the Bank online services.",Internet accessibility
http://arxiv.org/abs/1308.2454v1,"We introduce a comprehensive analytical framework to compare between open
access and closed access in two-tier femtocell networks, with regard to uplink
interference and outage. Interference at both the macrocell and femtocell
levels is considered. A stochastic geometric approach is employed as the basis
for our analysis. We further derive sufficient conditions for open access and
closed access to outperform each other in terms of the outage probability,
leading to closed-form expressions to upper and lower bound the difference in
the targeted received power between the two access modes. Simulations are
conducted to validate the accuracy of the analytical model and the correctness
of the bounds.",Internet accessibility
http://arxiv.org/abs/1210.2911v1,"Ad Hoc and Mesh networks are good samples of multi agent systems, where their
nodes access the channel through carrier sense multiple access method, while a
node channel access influence the access of neighbor nodes to the channel.
Hence, game theory is a strong tool for studying this kind of networks. Carrier
sense multiple access parameters such as minimum and maximum size of contention
window and persistence factor can be modified based on game theoretic methods.
In this study different games for tuning the parameters is investigated and
different challenges are examined.",Internet accessibility
http://arxiv.org/abs/1401.1513v1,"In this paper, we study the stability of two interacting queues under random
multiple access in which the queues leverage the feedback information. We
derive the stability region under random multiple access where one of the two
queues exploits the feedback information and backs off under negative
acknowledgement (NACK) and the other, higher priority, queue will access the
channel with probability one. We characterize the stability region of this
feedback-based random access protocol and prove that this derived stability
region encloses the stability region of the conventional random access (RA)
scheme that does not exploit the feedback information.",Internet accessibility
http://arxiv.org/abs/1901.07100v1,"A new multiple access method, namely, delta-orthogonal multiple access
(D-OMA) is introduced for massive access in future generation 6G cellular
networks. D-OMA is based on the concept of distributed large coordinated
multipoint transmission-enabled non-orthogonal multiple access (NOMA) using
partially overlapping sub-bands for NOMA clusters. Performance of this scheme
is demonstrated in terms of outage capacity for different degrees of
overlapping of NOMA sub-bands. D-OMA can also be used for enhanced security
provisioning in both uplink and downlink wireless access networks. Practical
implementation issues and open challenges for optimizing D-OMA are also
discussed.",Internet accessibility
http://arxiv.org/abs/1607.05017v1,"Rateless Multiple Access (RMA) is a novel non-orthogonal multiple access
framework that is promising for massive access in Internet of Things (IoT) due
to its high efficiency and low complexity. In the framework, after certain
\emph{registration}, each active user respectively transmits to the access
point (AP) randomly based on an assigned random access control function (RACf)
until receiving an acknowledgement (ACK). In this work, by exploiting the
intrinsic access pattern of each user, we propose a grant-free RMA scheme,
which no longer needs the registration process as in the original RMA, thus
greatly reduces the signalling overhead and system latency. Furthermore, we
propose a low-complexity joint iterative detection and decoding algorithm in
which the channel estimation, active user detection, and information decoding
are done simultaneously. Finally, we propose a method based on density
evolution (DE) to evaluate the system performance.",Internet accessibility
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",Internet accessibility
http://arxiv.org/abs/1701.00220v1,"Today, smartphone devices are owned by a large portion of the population and
have become a very popular platform for accessing the Internet. Smartphones
provide the user with immediate access to information and services. However,
they can easily expose the user to many privacy risks. Applications that are
installed on the device and entities with access to the device's Internet
traffic can reveal private information about the smartphone user and steal
sensitive content stored on the device or transmitted by the device over the
Internet. In this paper, we present a method to reveal various demographics and
technical computer skills of smartphone users by their Internet traffic
records, using machine learning classification models. We implement and
evaluate the method on real life data of smartphone users and show that
smartphone users can be classified by their gender, smoking habits, software
programming experience, and other characteristics.",Internet accessibility
http://arxiv.org/abs/1302.5491v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a study of Sri Lankan hotel websites.
A web survey and a web content analysis were conducted as the methods to elicit
data on web accessibility. Factors preventing accessibility were hypothesized
as an initial experiment. Affecting design elements are identified through web
content analysis, the results of which are utilized to develop specific
implications for improving web accessibility. The hypothesis tests show that
there is no significant correlation between accessibility and geographical or
economic factors. However, physical impairments of users have a considerable
influence on the accessibility of web page user interface if it has been
designed without full consideration of the needs of all users. Especially,
visual and mobility impaired users experience poor accessibility. Poor
readability and less navigable page designs are two observable issues, which
pose threats to accessibility. The lack of conformance to W3C accessibility
guidelines and the poor design process are the specific shortcomings which
reduce the overall accessibility. Guidelines aim to improve the accessibility
of sites with a strategic focus. Further enhancements are suggested with
adherence to principles, user centered design and developing customizable web
portals compatible for connections with differing speeds. Re-ordering search
results has been suggested as one of the finest step towards making the web
content accessible for users with differing needs. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1908.02804v1,"The web is the prominent way information is exchanged in the 21st century.
However, ensuring web-based information is accessible is complicated,
particularly with web applications that rely on JavaScript and other
technologies to deliver and build representations; representations are often
the HTML, images, or other code a server delivers for a web resource. Static
representations are becoming rarer and assessing the accessibility of web-based
information to ensure it is available to all users is increasingly difficult
given the dynamic nature of representations.
  In this work, we survey three ongoing research threads that can inform web
accessibility solutions: assessing web accessibility, modeling web user
activity, and web application crawling. Current web accessibility research is
continually focused on increasing the percentage of automatically testable
standards, but still relies heavily upon manual testing for complex interactive
applications. Along-side web accessibility research, there are mechanisms
developed by researchers that replicate user interactions with web pages based
on usage patterns. Crawling web applications is a broad research domain;
exposing content in web applications is difficult because of incompatibilities
in web crawlers and the technologies used to create the applications. We
describe research on crawling the deep web by exercising user forms. We close
with a thought exercise regarding the convergence of these three threads and
the future of automated, web-based accessibility evaluation and assurance
through a use case in web archiving. These research efforts provide insight
into how users interact with websites, how to automate and simulate user
interactions, how to record the results of user interactions, and how to
analyze, evaluate, and map resulting website content to determine its relative
accessibility.",web accessibility
http://arxiv.org/abs/1112.5728v1,"Web Accessibility for disabled people has posed a challenge to the civilized
societies that claim to uphold the principles of equal opportunity and
nondiscrimination. Certain concrete measures have been taken to narrow down the
digital divide between normal and disabled users of Internet technology. The
efforts have resulted in enactment of legislations and laws and mass awareness
about the discriminatory nature of the accessibility issue, besides the efforts
have resulted in the development of commensurate technological tools to develop
and test the Web accessibility. World Wide Web consortium's (W3C) Web
Accessibility Initiative (WAI) has framed a comprehensive document comprising
of set of guidelines to make the Web sites accessible to the users with
disabilities. This paper is about the issues and aspects surrounding Web
Accessibility. The details and scope are kept limited to comply with the aim of
the paper which is to create awareness and to provide basis for in-depth
investigation.",web accessibility
http://arxiv.org/abs/1405.7868v1,"Today most of the information in all areas is available over the web. It
increases the web utilization as well as attracts the interest of researchers
to improve the effectiveness of web access and web utilization. As the number
of web clients gets increased, the bandwidth sharing is performed that
decreases the web access efficiency. Web page prefetching improves the
effectiveness of web access by availing the next required web page before the
user demand. It is an intelligent predictive mining that analyze the user web
access history and predict the next page. In this work, vague improved markov
model is presented to perform the prediction. In this work, vague rules are
suggested to perform the pruning at different levels of markov model. Once the
prediction table is generated, the association mining will be implemented to
identify the most effective next page. In this paper, an integrated model is
suggested to improve the prediction accuracy and effectiveness.",web accessibility
http://arxiv.org/abs/1004.1257v1,"World Wide Web is a huge repository of web pages and links. It provides
abundance of information for the Internet users. The growth of web is
tremendous as approximately one million pages are added daily. Users' accesses
are recorded in web logs. Because of the tremendous usage of web, the web log
files are growing at a faster rate and the size is becoming huge. Web data
mining is the application of data mining techniques in web data. Web Usage
Mining applies mining techniques in log data to extract the behavior of users
which is used in various applications like personalized services, adaptive web
sites, customer profiling, prefetching, creating attractive web sites etc., Web
usage mining consists of three phases preprocessing, pattern discovery and
pattern analysis. Web log data is usually noisy and ambiguous and preprocessing
is an important process before mining. For discovering patterns sessions are to
be constructed efficiently. This paper reviews existing work done in the
preprocessing stage. A brief overview of various data mining techniques for
discovering patterns, and pattern analysis are discussed. Finally a glimpse of
various applications of web usage mining is also presented.",web accessibility
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",web accessibility
http://arxiv.org/abs/1302.5198v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a subjective study of Sri Lankan
hotel websites. A web survey and a web content analysis were conducted as the
methods to elicit data on web accessibility. Factors preventing accessibility
were hypothesized as an initial experiment. Hazardous design elements are
identified through web content analysis, the results of which are utilized to
develop specific implications for improving web accessibility. The hypothesis
tests show that there is no significant correlation between accessibility and
geographical or economic factors. However, physical impairments of users have a
considerable influence on the accessibility. Especially, visual and mobility
impaired users experience poor accessibility. Poor readability and less
navigable page designs are two observable issues, which pose threats to
accessibility. The lack of conformance to W3C accessibility guidelines and the
poor design process are the specific shortcomings which reduce the overall
accessibility. Guidelines aim to improve the accessibility of sites with a
strategic focus. Further enhancements are suggested with adherence to
principles and user centered design and developing customizable web portals
compatible for connections with differing speeds. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1105.0141v1,"Semantic Web is an open, distributed, and dynamic environment where access to
resources cannot be controlled in a safe manner unless the access decision
takes into account during discovery of web services. Security becomes the
crucial factor for the adoption of the semantic based web services. An access
control means that the users must fulfill certain conditions in order to gain
access over web services. Access control is important in both perspectives i.e.
legal and security point of view. This paper discusses important requirements
for effective access control in semantic web services which have been extracted
from the literature surveyed. I have also discussed open research issues in
this context, focusing on access control policies and models in this paper.",web accessibility
http://arxiv.org/abs/1511.04493v1,"Mobile devices that connect to the Internet via cellular networks are rapidly
becoming the primary medium for accessing Web content. Cellular service
providers (CSPs) commonly deploy Web proxies and other middleboxes for
security, performance optimization and traffic engineering reasons. However,
the prevalence and policies of these Web proxies are generally opaque to users
and difficult to measure without privileged access to devices and servers. In
this paper, we present a methodology to detect the presence of Web proxies
without requiring access to low-level packet traces on a device, nor access to
servers being contacted. We demonstrate the viability of this technique using
controlled experiments, and present the results of running our approach on
several production networks and popular Web sites. Next, we characterize the
behaviors of these Web proxies, including caching, redirecting, and content
rewriting. Our analysis can identify how Web proxies impact network
performance, and inform policies for future deployments. Last, we release an
Android app called Proxy Detector on the Google Play Store, allowing average
users with unprivileged (non-rooted) devices to understand Web proxy
deployments and contribute to our IRB-approved study. We report on results of
using this app on 11 popular carriers from the US, Canada, Austria, and China.",web accessibility
http://arxiv.org/abs/1710.07899v1,"Web portals are being considered as excellent means for conducting teaching
and learning activities electronically. The number of online services such as
course enrollment, tutoring through online course materials, evaluation and
even certification through web portals is increasing day by day. However, the
effectiveness of an educational web portal depends on its accessibility to a
wide range of students irrespective of their age, and physical abilities.
Accessibility of web portals largely depends on their userfriendliness in terms
of design, contents, assistive features, and online support. In this paper, we
have critically analyzed the web portals of thirty Indian Universities of
different categories based on the WCAG 2.0 guidelines. The purpose of this
study is to point out the deficiencies that are commonly observed in web
portals and help web designers to remove such deficiencies from the academic
web portals with a view to enhance their accessibility.",web accessibility
http://arxiv.org/abs/0907.5433v1,"World Wide Web is a huge data repository and is growing with the explosive
rate of about 1 million pages a day. As the information available on World Wide
Web is growing the usage of the web sites is also growing. Web log records each
access of the web page and number of entries in the web logs is increasing
rapidly. These web logs, when mined properly can provide useful information for
decision-making. The designer of the web site, analyst and management
executives are interested in extracting this hidden information from web logs
for decision making. Web access pattern, which is the frequently used sequence
of accesses, is one of the important information that can be mined from the web
logs. This information can be used to gather business intelligence to improve
sales and advertisement, personalization for a user, to analyze system
performance and to improve the web site organization. There exist many
techniques to mine access patterns from the web logs. This paper describes the
powerful algorithm that mines the web logs efficiently. Proposed algorithm
firstly converts the web access data available in a special doubly linked tree.
Each access is called an event. This tree keeps the critical mining related
information in very compressed form based on the frequent event count. Proposed
recursive algorithm uses this tree to efficiently find all access patterns that
satisfy user specified criteria. To prove that our algorithm is efficient from
the other GSP (Generalized Sequential Pattern) algorithms we have done
experimental studies on sample data.",web accessibility
http://arxiv.org/abs/1305.5959v2,"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.",web accessibility
http://arxiv.org/abs/1408.5460v1,"The World Wide Web is the most wide known information source that is easily
available and searchable. It consists of billions of interconnected documents
Web pages are authored by millions of people. Accesses made by various users to
pages are recorded inside web logs. These log files exist in various formats.
Because of increase in usage of web, size of web log files is increasing at a
much faster rate. Web mining is application of data mining technique to these
log files. It can be of three types Web usage mining, Web structure mining and
Web content mining. Web Usage mining is mining of usage patterns of users which
can then be used to personalize web sites and create attractive web sites. It
consists of three main phases: Preprocessing, Pattern discovery and Pattern
analysis. In this paper we focus on Data cleaning and IP Address identification
stages of preprocessing. Methodology has been proposed for both the stages. At
the end conclusion is made about number of users left after IP address
identification.",web accessibility
http://arxiv.org/abs/1204.2225v1,"This paper support the concept of a community Web directory, as a Web
directory that is constructed according to the needs and interests of
particular user communities. Furthermore, it presents the complete method for
the construction of such directories by using web usage data. User community
models take the form of thematic hierarchies and are constructed by employing
clustering approach. We applied our methodology to the ODP directory and also
to an artificial Web directory, which was generated by clustering Web pages
that appear in the access log of an Internet Service Provider. For the
discovery of the community models, we introduced a new criterion that combines
a priori thematic informativeness of the Web directory categories with the
level of interest observed in the usage data. In this context, we introduced
and evaluated new clustering method. We have tested the methodology using
access log files which are collected from the proxy servers of an Internet
Service Provider and provided results that indicates the usability of the
community Web directories. The proposed clustering methodology is evaluated
both on a specialized artificial and a community Web directory, indicating its
value to the user of the web.",web accessibility
http://arxiv.org/abs/1312.3060v1,"Expert System is developed as consulting service for users spread or public
requires affordable access. The Internet has become a medium for such services,
but presence of mobile devices make the access becomes more widespread by
utilizing mobile web and WAP (Wireless Application Protocol). Applying expert
systems applications over the web and WAP requires a knowledge base
representation that can be accessed simultaneously. This paper proposes single
database to accommodate the knowledge representation with decision tree mapping
approach. Because of the database exist, consulting application through both
web and WAP can access it to provide expert system services options for more
affordable for public.",web accessibility
http://arxiv.org/abs/1803.09585v1,"The paper presents a flexible and efficient method to secure the access to a
Web site implemented in PHP script language. The algorithm is based on the PHP
session mechanism. The proposed method is a general one and offers the
possibility to implement a PHP based secured access to a Web site, through a
portal page and using an additional script included in any site page, which is
required to be accessed only by registered users. This paper presents the
design, implementation and integration of the algorithm on any generic WEB
site.",web accessibility
http://arxiv.org/abs/1506.05628v1,"We analyze 18 million rows of Wi-Fi access logs collected over a one year
period from over 120,000 anonymized users at an inner-city shopping mall. The
anonymized dataset gathered from an opt-in system provides users' approximate
physical location, as well as Web browsing and some search history. Such data
provides a unique opportunity to analyze the interaction between people's
behavior in physical retail spaces and their Web behavior, serving as a proxy
to their information needs. We find: (1) the use of Wi-Fi network maps the
opening hours of the mall; (2) there is a weekly periodicity in users' visits
to the mall; (3) around 60% of registered Wi-Fi users actively browse the Web
and around 10% of them use Wi-Fi for accessing Web search engines; (4) people
are likely to spend a relatively constant amount of time browsing the Web while
their visiting duration may vary; (5) people tend to visit similar mall
locations and Web content during their repeated visits to the mall; (6) the
physical spatial context has a small but significant influence on the Web
content that indoor users browse; (7) accompanying users tend to access
resources from the same Web domains.",web accessibility
http://arxiv.org/abs/1208.1679v1,"Colors play a particularly important role in both designing and accessing Web
pages. A well-designed color scheme improves Web pages' visual aesthetic and
facilitates user interactions. As far as we know, existing color assessment
studies focus on images; studies on color assessment and editing for Web pages
are rare. This paper investigates color assessment for Web pages based on
existing online color theme-rating data sets and applies this assessment to Web
color edit. This study consists of three parts. First, we study the extraction
of a Web page's color theme. Second, we construct color assessment models that
score the color compatibility of a Web page by leveraging machine learning
techniques. Third, we incorporate the learned color assessment model into a new
application, namely, color transfer for Web pages. Our study combines
techniques from computer graphics, Web mining, computer vision, and machine
learning. Experimental results suggest that our constructed color assessment
models are effective, and useful in the color transfer for Web pages, which has
received little attention in both Web mining and computer graphics communities.",web accessibility
http://arxiv.org/abs/1111.2530v1,"With the rapid growth of internet technologies, Web has become a huge
repository of information and keeps growing exponentially under no editorial
control. However the human capability to read, access and understand Web
content remains constant. This motivated researchers to provide Web
personalized online services such as Web recommendations to alleviate the
information overload problem and provide tailored Web experiences to the Web
users. Recent studies show that Web usage mining has emerged as a popular
approach in providing Web personalization. However conventional Web usage based
recommender systems are limited in their ability to use the domain knowledge of
the Web application. The focus is only on Web usage data. As a consequence the
quality of the discovered patterns is low. In this paper, we propose a novel
framework integrating semantic information in the Web usage mining process.
Sequential Pattern Mining technique is applied over the semantic space to
discover the frequent sequential patterns. The frequent navigational patterns
are extracted in the form of Ontology instances instead of Web page views and
the resultant semantic patterns are used for generating Web page
recommendations to the user. Experimental results shown are promising and
proved that incorporating semantic information into Web usage mining process
can provide us with more interesting patterns which consequently make the
recommendation system more functional, smarter and comprehensive.",web accessibility
http://arxiv.org/abs/1204.5267v1,"Though World Wide Web is the single largest source of information, it is
ill-equipped to serve the people with vision related problems. With the
prolific increase in the interest to make the web accessible to all sections of
the society, solving this accessibility problem becomes mandatory. This paper
presents a technique for making web pages accessible for people with low vision
issues. A model for making web pages accessible, WILI (Web Interface for people
with Low-vision Issues) has been proposed. The approach followed in this work
is to automatically replace the existing display style of a web page with a new
skin following the guidelines given by Clear Print Booklet provided by Royal
National Institute of Blind. ""Single Click Solution"" is one of the primary
advantages provided by WILI. A prototype using the WILI model is implemented
and various experiments are conducted. The results of experiments conducted on
WILI indicate 82% effective conversion rate.",web accessibility
http://arxiv.org/abs/1408.2695v1,"This paper addresses web object size which is one of important performance
measures and affects to service time in multiple access environment. Since
packets arrive according to Poission distribution and web service time has
arbitrary distribution, M/G/1 model can be used to describe the behavior of the
web server system. In the time division multiplexing (TDM), we can use M/D/1
with vacations model, because service time is constant and server may have a
vacation. We derive the mean web object size satisfying the constraint such
that mean waiting time by round-robin scheduling in multiple access environment
is equal to the mean queueing delay of M/D/1 with vacations model in TDM and
M/H2/1 model, respectively. Performance evaluation shows that the mean web
object size increases as the link utilization increases at the given maximum
segment size (MSS), but converges on the lower bound when the number of
embedded objects included in a web page is beyond the threshold. Our results
can be applied to the economic design and maintenance of web service.",web accessibility
http://arxiv.org/abs/1309.1792v1,"The broad proliferation of mobile devices in recent years has drastically
changed the means of accessing the World Wide Web. Describing a shift away from
the desktop computer era for content consumption, predictions indicate that the
main access of web-based content will come from mobile devices. Concurrently,
the manner of content presentation has changed as well; web artifacts are
allowing for richer media and higher levels of user interaction which is
enabled through increasing access networks speeds. This article provides an
overview of more than two years of high level web page characteristics by
comparing the desktop and mobile client versions. Our study is the first
long-term evaluation of differences as seen by desktop and mobile web browser
clients. We showcase the main differentiating factors with respect to the
number of web page object requests, their sizes, relationships, and web page
object caching. We additionally highlight long-term trends and discuss their
future implications.",web accessibility
http://arxiv.org/abs/1310.2375v1,"Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.",web accessibility
http://arxiv.org/abs/1508.02127v1,"Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results.",web accessibility
http://arxiv.org/abs/1103.5046v1,"The Semantic Web initiative puts emphasis not primarily on putting data on
the Web, but rather on creating links in a way that both humans and machines
can explore the Web of data. When such users access the Web, they leave a trail
as Web servers maintain a history of requests. Web usage mining approaches have
been studied since the beginning of the Web given the log's huge potential for
purposes such as resource annotation, personalization, forecasting etc.
However, the impact of any such efforts has not really gone beyond generating
statistics detailing who, when, and how Web pages maintained by a Web server
were visited.",web accessibility
http://arxiv.org/abs/1806.00871v1,"Personal and private Web archives are proliferating due to the increase in
the tools to create them and the realization that Internet Archive and other
public Web archives are unable to capture personalized (e.g., Facebook) and
private (e.g., banking) Web pages. We introduce a framework to mitigate issues
of aggregation in private, personal, and public Web archives without
compromising potential sensitive information contained in private captures. We
amend Memento syntax and semantics to allow TimeMap enrichment to account for
additional attributes to be expressed inclusive of the requirements for
dereferencing private Web archive captures. We provide a method to involve the
user further in the negotiation of archival captures in dimensions beyond time.
We introduce a model for archival querying precedence and short-circuiting, as
needed when aggregating private and personal Web archive captures with those
from public Web archives through Memento. Negotiation of this sort is novel to
Web archiving and allows for the more seamless aggregation of various types of
Web archives to convey a more accurate picture of the past Web.",web accessibility
http://arxiv.org/abs/1103.5002v1,"The paper proposes an approach to modeling users of large Web sites based on
combining different data sources: access logs and content of the accessed pages
are combined with semantic information about the Web pages, the users and the
accesses of the users to the Web site. The assumption is that we are dealing
with a large Web site providing content to a large number of users accessing
the site. The proposed approach represents each user by a set of features
derived from the different data sources, where some feature values may be
missing for some users. It further enables user modeling based on the provided
characteristics of the targeted user subset. The approach is evaluated on
real-world data where we compare performance of the automatic assignment of a
user to a predefined user segment when different data sources are used to
represent the users.",web accessibility
http://arxiv.org/abs/1104.1892v1,"In this paper we present clustering method is very sensitive to the initial
center values, requirements on the data set too high, and cannot handle noisy
data the proposal method is using information entropy to initialize the cluster
centers and introduce weighting parameters to adjust the location of cluster
centers and noise problems.The navigation datasets which are sequential in
nature, Clustering web data is finding the groups which share common interests
and behavior by analyzing the data collected in the web servers, this improves
clustering on web data efficiently using improved fuzzy c-means(FCM)
clustering. Web usage mining is the application of data mining techniques to
web log data repositories. It is used in finding the user access patterns from
web access log. Web data Clusters are formed using on MSNBC web navigation
dataset.",web accessibility
http://arxiv.org/abs/1405.0749v1,"Web crawlers visit internet applications, collect data, and learn about new
web pages from visited pages. Web crawlers have a long and interesting history.
Early web crawlers collected statistics about the web. In addition to
collecting statistics about the web and indexing the applications for search
engines, modern crawlers can be used to perform accessibility and vulnerability
checks on the application. Quick expansion of the web, and the complexity added
to web applications have made the process of crawling a very challenging one.
Throughout the history of web crawling many researchers and industrial groups
addressed different issues and challenges that web crawlers face. Different
solutions have been proposed to reduce the time and cost of crawling.
Performing an exhaustive crawl is a challenging question. Additionally
capturing the model of a modern web application and extracting data from it
automatically is another open question. What follows is a brief history of
different technique and algorithms used from the early days of crawling up to
the recent days. We introduce criteria to evaluate the relative performance of
web crawlers. Based on these criteria we plot the evolution of web crawlers and
compare their performance",web accessibility
http://arxiv.org/abs/1105.1929v1,"The World Wide Web no longer consists just of HTML pages. Our work sheds
light on a number of trends on the Internet that go beyond simple Web pages.
The hidden Web provides a wealth of data in semi-structured form, accessible
through Web forms and Web services. These services, as well as numerous other
applications on the Web, commonly use XML, the eXtensible Markup Language. XML
has become the lingua franca of the Internet that allows customized markups to
be defined for specific domains. On top of XML, the Semantic Web grows as a
common structured data source. In this work, we first explain each of these
developments in detail. Using real-world examples from scientific domains of
great interest today, we then demonstrate how these new developments can assist
the managing, harvesting, and organization of data on the Web. On the way, we
also illustrate the current research avenues in these domains. We believe that
this effort would help bridge multiple database tracks, thereby attracting
researchers with a view to extend database technology.",web accessibility
http://arxiv.org/abs/1105.3228v1,"Financial economic models often assume that investors know (or agree on) the
fundamental value of the shares of the firm, easing the passage from the
individual to the collective dimension of the financial system generated by the
Share Exchange over time. Our model relaxes that heroic assumption of one
unique ""true value"" and deals with the formation of share market prices through
the dynamic formation of individual and social opinions (or beliefs) based upon
a fundamental signal of economic performance and position of the firm, the
forecast revision by heterogeneous individual investors, and their social mood
or sentiment about the ongoing state of the market pricing process. Market
clearing price formation is then featured by individual and group dynamics that
make its collective dimension irreducible to its individual level. This dynamic
holistic approach can be applied to better understand the market exuberance
generated by the Share Exchange over time.",individual pricing
http://arxiv.org/abs/1809.03110v1,"Cloud spot markets rent VMs for a variable price that is typically much lower
than the price of on-demand VMs, which makes them attractive for a wide range
of large-scale applications. However, applications that run on spot VMs suffer
from cost uncertainty, since spot prices fluctuate, in part, based on supply,
demand, or both. The difficulty in predicting spot prices affects users and
applications: the former cannot effectively plan their IT expenditures, while
the latter cannot infer the availability and performance of spot VMs, which are
a function of their variable price. To address the problem, we use properties
of cloud infrastructure and workloads to show that prices become more stable
and predictable as they are aggregated together. We leverage this observation
to define an aggregate index price for spot VMs that serves as a reference for
what users should expect to pay. We show that, even when the spot prices for
individual VMs are volatile, the index price remains stable and predictable. We
then introduce cloud index tracking: a migration policy that tracks the index
price to ensure applications running on spot VMs incur a predictable cost by
migrating to a new spot VM if the current VM's price significantly deviates
from the index price.",individual pricing
http://arxiv.org/abs/1702.07032v2,"We show that the Revenue-Optimal Deterministic Mechanism Design problem for a
single additive buyer is #P-hard, even when the distributions have support size
2 for each item and, more importantly, even when the optimal solution is
guaranteed to be of a very simple kind: the seller picks a price for each
individual item and a price for the grand bundle of all the items; the buyer
can purchase either the grand bundle at its given price or any subset of items
at their total individual prices. The following problems are also #P-hard, as
immediate corollaries of the proof:
  1. determining if individual item pricing is optimal for a given instance,
  2. determining if grand bundle pricing is optimal, and
  3. computing the optimal (deterministic) revenue.
  On the positive side, we show that when the distributions are i.i.d. with
support size 2, the optimal revenue obtainable by any mechanism, even a
randomized one, can be achieved by a simple solution of the above kind
(individual item pricing with a discounted price for the grand bundle) and
furthermore, it can be computed in polynomial time. The problem can be solved
in polynomial time too when the number of items is constant.",individual pricing
http://arxiv.org/abs/1808.04039v1,"Mobile data demand is increasing tremendously in wireless social networks,
and thus an efficient pricing scheme for social-enabled services is urgently
needed. Though static pricing is dominant in the actual data market, price
intuitively ought to be dynamically changed to yield greater revenue. The
critical question is how to design the optimal dynamic pricing scheme, with
prospects for maximizing the expected long-term revenue. In this paper, we
study the sequential dynamic pricing scheme of a monopoly mobile network
operator in the social data market. In the market, the operator, i.e., the
seller, individually offers each mobile user, i.e., the buyer, a certain price
in multiple time periods dynamically and repeatedly. The proposed scheme
exploits the network effects in the mobile users' behaviors that boost the
social data demand. Furthermore, due to limited radio resource, the impact of
wireless network congestion is taken into account in the pricing scheme.
Thereafter, we propose a modified sequential pricing policy in order to ensure
social fairness among mobile users in terms of their individual utilities. We
analytically demonstrate that the proposed sequential dynamic pricing scheme
can help the operator gain greater revenue and mobile users achieve higher
total utilities than those of the baseline static pricing scheme. To gain more
insights, we further study a simultaneous dynamic pricing scheme in which the
operator determines the pricing strategy at the beginning of each time period.
Mobile users decide on their individual data demand in each time period
simultaneously, considering the network effects in the social domain and the
congestion effects in the network domain. We construct the social graph using
Erd\H{o}s-R\'enyi (ER) model and the real dataset based social network for
performance evaluation.",individual pricing
http://arxiv.org/abs/cs/0106028v1,"We describe a model of a communication network that allows us to price
complex network services as financial derivative contracts based on the spot
price of the capacity in individual routers. We prove a theorem of a Girsanov
transform that is useful for pricing linear derivatives on underlying assets,
which can be used to price many complex network services, and it is used to
price an option that gives access to one of several virtual channels between
two network nodes, during a specified future time interval. We give the
continuous time hedging strategy, for which the option price is independent of
the service providers attitude towards risk. The option price contains the
density function of a sum of lognormal variables, which has to be evaluated
numerically.",individual pricing
http://arxiv.org/abs/0906.4838v1,"This paper presents a model based on multilayer feedforward neural network to
forecast crude oil spot price direction in the short-term, up to three days
ahead. A great deal of attention was paid on finding the optimal ANN model
structure. In addition, several methods of data pre-processing were tested. Our
approach is to create a benchmark based on lagged value of pre-processed spot
price, then add pre-processed futures prices for 1, 2, 3,and four months to
maturity, one by one and also altogether. The results on the benchmark suggest
that a dynamic model of 13 lags is the optimal to forecast spot price direction
for the short-term. Further, the forecast accuracy of the direction of the
market was 78%, 66%, and 53% for one, two, and three days in future
conclusively. For all the experiments, that include futures data as an input,
the results show that on the short-term, futures prices do hold new information
on the spot price direction. The results obtained will generate comprehensive
understanding of the crude oil dynamic which help investors and individuals for
risk managements.",individual pricing
http://arxiv.org/abs/1701.08711v5,"In Chinese societies, superstition is of paramount importance, and vehicle
license plates with desirable numbers can fetch very high prices in auctions.
Unlike other valuable items, license plates are not allocated an estimated
price before auction. I propose that the task of predicting plate prices can be
viewed as a natural language processing (NLP) task, as the value depends on the
meaning of each individual character on the plate and its semantics. I
construct a deep recurrent neural network (RNN) to predict the prices of
vehicle license plates in Hong Kong, based on the characters on a plate. I
demonstrate the importance of having a deep network and of retraining.
Evaluated on 13 years of historical auction prices, the deep RNN's predictions
can explain over 80 percent of price variations, outperforming previous models
by a significant margin. I also demonstrate how the model can be extended to
become a search engine for plates and to provide estimates of the expected
price distribution.",individual pricing
http://arxiv.org/abs/1608.08744v1,"Crowdsourced wireless community networks can effectively alleviate the
limited coverage issue of Wi-Fi access points (APs), by encouraging individuals
(users) to share their private residential Wi-Fi APs with others. In this
paper, we provide a comprehensive economic analysis for such a crowdsourced
network, with the particular focus on the users' behavior analysis and the
community network operator's pricing design. Specifically, we formulate the
interactions between the network operator and users as a two-layer Stackelberg
model, where the operator determining the pricing scheme in Layer I, and then
users determining their Wi-Fi sharing schemes in Layer II. First, we analyze
the user behavior in Layer II via a two-stage membership selection and network
access game, for both small-scale networks and large-scale networks. Then, we
design a partial price differentiation scheme for the operator in Layer I,
which generalizes both the complete price differentiation scheme and the single
pricing scheme (i.e., no price differentiation). We show that the proposed
partial pricing scheme can achieve a good tradeoff between the revenue and the
implementation complexity. Numerical results demonstrate that when using the
partial pricing scheme with only two prices, we can increase the operator's
revenue up to 124.44% comparing with the single pricing scheme, and can achieve
an average of 80% of the maximum operator revenue under the complete price
differentiation scheme.",individual pricing
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",individual pricing
http://arxiv.org/abs/1811.07166v3,"In the isolated auction of a single item, second price often dominates first
price in properties of theoretical interest. But, single items are rarely sold
in true isolation, so considering the broader context is critical when adopting
a pricing strategy. In this paper, we study a model centrally relevant to
Internet advertising and show that when items (ad impressions) are individually
auctioned within the context of a larger system that is managing budgets,
theory offers surprising endorsement for using a first price auction to sell
each individual item. In particular, first price auctions offer theoretical
guarantees of equilibrium uniqueness, monotonicity, and other desirable
properties, as well as efficient computability as the solution to the
well-studied Eisenberg-Gale convex program. We also use simulations to
demonstrate that a bidder's incentive to deviate vanishes in thick markets.",individual pricing
http://arxiv.org/abs/1611.00123v1,"The concept of device-to-device (D2D) communications underlaying cellular
networks opens up potential benefits for improving system performance but also
brings new challenges such as interference management. In this paper, we
propose a pricing framework for interference management from the D2D users to
the cellular system, where the base station (BS) protects itself (or its
serving cellular users) by pricing the crosstier interference caused from the
D2D users. A Stackelberg game is formulated to model the interactions between
the BS and D2D users. Specifically, the BS sets prices to a maximize its
revenue (or any desired utility) subject to an interference temperature
constraint. For given prices, the D2D users competitively adapt their power
allocation strategies for individual utility maximization. We first analyze the
competition among the D2D users by noncooperative game theory and an iterative
based distributed power allocation algorithm is proposed. Then, depending on
how much network information the BS knows, we develop two optimal algorithms,
one for uniform pricing with limited network information and the other for
differentiated pricing with global network information. The uniform pricing
algorithm can be implemented by a fully distributed manner and requires minimum
information exchange between the BS and D2D users, and the differentiated
pricing algorithm is partially distributed and requires no iteration between
the BS and D2D users. Then a suboptimal differentiated pricing scheme is
proposed to reduce complexity and it can be implemented in a fully distributed
fashion. Extensive simulations are conducted to verify the proposed framework
and algorithms.",individual pricing
http://arxiv.org/abs/1208.4589v1,"A case study of the Singapore road network provides empirical evidence that
road pricing can significantly affect commuter trip timing behaviors. In this
paper, we propose a model of trip timing decisions that reasonably matches the
observed commuters' behaviors. Our model explicitly captures the difference in
individuals' sensitivity to price, travel time and early or late arrival at
destination. New pricing schemes are suggested to better spread peak travel and
reduce traffic congestion. Simulation results based on the proposed model are
provided in comparison with the real data for the Singapore case study.",individual pricing
http://arxiv.org/abs/0911.1502v1,"In this paper, we analyze a round-based pricing scheme that encourages
favorable behavior from users of real-time P2P applications like P2PTV. In the
design of pricing schemes, we consider price to be a function of usage and
capacity of download/upload streams, and quality of content served. Users are
consumers and servers at the same time in such networks, and often exhibit
behavior that is unfavorable towards maximization of social benefits.
Traditionally, network designers have overcome this difficulty by building-in
traffic latencies. However, using simulations, we show that appropriate pricing
schemes and usage terms can enable designers to limit required traffic
latencies, and be able to earn nearly 30% extra revenue from providing P2PTV
services. The service provider adjusts the prices of individual programs
incrementally within rounds, while making relatively large-scale adjustments at
the end of each round. Through simulations, we show that it is most beneficial
for the service provider to carry out 5 such rounds of price adjustments for
maximizing his average profit and minimizing the associated standard deviation
at the same time.",individual pricing
http://arxiv.org/abs/1909.00344v1,"Stock prices are driven by various factors. In particular, many individual
investors who have relatively little financial knowledge rely heavily on the
information from news stories when making investment decisions in the stock
market. However, these stories may not reflect future stock prices because of
the subjectivity in the news; stock prices may instead affect the news
contents. This study aims to discover whether it is news or stock prices that
have a greater impact on the other. To achieve this, we analyze the
relationship between news sentiment and stock prices based on time series
analysis using five different classification models. Our experimental results
show that stock prices have a bigger impact on the news contents than news does
on stock prices.",individual pricing
http://arxiv.org/abs/0905.3191v1,"We consider the Item Pricing problem for revenue maximization in the limited
supply setting, where a single seller with $n$ items caters to $m$ buyers with
unknown subadditive valuation functions who arrive in a sequence. The seller
sets the prices on individual items. Each buyer buys a subset of yet unsold
items that maximizes her utility. Our goal is to design pricing strategies that
guarantee an expected revenue that is within a small factor $\alpha$ of the
maximum possible social welfare -- an upper bound on the maximum revenue that
can be generated. Most earlier work has focused on the unlimited supply
setting, where selling items to some buyer does not affect their availability
to the future buyers. Balcan et. al. (EC 2008) studied the limited supply
setting, giving a randomized strategy that assigns a single price to all items
(uniform strategy), and never changes it (static strategy), that gives an
$2^{O(\sqrt{\log n \log \log n})}$-approximation, and moreover, no static
uniform pricing strategy can give better than $2^{\Omega(\log^{1/4} n)}$-
approximation. We improve this lower bound to $2^{\Omega(sqrt{\log n})}$.
  We consider dynamic uniform strategies, which can change the price upon the
arrival of each buyer but the price on all unsold items is the same at all
times, and static non-uniform strategies, which can assign different prices to
different items but can never change it after setting it initially. We design
such pricing strategies that give a poly-logarithmic approximation to maximum
revenue. Thus in the limited supply setting, our results highlight a strong
separation between the power of dynamic and non-uniform pricing versus static
uniform pricing. To our knowledge, this is the first non-trivial analysis of
dynamic and non-uniform pricing schemes for revenue maximization.",individual pricing
http://arxiv.org/abs/1710.01567v3,"The mining process in blockchain requires solving a proof-of-work puzzle,
which is resource expensive to implement in mobile devices due to the high
computing power and energy needed. In this paper, we, for the first time,
consider edge computing as an enabler for mobile blockchain. In particular, we
study edge computing resource management and pricing to support mobile
blockchain applications in which the mining process of miners can be offloaded
to an edge computing service provider. We formulate a two-stage Stackelberg
game to jointly maximize the profit of the edge computing service provider and
the individual utilities of the miners. In the first stage, the service
provider sets the price of edge computing nodes. In the second stage, the
miners decide on the service demand to purchase based on the observed prices.
We apply the backward induction to analyze the sub-game perfect equilibrium in
each stage for both uniform and discriminatory pricing schemes. For the uniform
pricing where the same price is applied to all miners, the existence and
uniqueness of Stackelberg equilibrium are validated by identifying the best
response strategies of the miners. For the discriminatory pricing where the
different prices are applied to different miners, the Stackelberg equilibrium
is proved to exist and be unique by capitalizing on the Variational Inequality
theory. Further, the real experimental results are employed to justify our
proposed model.",individual pricing
http://arxiv.org/abs/1711.01049v1,"As the core issue of blockchain, the mining requires solving a proof-of-work
puzzle, which is resource expensive to implement in mobile devices due to high
computing power needed. Thus, the development of blockchain in mobile
applications is restricted. In this paper, we consider the edge computing as
the network enabler for mobile blockchain. In particular, we study optimal
pricing-based edge computing resource management to support mobile blockchain
applications where the mining process can be offloaded to an Edge computing
Service Provider (ESP). We adopt a two-stage Stackelberg game to jointly
maximize the profit of the ESP and the individual utilities of different
miners. In Stage I, the ESP sets the price of edge computing services. In Stage
II, the miners decide on the service demand to purchase based on the observed
prices. We apply the backward induction to analyze the sub-game perfect
equilibrium in each stage for uniform and discriminatory pricing schemes.
Further, the existence and uniqueness of Stackelberg game are validated for
both pricing schemes. At last, the performance evaluation shows that the ESP
intends to set the maximum possible value as the optimal price for profit
maximization under uniform pricing. In addition, the discriminatory pricing
helps the ESP encourage higher total service demand from miners and achieve
greater profit correspondingly.",individual pricing
http://arxiv.org/abs/1208.4167v1,"As the complexity of enterprise systems increases, the need for monitoring
and analyzing such systems also grows. A number of companies have built
sophisticated monitoring tools that go far beyond simple resource utilization
reports. For example, based on instrumentation and specialized APIs, it is now
possible to monitor single method invocations and trace individual transactions
across geographically distributed systems. This high-level of detail enables
more precise forms of analysis and prediction but comes at the price of high
data rates (i.e., big data). To maximize the benefit of data monitoring, the
data has to be stored for an extended period of time for ulterior analysis.
This new wave of big data analytics imposes new challenges especially for the
application performance monitoring systems. The monitoring data has to be
stored in a system that can sustain the high data rates and at the same time
enable an up-to-date view of the underlying infrastructure. With the advent of
modern key-value stores, a variety of data storage systems have emerged that
are built with a focus on scalability and high data rates as predominant in
this monitoring use case. In this work, we present our experience and a
comprehensive performance evaluation of six modern (open-source) data stores in
the context of application performance monitoring as part of CA Technologies
initiative. We evaluated these systems with data and workloads that can be
found in application performance monitoring, as well as, on-line advertisement,
power monitoring, and many other use cases. We present our insights not only as
performance results but also as lessons learned and our experience relating to
the setup and configuration complexity of these data stores in an industry
setting.",monitoring individual pricing
http://arxiv.org/abs/1008.0147v1,"We consider a multi-user network where a network manager and selfish users
interact. The network manager monitors the behavior of users and intervenes in
the interaction among users if necessary, while users make decisions
independently to optimize their individual objectives. In this paper, we
develop a framework of intervention mechanism design, which is aimed to
optimize the objective of the manager, or the network performance, taking the
incentives of selfish users into account. Our framework is general enough to
cover a wide range of application scenarios, and it has advantages over
existing approaches such as Stackelberg strategies and pricing. To design an
intervention mechanism and to predict the resulting operating point, we
formulate a new class of games called intervention games and a new solution
concept called intervention equilibrium. We provide analytic results about
intervention equilibrium and optimal intervention mechanisms in the case of a
benevolent manager with perfect monitoring. We illustrate these results with a
random access model. Our illustrative example suggests that intervention
requires less knowledge about users than pricing.",monitoring individual pricing
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",monitoring individual pricing
http://arxiv.org/abs/1206.6487v1,"We present a new anytime algorithm that achieves near-optimal regret for any
instance of finite stochastic partial monitoring. In particular, the new
algorithm achieves the minimax regret, within logarithmic factors, for both
""easy"" and ""hard"" problems. For easy problems, it additionally achieves
logarithmic individual regret. Most importantly, the algorithm is adaptive in
the sense that if the opponent strategy is in an ""easy region"" of the strategy
space then the regret grows as if the problem was easy. As an implication, we
show that under some reasonable additional assumptions, the algorithm enjoys an
O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.
(2011).",monitoring individual pricing
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",monitoring individual pricing
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",monitoring individual pricing
http://arxiv.org/abs/1105.3228v1,"Financial economic models often assume that investors know (or agree on) the
fundamental value of the shares of the firm, easing the passage from the
individual to the collective dimension of the financial system generated by the
Share Exchange over time. Our model relaxes that heroic assumption of one
unique ""true value"" and deals with the formation of share market prices through
the dynamic formation of individual and social opinions (or beliefs) based upon
a fundamental signal of economic performance and position of the firm, the
forecast revision by heterogeneous individual investors, and their social mood
or sentiment about the ongoing state of the market pricing process. Market
clearing price formation is then featured by individual and group dynamics that
make its collective dimension irreducible to its individual level. This dynamic
holistic approach can be applied to better understand the market exuberance
generated by the Share Exchange over time.",monitoring individual pricing
http://arxiv.org/abs/1106.0235v1,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",monitoring individual pricing
http://arxiv.org/abs/1809.03110v1,"Cloud spot markets rent VMs for a variable price that is typically much lower
than the price of on-demand VMs, which makes them attractive for a wide range
of large-scale applications. However, applications that run on spot VMs suffer
from cost uncertainty, since spot prices fluctuate, in part, based on supply,
demand, or both. The difficulty in predicting spot prices affects users and
applications: the former cannot effectively plan their IT expenditures, while
the latter cannot infer the availability and performance of spot VMs, which are
a function of their variable price. To address the problem, we use properties
of cloud infrastructure and workloads to show that prices become more stable
and predictable as they are aggregated together. We leverage this observation
to define an aggregate index price for spot VMs that serves as a reference for
what users should expect to pay. We show that, even when the spot prices for
individual VMs are volatile, the index price remains stable and predictable. We
then introduce cloud index tracking: a migration policy that tracks the index
price to ensure applications running on spot VMs incur a predictable cost by
migrating to a new spot VM if the current VM's price significantly deviates
from the index price.",monitoring individual pricing
http://arxiv.org/abs/1810.01851v1,"In Advanced Metering Infrastructure (AMI) networks, smart meters should send
fine-grained power consumption readings to electric utilities to perform
real-time monitoring and energy management. However, these readings can leak
sensitive information about consumers' activities. Various privacy-preserving
schemes for collecting fine-grained readings have been proposed for AMI
networks. These schemes aggregate individual readings and send an aggregated
reading to the utility, but they extensively use asymmetric-key cryptography
which involves large computation/communication overhead. Furthermore, they do
not address End-to-End (E2E) data integrity, authenticity, and computing
electricity bills based on dynamic prices. In this paper, we propose EPIC, an
efficient and privacy-preserving data collection scheme with E2E data integrity
verification for AMI networks. Using efficient cryptographic operations, each
meter should send a masked reading to the utility such that all the masks are
canceled after aggregating all meters' masked readings, and thus the utility
can only obtain an aggregated reading to preserve consumers' privacy. The
utility can verify the aggregated reading integrity without accessing the
individual readings to preserve privacy. It can also identify the attackers and
compute electricity bills efficiently by using the fine-grained readings
without violating privacy. Furthermore, EPIC can resist collusion attacks in
which the utility colludes with a relay node to extract the meters' readings. A
formal proof, probabilistic analysis are used to evaluate the security of EPIC,
and ns-3 is used to implement EPIC and evaluate the network performance. In
addition, we compare EPIC to existing data collection schemes in terms of
overhead and security/privacy features.",monitoring individual pricing
http://arxiv.org/abs/1702.07032v2,"We show that the Revenue-Optimal Deterministic Mechanism Design problem for a
single additive buyer is #P-hard, even when the distributions have support size
2 for each item and, more importantly, even when the optimal solution is
guaranteed to be of a very simple kind: the seller picks a price for each
individual item and a price for the grand bundle of all the items; the buyer
can purchase either the grand bundle at its given price or any subset of items
at their total individual prices. The following problems are also #P-hard, as
immediate corollaries of the proof:
  1. determining if individual item pricing is optimal for a given instance,
  2. determining if grand bundle pricing is optimal, and
  3. computing the optimal (deterministic) revenue.
  On the positive side, we show that when the distributions are i.i.d. with
support size 2, the optimal revenue obtainable by any mechanism, even a
randomized one, can be achieved by a simple solution of the above kind
(individual item pricing with a discounted price for the grand bundle) and
furthermore, it can be computed in polynomial time. The problem can be solved
in polynomial time too when the number of items is constant.",monitoring individual pricing
http://arxiv.org/abs/1808.04039v1,"Mobile data demand is increasing tremendously in wireless social networks,
and thus an efficient pricing scheme for social-enabled services is urgently
needed. Though static pricing is dominant in the actual data market, price
intuitively ought to be dynamically changed to yield greater revenue. The
critical question is how to design the optimal dynamic pricing scheme, with
prospects for maximizing the expected long-term revenue. In this paper, we
study the sequential dynamic pricing scheme of a monopoly mobile network
operator in the social data market. In the market, the operator, i.e., the
seller, individually offers each mobile user, i.e., the buyer, a certain price
in multiple time periods dynamically and repeatedly. The proposed scheme
exploits the network effects in the mobile users' behaviors that boost the
social data demand. Furthermore, due to limited radio resource, the impact of
wireless network congestion is taken into account in the pricing scheme.
Thereafter, we propose a modified sequential pricing policy in order to ensure
social fairness among mobile users in terms of their individual utilities. We
analytically demonstrate that the proposed sequential dynamic pricing scheme
can help the operator gain greater revenue and mobile users achieve higher
total utilities than those of the baseline static pricing scheme. To gain more
insights, we further study a simultaneous dynamic pricing scheme in which the
operator determines the pricing strategy at the beginning of each time period.
Mobile users decide on their individual data demand in each time period
simultaneously, considering the network effects in the social domain and the
congestion effects in the network domain. We construct the social graph using
Erd\H{o}s-R\'enyi (ER) model and the real dataset based social network for
performance evaluation.",monitoring individual pricing
http://arxiv.org/abs/1606.08410v1,"The electricity grid is crucial to our lives. House- holds and institutions
count on it. In recent years, the sources of energy have become less and less
available and they are driving the price of electricity higher and higher. It
has been estimated that 40% of power is spent in residential and institutional
buildings. Most of this power is absorbed by space cooling and heating. In
modern buildings, the HVAC (heating, ventilation, and air conditioning) system
is centralised and operated by a department usually called the central plant.
The central plant produces chilled water and steam that is then consumed by the
building AHUs (Air Handling Units) to maintain the buildings at a comfortable
temperature. However, the heating and cooling model does not take into account
human occupancy. The AHU within the building distributes air according to the
design parameters of the building ignoring the occupancy. As a matter of fact,
there is a potential for optimization lowering consumption to utilize energy
efficiently and also to be able to adapt to the changing cost of energy in a
micro-grid environment. This system makes it possible to reduce the consumption
when needed minimizing impact on the consumer. In this study, we will show,
through a set of studies conducted at the University of Houston, that there is
a potential for energy conservation and efficiency in both the buildings and
the central plant. We also present a strategy that can be undertaken to meet
this goal. This strategy, airflow monitoring and control, is tested in a
software simulation and the results are presented. This system enables the user
to control and monitor the temperature in the individual rooms according the
locals needs.",monitoring individual pricing
http://arxiv.org/abs/cs/0106028v1,"We describe a model of a communication network that allows us to price
complex network services as financial derivative contracts based on the spot
price of the capacity in individual routers. We prove a theorem of a Girsanov
transform that is useful for pricing linear derivatives on underlying assets,
which can be used to price many complex network services, and it is used to
price an option that gives access to one of several virtual channels between
two network nodes, during a specified future time interval. We give the
continuous time hedging strategy, for which the option price is independent of
the service providers attitude towards risk. The option price contains the
density function of a sum of lognormal variables, which has to be evaluated
numerically.",monitoring individual pricing
http://arxiv.org/abs/0906.4838v1,"This paper presents a model based on multilayer feedforward neural network to
forecast crude oil spot price direction in the short-term, up to three days
ahead. A great deal of attention was paid on finding the optimal ANN model
structure. In addition, several methods of data pre-processing were tested. Our
approach is to create a benchmark based on lagged value of pre-processed spot
price, then add pre-processed futures prices for 1, 2, 3,and four months to
maturity, one by one and also altogether. The results on the benchmark suggest
that a dynamic model of 13 lags is the optimal to forecast spot price direction
for the short-term. Further, the forecast accuracy of the direction of the
market was 78%, 66%, and 53% for one, two, and three days in future
conclusively. For all the experiments, that include futures data as an input,
the results show that on the short-term, futures prices do hold new information
on the spot price direction. The results obtained will generate comprehensive
understanding of the crude oil dynamic which help investors and individuals for
risk managements.",monitoring individual pricing
http://arxiv.org/abs/1909.13426v1,"Negotiation is a complex activity involving strategic reasoning, persuasion,
and psychology. An average person is often far from an expert in negotiation.
Our goal is to assist humans to become better negotiators through a
machine-in-the-loop approach that combines machine's advantage at data-driven
decision-making and human's language generation ability. We consider a
bargaining scenario where a seller and a buyer negotiate the price of an item
for sale through a text-based dialog. Our negotiation coach monitors messages
between them and recommends tactics in real time to the seller to get a better
deal (e.g., ""reject the proposal and propose a price"", ""talk about your
personal experience with the product""). The best strategy and tactics largely
depend on the context (e.g., the current price, the buyer's attitude).
Therefore, we first identify a set of negotiation tactics, then learn to
predict the best strategy and tactics in a given dialog context from a set of
human-human bargaining dialogs. Evaluation on human-human dialogs shows that
our coach increases the profits of the seller by almost 60%.",monitoring personal pricing
http://arxiv.org/abs/1804.03178v1,"In practical crowdsourcing systems such as Amazon Mechanical Turk, posted
pricing is widely used due to its simplicity, where a task requester publishes
a pricing rule a priori, on which workers decide whether to accept and perform
the task or not, and are often paid according to the quality of their effort.
One of the key ingredients of a good posted pricing lies in how to recruit more
high-quality workers with less budget, for which the following two schemes are
considered: (i) personalized pricing by profiling users in terms of their
quality and cost, and (ii) additional bonus payment offered for more qualified
task completion. Despite their potential benefits in crowdsourced pricing, it
has been under-explored how much gain each or both of personalization and bonus
payment actually provides to the requester. In this paper, we study four
possible combinations of posted pricing made by pricing with/without
personalization and bonus. We aim at analytically quantifying when and how much
such two ideas contribute to the requester's utility. To this end, we first
derive the optimal personalized and common pricing schemes and analyze their
computational tractability. Next, we quantify the gap in the utility between
with and without bonus payment in both pricing schemes. We analytically prove
that the impact of bonus is negligible significantly marginal in personalized
pricing, whereas crucial in common pricing. Finally, we study the notion of
Price of Agnosticity that quantifies the utility gap between personalized and
common pricing policies. This implies that a complex personalized pricing with
privacy concerns can be replaced by a simple common pricing with bonus. We
validate our analytical findings through extensive simulations and real
experiments done in Amazon Mechanical Turk, and provide additional implications
that are useful in designing a pricing policy.",monitoring personal pricing
http://arxiv.org/abs/1907.11768v2,"Range uncertainties in proton therapy hamper treatment precision. Prompt
gamma-rays were suggested 16 years ago for real-time range verification, and
have already shown promising results in clinical studies with collimated
cameras. Simultaneously, alternative imaging concepts without collimation are
investigated to reduce the footprint and price of current prototypes. In this
manuscript, a compact range verification method is presented. It monitors
prompt gamma-rays with a single scintillation detector positioned coaxially to
the beam and behind the patient. Thanks to the solid angle effect, proton range
deviations can be derived from changes in the number of gamma-rays detected per
proton, provided that the number of incident protons is well known. A
theoretical background is formulated and the requirements for a future
proof-of-principle experiment are identified. The potential benefits and
disadvantages of the method are discussed, and the prospects and potential
obstacles for its use during patient treatments are assessed. The final
milestone is to monitor proton range differences in clinical cases with a
statistical precision of 1 mm, a material cost of 25000 USD and a weight below
10 kg. This technique could facilitate the widespread application of in vivo
range verification in proton therapy and eventually the improvement of
treatment quality.",monitoring personal pricing
http://arxiv.org/abs/1705.02982v2,"A personal data market is a platform including three participants: data
owners (individuals), data buyers and market maker. Data owners who provide
personal data are compensated according to their privacy loss. Data buyers can
submit a query and pay for the result according to their desired accuracy.
Market maker coordinates between data owner and buyer. This framework has been
previously studied based on differential privacy. However, the previous study
assumes data owners can accept any level of privacy loss and data buyers can
conduct the transaction without regard to the financial budget. In this paper,
we propose a practical personal data trading framework that is able to strike a
balance between money and privacy. In order to gain insights on user
preferences, we first conducted an online survey on human attitude to- ward
privacy and interest in personal data trading. Second, we identify the 5 key
principles of personal data market, which is important for designing a
reasonable trading frame- work and pricing mechanism. Third, we propose a
reason- able trading framework for personal data which provides an overview of
how the data is traded. Fourth, we propose a balanced pricing mechanism which
computes the query price for data buyers and compensation for data owners
(whose data are utilized) as a function of their privacy loss. The main goal is
to ensure a fair trading for both parties. Finally, we will conduct an
experiment to evaluate the output of our proposed pricing mechanism in
comparison with other previously proposed mechanism.",monitoring personal pricing
http://arxiv.org/abs/1709.04767v1,"In this paper we examine information privacy from a value-centered angle. We
review and compare different personal data valuation approaches. We consider
the value of personal data now and in the near future, and we find that the
enduring part of personal data will soon become common knowledge and that their
price and value will drop substantially. Therefore the sector that is based on
personal data extraction will need to focus on new ways of monetization and on
the of the dynamic part of personal information.",monitoring personal pricing
http://arxiv.org/abs/1604.04157v1,"In \cite{EK10} the use of VCG in matching markets is motivated by saying that
in order to compute market clearing prices in a matching market, the auctioneer
needs to know the true valuations of the bidders. Hence VCG and corresponding
personalized prices are proposed as an incentive compatible mechanism. The same
line of argument pops up in several lecture sheets and other documents related
to courses based on Easley and Kleinberg's book, seeming to suggest that
computing market clearing prices and corresponding assignments were \emph{not}
incentive compatible. Main purpose of our note is to observe that, in contrast,
assignments based on buyer optimal market clearing prices are indeed incentive
compatible.",monitoring personal pricing
http://arxiv.org/abs/1905.01526v2,"We study the problem of computing personalized reserve prices in eager second
price auctions without having any assumption on valuation distributions. Here,
the input is a dataset that contains the submitted bids of $n$ buyers in a set
of auctions and the goal is to return personalized reserve prices $\textbf r$
that maximize the revenue earned on these auctions by running eager second
price auctions with reserve $\textbf r$. We present a novel LP formulation to
this problem and a rounding procedure which achieves a
$(1+2(\sqrt{2}-1)e^{\sqrt{2}-2})^{-1} \approx 0.684$-approximation. This
improves over the $\frac{1}{2}$-approximation Algorithm due to Roughgarden and
Wang. We show that our analysis is tight for this rounding procedure. We also
bound the integrality gap of the LP, which bounds the performance of any
algorithm based on this LP.",monitoring personal pricing
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",monitoring personal pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",monitoring personal pricing
http://arxiv.org/abs/1702.02046v1,"Breathing signal monitoring can provide important clues for human's physical
health problems. Comparing to existing techniques that require wearable devices
and special equipment, a more desirable approach is to provide contact-free and
long-term breathing rate monitoring by exploiting wireless signals. In this
paper, we propose TensorBeat, a system to employ channel state information
(CSI) phase difference data to intelligently estimate breathing rates for
multiple persons with commodity WiFi devices. The main idea is to leverage the
tensor decomposition technique to handle the CSI phase difference data. The
proposed TensorBeat scheme first obtains CSI phase difference data between
pairs of antennas at the WiFi receiver to create CSI tensor data. Then
Canonical Polyadic (CP) decomposition is applied to obtain the desired
breathing signals. A stable signal matching algorithm is developed to find the
decomposed signal pairs, and a peak detection method is applied to estimate the
breathing rates for multiple persons. Our experimental study shows that
TensorBeat can achieve high accuracy under different environments for
multi-person breathing rate monitoring.",monitoring personal pricing
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",monitoring personal pricing
http://arxiv.org/abs/1011.3852v1,"This paper describes a mobile health monitoring system called iCare for the
elderly. We use wireless body sensors and smart phones to monitor the wellbeing
of the elderly. It can offer remote monitoring for the elderly anytime anywhere
and provide tailored services for each person based on their personal health
condition. When detecting an emergency, the smart phone will automatically
alert pre-assigned people who could be the old people's family and friends, and
call the ambulance of the emergency centre. It also acts as the personal health
information system and the medical guidance which offers one communication
platform and the medical knowledge database so that the family and friends of
the served people can cooperate with doctors to take care of him/her. The
system also features some unique functions that cater to the living demands of
the elderly, including regular reminder, quick alarm, medical guidance, etc.
iCare is not only a real-time health monitoring system for the elderly, but
also a living assistant which can make their lives more convenient and
comfortable.",monitoring personal pricing
http://arxiv.org/abs/physics/0611130v1,"The importance of the power law has been well realized in econophysics over
the last decade. For instance, the distribution of the rate of stock price
variation and of personal assets show the power law. While these results reveal
the striking scale invariance of financial markets, the behaviour of price in
real economy is less known in spite of its extreme importance. As an example of
markets in real economy, here we take up the price of precious stones which
increases with size while the amount of their production rapidly decreases with
size. We show for the first time that the price of natural precious stones
(quartz crystal ball, gemstones such as diamond, emerald, and sapphire) as a
function of weight obeys the power law. This indicates that the price is
determined by the same evaluation measure for different sizes. Our results
demonstrate that not only the distribution of an economical observable but also
the price itself obeys the power law. We anticipate our findings to be a
starting point for the quantitative study of scale invariance in real economy.
While the Black--Sholes model provided the framework for optimal pricing in
financial markets, our method of analysis prvides a new framework that
characterizes the market in real economy.",monitoring personal pricing
http://arxiv.org/abs/1407.0566v2,"In the context of a myriad of mobile apps which collect personally
identifiable information (PII) and a prospective market place of personal data,
we investigate a user-centric monetary valuation of mobile PII. During a 6-week
long user study in a living lab deployment with 60 participants, we collected
their daily valuations of 4 categories of mobile PII (communication, e.g.
phonecalls made/received, applications, e.g. time spent on different apps,
location and media, photos taken) at three levels of complexity (individual
data points, aggregated statistics and processed, i.e. meaningful
interpretations of the data). In order to obtain honest valuations, we employ a
reverse second price auction mechanism. Our findings show that the most
sensitive and valued category of personal information is location. We report
statistically significant associations between actual mobile usage, personal
dispositions, and bidding behavior. Finally, we outline key implications for
the design of mobile services and future markets of personal data.",monitoring personal pricing
http://arxiv.org/abs/1906.05457v2,"As personal data have been the new oil of the digital era, there is a growing
trend perceiving personal data as a commodity. Although some people are willing
to trade their personal data for money, they might still expect limited
individual privacy loss, and the maximum tolerable privacy loss varies with
each individual. In this paper, we propose a framework that enables individuals
to trade their location data streams under personalized privacy loss, which can
be bounded in w successive time points. However, the introduction of such
personalized bounds of individual privacy loss over streaming data raises
several technical challenges in the aspects of budget allocation, utility
estimation of personalized differentially private mechanism, and arbitrage-free
pricing. To deal with those challenges, we modularize three key modules in our
framework and propose arbitrage-free trading mechanisms by combining instances
of the modules. Finally, our experiments verify the effectiveness of the
proposed mechanisms.",monitoring personal pricing
http://arxiv.org/abs/1701.07484v1,"Our machines, products, utilities, and environments have long been monitored
by embedded software systems. Our professional, commercial, social and personal
lives are also subject to monitoring as they are mediated by software systems.
Data on nearly everything now exists, waiting to be collected and analysed for
all sorts of reasons. Given the rising tide of data we pose the questions: What
is monitoring? Do diverse and disparate monitoring systems have anything in
common? We attempt answer these questions by proposing an abstract conceptual
framework for studying monitoring. We argue that it captures a structure common
to many different monitoring practices, and that from it detailed formal models
can be derived, customised to applications. The framework formalises the idea
that monitoring is a process that observes the behaviour of people and objects
in a context. The entities and their behaviours are represented by abstract
data types and the observable attributes by logics. Since monitoring usually
has a specific purpose, we extend the framework with protocols for detecting
attributes or events that require interventions and, possibly, a change in
behaviour. Our theory is illustrated by a case study from criminal justice,
that of electronic tagging.",monitoring personal pricing
http://arxiv.org/abs/1605.03035v1,"For improving e-health services, we propose a context-aware framework to
monitor the activities of daily living of dependent persons. We define a
strategy for generating long-term realistic scenarios and a framework
containing an adaptive monitoring algorithm based on three approaches for
optimizing resource usage. The used approaches provide a deep knowledge about
the person's context by considering: the person's profile, the activities and
the relationships between activities. We evaluate the performances of our
framework and show its adaptability and significant reduction in network,
energy and processing usage over a traditional monitoring implementation.",monitoring personal pricing
http://arxiv.org/abs/1811.10073v1,"Objective: Asthma is a chronic pulmonary disease with multiple triggers
manifesting as symptoms with various intensities. This paper evaluates the
suitability of long-term monitoring of pediatric asthma using diverse data to
qualify and quantify triggers that contribute to the asthma symptoms and
control to enable a personalized management plan.
  Materials and Methods: Asthma condition, environment, and adherence to the
prescribed care plan were continuously tracked for 97 pediatric patients using
kHealth-Asthma technology for one or three months.
  Result: At the cohort level, among 21% of the patients deployed in spring,
63% and 19% indicated pollen and Particulate Matter (PM2.5), respectively, as
the major asthma contributors. Of the 18% of the patients deployed in fall, 29%
and 21% found pollen and PM2.5 respectively, to be the contributors. For the
28% of the patients deployed in winter, PM2.5 was identified as the major
contributor for 80% of them. One patient across each season has been chosen to
explain the determination of personalized triggers by observing correlations
between triggers and asthma symptoms gathered from anecdotal evidence.
  Discussion and Conclusion: Both public and personal health signals including
compliance to prescribed care plan have been captured through continuous
monitoring using the kHealth-Asthma technology which generated insights on
causes of asthma symptoms across different seasons. Collectively, they can form
the underlying basis for personalized management plan and intervention.
  KEYWORDS: Personalized Digital Health, Medical Internet of Things, Pediatric
Asthma Management, Patient Generated Health Data, Personalized Triggers,
Telehealth,",monitoring personal pricing
http://arxiv.org/abs/1910.01770v1,"Because stress is subjective and is expressed differently from one person to
another, generic stress prediction models (i.e., models that predict the stress
of any person) perform crudely. Only person-specific ones (i.e., models that
predict the stress of a preordained person) yield reliable predictions, but
they are not adaptable and costly to deploy in real-world environments. For
illustration, in an office environment, a stress monitoring system that uses
person-specific models would require collecting new data and training a new
model for every employee. Moreover, once deployed, the models would deteriorate
and need expensive periodic upgrades because stress is dynamic and depends on
unforeseeable factors. We propose a simple, yet practical and cost effective
calibration technique that derives an accurate and personalized stress
prediction model from physiological samples collected from a large population.
We validate our approach on two stress datasets. The results show that our
technique performs much better than a generic model. For instance, a generic
model achieved only a 42.5% accuracy. However, with only 100 calibration
samples, we raised its accuracy to 95.2% We also propose a blueprint for a
stress monitoring system based on our strategy, and we debate its merits and
limitation. Finally, we made public our source code and the relevant datasets
to allow other researchers to replicate our findings.",monitoring personal pricing
http://arxiv.org/abs/1906.11657v1,"What fraction of the single item $n$ buyers setting's expected optimal
revenue MyeRev can the second price auction with reserves achieve? In the
special case where the buyers' valuation distributions are all drawn i.i.d. and
the distributions satisfy the regularity condition, the second price auction
with an anonymous reserve (ASP) is the optimal auction itself. As the setting
gets more complex, there are established upper bounds on the fraction of MyeRev
that ASP can achieve. On the contrary, no such upper bounds are known for the
fraction of MyeRev achievable by the second price auction with eager
personalized reserves (ESP). In particular, no separation was earlier known
between ESP's revenue and MyeRev even in the most general setting of
non-identical product distributions that don't satisfy the regularity
condition. In this paper we establish the first separation results for ESP: we
show that even in the case of distributions drawn i.i.d., but not necessarily
satisfying the regularity condition, the ESP cannot achieve more than a $0.778$
fraction of MyeRev in general. Combined with Correa et al.'s result (EC 2017)
that ESP can achieve at least a $0.745$ fraction of MyeRev, this nearly bridges
the gap between upper and lower bounds on ESP's approximation factor.",monitoring personal pricing
http://arxiv.org/abs/physics/0604179v1,"Involving effects of media, opinion leader and other agents on the opinion of
individuals of market society, a trader based model is developed and utilized
to simulate price via supply and demand. Pronounced effects are considered with
several weights and some personal differences between traders are taken into
account. Resulting time series and probabilty distribution function involving a
power law for price come out similar to the real ones.",monitoring personal pricing
http://arxiv.org/abs/1804.03178v1,"In practical crowdsourcing systems such as Amazon Mechanical Turk, posted
pricing is widely used due to its simplicity, where a task requester publishes
a pricing rule a priori, on which workers decide whether to accept and perform
the task or not, and are often paid according to the quality of their effort.
One of the key ingredients of a good posted pricing lies in how to recruit more
high-quality workers with less budget, for which the following two schemes are
considered: (i) personalized pricing by profiling users in terms of their
quality and cost, and (ii) additional bonus payment offered for more qualified
task completion. Despite their potential benefits in crowdsourced pricing, it
has been under-explored how much gain each or both of personalization and bonus
payment actually provides to the requester. In this paper, we study four
possible combinations of posted pricing made by pricing with/without
personalization and bonus. We aim at analytically quantifying when and how much
such two ideas contribute to the requester's utility. To this end, we first
derive the optimal personalized and common pricing schemes and analyze their
computational tractability. Next, we quantify the gap in the utility between
with and without bonus payment in both pricing schemes. We analytically prove
that the impact of bonus is negligible significantly marginal in personalized
pricing, whereas crucial in common pricing. Finally, we study the notion of
Price of Agnosticity that quantifies the utility gap between personalized and
common pricing policies. This implies that a complex personalized pricing with
privacy concerns can be replaced by a simple common pricing with bonus. We
validate our analytical findings through extensive simulations and real
experiments done in Amazon Mechanical Turk, and provide additional implications
that are useful in designing a pricing policy.",personal pricing
http://arxiv.org/abs/1705.02982v2,"A personal data market is a platform including three participants: data
owners (individuals), data buyers and market maker. Data owners who provide
personal data are compensated according to their privacy loss. Data buyers can
submit a query and pay for the result according to their desired accuracy.
Market maker coordinates between data owner and buyer. This framework has been
previously studied based on differential privacy. However, the previous study
assumes data owners can accept any level of privacy loss and data buyers can
conduct the transaction without regard to the financial budget. In this paper,
we propose a practical personal data trading framework that is able to strike a
balance between money and privacy. In order to gain insights on user
preferences, we first conducted an online survey on human attitude to- ward
privacy and interest in personal data trading. Second, we identify the 5 key
principles of personal data market, which is important for designing a
reasonable trading frame- work and pricing mechanism. Third, we propose a
reason- able trading framework for personal data which provides an overview of
how the data is traded. Fourth, we propose a balanced pricing mechanism which
computes the query price for data buyers and compensation for data owners
(whose data are utilized) as a function of their privacy loss. The main goal is
to ensure a fair trading for both parties. Finally, we will conduct an
experiment to evaluate the output of our proposed pricing mechanism in
comparison with other previously proposed mechanism.",personal pricing
http://arxiv.org/abs/1709.04767v1,"In this paper we examine information privacy from a value-centered angle. We
review and compare different personal data valuation approaches. We consider
the value of personal data now and in the near future, and we find that the
enduring part of personal data will soon become common knowledge and that their
price and value will drop substantially. Therefore the sector that is based on
personal data extraction will need to focus on new ways of monetization and on
the of the dynamic part of personal information.",personal pricing
http://arxiv.org/abs/1604.04157v1,"In \cite{EK10} the use of VCG in matching markets is motivated by saying that
in order to compute market clearing prices in a matching market, the auctioneer
needs to know the true valuations of the bidders. Hence VCG and corresponding
personalized prices are proposed as an incentive compatible mechanism. The same
line of argument pops up in several lecture sheets and other documents related
to courses based on Easley and Kleinberg's book, seeming to suggest that
computing market clearing prices and corresponding assignments were \emph{not}
incentive compatible. Main purpose of our note is to observe that, in contrast,
assignments based on buyer optimal market clearing prices are indeed incentive
compatible.",personal pricing
http://arxiv.org/abs/1905.01526v2,"We study the problem of computing personalized reserve prices in eager second
price auctions without having any assumption on valuation distributions. Here,
the input is a dataset that contains the submitted bids of $n$ buyers in a set
of auctions and the goal is to return personalized reserve prices $\textbf r$
that maximize the revenue earned on these auctions by running eager second
price auctions with reserve $\textbf r$. We present a novel LP formulation to
this problem and a rounding procedure which achieves a
$(1+2(\sqrt{2}-1)e^{\sqrt{2}-2})^{-1} \approx 0.684$-approximation. This
improves over the $\frac{1}{2}$-approximation Algorithm due to Roughgarden and
Wang. We show that our analysis is tight for this rounding procedure. We also
bound the integrality gap of the LP, which bounds the performance of any
algorithm based on this LP.",personal pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",personal pricing
http://arxiv.org/abs/physics/0611130v1,"The importance of the power law has been well realized in econophysics over
the last decade. For instance, the distribution of the rate of stock price
variation and of personal assets show the power law. While these results reveal
the striking scale invariance of financial markets, the behaviour of price in
real economy is less known in spite of its extreme importance. As an example of
markets in real economy, here we take up the price of precious stones which
increases with size while the amount of their production rapidly decreases with
size. We show for the first time that the price of natural precious stones
(quartz crystal ball, gemstones such as diamond, emerald, and sapphire) as a
function of weight obeys the power law. This indicates that the price is
determined by the same evaluation measure for different sizes. Our results
demonstrate that not only the distribution of an economical observable but also
the price itself obeys the power law. We anticipate our findings to be a
starting point for the quantitative study of scale invariance in real economy.
While the Black--Sholes model provided the framework for optimal pricing in
financial markets, our method of analysis prvides a new framework that
characterizes the market in real economy.",personal pricing
http://arxiv.org/abs/1407.0566v2,"In the context of a myriad of mobile apps which collect personally
identifiable information (PII) and a prospective market place of personal data,
we investigate a user-centric monetary valuation of mobile PII. During a 6-week
long user study in a living lab deployment with 60 participants, we collected
their daily valuations of 4 categories of mobile PII (communication, e.g.
phonecalls made/received, applications, e.g. time spent on different apps,
location and media, photos taken) at three levels of complexity (individual
data points, aggregated statistics and processed, i.e. meaningful
interpretations of the data). In order to obtain honest valuations, we employ a
reverse second price auction mechanism. Our findings show that the most
sensitive and valued category of personal information is location. We report
statistically significant associations between actual mobile usage, personal
dispositions, and bidding behavior. Finally, we outline key implications for
the design of mobile services and future markets of personal data.",personal pricing
http://arxiv.org/abs/1906.05457v2,"As personal data have been the new oil of the digital era, there is a growing
trend perceiving personal data as a commodity. Although some people are willing
to trade their personal data for money, they might still expect limited
individual privacy loss, and the maximum tolerable privacy loss varies with
each individual. In this paper, we propose a framework that enables individuals
to trade their location data streams under personalized privacy loss, which can
be bounded in w successive time points. However, the introduction of such
personalized bounds of individual privacy loss over streaming data raises
several technical challenges in the aspects of budget allocation, utility
estimation of personalized differentially private mechanism, and arbitrage-free
pricing. To deal with those challenges, we modularize three key modules in our
framework and propose arbitrage-free trading mechanisms by combining instances
of the modules. Finally, our experiments verify the effectiveness of the
proposed mechanisms.",personal pricing
http://arxiv.org/abs/1906.11657v1,"What fraction of the single item $n$ buyers setting's expected optimal
revenue MyeRev can the second price auction with reserves achieve? In the
special case where the buyers' valuation distributions are all drawn i.i.d. and
the distributions satisfy the regularity condition, the second price auction
with an anonymous reserve (ASP) is the optimal auction itself. As the setting
gets more complex, there are established upper bounds on the fraction of MyeRev
that ASP can achieve. On the contrary, no such upper bounds are known for the
fraction of MyeRev achievable by the second price auction with eager
personalized reserves (ESP). In particular, no separation was earlier known
between ESP's revenue and MyeRev even in the most general setting of
non-identical product distributions that don't satisfy the regularity
condition. In this paper we establish the first separation results for ESP: we
show that even in the case of distributions drawn i.i.d., but not necessarily
satisfying the regularity condition, the ESP cannot achieve more than a $0.778$
fraction of MyeRev in general. Combined with Correa et al.'s result (EC 2017)
that ESP can achieve at least a $0.745$ fraction of MyeRev, this nearly bridges
the gap between upper and lower bounds on ESP's approximation factor.",personal pricing
http://arxiv.org/abs/physics/0604179v1,"Involving effects of media, opinion leader and other agents on the opinion of
individuals of market society, a trader based model is developed and utilized
to simulate price via supply and demand. Pronounced effects are considered with
several weights and some personal differences between traders are taken into
account. Resulting time series and probabilty distribution function involving a
power law for price come out similar to the real ones.",personal pricing
http://arxiv.org/abs/1712.06236v2,"A smartphone user's personal hotspot (pH) allows him to share cellular
connection to another (e.g., a traveler) in the vicinity, but such sharing
consumes the limited data quota in his two-part tariff plan and may lead to
overage charge. This paper studies how to motivate such pH-enabled data-plan
sharing between local users and travelers in the ever-growing roaming markets,
and proposes pricing incentive for a data-plan buyer to reward surrounding pH
sellers (if any). The pricing scheme practically takes into account the
information uncertainty at the traveler side, including the random mobility and
the sharing cost distribution of selfish local users who potentially share pHs.
Though the pricing optimization problem is non-convex, we show that there
always exists a unique optimal price to tradeoff between the successful sharing
opportunity and the sharing price. We further generalize the optimal pricing to
the case of heterogeneous selling pHs who have diverse data usage behaviors in
the sharing cost distributions, and we show such diversity may or may not
benefit the traveler. Lacking selfish pHs' information, the traveler's expected
cost is higher than that under the complete information, but the gap diminishes
as the pHs' spatial density increases. Finally, we analyze the challenging
scenario that multiple travelers overlap for demanding data-plan sharing, by
resorting to a near-optimal pricing scheme. We show that a traveler suffers as
the travelers' spatial density increases.",personal pricing
http://arxiv.org/abs/1602.07720v1,"We study the question of setting and testing reserve prices in single item
auctions when the bidders are not identical. At a high level, there are two
generalizations of the standard second price auction: in the lazy version we
first determine the winner, and then apply reserve prices; in the eager version
we first discard the bidders not meeting their reserves, and then determine the
winner among the rest. We show that the two versions have dramatically
different properties: lazy reserves are easy to optimize, and A/B test in
production, whereas eager reserves always lead to higher welfare, but their
optimization is NP-complete, and naive A/B testing will lead to incorrect
conclusions. Despite their different characteristics, we show that the overall
revenue for the two scenarios is always within a factor of 2 of each other,
even in the presence of correlated bids. Moreover, we prove that the eager
auction dominates the lazy auction on revenue whenever the bidders are
independent or symmetric. We complement our theoretical results with
simulations on real world data that show that even suboptimally set eager
reserve prices are preferred from a revenue standpoint.",personal pricing
http://arxiv.org/abs/1208.5258v2,"Personal data has value to both its owner and to institutions who would like
to analyze it. Privacy mechanisms protect the owner's data while releasing to
analysts noisy versions of aggregate query results. But such strict protections
of individual's data have not yet found wide use in practice. Instead, Internet
companies, for example, commonly provide free services in return for valuable
sensitive information from users, which they exploit and sometimes sell to
third parties.
  As the awareness of the value of the personal data increases, so has the
drive to compensate the end user for her private information. The idea of
monetizing private data can improve over the narrower view of hiding private
data, since it empowers individuals to control their data through financial
means.
  In this paper we propose a theoretical framework for assigning prices to
noisy query answers, as a function of their accuracy, and for dividing the
price amongst data owners who deserve compensation for their loss of privacy.
Our framework adopts and extends key principles from both differential privacy
and query pricing in data markets. We identify essential properties of the
price function and micro-payments, and characterize valid solutions.",personal pricing
http://arxiv.org/abs/1909.13426v1,"Negotiation is a complex activity involving strategic reasoning, persuasion,
and psychology. An average person is often far from an expert in negotiation.
Our goal is to assist humans to become better negotiators through a
machine-in-the-loop approach that combines machine's advantage at data-driven
decision-making and human's language generation ability. We consider a
bargaining scenario where a seller and a buyer negotiate the price of an item
for sale through a text-based dialog. Our negotiation coach monitors messages
between them and recommends tactics in real time to the seller to get a better
deal (e.g., ""reject the proposal and propose a price"", ""talk about your
personal experience with the product""). The best strategy and tactics largely
depend on the context (e.g., the current price, the buyer's attitude).
Therefore, we first identify a set of negotiation tactics, then learn to
predict the best strategy and tactics in a given dialog context from a set of
human-human bargaining dialogs. Evaluation on human-human dialogs shows that
our coach increases the profits of the seller by almost 60%.",personal pricing
http://arxiv.org/abs/1905.00052v1,"We address the problem of personalization in the context of eCommerce search.
Specifically, we develop personalization ranking features that use in-session
context to augment a generic ranker optimized for conversion and relevance. We
use a combination of latent features learned from item co-clicks in historic
sessions and content-based features that use item title and price.
Personalization in search has been discussed extensively in the existing
literature. The novelty of our work is combining and comparing content-based
and content-agnostic features and showing that they complement each other to
result in a significant improvement of the ranker. Moreover, our technique does
not require an explicit re-ranking step, does not rely on learning user
profiles from long term search behavior, and does not involve complex modeling
of query-item-user features. Our approach captures item co-click propensity
using lightweight item embeddings. We experimentally show that our technique
significantly outperforms a generic ranker in terms of Mean Reciprocal Rank
(MRR). We also provide anecdotal evidence for the semantic similarity captured
by the item embeddings on the eBay search engine.",personal pricing
http://arxiv.org/abs/1203.3870v1,"Every time the customer (individual or company) has to release personal
information to its service provider (e.g., an online store or a cloud computing
provider), it faces a trade-off between the benefits gained (enhanced or
cheaper services) and the risks it incurs (identity theft and fraudulent uses).
The amount of personal information released is the major decision variable in
that trade-off problem, and has a proxy in the maximum loss the customer may
incur. We find the conditions for a unique optimal solution to exist for that
problem as that maximizing the customer's surplus. We also show that the
optimal amount of personal information is influenced most by the immediate
benefits the customer gets, i.e., the price and the quantity of service offered
by the service provider, rather than by maximum loss it may incur. Easy
spenders take larger risks with respect to low-spenders, but an increase in
price drives customers towards a more careful risk-taking attitude anyway. A
major role is also played by the privacy level, which the service provider
employs to regulate the benefits released to the customers. We also provide a
closed form solution for the limit case of a perfectly secure provider, showing
that the results do not differ significantly from those obtained in the general
case. The trade-off analysis may be employed by the customer to determine its
level of exposure in the relationship with its service provider.",personal pricing
http://arxiv.org/abs/1906.02635v1,"This paper proposes a method for estimating consumer preferences among
discrete choices, where the consumer chooses at most one product in a category,
but selects from multiple categories in parallel. The consumer's utility is
additive in the different categories. Her preferences about product attributes
as well as her price sensitivity vary across products and are in general
correlated across products. We build on techniques from the machine learning
literature on probabilistic models of matrix factorization, extending the
methods to account for time-varying product attributes and products going out
of stock. We evaluate the performance of the model using held-out data from
weeks with price changes or out of stock products. We show that our model
improves over traditional modeling approaches that consider each category in
isolation. One source of the improvement is the ability of the model to
accurately estimate heterogeneity in preferences (by pooling information across
categories); another source of improvement is its ability to estimate the
preferences of consumers who have rarely or never made a purchase in a given
category in the training data. Using held-out data, we show that our model can
accurately distinguish which consumers are most price sensitive to a given
product. We consider counterfactuals such as personally targeted price
discounts, showing that using a richer model such as the one we propose
substantially increases the benefits of personalization in discounts.",personal pricing
http://arxiv.org/abs/1501.04850v1,"With the rapid development of applications in open distributed environments
such as eCommerce, privacy of information is becoming a critical issue. Today,
many online companies are gathering information and have assembled
sophisticated databases that know a great deal about many people, generally
without the knowledge of those people. Such information changes hands or
ownership as a normal part of eCommerce transactions, or through strategic
decisions that often includes the sale of users' information to other firms.
The key commercial value of users' personal information derives from the
ability of firms to identify consumers and charge them personalized prices for
goods and services they have previously used or may wish to use in the future.
A look at present-day practices reveals that consumers' profile data is now
considered as one of the most valuable assets owned by online businesses. In
this thesis, we argue the following: if consumers' private data is such a
valuable asset, should they not be entitled to commercially benefit from their
asset as well? The scope of this thesis is on developing architecture for
privacy payoff as a means of rewarding consumers for sharing their personal
information with online businesses. The architecture is a multi-agent system in
which several agents employ various requirements for personal information
valuation and interaction capabilities that most users cannot do on their own.
The agents in the system bear the responsibility of working on behalf of
consumers to categorize their personal data objects, report to consumers on
online businesses' trustworthiness and reputation, determine the value of their
compensation using risk-based financial models, and, finally, negotiate for a
payoff value in return for the dissemination of users' information.",personal pricing
http://arxiv.org/abs/1812.09234v1,"A firm is selling a product to different types (based on the features such as
education backgrounds, ages, etc.) of customers over a finite season with
non-replenishable initial inventory. The type label of an arriving customer can
be observed but the demand function associated with each type is initially
unknown. The firm sets personalized prices dynamically for each type and
attempts to maximize the revenue over the season. We provide a learning
algorithm that is near-optimal when the demand and capacity scale in
proportion. The algorithm utilizes the primal-dual formulation of the problem
and learns the dual optimal solution explicitly. It allows the algorithm to
overcome the curse of dimensionality (the rate of regret is independent of the
number of types) and sheds light on novel algorithmic designs for learning
problems with resource constraints.",personal pricing
http://arxiv.org/abs/1508.07292v1,"Uber has recently been introducing novel practices in urban taxi transport.
Journey prices can change dynamically in almost real time and also vary
geographically from one area to another in a city, a strategy known as surge
pricing. In this paper, we explore the power of the new generation of open
datasets towards understanding the impact of the new disruption technologies
that emerge in the area of public transport. With our primary goal being a more
transparent economic landscape for urban commuters, we provide a direct price
comparison between Uber and the Yellow Cab company in New York. We discover
that Uber, despite its lower standard pricing rates, effectively charges higher
fares on average, especially during short in length, but frequent in
occurrence, taxi journeys. Building on this insight, we develop a smartphone
application, OpenStreetCab, that offers a personalized consultation to mobile
users on which taxi provider is cheaper for their journey. Almost five months
after its launch, the app has attracted more than three thousand users in a
single city. Their journey queries have provided additional insights on the
potential savings similar technologies can have for urban commuters, with a
highlight being that on average, a user in New York saves 6 U.S. Dollars per
taxi journey if they pick the cheapest taxi provider. We run extensive
experiments to show how Uber's surge pricing is the driving factor of higher
journey prices and therefore higher potential savings for our application's
users. Finally, motivated by the observation that Uber's surge pricing is
occurring more frequently that intuitively expected, we formulate a prediction
task where the aim becomes to predict a geographic area's tendency to surge.
Using exogenous to Uber datasets we show how it is possible to estimate
customer demand within an area, and by extension surge pricing, with high
accuracy.",personal pricing
http://arxiv.org/abs/physics/0608087v3,"Foreign exchange markets show that currency units (= accounting or nominal
price units) are variables. Technical and economic progress evidence that the
consumer baskets (= purchasing power units or real price units) are also
variables. In contrast, all physical measurement units are constants and either
defined in the SI (= metric) convention or based upon natural constants (=
""natural"" or Planck units). Econophysics can identify a constant natural value
scale or vaue unit (natural numaraire) based upon Planck energy. In honour of
the economist L. Walras, this ""Planck value"" could be called walras (Wal),
thereby using the SI naming convention. One Wal can be shown to have a
physiological and an economic interpretation in that it is equal to the annual
minimal real cost of physiological life of a reference person at minimal
activity. The price of one Wal in terms of any currency can be estimated by
hedonic regression techniques used in inflation measurement (axiometry). This
pilot research uses official disaggregated Swiss Producer and Consumer Price
Index data and estimates the hedonic walras price (HWP), quoted in Swiss francs
in 2003, and its inverse, the physical purchasing power (PhPP) of the Swiss
franc in 2003.",personal pricing
http://arxiv.org/abs/0801.2931v1,"We consider the ""Offline Ad Slot Scheduling"" problem, where advertisers must
be scheduled to ""sponsored search"" slots during a given period of time.
Advertisers specify a budget constraint, as well as a maximum cost per click,
and may not be assigned to more than one slot for a particular search.
  We give a truthful mechanism under the utility model where bidders try to
maximize their clicks, subject to their personal constraints. In addition, we
show that the revenue-maximizing mechanism is not truthful, but has a Nash
equilibrium whose outcome is identical to our mechanism. As far as we can tell,
this is the first treatment of sponsored search that directly incorporates both
multiple slots and budget constraints into an analysis of incentives.
  Our mechanism employs a descending-price auction that maintains a solution to
a certain machine scheduling problem whose job lengths depend on the price, and
hence is variable over the auction. The price stops when the set of bidders
that can afford that price pack exactly into a block of ad slots, at which
point the mechanism allocates that block and continues on the remaining slots.
To prove our result on the equilibrium of the revenue-maximizing mechanism, we
first show that a greedy algorithm suffices to solve the revenue-maximizing
linear program; we then use this insight to prove that bidders allocated in the
same block of our mechanism have no incentive to deviate from bidding the fixed
price of that block.",personal pricing
http://arxiv.org/abs/1409.0069v1,"There is a relatively small amount of research covering urban freight
movements. Most research dealing with the subject of urban mobility focuses on
passenger vehicles, not commercial vehicles hauling freight. However, in many
ways, urban freight transport contributes to congestion, air pollution, noise,
accident and more fuel consumption which raises logistic costs, and hence the
price of products. The main focus of this paper is to propose a new solution
for congestion in order to improve the distribution process of goods in urban
areas and optimize transportation cost, time of delivery, fuel consumption, and
environmental impact, while guaranteeing the safety of goods and passengers. A
novel technique for personalization in itinerary search based on city logistics
ontology and rules is proposed to overcome this problem. The integration of
personalization plays a key role in capturing or inferring the needs of each
stakeholder (user), and then satisfying these needs in a given context. The
proposed approach is implemented to an itinerary search problem for freight
transportation in urban areas to demonstrate its ability in facilitating
intelligent decision support by retrieving the best itinerary that satisfies
the most users preferences (stakeholders).",personal pricing
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud pricing
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud pricing
http://arxiv.org/abs/cs/0110006v1,"This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud pricing
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud pricing
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud pricing
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud pricing
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud pricing
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",e-commerce fraud pricing
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud pricing
http://arxiv.org/abs/1707.03367v1,"In the e-commerce world, the follow-up of prices in detail web pages is of
great interest for things like buying a product when it falls below some
threshold. For doing this task, instead of bookmarking the pages and revisiting
them, in this paper we propose a novel web data extraction system, called
Wextractor. It consists of an extraction method and a web app for listing the
retrieved prices. As for the final user, the main feature of Wextractor is
usability because (s)he only has to signal the pages of interest and our system
automatically extracts the price from the page.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud pricing
http://arxiv.org/abs/1806.05799v2,"As the largest e-commerce platform, Taobao helps advertisers reach billions
of search queries each day via sponsored search, which has also contributed
considerable revenue to the platform. An efficient bidding strategy to cater to
diverse advertiser demands while balancing platform revenue and consumer
experience is significant to a healthy and sustainable marketing ecosystem. In
this paper we propose \emph{Customer Intelligent Agent (CIA)}, a bidding
optimization framework which implements an impression-level bidding to reflect
advertisers' conversion willingness and budget control. In this way, CIA is
capable of fulfilling various e-commerce advertiser demands on different
levels, such as Gross Merchandise Volume optimization, style comparison etc.
Additionally, a replay based simulation system is designed to predict the
performance of different take-rate. CIA unifies the benefits of three parties
in the marketing ecosystem without changing the Generalized Second Price
mechanism. Our extensive offline simulations and large-scale online experiments
on \emph{Taobao Search Advertising (TSA)} platform verify the high
effectiveness of the CIA framework. Moreover, CIA has been deployed online as a
major bidding tool in TSA.",e-commerce fraud pricing
http://arxiv.org/abs/1806.09793v1,"With the considerable development of customer-to-customer (C2C) e-commerce in
the recent years, there is a big demand for an effective recommendation system
that suggests suitable websites for users to sell their items with some
specified needs. Nonetheless, e-commerce recommendation systems are mostly
designed for business-to-customer (B2C) websites, where the systems offer the
consumers the products that they might like to buy. Almost none of the related
research works focus on choosing selling sites for target items. In this paper,
we introduce an approach that recommends the selling websites based upon the
item's description, category, and desired selling price. This approach employs
NoSQL data-based machine learning techniques for building and training topic
models and classification models. The trained models can then be used to rank
the websites dynamically with respect to the user needs. The experimental
results with real-world datasets from Vietnam C2C websites will demonstrate the
effectiveness of our proposed method.",e-commerce fraud pricing
http://arxiv.org/abs/1909.13221v2,"Online advertising in E-commerce platforms provides sellers an opportunity to
achieve potential audiences with different target goals. Ad serving systems
(like display and search advertising systems) that assign ads to pages should
satisfy objectives such as plenty of audience for branding advertisers, clicks
or conversions for performance-based advertisers, at the same time try to
maximize overall revenue of the platform. In this paper, we propose an approach
based on linear programming subjects to constraints in order to optimize the
revenue and improve different performance goals simultaneously. We have
validated our algorithm by implementing an offline simulation system in Alibaba
E-commerce platform and running the auctions from online requests which takes
system performance, ranking and pricing schemas into account. We have also
compared our algorithm with related work, and the results show that our
algorithm can effectively improve campaign performance and revenue of the
platform.",e-commerce fraud pricing
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud pricing
http://arxiv.org/abs/physics/0608232v1,"We characterize the statistical properties of a large number of online
auctions run on eBay. Both stationary and dynamic properties, like
distributions of prices, number of bids etc., as well as relations between
these quantities are studied. The analysis of the data reveals surprisingly
simple distributions and relations, typically of power-law form. Based on these
findings we introduce a simple method to identify suspicious auctions that
could be influenced by a form of fraud known as shill bidding. Furthermore the
influence of bidding strategies is discussed. The results indicate that the
observed behavior is related to a mixture of agents using a variety of
strategies.",e-commerce fraud pricing
http://arxiv.org/abs/1905.04770v1,"Motivated by the dynamic assortment offerings and item pricings occurring in
e-commerce, we study a general problem of allocating finite inventories to
heterogeneous customers arriving sequentially. We analyze this problem under
the framework of competitive analysis, where the sequence of customers is
unknown and does not necessarily follow any pattern. Previous work in this
area, studying online matching, advertising, and assortment problems, has
focused on the case where each item can only be sold at a single price,
resulting in algorithms which achieve the best-possible competitive ratio of
1-1/e.
  In this paper, we extend all of these results to allow for items having
multiple feasible prices. Our algorithms achieve the best-possible
weight-dependent competitive ratios, which depend on the sets of feasible
prices given in advance. Our algorithms are also simple and intuitive; they are
based on constructing a class of universal ``value functions'' which integrate
the selection of items and prices offered.
  Finally, we test our algorithms on the publicly-available hotel data set of
Bodea et al. (2009), where there are multiple items (hotel rooms) each with
multiple prices (fares at which the room could be sold). We find that applying
our algorithms, as a ``hybrid'' with algorithms which attempt to forecast and
learn the future transactions, results in the best performance.",e-commerce fraud pricing
http://arxiv.org/abs/1711.02661v1,"In recent years, many new and interesting models of successful online
business have been developed, including competitive models such as auctions,
where the product price tends to rise, and group-buying, where users cooperate
obtaining a dynamic price that tends to go down. We propose the e-fair as a
business model for social commerce, where both sellers and buyers are grouped
to maximize benefits. e-Fairs extend the group-buying model aggregating demand
and supply for price optimization as well as consolidating shipments and
optimize withdrawals for guaranteeing additional savings. e-Fairs work upon
multiple dimensions: time to aggregate buyers, their geographical distribution,
price/quantity curves provided by sellers, and location of withdrawal points.
We provide an analytical model for time and spatial optimization and simulate
realistic scenarios using both real purchase data from an Italian marketplace
and simulated ones. Experimental results demonstrate the potentials offered by
e-fairs and show benefits for all the involved actors.",e-commerce fraud pricing
http://arxiv.org/abs/1211.3148v1,"Retailers in Saudi Arabia have been reserved in their adoption of
electronically delivered aspects of their business. This paper reports research
that identifies and explores key issues to enhance the diffusion of online
retailing in Saudi Arabia. Despite the fact that Saudi Arabia has the largest
and fastest growth of ICT marketplaces in the Arab region, e-commerce
activities are not progressing at the same speed. Only very few Saudi
companies, mostly medium and large companies from the manufacturing sector, are
involved in e-commerce implementation. Based on qualitative data collected by
conducting interviews with 16 retailers and 16 potential customers in Saudi
Arabia, 7 key drivers to online retailing diffusion in Saudi Arabia are
identified. These key drivers are government support, providing trustworthy and
secure online payments options, provision of individual house mailboxes,
providing high speed Internet connection at low cost, providing educational
programs, the success of bricks-and-clicks model, and competitive prices.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07607v3,"We study the problem of allocating impressions to sellers in e-commerce
websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue
generated by the platform. We employ a general framework of reinforcement
mechanism design, which uses deep reinforcement learning to design efficient
algorithms, taking the strategic behaviour of the sellers into account.
Specifically, we model the impression allocation problem as a Markov decision
process, where the states encode the history of impressions, prices,
transactions and generated revenue and the actions are the possible impression
allocations in each round. To tackle the problem of continuity and
high-dimensionality of states and actions, we adopt the ideas of the DDPG
algorithm to design an actor-critic policy gradient algorithm which takes
advantage of the problem domain in order to achieve convergence and stability.
We evaluate our proposed algorithm, coined IA(GRU), by comparing it against
DDPG, as well as several natural heuristics, under different rationality models
for the sellers - we assume that sellers follow well-known no-regret type
strategies which may vary in their degree of sophistication. We find that
IA(GRU) outperforms all algorithms in terms of the total revenue.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07946v1,"Sales forecast is an essential task in E-commerce and has a crucial impact on
making informed business decisions. It can help us to manage the workforce,
cash flow and resources such as optimizing the supply chain of manufacturers
etc. Sales forecast is a challenging problem in that sales is affected by many
factors including promotion activities, price changes, and user preferences
etc. Traditional sales forecast techniques mainly rely on historical sales data
to predict future sales and their accuracies are limited. Some more recent
learning-based methods capture more information in the model to improve the
forecast accuracy. However, these methods require case-by-case manual feature
engineering for specific commercial scenarios, which is usually a difficult,
time-consuming task and requires expert knowledge. To overcome the limitations
of existing methods, we propose a novel approach in this paper to learn
effective features automatically from the structured data using the
Convolutional Neural Network (CNN). When fed with raw log data, our approach
can automatically extract effective features from that and then forecast sales
using those extracted features. We test our method on a large real-world
dataset from CaiNiao.com and the experimental results validate the
effectiveness of our method.",e-commerce fraud pricing
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud pricing
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud pricing
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud pricing
http://arxiv.org/abs/1409.6559v1,"Online auctions are among the most influential e-business applications. Their
impact on trading for businesses, as well as consumers, is both remarkable and
inevitable. There have been considerable efforts in setting up market places,
but, with respects to market volume, online trading is still in its early
stages. This chapter discusses the benefits of the concept of Internet
marketplaces, with the highest impact on pricing strategies, namely, the
conduction of online business auctions. We discuss their benefits, problems and
possible solutions. In addition, we sketch actions for suppliers to achieve a
better strategic position in the upcoming Internet market places.",e-commerce fraud pricing
http://arxiv.org/abs/1809.09621v1,"Complementary products recommendation is an important problem in e-commerce.
Such recommendations increase the average order price and the number of
products in baskets. Complementary products are typically inferred from basket
data. In this study, we propose the BB2vec model. The BB2vec model learns
vector representations of products by analyzing jointly two types of data -
Baskets and Browsing sessions (visiting web pages of products). These vector
representations are used for making complementary products recommendation. The
proposed model alleviates the cold start problem by delivering better
recommendations for products having few or no purchases. We show that the
BB2vec model has better performance than other models which use only basket
data.",e-commerce fraud pricing
http://arxiv.org/abs/1510.02377v3,"In a world where traditional notions of privacy are increasingly challenged
by the myriad companies that collect and analyze our data, it is important that
decision-making entities are held accountable for unfair treatments arising
from irresponsible data usage. Unfortunately, a lack of appropriate
methodologies and tools means that even identifying unfair or discriminatory
effects can be a challenge in practice. We introduce the unwarranted
associations (UA) framework, a principled methodology for the discovery of
unfair, discriminatory, or offensive user treatment in data-driven
applications. The UA framework unifies and rationalizes a number of prior
attempts at formalizing algorithmic fairness. It uniquely combines multiple
investigative primitives and fairness metrics with broad applicability,
granular exploration of unfair treatment in user subgroups, and incorporation
of natural notions of utility that may account for observed disparities. We
instantiate the UA framework in FairTest, the first comprehensive tool that
helps developers check data-driven applications for unfair user treatment. It
enables scalable and statistically rigorous investigation of associations
between application outcomes (such as prices or premiums) and sensitive user
attributes (such as race or gender). Furthermore, FairTest provides debugging
capabilities that let programmers rule out potential confounders for observed
unfair effects. We report on use of FairTest to investigate and in some cases
address disparate impact, offensive labeling, and uneven rates of algorithmic
error in four data-driven applications. As examples, our results reveal subtle
biases against older populations in the distribution of error in a predictive
health application and offensive racial labeling in an image tagger.",algorithm detect unfair pricing website
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",algorithm detect unfair pricing website
http://arxiv.org/abs/1503.05414v3,"Social networks help to bond people who share similar interests all over the
world. As a complement, the Facebook ""Like"" button is an efficient tool that
bonds people with the online information. People click on the ""Like"" button to
express their fondness of a particular piece of information and in turn tend to
visit webpages with high ""Like"" count. The important fact of the Like count is
that it reflects the number of actual users who ""liked"" this information.
However, according to our study, one can easily exploit the defects of the
""Like"" button to counterfeit a high ""Like"" count. We provide a proof-of-concept
implementation of these exploits, and manage to generate 100 fake Likes in 5
minutes with a single account. We also reveal existing counterfeiting
techniques used by some online sellers to achieve unfair advantage for
promoting their products. To address this fake Like problem, we study the
varying patterns of Like count and propose an innovative fake Like detection
method based on clustering. To evaluate the effectiveness of our algorithm, we
collect the Like count history of more than 9,000 websites. Our experiments
successfully uncover 16 suspicious fake Like buyers that show abnormal Like
count increase patterns.",algorithm detect unfair pricing website
http://arxiv.org/abs/1211.0963v1,"Online rating systems are subject to malicious behaviors mainly by posting
unfair rating scores. Users may try to individually or collaboratively promote
or demote a product. Collaborating unfair rating 'collusion' is more damaging
than individual unfair rating. Although collusion detection in general has been
widely studied, identifying collusion groups in online rating systems is less
studied and needs more investigation. In this paper, we study impact of
collusion in online rating systems and asses their susceptibility to collusion
attacks. The proposed model uses a frequent itemset mining algorithm to detect
candidate collusion groups. Then, several indicators are used for identifying
collusion groups and for estimating how damaging such colluding groups might
be. Also, we propose an algorithm for finding possible collusive subgroup
inside larger groups which are not identified as collusive. The model has been
implemented and we present results of experimental evaluation of our
methodology.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.10201v1,"High performance grid computing is a key enabler of large scale collaborative
computational science. With the promise of exascale computing, high performance
grid systems are expected to incur electricity bills that grow super-linearly
over time. In order to achieve cost effectiveness in these systems, it is
essential for the scheduling algorithms to exploit electricity price
variations, both in space and time, that are prevalent in the dynamic
electricity price markets. In this paper, we present a metascheduling algorithm
to optimize the placement of jobs in a compute grid which consumes electricity
from the day-ahead wholesale market. We formulate the scheduling problem as a
Minimum Cost Maximum Flow problem and leverage queue waiting time and
electricity price predictions to accurately estimate the cost of job execution
at a system. Using trace based simulation with real and synthetic workload
traces, and real electricity price data sets, we demonstrate our approach on
two currently operational grids, XSEDE and NorduGrid. Our experimental setup
collectively constitute more than 433K processors spread across 58 compute
systems in 17 geographically distributed locations. Experiments show that our
approach simultaneously optimizes the total electricity cost and the average
response time of the grid, without being unfair to users of the local batch
systems.",algorithm detect unfair pricing website
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",algorithm detect unfair pricing website
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",algorithm detect unfair pricing website
http://arxiv.org/abs/1408.1993v1,"Malicious websites are a major cyber attack vector, and effective detection
of them is an important cyber defense task. The main defense paradigm in this
regard is that the defender uses some kind of machine learning algorithms to
train a detection model, which is then used to classify websites in question.
Unlike other settings, the following issue is inherent to the problem of
malicious websites detection: the attacker essentially has access to the same
data that the defender uses to train its detection models. This 'symmetry' can
be exploited by the attacker, at least in principle, to evade the defender's
detection models. In this paper, we present a framework for characterizing the
evasion and counter-evasion interactions between the attacker and the defender,
where the attacker attempts to evade the defender's detection models by taking
advantage of this symmetry. Within this framework, we show that an adaptive
attacker can make malicious websites evade powerful detection models, but
proactive training can be an effective counter-evasion defense mechanism. The
framework is geared toward the popular detection model of decision tree, but
can be adapted to accommodate other classifiers.",algorithm detect unfair pricing website
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",algorithm detect unfair pricing website
http://arxiv.org/abs/1711.06955v1,"Web spam is a big problem for search engine users in World Wide Web. They use
deceptive techniques to achieve high rankings. Although many researchers have
presented the different approach for classification and web spam detection
still it is an open issue in computer science. Analyzing and evaluating these
websites can be an effective step for discovering and categorizing the features
of these websites. There are several methods and algorithms for detecting those
websites, such as decision tree algorithm. In this paper, we present a
systematic framework based on CHAID algorithm and a modified string matching
algorithm (KMP) for extract features and analysis of these websites. We
evaluated our model and other methods with a dataset of Alexa Top 500 Global
Sites and Bing search engine results in 500 queries.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",algorithm detect unfair pricing website
http://arxiv.org/abs/1907.12649v2,"In recent years, Header Bidding (HB) has gained popularity among web
publishers, challenging the status quo in the ad ecosystem. Contrary to the
traditional waterfall standard, HB aims to give back to publishers control of
their ad inventory, increase transparency, fairness and competition among
advertisers, resulting in higher ad-slot prices. Although promising, little is
known about how this ad protocol works: What are HB's possible implementations,
who are the major players, and what is its network and UX overhead? To address
these questions, we design and implement HBDetector: a novel methodology to
detect HB auctions on a website at real time. By crawling 35,000 top Alexa
websites, we collect and analyze a dataset of 800k auctions. We find that: (i)
14.28% of top websites utilize HB. (ii) Publishers prefer to collaborate with a
few Demand Partners who also dominate the waterfall market. (iii) HB latency
can be significantly higher (up to 3x in median case) than waterfall.",algorithm detect unfair pricing website
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",algorithm detect unfair pricing website
http://arxiv.org/abs/1808.07359v1,"Recent works showed that websites can detect browser extensions that users
install and websites they are logged into. This poses significant privacy
risks, since extensions and Web logins that reflect user's behavior, can be
used to uniquely identify users on the Web. This paper reports on the first
large-scale behavioral uniqueness study based on 16,393 users who visited our
website. We test and detect the presence of 16,743 Chrome extensions, covering
28% of all free Chrome extensions. We also detect whether the user is connected
to 60 different websites.
  We analyze how unique users are based on their behavior, and find out that
54.86% of users that have installed at least one detectable extension are
unique; 19.53% of users are unique among those who have logged into one or more
detectable websites; and 89.23% are unique among users with at least one
extension and one login. We use an advanced fingerprinting algorithm and show
that it is possible to identify a user in less than 625 milliseconds by
selecting the most unique combinations of extensions.
  Because privacy extensions contribute to the uniqueness of users, we study
the trade-off between the amount of trackers blocked by such extensions and how
unique the users of these extensions are. We have found that privacy extensions
should be considered more useful than harmful. The paper concludes with
possible countermeasures.",algorithm detect unfair pricing website
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",algorithm detect unfair pricing website
http://arxiv.org/abs/1708.01348v4,"While page views are often sold instantly through real-time auctions when
users visit websites, they can also be sold in advance via guaranteed
contracts. In this paper, we present a dynamic programming model to study how
an online publisher should optimally allocate and price page views between
guaranteed and spot markets. The problem is challenging because the allocation
and pricing of guaranteed contracts affect advertisers' purchase between the
two markets, and the terminal value of the model is endogenously determined by
the updated dual force of supply and demand in auctions. We take the
advertisers' purchasing behaviour into consideration, i.e., risk aversion and
stochastic demand arrivals, and present a scalable and efficient algorithm for
the optimal solution. The model is also empirically validated with a commercial
dataset. The experimental results show that selling page views via both
guaranteed contracts and auctions can increase the publisher's expected total
revenue, and the optimal pricing and allocation strategies are robust to
different market and advertiser types.",algorithm detect unfair pricing website
http://arxiv.org/abs/1308.1382v1,"We consider the optimal pricing problem for a model of the rich media
advertisement market, as well as other related applications. In this market,
there are multiple buyers (advertisers), and items (slots) that are arranged in
a line such as a banner on a website. Each buyer desires a particular number of
{\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on
the ad only) while each slot $j$ has a quality $q_j$ (dependent on the position
only such as click-through rate in position auctions). Hence, the valuation of
the buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and
the prices in order to maximize the total revenue of the market maker.
  A key difference from the traditional position auction is the advertiser's
requirement of a fixed number of consecutive slots. Consecutive slots may be
needed for a large size rich media ad. We study three major pricing mechanisms,
the Bayesian pricing model, the maximum revenue market equilibrium model and an
envy-free solution model. Under the Bayesian model, we design a polynomial time
computable truthful mechanism which is optimum in revenue. For the market
equilibrium paradigm, we find a polynomial time algorithm to obtain the maximum
revenue market equilibrium solution. In envy-free settings, an optimal solution
is presented when the buyers have the same demand for the number of consecutive
slots. We conduct a simulation that compares the revenues from the above
schemes and gives convincing results.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0108015v1,"Recent trends reveal the search by companies for a legal hook to prevent the
undesired and unauthorized copying of information posted on websites. In the
center of this controversy are metasites, websites that display prices for a
variety of vendors. Metasites function by implementing shopbots, which extract
pricing data from other vendors' websites. Technological mechanisms have proved
unsuccessful in blocking shopbots, and in response, websites have asserted a
variety of legal claims. Two recent cases, which rely on the troublesome
trespass to chattels doctrine, suggest that contract law may provide a less
demanding legal method of preventing the search of websites by data robots. If
blocking collection of pricing data is as simple as posting an online contract,
the question arises whether this end result is desirable and legally viable.",algorithm detect unfair pricing website
http://arxiv.org/abs/1905.07026v1,"Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.",algorithm detect unfair pricing website
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",price discrimination
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination
http://arxiv.org/abs/1804.00480v1,"We consider the simplest and most fundamental problem of selling a single
item to a number of buyers whose values are drawn from known independent and
regular distributions. There are four most widely-used and widely-studied
mechanisms in this literature: Anonymous Posted-Pricing (AP), Second-Price
Auction with Anonymous Reserve (AR), Sequential Posted-Pricing (SPM), and
Myerson Auction (OPT). Myerson Auction is optimal but complicated, which also
suffers a few issues in practice such as fairness; AP is the simplest one but
its revenue is also the lowest among these four; AR and SPM are of intermediate
complexity and revenue. We study the revenue gaps among these four mechanisms,
which is defined as the largest ratio between revenues from two mechanisms. We
establish two tight ratios and one tighter bound:
  1. SPM/AP. This ratio studies the power of discrimination in pricing schemes.
We obtain the tight ratio of roughly $2.62$, closing the previous known bounds
$[e / (e - 1), e]$.
  2. AR/AP. This ratio studies the relative power of auction vs. pricing
schemes, when no discrimination is allowed. We get the tight ratio of $\pi^2 /
6 \approx 1.64$, closing the previous known bounds $[e / (e - 1), e]$.
  3. OPT/AR. This ratio studies the power of discrimination in auctions.
Previously, the revenue gap is known to be in interval $[2, e]$, and the
lower-bound of $2$ is conjectured to be tight~\cite{HR09,H13,AHNPY15}. We
disprove this conjecture by obtaining a better lower-bound of $2.15$.",price discrimination
http://arxiv.org/abs/1803.06797v5,"I consider the optimal hourly (or per-unit-time in general) pricing problem
faced by a freelance worker (or a service provider) on an on-demand service
platform. Service requests arriving while the worker is busy are lost forever.
Thus, the optimal hourly prices need to capture the average hourly opportunity
costs incurred by accepting jobs. Due to potential asymmetries in these costs,
price discrimination across jobs based on duration, characteristics of the
arrival process, etc., may be necessary for optimality, even if the customers'
hourly willingness to pay is identically distributed. I first establish that
such price discrimination is not necessary if the customer arrival process is
Poisson: in this case, the optimal policy charges an identical hourly rate for
all jobs. This result holds even if the earnings are discounted over time. I
then consider the case where the customers belong to different classes that are
differentiated in their willingness to pay. I present a simple and practical
iterative procedure to compute the optimal prices in this case under standard
regularity assumptions on the distributions of customer valuations. I finally
show that these insights continue to hold in the presence of competition
between multiple quality-differentiated workers, assuming a natural customer
choice model in which a customer always chooses the best available worker that
she can afford.",price discrimination
http://arxiv.org/abs/1405.5189v3,"There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling.",price discrimination
http://arxiv.org/abs/1710.10695v1,"There has been a great effort to transfer linear discriminant techniques that
operate on vector data to high-order data, generally referred to as Multilinear
Discriminant Analysis (MDA) techniques. Many existing works focus on maximizing
the inter-class variances to intra-class variances defined on tensor data
representations. However, there has not been any attempt to employ
class-specific discrimination criteria for the tensor data. In this paper, we
propose a multilinear subspace learning technique suitable for applications
requiring class-specific tensor models. The method maximizes the discrimination
of each individual class in the feature space while retains the spatial
structure of the input. We evaluate the efficiency of the proposed method on
two problems, i.e. facial image analysis and stock price prediction based on
limit order book data.",price discrimination
http://arxiv.org/abs/1703.06660v1,"We present in this work an economic analysis of ransomware, with relevant
data from Cryptolocker, CryptoWall, TeslaCrypt and other major strands. We
include a detailed study of the impact that different price discrimination
strategies can have on the success of a ransomware family, examining uniform
pricing, optimal price discrimination and bargaining strategies and analysing
their advantages and limitations. In addition, we present results of a
preliminary survey that can helps in estimating an optimal ransom value. We
discuss at each stage whether the different schemes we analyse have been
encountered already in existing malware, and the likelihood of them being
implemented and becoming successful. We hope this work will help to gain some
useful insights for predicting how ransomware may evolve in the future and be
better prepared to counter its current and future threat.",price discrimination
http://arxiv.org/abs/1411.1381v1,"We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or ""buy-it-now"" pricing (BIN),
and ""pay-per-play"" (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0.",price discrimination
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination
http://arxiv.org/abs/1703.00980v3,"This paper analyzes the impact of peer effects on electricity consumption of
a network of rational, utility-maximizing users. Users derive utility from
consuming electricity as well as consuming less energy than their neighbors.
However, a disutility is incurred for consuming more than their neighbors. To
maximize the profit of the load-serving entity that provides electricity to
such users, we develop a two-stage game-theoretic model, where the entity sets
the prices in the first stage. In the second stage, consumers decide on their
demand in response to the observed price set in the first stage so as to
maximize their utility. To this end, we derive theoretical statements under
which such peer effects reduce aggregate user consumption. Further, we obtain
expressions for the resulting electricity consumption and profit of the load
serving entity for the case of perfect price discrimination and a single price
under complete information, and approximations under incomplete information.
Simulations suggest that exposing only a selected subset of all users to peer
effects maximizes the entity's profit.",price discrimination
http://arxiv.org/abs/cs/0508028v1,"We present a mechanism for reservations of bursty resources that is both
truthful and robust. It consists of option contracts whose pricing structure
induces users to reveal the true likelihoods that they will purchase a given
resource. Users are also allowed to adjust their options as their likelihood
changes. This scheme helps users save cost and the providers to plan ahead so
as to reduce the risk of under-utilization and overbooking. The mechanism
extracts revenue similar to that of a monopoly provider practicing temporal
pricing discrimination with a user population whose preference distribution is
known in advance.",price discrimination
http://arxiv.org/abs/1304.6806v2,"We study scenarios where multiple sellers of a homogeneous good compete on
prices, where each seller can only sell to some subset of the buyers.
Crucially, sellers cannot price-discriminate between buyers. We model the
structure of the competition by a graph (or hyper-graph), with nodes
representing the sellers and edges representing populations of buyers. We study
equilibria in the game between the sellers, prove that they always exist, and
present various structural, quantitative, and computational results about them.
We also analyze the equilibria completely for a few cases. Many questions are
left open.",price discrimination
http://arxiv.org/abs/1706.01131v2,"We study the optimal pricing strategy of a monopolist selling homogeneous
goods to customers over multiple periods. The customers choose their time of
purchase to maximize their payoff that depends on their valuation of the
product, the purchase price, and the utility they derive from past purchases of
others, termed the network effect. We first show that the optimal price
sequence is non-decreasing. Therefore, by postponing purchase to future rounds,
customers trade-off a higher utility from the network effects with a higher
price. We then show that a customer's equilibrium strategy can be characterized
by a threshold rule in which at each round a customer purchases the product if
and only if her valuation exceeds a certain threshold. This implies that
customers face an inference problem regarding the valuations of others, i.e.,
observing that a customer has not yet purchased the product, signals that her
valuation is below a threshold. We consider a block model of network
interactions, where there are blocks of buyers subject to the same network
effect. A natural benchmark, this model allows us to provide an explicit
characterization of the optimal price sequence asymptotically as the number of
agents goes to infinity, which notably is linearly increasing in time with a
slope that depends on the network effect through a scalar given by the sum of
entries of the inverse of the network weight matrix. Our characterization shows
that increasing the ""imbalance"" in the network defined as the difference
between the in and out degree of the nodes increases the revenue of the
monopolist. We further study the effects of price discrimination and show that
in earlier periods monopolist offers lower prices to blocks with higher
Bonacich centrality to encourage them to purchase, which in turn further
incentivizes other customers to buy in subsequent periods.",price discrimination
http://arxiv.org/abs/1903.11469v1,"Increased data gathering capacity, together with the spreading of data
analytics techniques, has allowed an unprecedented concentration of information
related to the individuals' preferences in the hands of a few gatekeepers. In
such context, the traditional economic literature has been attempting to frame
all the data-driven economy features. Such features, although being able to
bring about a more efficient matching of people and relevant purchase
opportunities, also result into distortions and disequilibria, up to market
failures. Data-economy market disequilibria can be decrypted by leveraging on
some of the known network properties, thus obtaining general results suitable
for building a new theoretical framework for economic phenomena. Starting from
the hypothesis that a digital company can always benefit from an underlying
network of consumers or items related to its market, their representation can
indeed provide significant competitive advantages, also enhancing the
platforms' capacity to implement discriminatory practices by means of an
increased ability to estimate individuals' preferences. In the present paper,
we propose a measure called Information Patrimony, considering the amount of
information available within the system and we look into how platforms may
exploit data stemming from connected profiles within a network, with a view to
obtaining competitive advantages. Such information flow may eventually allow to
envisage the emergence of a new hybrid price discrimination pattern, through
which platforms may influence and steer individuals' purchase choices, as well
as to apply different prices to different customers.",price discrimination
http://arxiv.org/abs/1708.00754v1,"Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.",price discrimination
http://arxiv.org/abs/1803.04376v2,"One property that remains lacking in image captions generated by contemporary
methods is discriminability: being able to tell two images apart given the
caption for one of them. We propose a way to improve this aspect of caption
generation. By incorporating into the captioning training objective a loss
component directly related to ability (by a machine) to disambiguate
image/caption matches, we obtain systems that produce much more discriminative
caption, according to human evaluation. Remarkably, our approach leads to
improvement in other aspects of generated captions, reflected by a battery of
standard scores such as BLEU, SPICE etc. Our approach is modular and can be
applied to a variety of model/loss combinations commonly proposed for image
captioning.",price discrimination
http://arxiv.org/abs/cs/0308037v1,"A very complex vision system is developed to detect luminosity variations
connected with the discovery of new planets in the Universe. The traditional
imaging system can not manage a so large load. A private net is implemented to
perform an automatic vision and decision architecture. It lets to carry out an
on-line discrimination of interesting events by using two levels of triggers.
This system can even manage many Tbytes of data per day. The architecture
avails itself of a distributed parallel network system based on a maximum of
256 standard workstations with Microsoft Window as OS.",price discrimination detection
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination detection
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination detection
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination detection
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination detection
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination detection
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",price discrimination detection
http://arxiv.org/abs/1508.03928v1,"In this paper, we propose a novel deep neural network framework embedded with
low-level features (LCNN) for salient object detection in complex images. We
utilise the advantage of convolutional neural networks to automatically learn
the high-level features that capture the structured information and semantic
context in the image. In order to better adapt a CNN model into the saliency
task, we redesign the network architecture based on the small-scale datasets.
Several low-level features are extracted, which can effectively capture
contrast and spatial information in the salient regions, and incorporated to
compensate with the learned high-level features at the output of the last fully
connected layer. The concatenated feature vector is further fed into a
hinge-loss SVM detector in a joint discriminative learning manner and the final
saliency score of each region within the bounding box is obtained by the linear
combination of the detector's weights. Experiments on three challenging
benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be
effective and superior than most low-level oriented state-of-the-arts in terms
of P-R curves, F-measure and mean absolute errors.",price discrimination detection
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination detection
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",price discrimination detection
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination detection
http://arxiv.org/abs/1804.00480v1,"We consider the simplest and most fundamental problem of selling a single
item to a number of buyers whose values are drawn from known independent and
regular distributions. There are four most widely-used and widely-studied
mechanisms in this literature: Anonymous Posted-Pricing (AP), Second-Price
Auction with Anonymous Reserve (AR), Sequential Posted-Pricing (SPM), and
Myerson Auction (OPT). Myerson Auction is optimal but complicated, which also
suffers a few issues in practice such as fairness; AP is the simplest one but
its revenue is also the lowest among these four; AR and SPM are of intermediate
complexity and revenue. We study the revenue gaps among these four mechanisms,
which is defined as the largest ratio between revenues from two mechanisms. We
establish two tight ratios and one tighter bound:
  1. SPM/AP. This ratio studies the power of discrimination in pricing schemes.
We obtain the tight ratio of roughly $2.62$, closing the previous known bounds
$[e / (e - 1), e]$.
  2. AR/AP. This ratio studies the relative power of auction vs. pricing
schemes, when no discrimination is allowed. We get the tight ratio of $\pi^2 /
6 \approx 1.64$, closing the previous known bounds $[e / (e - 1), e]$.
  3. OPT/AR. This ratio studies the power of discrimination in auctions.
Previously, the revenue gap is known to be in interval $[2, e]$, and the
lower-bound of $2$ is conjectured to be tight~\cite{HR09,H13,AHNPY15}. We
disprove this conjecture by obtaining a better lower-bound of $2.15$.",price discrimination detection
http://arxiv.org/abs/1505.01546v2,"There is a key research issue to accurately select out neutron signals and
discriminate gamma signals from a mixed radiation field in the neutron
detection. This paper proposes a fractal spectrum discrimination approach by
means of different spectrum characteristics of neutron and gamma. Figure of
merit and average discriminant error ratio are adopted together to evaluate the
discriminant effects. Different neutron and gamma signals with various noises
and pulse pile-ups are simulated according to real data in the literature. The
proposed approach is compared with the digital charge integration and pulse
gradient methods. It is found that the fractal approach exhibits the best
discriminant performance among three methods. The fractal spectrum approach is
not sensitive to the high frequency noises and pulse pile-ups. It means that
the proposed approach takes the advantages of anti-noises and high discriminant
ability, and can be used to better discriminate neutron and gamma in neutron
detection.",price discrimination detection
http://arxiv.org/abs/1803.06797v5,"I consider the optimal hourly (or per-unit-time in general) pricing problem
faced by a freelance worker (or a service provider) on an on-demand service
platform. Service requests arriving while the worker is busy are lost forever.
Thus, the optimal hourly prices need to capture the average hourly opportunity
costs incurred by accepting jobs. Due to potential asymmetries in these costs,
price discrimination across jobs based on duration, characteristics of the
arrival process, etc., may be necessary for optimality, even if the customers'
hourly willingness to pay is identically distributed. I first establish that
such price discrimination is not necessary if the customer arrival process is
Poisson: in this case, the optimal policy charges an identical hourly rate for
all jobs. This result holds even if the earnings are discounted over time. I
then consider the case where the customers belong to different classes that are
differentiated in their willingness to pay. I present a simple and practical
iterative procedure to compute the optimal prices in this case under standard
regularity assumptions on the distributions of customer valuations. I finally
show that these insights continue to hold in the presence of competition
between multiple quality-differentiated workers, assuming a natural customer
choice model in which a customer always chooses the best available worker that
she can afford.",price discrimination detection
http://arxiv.org/abs/1405.5189v3,"There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling.",price discrimination detection
http://arxiv.org/abs/1710.10695v1,"There has been a great effort to transfer linear discriminant techniques that
operate on vector data to high-order data, generally referred to as Multilinear
Discriminant Analysis (MDA) techniques. Many existing works focus on maximizing
the inter-class variances to intra-class variances defined on tensor data
representations. However, there has not been any attempt to employ
class-specific discrimination criteria for the tensor data. In this paper, we
propose a multilinear subspace learning technique suitable for applications
requiring class-specific tensor models. The method maximizes the discrimination
of each individual class in the feature space while retains the spatial
structure of the input. We evaluate the efficiency of the proposed method on
two problems, i.e. facial image analysis and stock price prediction based on
limit order book data.",price discrimination detection
http://arxiv.org/abs/1703.06660v1,"We present in this work an economic analysis of ransomware, with relevant
data from Cryptolocker, CryptoWall, TeslaCrypt and other major strands. We
include a detailed study of the impact that different price discrimination
strategies can have on the success of a ransomware family, examining uniform
pricing, optimal price discrimination and bargaining strategies and analysing
their advantages and limitations. In addition, we present results of a
preliminary survey that can helps in estimating an optimal ransom value. We
discuss at each stage whether the different schemes we analyse have been
encountered already in existing malware, and the likelihood of them being
implemented and becoming successful. We hope this work will help to gain some
useful insights for predicting how ransomware may evolve in the future and be
better prepared to counter its current and future threat.",price discrimination detection
http://arxiv.org/abs/1906.05740v2,"Information transfer between time series is calculated by using the
asymmetric information-theoretic measure known as transfer entropy. Geweke's
autoregressive formulation of Granger causality is used to find linear transfer
entropy, and Schreiber's general, non-parametric, information-theoretic
formulation is used to detect non-linear transfer entropy.
  We first validate these measures against synthetic data. Then we apply these
measures to detect causality between social sentiment and cryptocurrency
prices. We perform significance tests by comparing the information transfer
against a null hypothesis, determined via shuffled time series, and calculate
the Z-score. We also investigate different approaches for partitioning in
nonparametric density estimation which can improve the significance of results.
  Using these techniques on sentiment and price data over a 48-month period to
August 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple
(XRP), litecoin (LTC) and ethereum (ETH), we detect significant information
transfer, on hourly timescales, in directions of both sentiment to price and of
price to sentiment. We report the scale of non-linear causality to be an order
of magnitude greater than linear causality.",price discrimination detection
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination algorithm
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination algorithm
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination algorithm
http://arxiv.org/abs/1708.00754v1,"Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.",price discrimination algorithm
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination algorithm
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination algorithm
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination algorithm
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination algorithm
http://arxiv.org/abs/1905.05922v1,"An effective way for a Mobile network operator (MNO) to improve its revenue
is price discrimination, i.e., providing different combinations of data caps
and subscription fees. Rollover data plan (allowing the unused data in the
current month to be used in the next month) is an innovative data mechanism
with time flexibility. In this paper, we study the MNO's optimal multi-cap data
plans with time flexibility in a realistic asymmetric information scenario.
Specifically, users are associated with multi-dimensional private information,
and the MNO designs a contract (with different data caps and subscription fees)
to induce users to truthfully reveal their private information. This problem is
quite challenging due to the multi-dimensional private information. We address
the challenge in two aspects. First, we find that a feasible contract
(satisfying incentive compatibility and individual rationality) should allocate
the data caps according to users' willingness-to-pay (captured by the slopes of
users' indifference curves). Second, for the non-convex data cap allocation
problem, we propose a Dynamic Quota Allocation Algorithm, which has a low
complexity and guarantees the global optimality. Numerical results show that
the time-flexible data mechanisms increase both the MNO's profit (25% on
average) and users' payoffs (8.2% on average) under price discrimination.",price discrimination algorithm
http://arxiv.org/abs/1508.05347v3,"Data as a commodity has always been purchased and sold. Recently, web
services that are data marketplaces have emerged that match data buyers with
data sellers. So far there are no guidelines how to price queries against a
database. We consider the recently proposed query-based pricing framework of
Koutris et al and ask the question of computing optimal input prices in this
framework by formulating a buyer utility model.
  We establish the interesting and deep equivalence between arbitrage-freeness
in the query-pricing framework and envy-freeness in pricing theory for
appropriately chosen buyer valuations. Given the approximation hardness results
from envy-free pricing we then develop logarithmic approximation pricing
algorithms exploiting the max flow interpretation of the arbitrage-free pricing
for the restricted query language proposed by Koutris et al. We propose a novel
polynomial-time logarithmic approximation pricing scheme and show that our new
scheme performs better than the existing envy-free pricing algorithms
instance-by-instance. We also present a faster pricing algorithm that is always
greater than the existing solutions, but worse than our previous scheme. We
experimentally show how our pricing algorithms perform with respect to the
existing envy-free pricing algorithms and to the optimal exponentially
computable solution, and our experiments show that our approximation algorithms
consistently arrive at about 99% of the optimal.",price discrimination algorithm
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination algorithm
http://arxiv.org/abs/1202.2840v2,"Consider the following toy problem. There are $m$ rectangles and $n$ points
on the plane. Each rectangle $R$ is a consumer with budget $B_R$, who is
interested in purchasing the cheapest item (point) inside R, given that she has
enough budget. Our job is to price the items to maximize the revenue. This
problem can also be defined on higher dimensions. We call this problem the
geometric pricing problem.
  In this paper, we study a new class of problems arising from a geometric
aspect of the pricing problem. It intuitively captures typical real-world
assumptions that have been widely studied in marketing research, healthcare
economics, etc. It also helps classify other well-known pricing problems, such
as the highway pricing problem and the graph vertex pricing problem on planar
and bipartite graphs. Moreover, this problem turns out to have close
connections to other natural geometric problems such as the geometric versions
of the unique coverage and maximum feasible subsystem problems.
  We show that the low dimensionality arising in this pricing problem does lead
to improved approximation ratios, by presenting sublinear-approximation
algorithms for two central versions of the problem: unit-demand uniform-budget
min-buying and single-minded pricing problems. Our algorithm is obtained by
combining algorithmic pricing and geometric techniques. These results suggest
that considering geometric aspect might be a promising research direction in
obtaining improved approximation algorithms for such pricing problems. To the
best of our knowledge, this is one of very few problems in the intersection
between geometry and algorithmic pricing areas. Thus its study may lead to new
algorithmic techniques that could benefit both areas.",price discrimination algorithm
http://arxiv.org/abs/0910.0110v1,"We consider the Stackelberg shortest-path pricing problem, which is defined
as follows. Given a graph G with fixed-cost and pricable edges and two distinct
vertices s and t, we may assign prices to the pricable edges. Based on the
predefined fixed costs and our prices, a customer purchases a cheapest s-t-path
in G and we receive payment equal to the sum of prices of pricable edges
belonging to the path. Our goal is to find prices maximizing the payment
received from the customer. While Stackelberg shortest-path pricing was known
to be APX-hard before, we provide the first explicit approximation threshold
and prove hardness of approximation within 2-o(1).",price discrimination algorithm
http://arxiv.org/abs/1709.07534v1,"E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.",price discrimination algorithm
http://arxiv.org/abs/0908.2834v1,"Most recent papers addressing the algorithmic problem of allocating
advertisement space for keywords in sponsored search auctions assume that
pricing is done via a first-price auction, which does not realistically model
the Generalized Second Price (GSP) auction used in practice. Towards the goal
of more realistically modeling these auctions, we introduce the Second-Price Ad
Auctions problem, in which bidders' payments are determined by the GSP
mechanism. We show that the complexity of the Second-Price Ad Auctions problem
is quite different than that of the more studied First-Price Ad Auctions
problem. First, unlike the first-price variant, for which small constant-factor
approximations are known, it is NP-hard to approximate the Second-Price Ad
Auctions problem to any non-trivial factor. Second, this discrepancy extends
even to the 0-1 special case that we call the Second-Price Matching problem
(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This stands
in contrast to the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a classic result on the
performance of the ""Ranking"" algorithm for online bipartite matching.",price discrimination algorithm
http://arxiv.org/abs/1611.02442v3,"We study approximation algorithms for revenue maximization based on static
item pricing, where a seller chooses prices for various goods in the market,
and then the buyers purchase utility-maximizing bundles at these given prices.
We formulate two somewhat general techniques for designing good pricing
algorithms for this setting: Price Doubling and Item Halving. Using these
techniques, we unify many of the existing results in the item pricing
literature under a common framework, as well as provide several new item
pricing algorithms for approximating both revenue and social welfare. More
specifically, for a variety of settings with item pricing, we show that it is
possible to deterministically obtain a log-approximation for revenue and a
constant-approximation for social welfare simultaneously: thus one need not
sacrifice revenue if the goal is to still have decent welfare guarantees. %In
addition, we provide a new black-box reduction from revenue to welfare based on
item pricing, which immediately gives us new revenue-approximation algorithms
(e.g., for gross substitutes valuations).
  The main technical contribution of this paper is a $O((\log m + \log
k)^2)$-approximation algorithm for revenue maximization based on the Item
Halving technique, for settings where buyers have XoS valuations, where $m$ is
the number of goods and $k$ is the average supply. Surprisingly, ours is the
first known item pricing algorithm with polylogarithmic approximations for such
general classes of valuations, and partially resolves an important open
question from the algorithmic pricing literature about the existence of item
pricing algorithms with logarithmic factors for general valuations. We also use
the Item Halving framework to form envy-free item pricing mechanisms for the
popular setting of multi-unit markets, providing a log-approximation to revenue
in this case.",price discrimination algorithm
http://arxiv.org/abs/1805.02574v1,"We study revenue optimization pricing algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation. We show that, in the case when both the seller and the
buyer have the same discounting in their cumulative utilities (revenue and
surplus), there exist two optimal algorithms. The first one constantly offers
the Myerson price, while the second pricing proposes a ""big deal"": pay for all
goods in advance (at the first round) or get nothing. However, when there is an
imbalance between the seller and the buyer in the patience to wait for utility,
we find that the constant pricing, surprisingly, is no longer optimal. First,
it is outperformed by the pricing algorithm ""big deal"", when the seller's
discount rate is lower than the one of the buyer. Second, in the inverse case
of a less patient buyer, we reduce the problem of finding an optimal algorithm
to a multidimensional optimization problem (a multivariate analogue of the
functional used to determine Myerson's price) that does not admit a closed form
solution in general, but can be solved by numerical optimization techniques
(e.g., gradient ones). We provide extensive analysis of numerically found
optimal algorithms to demonstrate that they are non-trivial, may be
non-consistent, and generate larger expected revenue than the constant pricing
with the Myerson price.",price discrimination algorithm
http://arxiv.org/abs/1610.08890v2,"Parking prices in cities are uniform over large areas and do not reflect
spatially heterogeneous parking supply and demand. Underpricing results in high
parking occupancy in the subareas where the demand exceeds supply and long
search for the vacant parking, whereas overpricing leads to low occupancy and
hampered economic vitality. We present Nearest Pocket for Prices Algorithm
(NPPA), a spatially explicit algorithm for establishing on-and off-street
parking prices that guarantee a predetermined uniform level of occupation over
the entire parking space. We apply NPPA for establishing heterogeneous parking
prices that guarantee 90% parking occupancy in the Israeli city of Bat Yam.",price discrimination algorithm
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination algorithm
http://arxiv.org/abs/1709.01654v2,"Hormozgan Province, located in the south of Iran, faces several challenges
regarding water resources management. The first one is the discharge of a
massive volume of water to the Persian Gulf because of the concentration of the
annual rainfalls in a short period of time and the narrow distance between the
headwater and the coast. The second one is the unbalanced development of
economic sectors in comparison with distribution of fresh water resources.
Finally, long-term drought is also common in this area. The construction of a
carry-over dam (Esteghlal Dam) and several conveyance pipelines and withdrawing
of the surface water and groundwater resources were considered as the solution
to deal with those challenges. During recent drought, severe overdraft and
inefficient use of recourses confirmed the fact that all done before are not
enough. During this period, there was a tendency to store water in reservoir in
order to meet the demand of the urban sector. Therefore, the agricultural
demand was the first victim of water allocation policy. It caused over
exploitation of the groundwater resources (to meet the agricultural demand) and
considerable losses (evaporation and leakage) from the reservoir. All of the
above-mentioned problems confirm the necessity of the development of a
conjunctive use policy. In this paper, all demand related to the Esteghlal Dam
and the Minab Aquifer (Bandarabbas City and agriculture in the Minab Plain)
were considered as the case study. The main objective was to find the best
applicable conjunctive policy which as well guarantees the conservation of the
Minab Aquifer. Alternative water allocation policies have been developed based
on the present capacities and the experience of local operating staff.",short withdrawal period
http://arxiv.org/abs/1209.5147v2,"We study numerically the hydrodynamics of dip coating from a suspension and
report a mechanism for colloidal assembly and pattern formation on smooth and
uniform substrates. Below a critical withdrawal speed of the substrate,
capillary forces required to deform the meniscus prevent colloidal particles
from entering the coating film. Capillary forces are overcome by hydrodynamic
drag only after a minimum number of particles organize in a close-packed
formation within the meniscus. Once within the film, the formed assembly moves
at nearly the withdrawal speed and rapidly separates from the next assembly.
The interplay between hydrodynamic and capillary forces can thus produce
periodic and regular structures within the curved meniscus that extends below
the withdrawn film. The hydrodynamically-driven assembly documented here is
consistent with stripe pattern formations observed experimentally in the
so-called thin-film entrainment regime.",short withdrawal period
http://arxiv.org/abs/1807.05782v2,"Rabin encryption and a secure ownership transfer protocol based on the
difficulty of factorization of a public key use a small public exponent. Such
encryption requires random number padding. The Coppersmith's shortpad attack
works effectively on short padding, thereby allowing an adversary to extract
the secret message. However, the criteria for determining the appropriate
padding size remains unclear. In this paper, we derived the processing-time
formula for the shortpad attack and determined the optimal random-padding size
in order to achieve the desired security.",short withdrawal period
http://arxiv.org/abs/1407.2137v1,"The above article, published online on 28 April 2014 in Wiley Online Library
(wileyonlinelibrary.com), has been withdrawn with agreement from the journal
Editor-in-Chief, Blaise Cronin, and Wiley Periodicals, Inc. The withdrawal has
been agreed for legal reasons.",short withdrawal period
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",short withdrawal period
http://arxiv.org/abs/1708.00157v1,"An existing pseudo-commodity and a smart contracts framework allow the
creation of a purely automatic and self-sufficient price-stable cryptocurrency,
without human intervention. This new currency, we denominated Toroid or TRD,
can be used more extensively for commerce than pseudo commodity
cryptocurrencies due to its lower volatility. Also, is suitable for investment,
as the tokens in each account multiply, return interest, when the market grows.
Like the controlled fiat money of a central bank plus the benefits of an
inflation-adjusted perpetuity bond. Collateral in base coin, for example BTC or
ETH, can be added to bootstrap your own Toroid investment or withdrawed after a
very small investment period. So, the Toroids are not created from nothing nor
have a limited monetary base. The minimum investment period can be very small,
for example one day, and you keep the interest but you can return the Toroids
and refund your collateral. That is a one-side only peg to a deflationary
crypto-commodity. The stability is guaranteed by endogenous measurements of
number of transactions and wallet pro-rated rebasement of balance to reduce
volatility of price. Each account has its own rebasement due to the account
creation timestamp. Rebasement control mechanism is progressive during initial
bootstrap period because price manipulation protection is more severe when the
capital involved is smaller. Rebasement has a quick positive start to
incentivize early adopters that see only big growth in their TRD account during
bootstrap period. Finally, the new rebasement control makes it economically
infeasible for an attacker targeting the coin with manipulated transaction
volume if we set the minimum rebasement greater than profits from massive
currency manipulation.",short withdrawal period
http://arxiv.org/abs/1002.3916v1,"Short-interval water level measurements using automatic water level recorder
in a deep well in an unconfined crystalline rock aquifer at the campus of NGRI,
near Hyderabad shows a cyclic fluctuation in the water levels. The observed
values clearly show the principal trend due to rainfall recharge. Spectral
analysis was carried out to evaluate correlation of the cyclic fluctuation to
the synthetic earth tides as well as groundwater withdrawal time series in the
surrounding. It was found that these fluctuations have considerably high
correlation with earth tides whereas groundwater pumping does not show any
significant correlation with water table fluctuations. It is concluded that
earth tides cause the fluctuation in the water table. These fluctuations were
hitherto unobserved during manual observations made over larger time intervals.
It indicates that the unconfined aquifer is characterised by a low porosity.",short withdrawal period
http://arxiv.org/abs/1710.11271v2,"Most social platforms offer mechanisms allowing users to delete their posts,
and a significant fraction of users exercise this right to be forgotten.
However, ironically, users' attempt to reduce attention to sensitive posts via
deletion, in practice, attracts unwanted attention from stalkers specifically
to those posts. Thus, deletions may leave users more vulnerable to attacks on
their privacy in general. Users hoping to make their posts forgotten face a
""damned if I do, damned if I don't"" dilemma. Many are shifting towards
ephemeral social platform like Snapchat, which will deprive us of important
user-data archival. In the form of intermittent withdrawals, we present, Lethe,
a novel solution to this problem of forgetting the forgotten. If the
next-generation social platforms are willing to give up the uninterrupted
availability of non-deleted posts by a very small fraction, Lethe provides
privacy to the deleted posts over long durations. In presence of Lethe, an
adversarial observer becomes unsure if some posts are permanently deleted or
just temporarily withdrawn by Lethe; at the same time, the adversarial observer
is overwhelmed by a large number of falsely flagged undeleted posts. To
demonstrate the feasibility and performance of Lethe, we analyze large-scale
real data about users' deletion over Twitter and thoroughly investigate how to
choose time duration distributions for alternating between temporary
withdrawals and resurrections of non-deleted posts. We find a favorable
trade-off between privacy, availability and adversarial overhead in different
settings for users exercising their right to delete. We show that, even against
an ultimate adversary with an uninterrupted access to the entire platform,
Lethe offers deletion privacy for up to 3 months from the time of deletion,
while maintaining content availability as high as 95% and keeping the
adversarial precision to 20%.",withdrawal rights
http://arxiv.org/abs/1703.07521v5,"We show that for a relation $f\subseteq \{0,1\}^n\times \mathcal{O}$ and a
function $g:\{0,1\}^{m}\times \{0,1\}^{m} \rightarrow \{0,1\}$ (with $m= O(\log
n)$), $$\mathrm{R}_{1/3}(f\circ g^n) = \Omega\left(\mathrm{R}_{1/3}(f) \cdot
\left(\log\frac{1}{\mathrm{disc}(M_g)} - O(\log n)\right)\right),$$ where
$f\circ g^n$ represents the composition of $f$ and $g^n$, $M_g$ is the sign
matrix for $g$, $\mathrm{disc}(M_g)$ is the discrepancy of $M_g$ under the
uniform distribution and $\mathrm{R}_{1/3}(f)$ ($\mathrm{R}_{1/3}(f\circ g^n)$)
denotes the randomized query complexity of $f$ (randomized communication
complexity of $f\circ g^n$) with worst case error $\frac{1}{3}$.
  In particular, this implies that for a relation $f\subseteq \{0,1\}^n\times
\mathcal{O}$, $$\mathrm{R}_{1/3}(f\circ \mathrm{IP}_m^n) =
\Omega\left(\mathrm{R}_{1/3}(f) \cdot m\right),$$ where
$\mathrm{IP}_m:\{0,1\}^m\times \{0,1\}^m\rightarrow \{0,1\}$ is the Inner
Product (modulo $2$) function and $m= O(\log(n))$.",withdrawal rights
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",withdrawal rights
http://arxiv.org/abs/1810.00392v1,"We study the classical, two-sided stable marriage problem under pairwise
preferences. In the most general setting, agents are allowed to express their
preferences as comparisons of any two of their edges and they also have the
right to declare a draw or even withdraw from such a comparison. This freedom
is then gradually restricted as we specify six stages of orderedness in the
preferences, ending with the classical case of strictly ordered lists. We study
all cases occurring when combining the three known notions of stability---weak,
strong and super-stability---under the assumption that each side of the
bipartite market obtains one of the six degrees of orderedness. By designing
three polynomial algorithms and two NP-completeness proofs we determine the
complexity of all cases not yet known, and thus give an exact boundary in terms
of preference structure between tractable and intractable cases.",withdrawal rights
http://arxiv.org/abs/0707.1859v2,"After receiving useful peer comments, we would like to withdraw this paper.",withdrawal rights
http://arxiv.org/abs/1701.06989v4,"We consider an efficiently decodable non-adaptive group testing (NAGT)
problem that meets theoretical bounds. The problem is to find a few specific
items (at most $d$) satisfying certain characteristics in a colossal number of
$N$ items as quickly as possible. Those $d$ specific items are called
\textit{defective items}. The idea of NAGT is to pool a group of items, which
is called \textit{a test}, then run a test on them. If the test outcome is
\textit{positive}, there exists at least one defective item in the test, and if
it is \textit{negative}, there exists no defective items. Formally, a binary $t
\times N$ measurement matrix $\mathcal{M} = (m_{ij})$ is the representation for
$t$ tests where row $i$ stands for test $i$ and $m_{ij} = 1$ if and only if
item $j$ belongs to test $i$.
  There are three main objectives in NAGT: minimize the number of tests $t$,
construct matrix $\mathcal{M}$, and identify defective items as quickly as
possible. In this paper, we present a strongly explicit construction of
$\mathcal{M}$ for when the number of defective items is at most 2, with the
number of tests $t \simeq 16 \log{N} = O(\log{N})$. In particular, we need only
$K \simeq N \times 16\log{N} = O(N\log{N})$ bits to construct such matrices,
which is optimal. Furthermore, given these $K$ bits, any entry in the matrix
can be constructed in time $O \left(\ln{N}/ \ln{\ln{N}} \right)$. Moreover,
$\mathcal{M}$ can be decoded with high probability in time $O\left(
\frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$. When the number of defective items is
greater than 2, we present a scheme that can identify at least $(1-\epsilon)d$
defective items with $t \simeq 32 C(\epsilon) d \log{N} = O(d \log{N})$ in time
$O \left( d \frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$ for any close-to-zero
$\epsilon$, where $C(\epsilon)$ is a constant that depends only on $\epsilon$.",withdrawal rights
http://arxiv.org/abs/cs/0207050v2,"Constraint logic programming combines declarativity and efficiency thanks to
constraint solvers implemented for specific domains. Value withdrawal
explanations have been efficiently used in several constraints programming
environments but there does not exist any formalization of them. This paper is
an attempt to fill this lack. Furthermore, we hope that this theoretical tool
could help to validate some programming environments. A value withdrawal
explanation is a tree describing the withdrawal of a value during a domain
reduction by local consistency notions and labeling. Domain reduction is
formalized by a search tree using two kinds of operators: operators for local
consistency notions and operators for labeling. These operators are defined by
sets of rules. Proof trees are built with respect to these rules. For each
removed value, there exists such a proof tree which is the withdrawal
explanation of this value.",withdrawal rights
http://arxiv.org/abs/0804.4628v1,"Recently, Rawat and Saxena proposed a method for protecting data using
``Disclaimer Statement''. This paper presents some issues and several flaws in
their proposal.",withdrawal rights disclaimer
http://arxiv.org/abs/1710.11271v2,"Most social platforms offer mechanisms allowing users to delete their posts,
and a significant fraction of users exercise this right to be forgotten.
However, ironically, users' attempt to reduce attention to sensitive posts via
deletion, in practice, attracts unwanted attention from stalkers specifically
to those posts. Thus, deletions may leave users more vulnerable to attacks on
their privacy in general. Users hoping to make their posts forgotten face a
""damned if I do, damned if I don't"" dilemma. Many are shifting towards
ephemeral social platform like Snapchat, which will deprive us of important
user-data archival. In the form of intermittent withdrawals, we present, Lethe,
a novel solution to this problem of forgetting the forgotten. If the
next-generation social platforms are willing to give up the uninterrupted
availability of non-deleted posts by a very small fraction, Lethe provides
privacy to the deleted posts over long durations. In presence of Lethe, an
adversarial observer becomes unsure if some posts are permanently deleted or
just temporarily withdrawn by Lethe; at the same time, the adversarial observer
is overwhelmed by a large number of falsely flagged undeleted posts. To
demonstrate the feasibility and performance of Lethe, we analyze large-scale
real data about users' deletion over Twitter and thoroughly investigate how to
choose time duration distributions for alternating between temporary
withdrawals and resurrections of non-deleted posts. We find a favorable
trade-off between privacy, availability and adversarial overhead in different
settings for users exercising their right to delete. We show that, even against
an ultimate adversary with an uninterrupted access to the entire platform,
Lethe offers deletion privacy for up to 3 months from the time of deletion,
while maintaining content availability as high as 95% and keeping the
adversarial precision to 20%.",withdrawal rights disclaimer
http://arxiv.org/abs/1703.07521v5,"We show that for a relation $f\subseteq \{0,1\}^n\times \mathcal{O}$ and a
function $g:\{0,1\}^{m}\times \{0,1\}^{m} \rightarrow \{0,1\}$ (with $m= O(\log
n)$), $$\mathrm{R}_{1/3}(f\circ g^n) = \Omega\left(\mathrm{R}_{1/3}(f) \cdot
\left(\log\frac{1}{\mathrm{disc}(M_g)} - O(\log n)\right)\right),$$ where
$f\circ g^n$ represents the composition of $f$ and $g^n$, $M_g$ is the sign
matrix for $g$, $\mathrm{disc}(M_g)$ is the discrepancy of $M_g$ under the
uniform distribution and $\mathrm{R}_{1/3}(f)$ ($\mathrm{R}_{1/3}(f\circ g^n)$)
denotes the randomized query complexity of $f$ (randomized communication
complexity of $f\circ g^n$) with worst case error $\frac{1}{3}$.
  In particular, this implies that for a relation $f\subseteq \{0,1\}^n\times
\mathcal{O}$, $$\mathrm{R}_{1/3}(f\circ \mathrm{IP}_m^n) =
\Omega\left(\mathrm{R}_{1/3}(f) \cdot m\right),$$ where
$\mathrm{IP}_m:\{0,1\}^m\times \{0,1\}^m\rightarrow \{0,1\}$ is the Inner
Product (modulo $2$) function and $m= O(\log(n))$.",withdrawal rights disclaimer
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",withdrawal rights disclaimer
http://arxiv.org/abs/1201.1188v3,"Biometrical authentication systems are often presented as the best and
simplest way to reach higher security levels. But a deeper analysis shows that
several risks are hidden and the service provider adopting those system has to
carefully check its liabilities before deploying them.",withdrawal rights disclaimer
http://arxiv.org/abs/1402.2757v1,"I have been asked to write brief, gentle introduction to the basic idea
behind the field of ""quantum gravity"" in 1500 words or less. Doing so appears
to be almost as great a challenge as coming up with a consistent theory of
quantum gravity. However, I will try. Disclaimer: \emph{The views expressed in
this article are my own and do not represent the consensus of the quantum
gravity community}.",withdrawal rights disclaimer
http://arxiv.org/abs/1810.00392v1,"We study the classical, two-sided stable marriage problem under pairwise
preferences. In the most general setting, agents are allowed to express their
preferences as comparisons of any two of their edges and they also have the
right to declare a draw or even withdraw from such a comparison. This freedom
is then gradually restricted as we specify six stages of orderedness in the
preferences, ending with the classical case of strictly ordered lists. We study
all cases occurring when combining the three known notions of stability---weak,
strong and super-stability---under the assumption that each side of the
bipartite market obtains one of the six degrees of orderedness. By designing
three polynomial algorithms and two NP-completeness proofs we determine the
complexity of all cases not yet known, and thus give an exact boundary in terms
of preference structure between tractable and intractable cases.",withdrawal rights disclaimer
http://arxiv.org/abs/0707.1859v2,"After receiving useful peer comments, we would like to withdraw this paper.",withdrawal rights disclaimer
http://arxiv.org/abs/1701.06989v4,"We consider an efficiently decodable non-adaptive group testing (NAGT)
problem that meets theoretical bounds. The problem is to find a few specific
items (at most $d$) satisfying certain characteristics in a colossal number of
$N$ items as quickly as possible. Those $d$ specific items are called
\textit{defective items}. The idea of NAGT is to pool a group of items, which
is called \textit{a test}, then run a test on them. If the test outcome is
\textit{positive}, there exists at least one defective item in the test, and if
it is \textit{negative}, there exists no defective items. Formally, a binary $t
\times N$ measurement matrix $\mathcal{M} = (m_{ij})$ is the representation for
$t$ tests where row $i$ stands for test $i$ and $m_{ij} = 1$ if and only if
item $j$ belongs to test $i$.
  There are three main objectives in NAGT: minimize the number of tests $t$,
construct matrix $\mathcal{M}$, and identify defective items as quickly as
possible. In this paper, we present a strongly explicit construction of
$\mathcal{M}$ for when the number of defective items is at most 2, with the
number of tests $t \simeq 16 \log{N} = O(\log{N})$. In particular, we need only
$K \simeq N \times 16\log{N} = O(N\log{N})$ bits to construct such matrices,
which is optimal. Furthermore, given these $K$ bits, any entry in the matrix
can be constructed in time $O \left(\ln{N}/ \ln{\ln{N}} \right)$. Moreover,
$\mathcal{M}$ can be decoded with high probability in time $O\left(
\frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$. When the number of defective items is
greater than 2, we present a scheme that can identify at least $(1-\epsilon)d$
defective items with $t \simeq 32 C(\epsilon) d \log{N} = O(d \log{N})$ in time
$O \left( d \frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$ for any close-to-zero
$\epsilon$, where $C(\epsilon)$ is a constant that depends only on $\epsilon$.",withdrawal rights disclaimer
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",mandatory disclosures detection
http://arxiv.org/abs/1711.02828v1,"Supervisory Control and Data Acquisition (SCADA) systems face the absence of
a protection technique that can beat different types of intrusions and protect
the data from disclosure while handling this data using other applications,
specifically Intrusion Detection System (IDS). The SCADA system can manage the
critical infrastructure of industrial control environments. Protecting
sensitive information is a difficult task to achieve in reality with the
connection of physical and digital systems. Hence, privacy preservation
techniques have become effective in order to protect sensitive/private
information and to detect malicious activities, but they are not accurate in
terms of error detection, sensitivity percentage of data disclosure. In this
paper, we propose a new Privacy Preservation Intrusion Detection (PPID)
technique based on the correlation coefficient and Expectation Maximisation
(EM) clustering mechanisms for selecting important portions of data and
recognizing intrusive events. This technique is evaluated on the power system
datasets for multiclass attacks to measure its reliability for detecting
suspicious activities. The experimental results outperform three techniques in
the above terms, showing the efficiency and effectiveness of the proposed
technique to be utilized for current SCADA systems.",mandatory disclosures detection
http://arxiv.org/abs/0709.3504v1,"A forty-four pass fibre optic surface plasmon resonance sensor that enhances
detection sensitivity according to the number of passes is demonstrated for the
first time. The technique employs a fibre optic recirculation loop that passes
the detection spot forty- four times, thus enhancing sensitivity by a factor of
forty-four. Presently, the total number of passes is limited by the onset of
lasing action of the recirculation loop. This technique offers a significant
sensitivity improvement for various types of plasmon resonance sensors that may
be used in chemical and biomolecule detections.",mandatory disclosures detection
http://arxiv.org/abs/1402.3198v1,"Certain methods of analysis require the knowledge of the spatial distances
between entities whose data are stored in a microdata table. For instance, such
knowledge is necessary and sufficient to perform data mining tasks such as
nearest neighbour searches or clustering. However, when inter-record distances
are published in addition to the microdata for research purposes, the risk of
identity disclosure has to be taken into consideration again. In order to
tackle this problem, we introduce a flexible graph model for microdata in a
metric space and propose a linkage attack based on realistic assumptions of a
data snooper's background knowledge. This attack is based on the idea of
finding a maximum approximate common subgraph of two vertex-labelled and
edge-weighted graphs. By adapting a standard argument from algorithmic graph
theory to our setup, this task is transformed to the maximum clique detection
problem in a corresponding product graph. Using a toy example and experimental
results on simulated data show that publishing even approximate distances could
increase the risk of identity disclosure unreasonably.",mandatory disclosures detection
http://arxiv.org/abs/1208.2486v1,"Plagiarism is a burning problem that academics have been facing in all of the
varied levels of the educational system. With the advent of digital content,
the challenge to ensure the integrity of academic work has been amplified. This
paper discusses on defining a precise definition of plagiarized computer code,
various solutions available for detecting plagiarism and building a cloud
platform for plagiarism disclosure. 'CodeAliker', our application thus
developed automates the submission of assignments and the review process
associated for essay text as well as computer code. It has been made available
under the GNU's General Public License as a Free and Open Source Software.",mandatory disclosures detection
http://arxiv.org/abs/1901.07311v1,"A tremendous amount of individual-level data is generated each day, of use to
marketing, decision makers, and machine learning applications. This data often
contain private and sensitive information about individuals, which can be
disclosed by adversaries. An adversary can recognize the underlying
individual's identity for a data record by looking at the values of
quasi-identifier attributes, known as identity disclosure, or can uncover
sensitive information about an individual through attribute disclosure. In
Statistical Disclosure Control, multiple disclosure risk measures have been
proposed. These share two drawbacks: they do not consider identity and
attribute disclosure concurrently in the risk measure, and they make
restrictive assumptions on an adversary's knowledge by assuming certain
attributes are quasi-identifiers and there is a clear boundary between
quasi-identifiers and sensitive information. In this paper, we present a novel
disclosure risk measure that addresses these limitations, by presenting a
single combined metric of identity and attribute disclosure risk, and providing
flexibility in modeling adversary's knowledge. We have developed an efficient
algorithm for computing the proposed risk measure and evaluated the feasibility
and performance of our approach on a real-world data set from the domain of
social work.",mandatory disclosures detection
http://arxiv.org/abs/1809.00620v2,"Online advertisements that masquerade as non-advertising content pose
numerous risks to users. Such hidden advertisements appear on social media
platforms when content creators or ""influencers"" endorse products and brands in
their content. While the Federal Trade Commission (FTC) requires content
creators to disclose their endorsements in order to prevent deception and harm
to users, we do not know whether and how content creators comply with the FTC's
guidelines. In this paper, we studied disclosures within affiliate marketing,
an endorsement-based advertising strategy used by social media content
creators. We examined whether content creators follow the FTC's disclosure
guidelines, how they word the disclosures, and whether these disclosures help
users identify affiliate marketing content as advertisements. To do so, we
first measured the prevalence of and identified the types of disclosures in
over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a
user study with 1,791 participants to test the efficacy of these disclosures.
Our findings reveal that only about 10% of affiliate marketing content on both
platforms contains any disclosures at all. Further, users fail to understand
shorter, non-explanatory disclosures. Based on our findings, we make various
design and policy suggestions to help improve advertising disclosure practices
on social media platforms.",mandatory disclosures detection
http://arxiv.org/abs/1302.2028v1,"Protecting sensitive information from unauthorized disclosure is a major
concern of every organization. As an organizations employees need to access
such information in order to carry out their daily work, data leakage detection
is both an essential and challenging task. Whether caused by malicious intent
or an inadvertent mistake, data loss can result in significant damage to the
organization. Fingerprinting is a content-based method used for detecting data
leakage. In fingerprinting, signatures of known confidential content are
extracted and matched with outgoing content in order to detect leakage of
sensitive content. Existing fingerprinting methods, however, suffer from two
major limitations. First, fingerprinting can be bypassed by rephrasing (or
minor modification) of the confidential content, and second, usually the whole
content of document is fingerprinted (including non-confidential parts),
resulting in false alarms. In this paper we propose an extension to the
fingerprinting approach that is based on sorted k-skip-n-grams. The proposed
method is able to produce a fingerprint of the core confidential content which
ignores non-relevant (non-confidential) sections. In addition, the proposed
fingerprint method is more robust to rephrasing and can also be used to detect
a previously unseen confidential document and therefore provide better
detection of intentional leakage incidents.",mandatory disclosures detection
http://arxiv.org/abs/1308.4806v2,"We have developed a new method to measure krypton traces in xenon at
unprecedented low concentrations. This is a mandatory task for many near-future
low-background particle physics detectors. Our system separates krypton from
xenon using cryogenic gas chromatography. The amount of krypton is then
quantified using a mass spectrometer. We demonstrate that the system has
achieved a detection limit of 8 ppq (parts per quadrillion) and present results
of distilled xenon with krypton concentrations below 1 ppt.",mandatory disclosures detection
http://arxiv.org/abs/1803.08488v2,"While disclosures relating to various forms of Internet advertising are well
established and follow specific formats, endorsement marketing disclosures are
often open-ended in nature and written by individual publishers. Because such
marketing often appears as part of publishers' actual content, ensuring that it
is adequately disclosed is critical so that end-users can identify it as such.
In this paper, we characterize disclosures relating to affiliate marketing---a
type of endorsement based marketing---on two popular social media platforms:
YouTube & Pinterest. We find that only roughly one-tenth of affiliate content
on both platforms contains disclosures. Based on our findings, we make policy
recommendations geared towards various stakeholders in the affiliate marketing
industry, highlighting how both social media platforms and affiliate companies
can enable better disclosure practices.",mandatory disclosures detection
http://arxiv.org/abs/1504.08043v2,"From buying books to finding the perfect partner, we share our most intimate
wants and needs with our favourite online systems. But how far should we accept
promises of privacy in the face of personal profiling? In particular we ask how
can we improve detection of sensitive topic profiling by online systems? We
propose a definition of privacy disclosure we call
{\epsilon}-indistinguishability from which we construct scalable, practical
tools to assess an adversaries learning potential. We demonstrate our results
using openly available resources, detecting a learning rate in excess of 98%
for a range of sensitive topics during our experiments.",mandatory disclosures detection
http://arxiv.org/abs/1605.06466v1,"Web services are software systems designed for supporting interoperable
dynamic cross-enterprise interactions. The result of attacks to Web services
can be catastrophic and causing the disclosure of enterprises' confidential
data. As new approaches of attacking arise every day, anomaly detection systems
seem to be invaluable tools in this context. The aim of this work has been to
target the attacks that reside in the Web service layer and the extensible
markup language (XML)-structured simple object access protocol (SOAP) messages.
After studying the shortcomings of the existing solutions, a new approach for
detecting anomalies in Web services is outlined. More specifically, the
proposed technique illustrates how to identify anomalies by employing mining
methods on XML-structured SOAP messages. This technique also takes the
advantages of tree-based association rule mining to extract knowledge in the
training phase, which is used in the test phase to detect anomalies. In
addition, this novel composition of techniques brings nearly low false alarm
rate while maintaining the detection rate reasonably high, which is shown by a
case study.",mandatory disclosures detection
http://arxiv.org/abs/1009.5823v1,"Hyperspectral imaging has proven its efficiency for target detection
applications but the acquisition mode and the data rate are major issues when
dealing with real-time detection applications. It can be useful to use snapshot
spectral imagers able to acquire all the spectral channels simultaneously on a
single image sensor. Such snapshot spectral imagers suffer from the lack of
spectral resolution. It is then mandatory to carefully select the spectral
content of the acquired image with respect to the proposed application. We
present a novel approach of hyperspectral band selection for target detection
which maximizes the contrast between the background and the target by proper
optimization of positions and linewidths of a limited number of filters. Based
on a set of tunable band-pass filters such as Fabry-Perot filters, the device
should be able to adapt itself to the current scene and the target looked for.
Simulations based on real hyperspectral images show that such snapshot imagers
could compete well against hyperspectral imagers in terms of detection
efficiency while allowing snapshot acquisition, and real-time detection.",mandatory disclosures detection
http://arxiv.org/abs/1811.08212v1,"The automatic detection of frauds in banking transactions has been recently
studied as a way to help the analysts finding fraudulent operations. Due to the
availability of a human feedback, this task has been studied in the framework
of active learning: the fraud predictor is allowed to sequentially call on an
oracle. This human intervention is used to label new examples and improve the
classification accuracy of the latter. Such a setting is not adapted in the
case of fraud detection with financial data in European countries. Actually, as
a human verification is mandatory to consider a fraud as really detected, it is
not necessary to focus on improving the classifier. We introduce the setting of
'Computer-assisted fraud detection' where the goal is to minimize the number of
non fraudulent operations submitted to an oracle. The existing methods are
applied to this task and we show that a simple meta-algorithm provides
competitive results in this scenario on benchmark datasets.",mandatory disclosures detection
http://arxiv.org/abs/1405.2911v1,"Humanoid robots are designed to operate in human centered environments where
they execute a multitude of challenging tasks, each differing in complexity,
resource requirements, and execution time. In such highly dynamic surroundings
it is desirable to anticipate upcoming situations in order to predict future
resource requirements such as CPU or memory usage. Resource prediction
information is essential for detecting upcoming resource bottlenecks or
conflicts and can be used enhance resource negotiation processes or to perform
speculative resource allocation.
  In this paper we present a prediction model based on Markov chains for
predicting the behavior of the humanoid robot ARMAR-III in human robot
interaction scenarios. Robot state information required by the prediction
algorithm is gathered through self-monitoring and combined with environmental
context information. Adding resource profiles allows generating probability
distributions of possible future resource demands. Online learning of model
parameters is made possible through disclosure mechanisms provided by the robot
framework ArmarX.",mandatory disclosures detection
http://arxiv.org/abs/1412.0008v1,"We live and work in environments that are inundated with cameras embedded in
devices such as phones, tablets, laptops, and monitors. Newer wearable devices
like Google Glass, Narrative Clip, and Autographer offer the ability to quietly
log our lives with cameras from a `first person' perspective. While capturing
several meaningful and interesting moments, a significant number of images
captured by these wearable cameras can contain computer screens. Given the
potentially sensitive information that is visible on our displays, there is a
need to guard computer screens from undesired photography. People need
protection against photography of their screens, whether by other people's
cameras or their own cameras.
  We present ScreenAvoider, a framework that controls the collection and
disclosure of images with computer screens and their sensitive content.
ScreenAvoider can detect images with computer screens with high accuracy and
can even go so far as to discriminate amongst screen content. We also introduce
a ScreenTag system that aids in the identification of screen content, flagging
images with highly sensitive content such as messaging applications or email
webpages. We evaluate our concept on realistic lifelogging datasets, showing
that ScreenAvoider provides a practical and useful solution that can help users
manage their privacy.",mandatory disclosures detection
http://arxiv.org/abs/1702.01160v2,"Mobile applications (apps) often transmit sensitive data through network with
various intentions. Some transmissions are needed to fulfill the app's
functionalities. However, transmissions with malicious receivers may lead to
privacy leakage and tend to behave stealthily to evade detection. The problem
is twofold: how does one unveil sensitive transmissions in mobile apps, and
given a sensitive transmission, how does one determine if it is legitimate?
  In this paper, we propose LeakSemantic, a framework that can automatically
locate abnormal sensitive network transmissions from mobile apps. LeakSemantic
consists of a hybrid program analysis component and a machine learning
component. Our program analysis component combines static analysis and dynamic
analysis to precisely identify sensitive transmissions. Compared to existing
taint analysis approaches, LeakSemantic achieves better accuracy with fewer
false positives and is able to collect runtime data such as network traffic for
each transmission. Based on features derived from the runtime data, machine
learning classifiers are built to further differentiate between the legal and
illegal disclosures. Experiments show that LeakSemantic achieves 91% accuracy
on 2279 sensitive connections from 1404 apps.",mandatory disclosures detection
http://arxiv.org/abs/1704.02385v1,"An-ever increasing number of social media websites, electronic newspapers and
Internet forums allow visitors to leave comments for others to read and
interact. This exchange is not free from participants with malicious
intentions, which do not contribute with the written conversation. Among
different communities users adopt strategies to handle such users. In this
paper we present a comprehensive categorization of the trolling phenomena
resource, inspired by politeness research and propose a model that jointly
predicts four crucial aspects of trolling: intention, interpretation, intention
disclosure and response strategy. Finally, we present a new annotated dataset
containing excerpts of conversations involving trolls and the interactions with
other users that we hope will be a useful resource for the research community.",mandatory disclosures detection
http://arxiv.org/abs/1906.09829v1,"Privacy is of the utmost concern when it comes to releasing data to third
parties. Data owners rely on anonymization approaches to safeguard the released
datasets against re-identification attacks. However, even with strict
anonymization in place, re-identification attacks are still a possibility and
in many cases a reality. Prior art has focused on providing better
anonymization algorithms with minimal loss of information and how to prevent
data disclosure attacks. Our approach tries to tackle the issue of tracing
re-identification attacks based on the concept of honeytokens, decoy or ""bait""
records with the goal to lure malicious users. While the concept of honeytokens
has been widely used in the security domain, this is the first approach to
apply the concept on the data privacy domain. Records with high
re-identification risk, called AnonTokens, are inserted into anonymized
datasets. This work demonstrates the feasibility, detectability and usability
of AnonTokens and provides promising results for data owners who want to apply
our approach to real use cases. We evaluated our concept with real large-scale
population datasets. The results show that the introduction of decoy tokens is
feasible without significant impact on the released dataset.",mandatory disclosures detection
http://arxiv.org/abs/1410.4617v2,"We view a distributed system as a graph of active locations with
unidirectional channels between them, through which they pass messages. In this
context, the graph structure of a system constrains the propagation of
information through it.
  Suppose a set of channels is a cut set between an information source and a
potential sink. We prove that, if there is no disclosure from the source to the
cut set, then there can be no disclosure to the sink. We introduce a new
formalization of partial disclosure, called *blur operators*, and show that the
same cut property is preserved for disclosure to within a blur operator. This
cut-blur property also implies a compositional principle, which ensures limited
disclosure for a class of systems that differ only beyond the cut.",mandatory disclosures detection
http://arxiv.org/abs/1807.05738v1,"Opinion polls suggest that the public value their privacy, with majorities
calling for greater control of their data. However, individuals continue to use
online services which place their personal information at risk, comprising a
Privacy Paradox. Previous work has analysed this phenomenon through
after-the-fact comparisons, but not studied disclosure behaviour during
questioning. We physically surveyed UK cities to study how the British public
regard privacy and how perceptions differ between demographic groups. Through
analysis of optional data disclosure, we empirically examined whether those who
claim to value their privacy act privately with their own data. We found that
both opinions and self-reported actions have little effect on disclosure, with
over 99\% of individuals revealing private data needlessly. We show that not
only do individuals act contrary to their opinions, they disclose information
needlessly even whilst describing themselves as private. We believe our
findings encourage further analysis of data disclosure, as a means of studying
genuine privacy behaviour.",mandatory disclosures detection
http://arxiv.org/abs/1809.09682v2,"Motivated by applications where privacy is important, we consider planning
problems for robots acting in the presence of an observer. We first formulate
and then solve planning problems subject to stipulations on the information
divulged during plan execution --- the appropriate solution concept being both
a plan and an information disclosure policy. We pose this class of problem
under a worst-case model within the framework of procrustean graphs,
formulating the disclosure policy as a particular type of map on edge labels.
We devise algorithms that, given a planning problem supplemented with an
information stipulation, can find a plan, associated disclosure policy, or both
if some exists. Both the plan and associated disclosure policy may depend
subtlety on additional information available to the observer, such as whether
the observer knows the robot's plan (e.g., leaked via a side-channel). Our
implementation finds a plan and a suitable disclosure policy, jointly, when any
such pair exists, albeit for small problem instances.",mandatory disclosures detection
http://arxiv.org/abs/1701.08470v1,"The application of automatic theorem provers to discharge proof obligations
is necessary to apply formal methods in an efficient manner. Tools supporting
formal methods, such as Atelier~B, generate proof obligations fully
automatically. Consequently, such proof obligations are often cluttered with
information that is irrelevant to establish their validity.
  We present iapa, an ""Interface to Automatic Proof Agents"", a new tool that is
being integrated to Atelier~B, through which the user will access proof
obligations, apply operations to simplify these proof obligations, and then
dispatch the resulting, simplified, proof obligations to a portfolio of
automatic theorem provers.",non-compliance with information obligations
http://arxiv.org/abs/cs/0604081v1,"We consider the interpretations of notions of access control (permissions,
interdictions, obligations, and user rights) as run-time properties of
information systems specified as event systems with fairness. We give proof
rules for verifying that an access control policy is enforced in a system, and
consider preservation of access control by refinement of event systems. In
particular, refinement of user rights is non-trivial; we propose to combine
low-level user rights and system obligations to implement high-level user
rights.",non-compliance with information obligations
http://arxiv.org/abs/1210.6857v1,"We show that time complexity analysis of higher-order functional programs can
be effectively reduced to an arguably simpler (although computationally
equivalent) verification problem, namely checking first-order inequalities for
validity. This is done by giving an efficient inference algorithm for linear
dependent types which, given a PCF term, produces in output both a linear
dependent type and a cost expression for the term, together with a set of proof
obligations. Actually, the output type judgement is derivable iff all proof
obligations are valid. This, coupled with the already known relative
completeness of linear dependent types, ensures that no information is lost,
i.e., that there are no false positives or negatives. Moreover, the procedure
reflects the difficulty of the original problem: simple PCF terms give rise to
sets of proof obligations which are easy to solve. The latter can then be put
in a format suitable for automatic or semi-automatic verification by external
solvers. Ongoing experimental evaluation has produced encouraging results,
which are briefly presented in the paper.",non-compliance with information obligations
http://arxiv.org/abs/1606.00339v1,"We present a general formal argumentation system for dealing with the
detachment of conditional obligations. Given a set of facts, constraints, and
conditional obligations, we answer the question whether an unconditional
obligation is detachable by considering reasons for and against its detachment.
For the evaluation of arguments in favor of detaching obligations we use a
Dung-style argumentation-theoretical semantics. We illustrate the modularity of
the general framework by considering some extensions, and we compare the
framework to some related approaches from the literature.",non-compliance with information obligations
http://arxiv.org/abs/1206.5174v3,"We recently introduced p-automata, automata that read discrete-time Markov
chains. We used turn-based stochastic parity games to define acceptance of
Markov chains by a subclass of p-automata. Definition of acceptance required a
cumbersome and complicated reduction to a series of turn-based stochastic
parity games. The reduction could not support acceptance by general p-automata,
which was left undefined as there was no notion of games that supported it.
  Here we generalize two-player games by adding a structural acceptance
condition called obligations. Obligations are orthogonal to the linear winning
conditions that define winning. Obligations are a declaration that player 0 can
achieve a certain value from a configuration. If the obligation is met, the
value of that configuration for player 0 is 1.
  One cannot define value in obligation games by the standard mechanism of
considering the measure of winning paths on a Markov chain and taking the
supremum of the infimum of all strategies. Mainly because obligations need
definition even for Markov chains and the nature of obligations has the flavor
of an infinite nesting of supremum and infimum operators. We define value via a
reduction to turn-based games similar to Martin's proof of determinacy of
Blackwell games with Borel objectives. Based on this definition, we show that
games are determined. We show that for Markov chains with Borel objectives and
obligations, and finite turn-based stochastic parity games with obligations
there exists an alternative and simpler characterization of the value function.
Based on this simpler definition we give an exponential time algorithm to
analyze finite turn-based stochastic parity games with obligations. Finally, we
show that obligation games provide the necessary framework for reasoning about
p-automata and that they generalize the previous definition.",non-compliance with information obligations
http://arxiv.org/abs/cs/0106010v1,"This paper concentrates on the representation of the legal relations that
obtain between parties once they have entered a contractual agreement and their
evolution as the agreement progresses through time. Contracts are regarded as
process and they are analysed in terms of the obligations that are active at
various points during their life span. An informal notation is introduced that
summarizes conveniently the states of an agreement as it evolves over time.
Such a representation enables us to determine what the status of an agreement
is, given an event or a sequence of events that concern the performance of
actions by the agents involved. This is useful both in the context of contract
drafting (where parties might wish to preview how their agreement might evolve)
and in the context of contract performance monitoring (where parties might with
to establish what their legal positions are once their agreement is in force).
The discussion is based on an example that illustrates some typical patterns of
contractual obligations.",non-compliance with information obligations
http://arxiv.org/abs/1603.02964v1,"Some organizations use software applications to manage their customers'
personal, medical, or financial information. In the United States, those
software applications are obligated to preserve users' privacy and to comply
with the United States federal privacy laws and regulations. To formally
guarantee compliance with those regulations, it is essential to extract and
model the privacy rules from the text of the law using a formal framework. In
this work we propose a goal-oriented framework for modeling and extracting the
privacy requirements from regulatory text using natural language processing
techniques.",non-compliance with information obligations
http://arxiv.org/abs/1905.08819v1,"Data cooperatives with fiduciary obligations to members provide a promising
direction for the empowerment of individuals through their own personal data. A
data cooperative can manage, curate and protect access to the personal data of
citizen members. Furthermore, the data cooperative can run internal analytics
in order to obtain insights regarding the well-being of its members. Armed with
these insights, the data cooperative would be in a good position to negotiate
better services and discounts for its members. Credit Unions and similar
institutions can provide a suitable realization of data cooperatives.",non-compliance with information obligations
http://arxiv.org/abs/1402.4741v1,"In this paper, we provide more evidence for the contention that logical
consequence should be understood in normative terms. Hartry Field and John
MacFarlane covered the classical case. We extend their work, examining what it
means for an agent to be obliged to infer a conclusion when faced with
uncertain information or reasoning within a non-monotonic, defeasible, logical
framework (which allows e. g. for inference to be drawn from premises
considered true unless evidence to the contrary is presented).",non-compliance with information obligations
http://arxiv.org/abs/1607.01485v1,"Normative texts are documents based on the deontic notions of obligation,
permission, and prohibition. Our goal is to model such texts using the C-O
Diagram formalism, making them amenable to formal analysis, in particular
verifying that a text satisfies properties concerning causality of actions and
timing constraints. We present an experimental, semi-automatic aid to bridge
the gap between a normative text and its formal representation. Our approach
uses dependency trees combined with our own rules and heuristics for extracting
the relevant components. The resulting tabular data can then be converted into
a C-O Diagram.",non-compliance with information obligations
http://arxiv.org/abs/1012.1131v1,"Nowadays we are faced with an increasing popularity of social software
including wikis, blogs, micro-blogs and online social networks such as Facebook
and MySpace. Unfortunately, the mostly used social services are centralized and
personal information is stored at a single vendor. This results in potential
privacy problems as users do not have much control over how their private data
is disseminated. To overcome this limitation, some recent approaches envisioned
replacing the single authority centralization of services by a peer-to-peer
trust-based approach where users can decide with whom they want to share their
private data. In this peer-to-peer collaboration it is very difficult to ensure
that after data is shared with other peers, these peers will not misbehave and
violate data privacy. In this paper we propose a mechanism that addresses the
issue of data privacy violation due to data disclosure to malicious peers. In
our approach trust values between users are adjusted according to their
previous activities on the shared data. Users share their private data by
specifying some obligations the receivers must follow. We log modifications
done by users on the shared data as well as the obligations that must be
followed when data is shared. By a log-auditing mechanism we detect users that
misbehaved and we adjust their associated trust values by using any existing
decentralized trust model.",non-compliance with information obligations
http://arxiv.org/abs/1208.0944v1,"In a global and technology oriented world the requirements that products and
services have to fulfill are increasing and are getting more complicated.
Research and development (R&D) is becoming increasingly important in creating
the knowledge that makes research and business more competitive. Companies are
obliged to produce more rapidly, more effectively and more efficiently. In
order to meet these requirements and to secure the viability of business
processes, services and products R&D teams need to access and retrieve
information from as many sources as possible. From the other perspective
virtual teams are important mechanisms for organizations seeking to leverage
scarce resources across geographic and other boundaries moreover; virtual
collaboration has become vital for most organizations. This is particularly
true in the context of designing new product and service innovation. Such
collaboration often involves a network of partners located around the world.
However at the R&D project level, dealing with such distributed teams
challenges both managers and specialists. In new product development, it is
necessary to put together the growing different capabilities and services with
the goal, through cooperation between suppliers and customers, service
providers and scientific institutions to achieve innovations of high quality.
In this paper based on comprehensive literature review of recent articles, at
the first step provides an primary definition and characterization of virtual
R&D team; next, the potential value created by virtual R&D teams for new
product development is explored and lastly along with a guide line for future
study, it is argued that the establishing of virtual R&D teams should be given
consideration in the management of R&D projects.",non-compliance with information obligations
http://arxiv.org/abs/1311.2023v1,"Social networks can have asymmetric relationships. In the online social
network Twitter, a follower receives tweets from a followed person but the
followed person is not obliged to subscribe to the channel of the follower.
Thus, it is natural to consider the dissemination of information in directed
networks. In this work we use the mean-field approach to derive differential
equations that describe the dissemination of information in a social network
with asymmetric relationships. In particular, our model reflects the impact of
the degree distribution on the information propagation process. We further show
that for an important subclass of our model, the differential equations can be
solved analytically.",non-compliance with information obligations
http://arxiv.org/abs/1811.07271v2,"Visualizations have a potentially enormous influence on how data are used to
make decisions across all areas of human endeavor. However, it is not clear how
this power connects to ethical duties: what obligations do we have when it
comes to visualizations and visual analytics systems, beyond our duties as
scientists and engineers? Drawing on historical and contemporary examples, I
address the moral components of the design and use of visualizations, identify
some ongoing areas of visualization research with ethical dilemmas, and propose
a set of additional moral obligations that we have as designers, builders, and
researchers of visualizations.",non-compliance with information obligations
http://arxiv.org/abs/1206.5132v2,"In many countries, information and communication technology (ICT) has a lucid
impact on the development of educational curriculum. This is the era of
Information Communication Technology, so to perk up educational planning it is
indispensable to implement the ICT in Education sector. Student can perform
well throughout the usage of ICT. ICT helps the students to augment their
knowledge skills as well as to improve their learning skills. To know with
reference to the usage and Impact of ICT in Education sector of Pakistan, we
accumulate data from 429 respondents from 5 colleges and universities, we use
convenient sampling to accumulate the data from district Rawalpindi of
Pakistan. The consequences show that Availability and Usage of ICT improves the
knowledge and learning skills of students. This indicates that existence of ICT
is improving the educational efficiency as well as obliging for making policies
regarding education sector.",non-compliance with information obligations
http://arxiv.org/abs/physics/0309091v1,"The electric charge of the quantization condition of Dirac's monopole may
have any value, we are not obliged to identify it with the electron charge.
Consequently the magnetic charge of the monopole is quite arbitrary: Dirac's
monopole is a mere object of science fiction.",non-compliance with information obligations
http://arxiv.org/abs/1407.6124v1,"We consider the problem of automated reasoning about dynamically manipulated
data structures. The state-of-the-art methods are limited to the
unfold-and-match (U+M) paradigm, where predicates are transformed via
(un)folding operations induced from their definitions before being treated as
uninterpreted. However, proof obligations from verifying programs with
iterative loops and multiple function calls often do not succumb to this
paradigm. Our contribution is a proof method which -- beyond U+M -- performs
automatic formula re-writing by treating previously encountered obligations in
each proof path as possible induction hypotheses. This enables us, for the
first time, to systematically reason about a wide range of obligations, arising
from practical program verification. We demonstrate the power of our proof
rules on commonly used lemmas, thereby close the remaining gaps in existing
state-of-the-art systems. Another impact, probably more important, is that our
method regains the power of compositional reasoning, and shows that the usage
of user-provided lemmas is no longer needed for the existing set of benchmarks.
This not only removes the burden of coming up with the appropriate lemmas, but
also significantly boosts up the verification process, since lemma
applications, coupled with unfolding, often induce very large search space.",non-compliance with information obligations
http://arxiv.org/abs/0811.1914v1,"We describe an extension to the TLA+ specification language with constructs
for writing proofs and a proof environment, called the Proof Manager (PM), to
checks those proofs. The language and the PM support the incremental
development and checking of hierarchically structured proofs. The PM translates
a proof into a set of independent proof obligations and calls upon a collection
of back-end provers to verify them. Different provers can be used to verify
different obligations. The currently supported back-ends are the tableau prover
Zenon and Isabelle/TLA+, an axiomatisation of TLA+ in Isabelle/Pure. The proof
obligations for a complete TLA+ proof can also be used to certify the theorem
in Isabelle/TLA+.",non-compliance with information obligations
http://arxiv.org/abs/1809.05369v1,"Data protection regulations generally afford individuals certain rights over
their personal data, including the rights to access, rectify, and delete the
data held on them. Exercising such rights naturally requires those with data
management obligations (service providers) to be able to match an individual
with their data. However, many mobile apps collect personal data, without
requiring user registration or collecting details of a user's identity (email
address, names, phone number, and so forth). As a result, a user's ability to
exercise their rights will be hindered without means for an individual to link
themselves with this 'nameless' data. Current approaches often involve those
seeking to exercise their legal rights having to give the app's provider more
personal information, or even to register for a service; both of which seem
contrary to the spirit of data protection law. This paper explores these
concerns, and indicates simple means for facilitating data subject rights
through both application and mobile platform (OS) design.",non-compliance with information obligations
http://arxiv.org/abs/cs/0701142v1,"We introduce the concept of knowledge states; many well-known algorithms can
be viewed as knowledge state algorithms. The knowledge state approach can be
used to to construct competitive randomized online algorithms and study the
tradeoff between competitiveness and memory. A knowledge state simply states
conditional obligations of an adversary, by fixing a work function, and gives a
distribution for the algorithm. When a knowledge state algorithm receives a
request, it then calculates one or more ""subsequent"" knowledge states, together
with a probability of transition to each. The algorithm then uses randomization
to select one of those subsequents to be the new knowledge state. We apply the
method to the paging problem. We present optimally competitive algorithm for
paging for the cases where the cache sizes are k=2 and k=3. These algorithms
use only a very limited number of bookmarks.",non-compliance with information obligations
http://arxiv.org/abs/1108.2349v1,"Current approaches for the discovery, specification, and provision of
services ignore the relationship between the service contract and the
conditions in which the service can guarantee its contract. Moreover, they do
not use formal methods for specifying services, contracts, and compositions.
Without a formal basis it is not possible to justify through formal
verification the correctness conditions for service compositions and the
satisfaction of contractual obligations in service provisions. We remedy this
situation in this paper. We present a formal definition of services with
context-dependent contracts. We define a composition theory of services with
context-dependent contracts taking into consideration functional,
nonfunctional, legal and contextual information. Finally, we present a formal
verification approach that transforms the formal specification of service
composition into extended timed automata that can be verified using the model
checking tool UPPAAL.",non-compliance with information obligations
http://arxiv.org/abs/1007.3589v1,"Registries play a key role in service-oriented applications. Originally, they
were neutral players between service providers and clients. The UDDI Business
Registry (UBR) was meant to foster these concepts and provide a common
reference for companies interested in Web services. The more Web services were
used, the more companies started create their own local registries: more
efficient discovery processes, better control over the quality of published
information, and also more sophisticated publication policies motivated the
creation of private repositories. The number and heterogeneity of the different
registries - besides the decision to close the UBR are pushing for new and
sophisticated means to make different registries cooperate. This paper proposes
DIRE (DIstributed REgistry), a novel approach based on a publish and subscribe
(P/S) infrastructure to federate different heterogeneous registries and make
them exchange information about published services. The paper discusses the
main motivations for the P/S-based infrastructure, proposes an integrated
service model, introduces the main components of the framework, and exemplifies
them on a simple case study.",do-not-call registry
http://arxiv.org/abs/cs/0605111v1,"The NSDL Metadata Registry is designed to provide humans and machines with
the means to discover, create, access and manage metadata schemes, schemas,
application profiles, crosswalks and concept mappings. This paper describes the
general goals and architecture of the NSDL Metadata Registry as well as issues
encountered during the first year of the project's implementation.",do-not-call registry
http://arxiv.org/abs/1609.09211v1,"Advancements in technology have transformed mobile devices from being mere
communication widgets to versatile computing devices. Proliferation of these
hand held devices has made them a common means to access and process digital
information. Most web based applications are today available in a form that can
conveniently be accessed over mobile devices. However, webservices
(applications meant for consumption by other applications rather than humans)
are not as commonly provided and consumed over mobile devices. Facilitating
this and in effect realizing a service-oriented system over mobile devices has
the potential to further enhance the potential of mobile devices. One of the
major challenges in this integration is the lack of an efficient service
registry system that caters to issues associated with the dynamic and volatile
mobile environments. Existing service registry technologies designed for
traditional systems fall short of accommodating such issues. In this paper, we
propose a novel approach to manage service registry systems provided 'solely'
over mobile devices, and thus realising an SOA without the need for high-end
computing systems. The approach manages a dynamic service registry system in
the form of light weight and distributed registries. We assess the feasibility
of our approach by engineering and deploying a working prototype of the
proposed registry system over actual mobile devices. A comparative study of the
proposed approach and the traditional UDDI (Universal Description, Discovery,
and Integration) registry is also included. The evaluation of our framework has
shown propitious results in terms of battery cost, scalability, hindrance with
native applications.",do-not-call registry
http://arxiv.org/abs/cs/0212052v1,"In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.",do-not-call registry
http://arxiv.org/abs/1608.01019v1,"The entity registry system (ERS) is a decentralized entity registry that can
be used to replace the Web as a platform for publishing linked data when the
latter is not available. In developing countries, where off-line is the default
mode of operation, centralized linked data solutions fail to address the needs
of the communities. Although the features are mostly completed, the system is
not yet ready for deployment. This project aims to provide extensive tests and
scalability investigations that would make it ready for a real scenario.",do-not-call registry
http://arxiv.org/abs/1111.5733v1,"The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.",do-not-call registry
http://arxiv.org/abs/1809.01756v1,"Token curated registries (TCRs) have been proposed recently as an approach to
create and maintain high quality lists of resources or recommendations in a
decentralized manner. Applications range from maintaining registries of web
domains for advertising purposes (e.g., adChain) or restaurants, consumer
products, etc. The registry is maintained through a combination of candidate
applications requiring a token deposit, challenges based on token staking and
token-weighted votes with a redistribution of tokens occurring as a consequence
of the vote. We present a simplified mathematical model of a TCR and its
challenge and voting process analyze it from a game-theoretic perspective. We
derive some insights into conditions with respect to the quality of a candidate
under which challenges occur, and under which the outcome is reject or accept.
We also show that there are conditions under which the outcome may not be
entirely predictable in the sense that everyone voting for accept and everyone
voting for reject could both be Nash Equilibria outcomes. For such conditions,
we also explore when a particular strategy profile may be payoff dominant. We
identify ways in which our modeling can be extended and also some implications
of our model with respect to the composition of TCRs.",do-not-call registry
http://arxiv.org/abs/1602.03681v1,"The public package registry npm is one of the biggest software registry. With
its 216 911 software packages, it forms a big network of software dependencies.
In this paper we evaluate various methods for finding similar packages in the
npm network, using only the structure of the graph. Namely, we want to find a
way of categorizing similar packages, which would be useful for recommendation
systems. This size enables us to compute meaningful results, as it softened the
particularities of the graph. Npm is also quite famous as it is the default
package repository of Node.js. We believe that it will make our results
interesting for more people than a less used package repository. This makes it
a good subject of analysis of software networks.",do-not-call registry
http://arxiv.org/abs/1512.08612v1,"At dry and clean material junctions of rigid materials the corrugation of the
sliding energy landscape is dominated by variations of Pauli repulsions. These
occur when electron clouds centered around atoms in adjacent layers overlap as
they slide across each other. In such cases there exists a direct relation
between interfacial surface (in)commensurability and superlubricity, a
frictionless and wearless tribological state. The Registry Index is a purely
geometrical parameter that quantifies the degree of interlayer
commensurability, thus providing a simple and intuitive method for the
prediction of sliding energy landscapes at rigid material interfaces. In the
present study, we extend the applicability of the Registry Index to
non-parallel surfaces, using a model system of nanotubes motion on flat
hexagonal materials. Our method successfully reproduces sliding energy
landscapes of carbon nanotubes on Graphene calculated using a Lennard-Jones
type and the Kolmogorov-Crespi interlayer potentials. Furthermore, it captures
the sliding energy corrugation of a boron nitride nanotube on hexagonal boron
nitride calculated using the h-BN ILP. Finally, we use the Registry Index to
predict the sliding energy landscapes of the heterogeneous junctions of a
carbon nanotubes on hexagonal boron nitride and of boron nitride nanotubes on
graphene that are shown to exhibit a significantly reduced corrugation. For
such rigid interfaces this is expected to be manifested by superlubric motion.",do-not-call registry
http://arxiv.org/abs/1811.09680v1,"Token Curated Registries (TCR) are decentralized recommendation systems that
can be implemented using Blockchain smart contracts. They allow participants to
vote for or against adding items to a list through a process that involves
staking tokens intrinsic to the registry, with winners receiving the staked
tokens for each vote. A TCR aims to provide incentives to create a well-curated
list. In this work, we consider a challenge for these systems - incentivizing
token-holders to actually engage and participate in the voting process. We
propose a novel token-inflation mechanism for enhancing engagement, whereby
only voting participants see their token supply increased by a pre-defined
multiple after each round of voting. To evaluate this proposal, we propose a
simple 4-class model of voters that captures all possible combinations of two
key dimensions: whether they are engaged (likely to vote at all for a given
item) or disengaged, and whether they are informed (likely to vote in a way
that increases the quality of the list) or uninformed, and a simple metric to
evaluate the quality of the list as a function of the vote outcomes. We conduct
simulations using this model of voters and show that implementing
token-inflation results in greater wealth accumulation for engaged voters. In
particular, when the number of informed voters is sufficiently high, our
simulations show that voters that are both informed and engaged see the
greatest benefits from participating in the registry when our proposed
token-inflation mechanism is employed. We further validate this finding using a
simplified mathematical analysis.",do-not-call registry
http://arxiv.org/abs/1308.3357v1,"Linked Data applications often assume that connectivity to data repositories
and entity resolution services are always available. This may not be a valid
assumption in many cases. Indeed, there are about 4.5 billion people in the
world who have no or limited Web access. Many data-driven applications may have
a critical impact on the life of those people, but are inaccessible to those
populations due to the architecture of today's data registries. In this paper,
we propose and evaluate a new open-source system that can be used as a
general-purpose entity registry suitable for deployment in poorly-connected or
ad-hoc environments.",do-not-call registry
http://arxiv.org/abs/1903.03061v1,"This paper presents DIALOG (Digital Investigation Ontology); a framework for
the management, reuse, and analysis of Digital Investigation knowledge. DIALOG
provides a general, application independent vocabulary that can be used to
describe an investigation at different levels of detail. DIALOG is defined to
encapsulate all concepts of the digital forensics field and the relationships
between them. In particular, we concentrate on the Windows Registry, where
registry keys are modeled in terms of both their structure and function.
Registry analysis software tools are modeled in a similar manner and we
illustrate how the interpretation of their results can be done using the
reasoning capabilities of ontology",do-not-call registry
http://arxiv.org/abs/1906.03300v1,"In this study, we aim to incorporate the expertise of anonymous curators into
a token-curated registry (TCR), a decentralized recommender system for
collecting a list of high-quality content. This registry is important, because
previous studies on TCRs have not specifically focused on technical content,
such as academic papers and patents, whose effective curation requires
expertise in relevant fields. To measure expertise, curation in our model
focuses on both the content and its citation relationships, for which curator
assignment uses the Personalized PageRank (PPR) algorithm while reward
computation uses a multi-task peer-prediction mechanism. Our proposed CitedTCR
bridges the literature on network-based and token-based recommender systems and
contributes to the autonomous development of an evolving citation graph for
high-quality content. Moreover, we experimentally confirm the incentive for
registration and curation in CitedTCR using the simplification of a one-to-one
correspondence between users and content (nodes).",do-not-call registry
http://arxiv.org/abs/1611.01820v1,"Today, full-texts of scientific articles are often stored in different
locations than the used datasets. Dataset registries aim at a closer
integration by making datasets citable but authors typically refer to datasets
using inconsistent abbreviations and heterogeneous metadata (e.g. title,
publication year). It is thus hard to reproduce research results, to access
datasets for further analysis, and to determine the impact of a dataset.
Manually detecting references to datasets in scientific articles is
time-consuming and requires expert knowledge in the underlying research
domain.We propose and evaluate a semi-automatic three-step approach for finding
explicit references to datasets in social sciences articles.We first extract
pre-defined special features from dataset titles in the da|ra registry, then
detect references to datasets using the extracted features, and finally match
the references found with corresponding dataset titles. The approach does not
require a corpus of articles (avoiding the cold start problem) and performs
well on a test corpus. We achieved an F-measure of 0.84 for detecting
references in full-texts and an F-measure of 0.83 for finding correct matches
of detected references in the da|ra dataset registry.",do-not-call registry
http://arxiv.org/abs/1603.01979v1,"In this work, we compare GDELT and Event Registry, which monitor news
articles worldwide and provide big data to researchers regarding scale, news
sources, and news geography. We found significant differences in scale and news
sources, but surprisingly, we observed high similarity in news geography
between the two datasets.",do-not-call registry
http://arxiv.org/abs/1910.00286v1,"Detection and Analysis of a potential malware specifically, used for ransom
is a challenging task. Recently, intruders are utilizing advance cryptographic
techniques to get hold of digital assets and then demand ransom. It is believed
that generally, the files comprise of some attributes, states, and patterns
that can be recognized by a machine learning technique. This work thus focuses
on detection of Ransomware by performing feature engineering, which helps in
analyzing vital attributes and behaviors of the malware. The main contribution
of this work is the identification of important and distinct characteristics of
Ransomware that can help in detecting them. Finally, based on the selected
features, both conventional machine learning techniques and Transfer Learning
based Deep Convolutional Neural Networks have been used to detect Ransomware.
In order to perform feature engineering and analysis, two separate datasets
(static and dynamic) were generated. The static dataset has 3646 samples (1700
Ransomware and 1946 Goodware). On the other hand, the dynamic dataset comprised
of 3444 samples (1455 Ransomware and 1989 Goodware). Through various
experiments, it is observed that the Registry changes, API calls, and DLLs are
the most important features for Ransomware detection. Additionally, important
sequences are found with the help of N Gram technique. It is also observed that
in case of Registry Delete operation, if a malicious file tries to delete
registries, it follows a specific and repeated sequence. However for the benign
file, it doesnt follow any specific sequence or repetition. Similarly, an
interesting observation made through this study is that there is no common
Registry deleted sequence between malicious and benign file. And thus this
discernible fact can be readily exploited for Ransomware detection. The
relevant Python code and dataset are available at github.",do-not-call registry
http://arxiv.org/abs/cs/0212051v1,"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.",do-not-call registry
http://arxiv.org/abs/0711.1836v2,"The size distribution of land plots is a result of land allocation processes
in the past. In the absence of regulation this is a Markov process leading an
equilibrium described by a probabilistic equation used commonly in the
insurance and financial mathematics. We support this claim by analyzing the
distribution of two plot types, garden and build-up areas, in the Czech Land
Registry pointing out the coincidence with the distribution of prime number
factors described by Dickman function in the first case.",do-not-call registry
