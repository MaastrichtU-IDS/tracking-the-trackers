http://arxiv.org/abs/1305.5959v2,"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.",wayback machine
http://arxiv.org/abs/1309.4016v1,"The Internet Archive's (IA) Wayback Machine is the largest and oldest public
web archive and has become a significant repository of our recent history and
cultural heritage. Despite its importance, there has been little research about
how it is discovered and used. Based on web access logs, we analyze what users
are looking for, why they come to IA, where they come from, and how pages link
to IA. We find that users request English pages the most, followed by the
European languages. Most human users come to web archives because they do not
find the requested pages on the live web. About 65% of the requested archived
pages no longer exist on the live web. We find that more than 82% of human
sessions connect to the Wayback Machine via referrals from other web sites,
while only 15% of robots have referrers. Most of the links (86%) from websites
are to individual archived pages at specific points in time, and of those 83%
no longer exist on the live web.",wayback machine
http://arxiv.org/abs/1904.12636v1,"In designing a distributed service, three desirable attributes are
Consistency, Availability and Partition Tolerance. In this note we explore a
framework for characterizing these three in a manner that establishes definite
limits and relationships between them, and explore some implications of this
characterization.",wayback machine
http://arxiv.org/abs/1801.10396v2,"Web archiving services play an increasingly important role in today's
information ecosystem, by ensuring the continuing availability of information,
or by deliberately caching content that might get deleted or removed. Among
these, the Wayback Machine has been proactively archiving, since 2001, versions
of a large number of Web pages, while newer services like archive.is allow
users to create on-demand snapshots of specific Web pages, which serve as time
capsules that can be shared across the Web. In this paper, we present a
large-scale analysis of Web archiving services and their use on social media,
shedding light on the actors involved in this ecosystem, the content that gets
archived, and how it is shared. We crawl and study: 1) 21M URLs from
archive.is, spanning almost two years, and 2) 356K archive.is plus 391K Wayback
Machine URLs that were shared on four social networks: Reddit, Twitter, Gab,
and 4chan's Politically Incorrect board (/pol/) over 14 months. We observe that
news and social media posts are the most common types of content archived,
likely due to their perceived ephemeral and/or controversial nature. Moreover,
URLs of archiving services are extensively shared on ""fringe"" communities
within Reddit and 4chan to preserve possibly contentious content. Lastly, we
find evidence of moderators nudging or even forcing users to use archives,
instead of direct links, for news sources with opposing ideologies, potentially
depriving them of ad revenue.",wayback machine
http://arxiv.org/abs/1604.05923v1,"This paper describes how born digital primary sources could be used to
reconstruct the recent history of scientific institutions. The case study is an
analysis of the first 25 years online of the University of Bologna. The focus
of this work is primarily methodological: several different issues are
presented, starting with the fact that the University of Bologna website has
been excluded for thirteen years from the Internet Archive's Wayback Machine,
and possible solutions are proposed and applied. The article is organised in
three parts: in the first one, some of the fundamental concepts on web archives
and the preservation of born digital sources are introduced. Then the
reconstruction of the University of Bologna web's past is presented. Finally
the future of this research is described, presenting a specific case study in
which the historian's craft will be challenged by a completely different issue,
namely the large amount of data available in the university digital library.",wayback machine
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",wayback machine
http://arxiv.org/abs/1212.6177v2,"Although the Internet Archive's Wayback Machine is the largest and most
well-known web archive, there have been a number of public web archives that
have emerged in the last several years. With varying resources, audiences and
collection development policies, these archives have varying levels of overlap
with each other. While individual archives can be measured in terms of number
of URIs, number of copies per URI, and intersection with other archives, to
date there has been no answer to the question ""How much of the Web is
archived?"" We study the question by approximating the Web using sample URIs
from DMOZ, Delicious, Bitly, and search engine indexes; and, counting the
number of copies of the sample URIs exist in various public web archives. Each
sample set provides its own bias. The results from our sample sets indicate
that range from 35%-90% of the Web has at least one archived copy, 17%-49% has
between 2-5 copies, 1%-8% has 6-10 copies, and 8%-63% has more than 10 copies
in public web archives. The number of URI copies varies as a function of time,
but no more than 31.3% of URIs are archived more than once per month.",wayback machine
http://arxiv.org/abs/1506.06279v1,"A variety of fan-based wikis about episodic fiction (e.g., television shows,
novels, movies) exist on the World Wide Web. These wikis provide a wealth of
information about complex stories, but if readers are behind in their viewing
they run the risk of encountering ""spoilers"" -- information that gives away key
plot points before the intended time of the show's writers. Enterprising
readers might browse the wiki in a web archive so as to view the page prior to
a specific episode date and thereby avoid spoilers. Unfortunately, due to how
web archives choose the ""best"" page, it is still possible to see spoilers
(especially in sparse archives).
  In this paper we discuss how to use Memento to avoid spoilers. Memento uses
TimeGates to determine which best archived page to give back to the user,
currently using a minimum distance heuristic. We quantify how this heuristic is
inadequate for avoiding spoilers, analyzing data collected from fan wikis and
the Internet Archive. We create an algorithm for calculating the probability of
encountering a spoiler in a given wiki article. We conduct an experiment with
16 wiki sites for popular television shows. We find that 38% of those pages are
unavailable in the Internet Archive. We find that when accessing fan wiki pages
in the Internet Archive there is as much as a 66% chance of encountering a
spoiler. Using sample access logs from the Internet Archive, we find that 19%
of actual requests to the Wayback Machine for wikia.com pages ended in
spoilers. We suggest the use of a different minimum distance heuristic,
minpast, for wikis, using the desired datetime as an upper bound.",wayback machine
http://arxiv.org/abs/1701.08256v1,"Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.",wayback machine
http://arxiv.org/abs/1309.5503v1,"When a user views an archived page using the archive's user interface (UI),
the user selects a datetime to view from a list. The archived web page, if
available, is then displayed. From this display, the web archive UI attempts to
simulate the web browsing experience by smoothly transitioning between archived
pages. During this process, the target datetime changes with each link
followed; drifting away from the datetime originally selected. When browsing
sparsely-archived pages, this nearly-silent drift can be many years in just a
few clicks. We conducted 200,000 acyclic walks of archived pages, following up
to 50 links per walk, comparing the results of two target datetime policies.
The Sliding Target policy allows the target datetime to change as it does in
archive UIs such as the Internet Archive's Wayback Machine. The Sticky Target
policy, represented by the Memento API, keeps the target datetime the same
throughout the walk. We found that the Sliding Target policy drift increases
with the number of walk steps, number of domains visited, and choice (number of
links available). However, the Sticky Target policy controls temporal drift,
holding it to less than 30 days on average regardless of walk length or number
of domains visited. The Sticky Target policy shows some increase as choice
increases, but this may be caused by other factors. We conclude that based on
walk length, the Sticky Target policy generally produces at least 30 days less
drift than the Sliding Target policy.",wayback machine
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",wayback machine
http://arxiv.org/abs/1904.10629v1,"This paper focuses on reporting of Internet malicious activity (or
mal-activity in short) by public blacklists with the objective of providing a
systematic characterization of what has been reported over the years, and more
importantly, the evolution of reported activities. Using an initial seed of 22
blacklists, covering the period from January 2007 to June 2017, we collect more
than 51 million mal-activity reports involving 662K unique IP addresses
worldwide. Leveraging the Wayback Machine, antivirus (AV) tool reports and
several additional public datasets (e.g., BGP Route Views and Internet
registries) we enrich the data with historical meta-information including
geo-locations (countries), autonomous system (AS) numbers and types of
mal-activity. Furthermore, we use the initially labelled dataset of approx 1.57
million mal-activities (obtained from public blacklists) to train a machine
learning classifier to classify the remaining unlabeled dataset of approx 44
million mal-activities obtained through additional sources. We make our unique
collected dataset (and scripts used) publicly available for further research.
  The main contributions of the paper are a novel means of report collection,
with a machine learning approach to classify reported activities,
characterization of the dataset and, most importantly, temporal analysis of
mal-activity reporting behavior. Inspired by P2P behavior modeling, our
analysis shows that some classes of mal-activities (e.g., phishing) and a small
number of mal-activity sources are persistent, suggesting that either
blacklist-based prevention systems are ineffective or have unreasonably long
update periods. Our analysis also indicates that resources can be better
utilized by focusing on heavy mal-activity contributors, which constitute the
bulk of mal-activities.",wayback machine
http://arxiv.org/abs/1908.02819v1,"When a user requests a web page from a web archive, the user will typically
either get an HTTP 200 if the page is available, or an HTTP 404 if the web page
has not been archived. This is because web archives are typically accessed by
URI lookup, and the response is binary: the archive either has the page or it
does not, and the user will not know of other archived web pages that exist and
are potentially similar to the requested web page. In this paper, we propose
augmenting these binary responses with a model for selecting and ranking
recommended web pages in a Web archive. This is to enhance both HTTP 404
responses and HTTP 200 responses by surfacing web pages in the archive that the
user may not know existed. First, we check if the URI is already classified in
DMOZ or Wikipedia. If the requested URI is not found, we use ML to classify the
URI using DMOZ as our ontology and collect candidate URIs to recommended to the
user. Next, we filter the candidates based on if they are present in the
archive. Finally, we rank candidates based on several features, such as
archival quality, web page popularity, temporal similarity, and URI similarity.
We calculated the F1 score for different methods of classifying the requested
web page at the first level. We found that using all-grams from the URI after
removing numerals and the TLD produced the best result with F1=0.59. For
second-level classification, the micro-average F1=0.30. We found that 44.89% of
the correctly classified URIs contained at least one word that exists in a
dictionary and 50.07% of the correctly classified URIs contained long strings
in the domain. In comparison with the URIs from our Wayback access logs, only
5.39% of those URIs contained only words from a dictionary, and 26.74%
contained at least one word from a dictionary. These percentages are low and
may affect the ability for the requested URI to be correctly classified.",wayback machine
http://arxiv.org/abs/1611.00467v1,"Virtual machines have been widely adapted for high-level programming language
implementations and for providing a degree of platform neutrality. As the
overall use and adaptation of virtual machines grow, the overall performance of
virtual machines has become a widely-discussed topic. In this paper, we present
a survey on the performance differences of the two most widely adapted types of
virtual machines - the stack-based virtual machine and the register-based
virtual machine - using various benchmark programs. Additionally, we adopted a
new approach of measuring performance by measuring the overall dispatch time,
amount of dispatches, fetch time, and execution time while running benchmarks
on custom-implemented, lightweight virtual machines. Finally, we present two
lightweight, custom-designed, Turing-equivalent virtual machines that are
specifically designed in benchmarking virtual machine performance - the
""Conceptum"" stack-based virtual machine, and the ""Inertia"" register-based
virtual machine. Our result showed that while on average the register machine
spends 20.39% less time in executing benchmarks than the stack machine, the
stack-based virtual machine is still faster than the virtual machine regarding
the instruction fetch time.",wayback machine
http://arxiv.org/abs/1909.03550v1,"Lecture notes on optimization for machine learning, derived from a course at
Princeton University and tutorials given in MLSS, Buenos Aires, as well as
Simons Foundation, Berkeley.",wayback machine
http://arxiv.org/abs/0910.1761v1,"This paper will provide a method to decompose forging dies for machining
planning in the case of high speed machining finishing operations. This method
lies on a machining feature approach model presented in the following paper.
The two main decomposition phases, called Basic Machining Features Extraction
and Process Planning Generation, are presented. These two decomposition phases
integrates machining resources models and expert machining knowledge to provide
an outstanding process planning.",wayback machine
http://arxiv.org/abs/1811.04422v1,"I describe an optimal control view of adversarial machine learning, where the
dynamical system is the machine learner, the input are adversarial actions, and
the control costs are defined by the adversary's goals to do harm and be hard
to detect. This view encompasses many types of adversarial machine learning,
including test-item attacks, training-data poisoning, and adversarial reward
shaping. The view encourages adversarial machine learning researcher to utilize
advances in control theory and reinforcement learning.",wayback machine
http://arxiv.org/abs/1007.3303v1,"An abstract machine is a theoretical model designed to perform a rigorous
study of computation. Such a model usually consists of configurations,
instructions, programs, inputs and outputs for the machine. In this paper we
formalize these notions as a very simple algebraic system, called a
configuration machine. If an abstract machine is defined as a configuration
machine consisting of primitive recursive functions then the functions computed
by the machine are always recursive. The theory of configuration machines
provides a useful tool to study universal machines.",wayback machine
http://arxiv.org/abs/1909.09246v1,"In this chapter, we provide a brief overview of applying machine learning
techniques for clinical prediction tasks. We begin with a quick introduction to
the concepts of machine learning and outline some of the most common machine
learning algorithms. Next, we demonstrate how to apply the algorithms with
appropriate toolkits to conduct machine learning experiments for clinical
prediction tasks. The objectives of this chapter are to (1) understand the
basics of machine learning techniques and the reasons behind why they are
useful for solving clinical prediction problems, (2) understand the intuition
behind some machine learning models, including regression, decision trees, and
support vector machines, and (3) understand how to apply these models to
clinical prediction problems using publicly available datasets via case
studies.",wayback machine
http://arxiv.org/abs/1304.0053v2,"In the 1960's Gisbert Hasenjaeger built Turing Machines from
electromechanical relays and uniselectors. Recently, Glaschick reverse
engineered the program of one of these machines and found that it is a
universal Turing machine. In fact, its program uses only four states and two
symbols, making it a very small universal Turing machine. (The machine has
three tapes and a number of other features that are important to keep in mind
when comparing it to other small universal machines.) Hasenjaeger's machine
simulates Hao Wang's B machines, which were proved universal by Wang.
Unfortunately, Wang's original simulation algorithm suffers from an exponential
slowdown when simulating Turing machines. Hence, via this simulation,
Hasenjaeger's machine also has an exponential slowdown when simulating Turing
machines. In this work, we give a new efficient simulation algorithm for Wang's
B machines by showing that they simulate Turing machines with only a polynomial
slowdown. As a second result, we find that Hasenjaeger's machine also
efficiently simulates Turing machines in polynomial time. Thus, Hasenjaeger's
machine is both small and fast. In another application of our result, we show
that Hooper's small universal Turing machine simulates Turing machines in
polynomial time, an exponential improvement.",wayback machine
http://arxiv.org/abs/1503.03787v1,"This paper introduces a new computing model based on the cooperation among
Turing machines called orchestrated machines. Like universal Turing machines,
orchestrated machines are also designed to simulate Turing machines but they
can also modify the original operation of the included Turing machines to
create a new layer of some kind of collective behavior. Using this new model we
can define some interested notions related to cooperation ability of Turing
machines such as the intelligence quotient or the emotional intelligence
quotient for Turing machines.",wayback machine
http://arxiv.org/abs/1904.12054v1,"Machine learning has become a vital part in many aspects of our daily life.
However, building well performing machine learning applications requires highly
specialized data scientists and domain experts. Automated machine learning
(AutoML) aims to reduce the demand for data scientists by enabling domain
experts to automatically build machine learning applications without extensive
knowledge of statistics and machine learning. In this survey, we summarize the
recent developments in academy and industry regarding AutoML. First, we
introduce a holistic problem formulation. Next, approaches for solving various
subproblems of AutoML are presented. Finally, we provide an extensive empirical
evaluation of the presented approaches on synthetic and real data.",wayback machine
http://arxiv.org/abs/1812.01343v1,"This work introduces a natural variant of the online machine scheduling
problem on unrelated machines, which we refer to as the favorite machine model.
In this model, each job has a minimum processing time on a certain set of
machines, called favorite machines, and some longer processing times on other
machines. This type of costs (processing times) arise quite naturally in many
practical problems. In the online version, jobs arrive one by one and must be
allocated irrevocably upon each arrival without knowing the future jobs. We
consider online algorithms for allocating jobs in order to minimize the
makespan.
  We obtain tight bounds on the competitive ratio of the greedy algorithm and
characterize the optimal competitive ratio for the favorite machine model. Our
bounds generalize the previous results of the greedy algorithm and the optimal
algorithm for the unrelated machines and the identical machines. We also study
a further restriction of the model, called the symmetric favorite machine
model, where the machines are partitioned equally into two groups and each job
has one of the groups as favorite machines. We obtain a 2.675-competitive
algorithm for this case, and the best possible algorithm for the two machines
case.",wayback machine
http://arxiv.org/abs/1903.08801v1,"Traditional machine learning algorithms use data from databases that are
mutable, and therefore the data cannot be fully trusted. Also, the machine
learning process is difficult to automate. This paper proposes building a
trustable machine learning system by using blockchain technology, which can
store data in a permanent and immutable way. In addition, smart contracts are
used to automate the machine learning process. This paper makes three
contributions. First, it establishes a link between machine learning technology
and blockchain technology. Previously, machine learning and blockchain have
been considered two independent technologies without an obvious link. Second,
it proposes a unified analytical framework for trustable machine learning by
using blockchain technology. This unified framework solves both the
trustability and automation issues in machine learning. Third, it enables a
computer to translate core machine learning implementation from a single thread
on a single machine to multiple threads on multiple machines running with
blockchain by using a unified approach. The paper uses association rule mining
as an example to demonstrate how trustable machine learning can be implemented
with blockchain, and it shows how this approach can be used to analyze opioid
prescriptions to help combat the opioid crisis.",wayback machine
http://arxiv.org/abs/1606.05664v1,"In this paper, we study the support vector machine and introduced the notion
of generalized support vector machine for classification of data. We show that
the problem of generalized support vector machine is equivalent to the problem
of generalized variational inequality and establish various results for the
existence of solutions. Moreover, we provide various examples to support our
results.",wayback machine
http://arxiv.org/abs/1609.08874v1,"A signal machine is an abstract geometrical model for computation, proposed
as an extension to the one-dimensional cellular automata, in which discrete
time and space of cellular automata is replaced with continuous time and space
in signal machine. A signal machine is defined as a set of meta-signals and a
set of rules. A signal machine starts from an initial configuration which is a
set of moving signals. Signals are moving in space freely until a collision.
Rules of signal machine specify what happens after a collision, or in other
words, specify out-coming signals for each set of colliding signals. Originally
signal machine is defined by its rule as a deterministic machine. In this
paper, we introduce the concept of non-deterministic signal machine, which may
contain more than one defined rule for each set of colliding signals. We show
that for a specific class of nondeterministic signal machines, called
k-restricted nondeterministic signal machine, there is a deterministic signal
machine computing the same result as the nondeterministic one, on any given
initial configuration. k-restricted nondeterministic signal machine is a
nondeterministic signal machine which accepts an input iff produces a special
accepting signal, which have at most two nondeterministic rule for each
collision, and at most k collisions before any acceptance.",wayback machine
http://arxiv.org/abs/1707.09562v3,"We conduct an empirical study of machine learning functionalities provided by
major cloud service providers, which we call machine learning clouds. Machine
learning clouds hold the promise of hiding all the sophistication of running
large-scale machine learning: Instead of specifying how to run a machine
learning task, users only specify what machine learning task to run and the
cloud figures out the rest. Raising the level of abstraction, however, rarely
comes free - a performance penalty is possible. How good, then, are current
machine learning clouds on real-world machine learning workloads?
  We study this question with a focus on binary classication problems. We
present mlbench, a novel benchmark constructed by harvesting datasets from
Kaggle competitions. We then compare the performance of the top winning code
available from Kaggle with that of running machine learning clouds from both
Azure and Amazon on mlbench. Our comparative study reveals the strength and
weakness of existing machine learning clouds and points out potential future
directions for improvement.",wayback machine
http://arxiv.org/abs/1404.2863v1,"The preceding paper constructed tangle machines as diagrammatic models, and
illustrated their utility with a number of examples. The information content of
a tangle machine is contained in characteristic quantities associated to
equivalence classes of tangle machines, which are called invariants. This paper
constructs invariants of tangle machines. Chief among these are the prime
factorizations of a machine, which are essentially unique. This is proven using
low dimensional topology, through representing a colour-suppressed machine as a
diagram for a network of jointly embedded spheres and intervals in 4-space. The
complexity of a tangle machine is defined as its number of prime factors.",wayback machine
http://arxiv.org/abs/1501.00507v1,A sequence function alternative representation of state machines.,wayback machine
http://arxiv.org/abs/1904.05061v1,"One of the major objectives of Artificial Intelligence is to design learning
algorithms that are executed on a general purposes computational machines such
as human brain. Neural Turing Machine (NTM) is a step towards realizing such a
computational machine. The attempt is made here to run a systematic review on
Neural Turing Machine. First, the mind-map and taxonomy of machine learning,
neural networks, and Turing machine are introduced. Next, NTM is inspected in
terms of concepts, structure, variety of versions, implemented tasks,
comparisons, etc. Finally, the paper discusses on issues and ends up with
several future works.",wayback machine
http://arxiv.org/abs/1909.01866v1,"Bias is known to be an impediment to fair decisions in many domains such as
human resources, the public sector, health care etc. Recently, hope has been
expressed that the use of machine learning methods for taking such decisions
would diminish or even resolve the problem. At the same time, machine learning
experts warn that machine learning models can be biased as well. In this
article, our goal is to explain the issue of bias in machine learning from a
technical perspective and to illustrate the impact that biased data can have on
a machine learning model. To reach such a goal, we develop interactive plots to
visualizing the bias learned from synthetic data.",wayback machine
http://arxiv.org/abs/0904.3664v1,"Introduction to Machine learning covering Statistical Inference (Bayes, EM,
ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),
and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",wayback machine
http://arxiv.org/abs/0802.1123v2,"In this paper, we tackle the open problem of snap-stabilization in
message-passing systems. Snap-stabilization is a nice approach to design
protocols that withstand transient faults. Compared to the well-known
self-stabilizing approach, snap-stabilization guarantees that the effect of
faults is contained immediately after faults cease to occur. Our contribution
is twofold: we show that (1) snap-stabilization is impossible for a wide class
of problems if we consider networks with finite yet unbounded channel capacity;
(2) snap-stabilization becomes possible in the same setting if we assume
bounded-capacity channels. We propose three snap-stabilizing protocols working
in fully-connected networks. Our work opens exciting new research perspectives,
as it enables the snap-stabilizing paradigm to be implemented in actual
networks.",internet snap
http://arxiv.org/abs/1512.00822v2,"Early programming languages for software-defined networking (SDN) were built
on top of the simple match-action paradigm offered by OpenFlow 1.0. However,
emerging hardware and software switches offer much more sophisticated support
for persistent state in the data plane, without involving a central controller.
Nevertheless, managing stateful, distributed systems efficiently and correctly
is known to be one of the most challenging programming problems. To simplify
this new SDN problem, we introduce SNAP.
  SNAP offers a simpler ""centralized"" stateful programming model, by allowing
programmers to develop programs on top of one big switch rather than many.
These programs may contain reads and writes to global, persistent arrays, and
as a result, programmers can implement a broad range of applications, from
stateful firewalls to fine-grained traffic monitoring. The SNAP compiler
relieves programmers of having to worry about how to distribute, place, and
optimize access to these stateful arrays by doing it all for them. More
specifically, the compiler discovers read/write dependencies between arrays and
translates one-big-switch programs into an efficient internal representation
based on a novel variant of binary decision diagrams. This internal
representation is used to construct a mixed-integer linear program, which
jointly optimizes the placement of state and the routing of traffic across the
underlying physical topology. We have implemented a prototype compiler and
applied it to about 20 SNAP programs over various topologies to demonstrate our
techniques' scalability.",internet snap
http://arxiv.org/abs/1009.2282v1,"Given the respective advantages of the two complimentary techniques for
peer-to-peer media streaming (namely tree-based push and mesh-based pull),
there is a strong trend of combining them into a hybrid streaming system.
Backed by recently proposed mechanisms to identify stable peers, such a hybrid
system usually consists of backbone trees formed by the stable peers and other
overlay structures in the second tier to accommodate the remaining peers. In
this paper, we embrace the hybrid push-pull structure for peer-to-peer media
streaming. Our protocol is dominated by a multi-tree push mechanism to minimize
the delay in the backbone and is complemented by other overlay structures to
cope with peer dynamics. What mainly distinguishes our multi-tree pushing from
the conventional ones is an unbalanced tree design guided by the so called
snow-ball streaming, which has a provable minimum delay and can be smoothly
""melded"" with virtually any other existing overlay structures lying in the
second tier. We design algorithms to construct and maintain our SNowbAll
multi-tree Pushing (SNAP) overlay, and we also illustrate how to smoothly weld
the SNAP backbone with the second tier. Finally, we perform simulations in
ns-2; the results indicate that our approach outperforms a recently proposed
hybrid streaming system.",internet snap
http://arxiv.org/abs/1203.0065v1,"We investigated the reset time of superconducting nanowire avalanche
photodetectors (SNAPs) based on 30 nm wide nanowires. We studied the dependence
of the reset time of SNAPs on the device inductance and discovered that SNAPs
can provide a speed-up relative to SNSPDs with the same area, but with some
limitations: (1) reducing the series inductance of SNAPs (necessary for the
avalanche formation) could result in the detectors operating in an unstable
regime; (2) a trade-off exists between maximizing the bias current margin and
minimizing the reset time of SNAPs; and (3) reducing the reset time of SNAPs
below ~ 1 ns resulted in afterpulsing.",internet snap
http://arxiv.org/abs/1401.0943v1,"Business ontology can enhance the successful development of complex
enterprise system; this is being achieved through knowledge sharing and the
ease of communication between every entity in the domain. Through human
semantic interaction with the web resources, machines to interpret the data
published in a machine interpretable form under web. However, the theoretical
practice of business ontology in eCommerce domain is quite a few especially in
the section of electronic transaction, and the various techniques used to
obtain efficient communication across spheres are error prone and are not
always guaranteed to be efficient in obtaining desired result due to poor
semantic integration between entities. To overcome the poor semantic
integration this research focuses on proposed ontology called LB2CO, which
combines the framework of IDEF5 & SNAP as an analysis tool, for automated
recommendation of product and services and create effective ontological
framework for B2C transaction & communication across different business domains
that facilitates the interoperability & integration of B2C transactions over
the web.",internet snap
http://arxiv.org/abs/1905.03932v1,"Disruption Tolerant Networks (DTNs) are employed in applications where the
network is likely to be disrupted due to environmental conditions or where the
network topology makes it impossible to find a direct route from the sender to
the receiver. Underwater networks typically use acoustic waves for transmitting
data. However, these waves are susceptible to interference from sources of
noise such as the wake from ships, sounds from snapping shrimp, and collisions
from acoustic waves generated by other nodes.
  DTNs are good candidates for situations where successfully delivering the
message is more important than low delivery times and high network throughput.
This is true for certain applications of underwater networks. DTNs can also
create new options for network topologies, such as opening up the possibility
of using data muling nodes if the network is resilient to delays.
  The Acoustic Research Laboratory (ARL) at NUS has developed their own
Groovy-based underwater network simulator called UnetStack, in which network
protocols can be designed and tested in a simulator. These protocols can later
be directly deployed on physical hardware, such as Subnero's underwater modems.
Hence, this project revolves around creating a new UnetStack protocol called
DtnLink for enabling disruption tolerant networking in various use cases of the
ARL.",internet snap
http://arxiv.org/abs/1206.1748v1,"VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system",internet snap
http://arxiv.org/abs/1905.05358v1,"State-of-the-art theorem provers, combined with smart sampling heuristics,
can generate millions of test cases in just a few hours. But given the
heuristic nature of those methods, not all of those tests may be valid. Also,
test engineers may find it too burdensome to run all those tests.
  Within a large space of tests, there can be redundancies (duplicate entries
or similar entries that do not contribute much to overall diversity). Our
approach, called SNAP uses specialized sub-sampling heuristics to avoid finding
those repeated tests. By avoiding those repeated structures SNAP explores a
smaller space of options. Hence, it is possible for SNAP to verify all its
tests.
  To evaluate SNAP, this paper applied 27 real-world case studies from a recent
ICSE'18 paper. Compared to prior results, SNAP's test case generation was 10 to
3000 times faster (median to max). Also, while prior work showed that their
tests were 70% valid, our method generates 100% valid tests. Most importantly,
test engineers would find it relatively easiest to apply SNAP's tests since our
test suites are 10 to 750 times smaller (median to max) than those generated
using prior work.",internet snap
http://arxiv.org/abs/1606.07550v1,"Large networks are becoming a widely used abstraction for studying complex
systems in a broad set of disciplines, ranging from social network analysis to
molecular biology and neuroscience. Despite an increasing need to analyze and
manipulate large networks, only a limited number of tools are available for
this task.
  Here, we describe Stanford Network Analysis Platform (SNAP), a
general-purpose, high-performance system that provides easy to use, high-level
operations for analysis and manipulation of large networks. We present SNAP
functionality, describe its implementational details, and give performance
benchmarks. SNAP has been developed for single big-memory machines and it
balances the trade-off between maximum performance, compact in-memory graph
representation, and the ability to handle dynamic graphs where nodes and edges
are being added or removed over time. SNAP can process massive networks with
hundreds of millions of nodes and billions of edges. SNAP offers over 140
different graph algorithms that can efficiently manipulate large graphs,
calculate structural properties, generate regular and random graphs, and handle
attributes and meta-data on nodes and edges. Besides being able to handle large
graphs, an additional strength of SNAP is that networks and their attributes
are fully dynamic, they can be modified during the computation at low cost.
SNAP is provided as an open source library in C++ as well as a module in
Python.
  We also describe the Stanford Large Network Dataset, a set of social and
information real-world networks and datasets, which we make publicly available.
The collection is a complementary resource to our SNAP software and is widely
used for development and benchmarking of graph analytics algorithms.",internet snap
http://arxiv.org/abs/1604.02956v1,"Surface nanoscale axial photonics (SNAP) structures are fabricated with a
femtosecond laser for the first time. The inscriptions introduced by the laser
pressurize the fiber and cause its nanoscale effective radius variation. We
demonstrate the subangstrom precise fabrication of individual and coupled SNAP
microresonators having the effective radius variation of several nanometers.
Our results pave the way to a novel ultraprecise SNAP fabrication technology
based on the femtosecond laser inscription.",internet snap
http://arxiv.org/abs/1703.02707v1,"We develop a theory of optical frequency comb generation in ultra-compact
Surface Nanoscale Axial Photonic (SNAP) bottle microresonators, employing the
nonlinear interaction of whispering gallery modes which are confined along an
optical fiber with nanoscale radius variation. We predict that a SNAP
microresonator with a few-$\mu$m radius can generate a frequency comb with an
ultra-fine sub-GHz spectral spacing, which would require traditional ring
resonators of $cm$ radius. We identify regimes of stable or quasi-periodic comb
dynamics due to soliton excitation, and also show that special engineering of
the SNAP radius profile can be used to compensate nonlinearity induced
dispersion.",internet snap
http://arxiv.org/abs/1909.05953v1,"This paper deals with the following separability problem in 3D space: Given a
rigid polyhedron $P$ with $n$ vertices, does a semi-rigid polyhedron $G$ exist,
such that both polyhedra can be transformed into an inseparable assembled
state, where the fixture snaps on to $P$, by applying a linear force and
exploiting the mild flexibility of $G$? If such a flexible snapping polyhedron
exists, devise an efficient and robust algorithm that constructs it. In simple
words, we are looking for s semi-rigid polyhedron $G$, such that when $P$ and
$G$ are separate, we can push $G$ towards $P$, slightly bending $G$ on the way,
and obtain a configuration, where $G$ is back in its original shape, and both
$P$ and $G$ are inseparable as rigid bodies. We define certain properties such
a pair of polyhedron and its snapping fixture may possess, and prove two
theorems related to the pair. We introduce an algorithm that produces a
snapping fixture with such properties in $O(n^5)$ time, if a snapping fixture
exists, and an efficient and robust implementation of this algorithm.",internet snap
http://arxiv.org/abs/1110.0714v1,"The carnivorous aquatic Waterwheel Plant (Aldrovanda vesiculosa L.) and the
closely related terrestrial Venus Flytrap (Dionaea muscipula SOL. EX J. ELLIS)
both feature elaborate snap-traps, which shut after reception of an external
mechanical stimulus by prey animals. Traditionally, Aldrovanda is considered as
a miniature, aquatic Dionaea, an assumption which was already established by
Charles Darwin. However, videos of snapping traps from both species suggest
completely different closure mechanisms. Indeed, the well-described snapping
mechanism in Dionaea comprises abrupt curvature inversion of the two trap
lobes, while the closing movement in Aldrovanda involves deformation of the
trap midrib but not of the lobes, which do not change curvature. In this paper,
we present the first detailed mechanical models for these plants, which are
based on the theory of thin solid membranes and explain this difference by
showing that the fast snapping of Aldrovanda is due to kinematic amplification
of the bending deformation of the midrib, while that of Dionaea unambiguously
relies on the buckling instability that affects the two lobes.",internet snap
http://arxiv.org/abs/1806.05396v1,"We report on a flow velocity measurement technique based on snap-through
detection of an electrostatically actuated, bistable micromechanical beam. We
show that induced elecro-thermal Joule heating and the convective air cooling
change the beam curvature and consequently the critical snap-through voltage
($V_{ST}$). Using single crystal silicon beams, we demonstrate the snap-through
voltage to flow velocity sensitivity of $dV_{\text{ST}}/du \approx0.13$ V s
m$^{-1}$ with a power consumption of $\approx360\; \mu$W. Our experimental
results were in accord with the reduced order, coupled,
thermo-electro-mechanical model prediction. We anticipate that
electrostatically induced snap-through in curved, micromechanical beams will
open new directions for the design and implementation of downscaled flow
sensors for autonomous applications and environmental sensors.",internet snap
http://arxiv.org/abs/0712.2257v1,"Fluid dynamics video: Snapping shrimp produce a snapping sound by an
extremely rapid closure of their snapper claw. Our high speed imaging of the
claw closure has revealed that the sound is generated by the collapse of a
cavitation bubble formed in a fast flowing water jet forced out from the claws
during claw closure. The produced sound originates from the cavitation collapse
of the bubble. At collapse a short flash of light is emitted, just as in single
bubble sonoluminescence. A model based on the Rayleigh-Plesset equation can
quantitatively account for the visual and acoustical observations.",internet snap
http://arxiv.org/abs/1112.5175v1,"We experimentally demonstrate series of identical two, three, and five
coupled high Q-factor Surface Nanoscale Axial Photonics (SNAP) microresonators
formed by periodic nanoscale variation of the optical fiber radius. These
microresonators are fabricated with a 100 \mum period along an 18 \mum radius
optical fiber. The axial FWHM of these microresonators is 80 \mum and their
Q-factor exceeds 107. In addition, we demonstrate a SNAP microresonator with
the axial FWHM as small as 30 \mum and the axial FWHM of the fundamental mode
as small as 10 \mum. These results may potentially enable the dense integration
of record low loss coupled photonic microdevices on the optical fiber platform.",internet snap
http://arxiv.org/abs/1203.0065v1,"We investigated the reset time of superconducting nanowire avalanche
photodetectors (SNAPs) based on 30 nm wide nanowires. We studied the dependence
of the reset time of SNAPs on the device inductance and discovered that SNAPs
can provide a speed-up relative to SNSPDs with the same area, but with some
limitations: (1) reducing the series inductance of SNAPs (necessary for the
avalanche formation) could result in the detectors operating in an unstable
regime; (2) a trade-off exists between maximizing the bias current margin and
minimizing the reset time of SNAPs; and (3) reducing the reset time of SNAPs
below ~ 1 ns resulted in afterpulsing.",webpage snap
http://arxiv.org/abs/1311.6243v1,"In this world, globalization has become a basic and most popular human trend.
To globalize information, people are going to publish the documents in the
internet. As a result, information volume of internet has become huge. To
handle that huge volume of information, Web searcher uses search engines. The
Webpage indexing mechanism of a search engine plays a big role to retrieve Web
search results in a faster way from the huge volume of Web resources. Web
researchers have introduced various types of Web-page indexing mechanism to
retrieve Webpages from Webpage repository. In this paper, we have illustrated a
new approach of design and development of Webpage indexing. The proposed
Webpage indexing mechanism has applied on domain specific Webpages and we have
identified the Webpage domain based on an Ontology. In our approach, first we
prioritize the Ontology terms that exist in the Webpage content then apply our
own indexing mechanism to index that Webpage. The main advantage of storing an
index is to optimize the speed and performance while finding relevant documents
from the domain specific search engine storage area for a user given search
query.",webpage snap
http://arxiv.org/abs/0802.1123v2,"In this paper, we tackle the open problem of snap-stabilization in
message-passing systems. Snap-stabilization is a nice approach to design
protocols that withstand transient faults. Compared to the well-known
self-stabilizing approach, snap-stabilization guarantees that the effect of
faults is contained immediately after faults cease to occur. Our contribution
is twofold: we show that (1) snap-stabilization is impossible for a wide class
of problems if we consider networks with finite yet unbounded channel capacity;
(2) snap-stabilization becomes possible in the same setting if we assume
bounded-capacity channels. We propose three snap-stabilizing protocols working
in fully-connected networks. Our work opens exciting new research perspectives,
as it enables the snap-stabilizing paradigm to be implemented in actual
networks.",webpage snap
http://arxiv.org/abs/1409.2590v1,"Template extraction is the process of isolating the template of a given
webpage. It is widely used in several disciplines, including webpages
development, content extraction, block detection, and webpages indexing. One of
the main goals of template extraction is identifying a set of webpages with the
same template without having to load and analyze too many webpages prior to
identifying the template. This work introduces a new technique to automatically
discover a reduced set of webpages in a website that implement the template.
This set is computed with an hyperlink analysis that computes a very small set
with a high level of confidence.",webpage snap
http://arxiv.org/abs/1003.1497v1,"HTTP Server is a computer programs that serves webpage content to clients. A
webpage is a document or resource of information that is suitable for the World
Wide Web and can be accessed through a web browser and displayed on a computer
screen. This information is usually in HTML format, and may provide navigation
to other webpage's via hypertext links. WebPages may be retrieved from a local
computer or from a remote HTTP Server. WebPages are requested and served from
HTTP Servers using Hypertext Transfer Protocol (HTTP). WebPages may consist of
files of static or dynamic text stored within the HTTP Server's file system.
Client-side scripting can make WebPages more responsive to user input once in
the client browser. This paper encompasses the creation of HTTP server program
using java language, which is basically supporting for HTML and JavaScript.",webpage snap
http://arxiv.org/abs/1810.11593v1,"We introduce Reagent, a technology that readily converts ordinary webpages
containing structured data into software agents with which one can interact
naturally, via a combination of speech and pointing. Previous efforts to make
webpage content manipulable by third-party software components in browsers or
desktop applications have generally relied upon specialized instrumentation
included in the webpages -- a practice that neither scales well nor applies to
pre-existing webpages. In contrast, Reagent automatically captures semantic
details and semantically-meaningful mouse events from arbitrary webpages that
contain no pre-existing special instrumentation. Reagent combines these events
with text transcriptions of user speech to derive and execute parameterized
commands representing human intent. Thus, users may request various
visualization or analytic operations to be performed on data displayed on a
page by speaking to it and/or pointing to elements within it. When unable to
infer translations between event labels and human terminology, Reagent
proactively asks users for definitions and adds them to its dictionary. We
demonstrate Reagent in the context of a collection of pre-existing webpages
that contain football team and player statistics.",webpage snap
http://arxiv.org/abs/1905.05358v1,"State-of-the-art theorem provers, combined with smart sampling heuristics,
can generate millions of test cases in just a few hours. But given the
heuristic nature of those methods, not all of those tests may be valid. Also,
test engineers may find it too burdensome to run all those tests.
  Within a large space of tests, there can be redundancies (duplicate entries
or similar entries that do not contribute much to overall diversity). Our
approach, called SNAP uses specialized sub-sampling heuristics to avoid finding
those repeated tests. By avoiding those repeated structures SNAP explores a
smaller space of options. Hence, it is possible for SNAP to verify all its
tests.
  To evaluate SNAP, this paper applied 27 real-world case studies from a recent
ICSE'18 paper. Compared to prior results, SNAP's test case generation was 10 to
3000 times faster (median to max). Also, while prior work showed that their
tests were 70% valid, our method generates 100% valid tests. Most importantly,
test engineers would find it relatively easiest to apply SNAP's tests since our
test suites are 10 to 750 times smaller (median to max) than those generated
using prior work.",webpage snap
http://arxiv.org/abs/1606.07550v1,"Large networks are becoming a widely used abstraction for studying complex
systems in a broad set of disciplines, ranging from social network analysis to
molecular biology and neuroscience. Despite an increasing need to analyze and
manipulate large networks, only a limited number of tools are available for
this task.
  Here, we describe Stanford Network Analysis Platform (SNAP), a
general-purpose, high-performance system that provides easy to use, high-level
operations for analysis and manipulation of large networks. We present SNAP
functionality, describe its implementational details, and give performance
benchmarks. SNAP has been developed for single big-memory machines and it
balances the trade-off between maximum performance, compact in-memory graph
representation, and the ability to handle dynamic graphs where nodes and edges
are being added or removed over time. SNAP can process massive networks with
hundreds of millions of nodes and billions of edges. SNAP offers over 140
different graph algorithms that can efficiently manipulate large graphs,
calculate structural properties, generate regular and random graphs, and handle
attributes and meta-data on nodes and edges. Besides being able to handle large
graphs, an additional strength of SNAP is that networks and their attributes
are fully dynamic, they can be modified during the computation at low cost.
SNAP is provided as an open source library in C++ as well as a module in
Python.
  We also describe the Stanford Large Network Dataset, a set of social and
information real-world networks and datasets, which we make publicly available.
The collection is a complementary resource to our SNAP software and is widely
used for development and benchmarking of graph analytics algorithms.",webpage snap
http://arxiv.org/abs/1604.02956v1,"Surface nanoscale axial photonics (SNAP) structures are fabricated with a
femtosecond laser for the first time. The inscriptions introduced by the laser
pressurize the fiber and cause its nanoscale effective radius variation. We
demonstrate the subangstrom precise fabrication of individual and coupled SNAP
microresonators having the effective radius variation of several nanometers.
Our results pave the way to a novel ultraprecise SNAP fabrication technology
based on the femtosecond laser inscription.",webpage snap
http://arxiv.org/abs/1703.02707v1,"We develop a theory of optical frequency comb generation in ultra-compact
Surface Nanoscale Axial Photonic (SNAP) bottle microresonators, employing the
nonlinear interaction of whispering gallery modes which are confined along an
optical fiber with nanoscale radius variation. We predict that a SNAP
microresonator with a few-$\mu$m radius can generate a frequency comb with an
ultra-fine sub-GHz spectral spacing, which would require traditional ring
resonators of $cm$ radius. We identify regimes of stable or quasi-periodic comb
dynamics due to soliton excitation, and also show that special engineering of
the SNAP radius profile can be used to compensate nonlinearity induced
dispersion.",webpage snap
http://arxiv.org/abs/1901.02660v2,"Majority of the currently available webpages are dynamic in nature and are
changing frequently. New content gets added to webpages and existing content
gets updated or deleted. Hence, people find it useful to be alert for changes
in webpages which contain information valuable to them. In the current context,
keeping track of these webpages and getting alerts about different changes have
become significantly challenging. Change Detection and Notification (CDN)
systems were introduced to automate this monitoring process and notify users
when changes occur in webpages. This survey classifies and analyzes different
aspects of CDN systems and different techniques used for each aspect.
Furthermore, the survey highlights the current challenges and areas of
improvement present within the field of research.",webpage snap
http://arxiv.org/abs/1907.02906v1,"While there have been numerous calls to increase the participation of people
with disabilities in STEM, many postsecondary institutions are not equipped to
support students with disabilities. We examined the digital accessibility of
139 webpages from 73 postsecondary institutions that contained information
about the undergraduate physics curriculum and graduate research programs. We
selected these webpages as they are common entry points for students interested
in pursuing a physics degree. We used Tenon and Mac OS X's Voiceover software
to assess the accessibility of these webpages as measured by alignment with the
Web Content Accessibility Guidelines (WCAG) 2.0. We found only one webpage was
accessible for students with disabilities. We present five common accessibility
errors we identified in the webpages in our sample, suggested solutions for
these errors, and implications for students with disabilities, instructors and
staff, institutional administration, and the broader physics community.",webpage snap
http://arxiv.org/abs/1909.05953v1,"This paper deals with the following separability problem in 3D space: Given a
rigid polyhedron $P$ with $n$ vertices, does a semi-rigid polyhedron $G$ exist,
such that both polyhedra can be transformed into an inseparable assembled
state, where the fixture snaps on to $P$, by applying a linear force and
exploiting the mild flexibility of $G$? If such a flexible snapping polyhedron
exists, devise an efficient and robust algorithm that constructs it. In simple
words, we are looking for s semi-rigid polyhedron $G$, such that when $P$ and
$G$ are separate, we can push $G$ towards $P$, slightly bending $G$ on the way,
and obtain a configuration, where $G$ is back in its original shape, and both
$P$ and $G$ are inseparable as rigid bodies. We define certain properties such
a pair of polyhedron and its snapping fixture may possess, and prove two
theorems related to the pair. We introduce an algorithm that produces a
snapping fixture with such properties in $O(n^5)$ time, if a snapping fixture
exists, and an efficient and robust implementation of this algorithm.",webpage snap
http://arxiv.org/abs/1704.01220v1,"Clearly, no one likes webpages with poor quality of experience (QoE). Being
perceived as slow or fast is a key element in the overall perceived QoE of web
applications. While extensive effort has been put into optimizing web
applications (both in industry and academia), not a lot of work exists in
characterizing what aspects of webpage loading process truly influence human
end-user's perception of the ""Speed"" of a page. In this paper we present
""SpeedPerception"", a large-scale web performance crowdsourcing framework
focused on understanding the perceived loading performance of above-the-fold
(ATF) webpage content. Our end goal is to create free open-source benchmarking
datasets to advance the systematic analysis of how humans perceive webpage
loading process. In Phase-1 of our ""SpeedPerception"" study using Internet
Retailer Top 500 (IR 500) websites
(https://github.com/pahammad/speedperception), we found that commonly used
navigation metrics such as ""onLoad"" and ""Time To First Byte (TTFB)"" fail (less
than 60% match) to represent majority human perception when comparing the speed
of two webpages. We present a simple 3-variable-based machine learning model
that explains the majority end-user choices better (with $87 \pm 2\%$
accuracy). In addition, our results suggest that the time needed by end-users
to evaluate relative perceived speed of webpage is far less than the time of
its ""visualComplete"" event.",webpage snap
http://arxiv.org/abs/1507.01677v3,"This paper examines some of the potential challenges associated with enabling
a seamless web experience on underpowered mobile devices such as Google Glass
from the perspective of web content providers, device, and the network. We
conducted experiments to study the impact of webpage complexity, individual web
components and different application layer protocols while accessing webpages
on the performance of Glass browser, by measuring webpage load time,
temperature variation and power consumption and compare it to a smartphone. Our
findings suggest that (a) performance of Glass compared to a smartphone in
terms of power consumption and webpage load time deteriorates with increasing
webpage complexity (b) execution time for popular JavaScript benchmarks is
about 3-8 times higher on Glass compared to a smartphone, (c) WebP is more
energy efficient image format than JPEG and PNG, and (d) seven out of 50
websites studied are optimized for content delivery to Glass.",webpage snap
http://arxiv.org/abs/1708.07940v1,"Existing works for extracting navigation objects from webpages focus on
navigation menus, so as to reveal the information architecture of the site.
However, web 2.0 sites such as social networks, e-commerce portals etc. are
making the understanding of the content structure in a web site increasingly
difficult. Dynamic and personalized elements such as top stories, recommended
list in a webpage are vital to the understanding of the dynamic nature of web
2.0 sites. To better understand the content structure in web 2.0 sites, in this
paper we propose a new extraction method for navigation objects in a webpage.
Our method will extract not only the static navigation menus, but also the
dynamic and personalized page-specific navigation lists. Since the navigation
objects in a webpage naturally come in blocks, we first cluster hyperlinks into
different blocks by exploiting spatial locations of hyperlinks, the
hierarchical structure of the DOM-tree and the hyperlink density. Then we
identify navigation objects from those blocks using the SVM classifier with
novel features such as anchor text lengths etc. Experiments on real-world data
sets with webpages from various domains and styles verified the effectiveness
of our method.",webpage snap
http://arxiv.org/abs/1903.02939v1,"The visual appearance of a webpage carries valuable information about its
quality and can be used to improve the performance of learning to rank (LTR).
We introduce the Visual learning TO Rank (ViTOR) model that integrates
state-of-the-art visual features extraction methods by (i) transfer learning
from a pre-trained image classification model, and (ii) synthetic saliency heat
maps generated from webpage snapshots. Since there is currently no public
dataset for the task of LTR with visual features, we also introduce and release
the ViTOR dataset, containing visually rich and diverse webpages. The ViTOR
dataset consists of visual snapshots, non-visual features and relevance
judgments for ClueWeb12 webpages and TREC Web Track queries. We experiment with
the proposed ViTOR model on the ViTOR dataset and show that it significantly
improves the performance of LTR with visual features",webpage snap
http://arxiv.org/abs/1110.0714v1,"The carnivorous aquatic Waterwheel Plant (Aldrovanda vesiculosa L.) and the
closely related terrestrial Venus Flytrap (Dionaea muscipula SOL. EX J. ELLIS)
both feature elaborate snap-traps, which shut after reception of an external
mechanical stimulus by prey animals. Traditionally, Aldrovanda is considered as
a miniature, aquatic Dionaea, an assumption which was already established by
Charles Darwin. However, videos of snapping traps from both species suggest
completely different closure mechanisms. Indeed, the well-described snapping
mechanism in Dionaea comprises abrupt curvature inversion of the two trap
lobes, while the closing movement in Aldrovanda involves deformation of the
trap midrib but not of the lobes, which do not change curvature. In this paper,
we present the first detailed mechanical models for these plants, which are
based on the theory of thin solid membranes and explain this difference by
showing that the fast snapping of Aldrovanda is due to kinematic amplification
of the bending deformation of the midrib, while that of Dionaea unambiguously
relies on the buckling instability that affects the two lobes.",webpage snap
http://arxiv.org/abs/1806.05396v1,"We report on a flow velocity measurement technique based on snap-through
detection of an electrostatically actuated, bistable micromechanical beam. We
show that induced elecro-thermal Joule heating and the convective air cooling
change the beam curvature and consequently the critical snap-through voltage
($V_{ST}$). Using single crystal silicon beams, we demonstrate the snap-through
voltage to flow velocity sensitivity of $dV_{\text{ST}}/du \approx0.13$ V s
m$^{-1}$ with a power consumption of $\approx360\; \mu$W. Our experimental
results were in accord with the reduced order, coupled,
thermo-electro-mechanical model prediction. We anticipate that
electrostatically induced snap-through in curved, micromechanical beams will
open new directions for the design and implementation of downscaled flow
sensors for autonomous applications and environmental sensors.",webpage snap
http://arxiv.org/abs/0711.2867v1,"We analyze linkage strategies for a set I of webpages for which the webmaster
wants to maximize the sum of Google's PageRank scores. The webmaster can only
choose the hyperlinks starting from the webpages of I and has no control on the
hyperlinks from other webpages. We provide an optimal linkage strategy under
some reasonable assumptions.",webpage snap
http://arxiv.org/abs/1510.06501v2,"Phishing is a major problem on the Web. Despite the significant attention it
has received over the years, there has been no definitive solution. While the
state-of-the-art solutions have reasonably good performance, they require a
large amount of training data and are not adept at detecting phishing attacks
against new targets. In this paper, we begin with two core observations: (a)
although phishers try to make a phishing webpage look similar to its target,
they do not have unlimited freedom in structuring the phishing webpage; and (b)
a webpage can be characterized by a small set of key terms; how these key terms
are used in different parts of a webpage is different in the case of legitimate
and phishing webpages. Based on these observations, we develop a phishing
detection system with several notable properties: it is language-independent,
can be implemented entirely on client-side, has excellent classification
performance and is fast. In addition, we developed a target identification
component that can identify the target website that a phishing webpage is
attempting to mimic. The target detection component is faster than previously
reported systems and can help minimize false positives in our phishing
detection system.",webpage snap
http://arxiv.org/abs/1707.03908v1,"Game maps are useful for human players, general-game-playing agents, and
data-driven procedural content generation. These maps are generally made by
hand-assembling manually-created screenshots of game levels. Besides being
tedious and error-prone, this approach requires additional effort for each new
game and level to be mapped. The results can still be hard for humans or
computational systems to make use of, privileging visual appearance over
semantic information. We describe a software system, Mappy, that produces a
good approximation of a linked map of rooms given a Nintendo Entertainment
System game program and a sequence of button inputs exploring its world. In
addition to visual maps, Mappy outputs grids of tiles (and how they change over
time), positions of non-tile objects, clusters of similar rooms that might in
fact be the same room, and a set of links between these rooms. We believe this
is a necessary step towards developing larger corpora of high-quality
semantically-annotated maps for PCG via machine learning and other
applications.",programmed screenshot
http://arxiv.org/abs/1810.11536v1,"Recent progress on deep learning has made it possible to automatically
transform the screenshot of Graphic User Interface (GUI) into code by using the
encoder-decoder framework. While the commonly adopted image encoder (e.g., CNN
network), might be capable of extracting image features to the desired level,
interpreting these abstract image features into hundreds of tokens of code puts
a particular challenge on the decoding power of the RNN-based code generator.
Considering the code used for describing GUI is usually hierarchically
structured, we propose a new attention-based hierarchical code generation
model, which can describe GUI images in a finer level of details, while also
being able to generate hierarchically structured code in consistency with the
hierarchical layout of the graphic elements in the GUI. Our model follows the
encoder-decoder framework, all the components of which can be trained jointly
in an end-to-end manner. The experimental results show that our method
outperforms other current state-of-the-art methods on both a publicly available
GUI-code dataset as well as a dataset established by our own.",programmed screenshot
http://arxiv.org/abs/1901.02701v2,"A significant proportion of individuals' daily activities is experienced
through digital devices. Smartphones in particular have become one of the
preferred interfaces for content consumption and social interaction.
Identifying the content embedded in frequently-captured smartphone screenshots
is thus a crucial prerequisite to studies of media behavior and health
intervention planning that analyze activity interplay and content switching
over time. Screenshot images can depict heterogeneous contents and
applications, making the a priori definition of adequate taxonomies a
cumbersome task, even for humans. Privacy protection of the sensitive data
captured on screens means the costs associated with manual annotation are
large, as the effort cannot be crowd-sourced. Thus, there is need to examine
utility of unsupervised and semi-supervised methods for digital screenshot
classification. This work introduces the implications of applying clustering on
large screenshot sets when only a limited amount of labels is available. In
this paper we develop a framework for combining K-Means clustering with Active
Learning for efficient leveraging of labeled and unlabeled samples, with the
goal of discovering latent classes and describing a large collection of
screenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions
for class probability propagation provide for more well-formed cluster
configurations. Visual and textual vector representations of the screenshot
images are derived and combined to assess the relative contribution of
multi-modal features to the overall performance.",programmed screenshot
http://arxiv.org/abs/1611.03906v2,"Non-programming users should be able to create their own customized scripts
to perform computer-based tasks for them, just by demonstrating to the machine
how it's done. To that end, we develop a system prototype which
learns-by-demonstration called HILC (Help, It Looks Confusing). Users train
HILC to synthesize a task script by demonstrating the task, which produces the
needed screenshots and their corresponding mouse-keyboard signals. After the
demonstration, the user answers follow-up questions.
  We propose a user-in-the-loop framework that learns to generate scripts of
actions performed on visible elements of graphical applications. While pure
programming-by-demonstration is still unrealistic, we use quantitative and
qualitative experiments to show that non-programming users are willing and
effective at answering follow-up queries posed by our system. Our models of
events and appearance are surprisingly simple, but are combined effectively to
cope with varying amounts of supervision.
  The best available baseline, Sikuli Slides, struggled with the majority of
the tests in our user study experiments. The prototype with our proposed
approach successfully helped users accomplish simple linear tasks, complicated
tasks (monitoring, looping, and mixed), and tasks that span across multiple
executables. Even when both systems could ultimately perform a task, ours was
trained and refined by the user in less time.",programmed screenshot
http://arxiv.org/abs/1705.07962v2,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",programmed screenshot
http://arxiv.org/abs/cs/0504039v1,"This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later).",programmed screenshot
http://arxiv.org/abs/1805.02763v2,"Crowdtesting is effective especially when it comes to the feedback on GUI
systems, or subjective opinions about features. Despite of this, we find
crowdtesting reports are highly replicated, i.e., 82% of them are replicates of
others. Hence automatically detecting replicate reports could help reduce
triaging efforts. Most of the existing approaches mainly adopted textual
information for replicate detection, and suffered from low accuracy because of
the expression gap. Our observation on real industrial crowdtesting data found
that when dealing with crowdtesting reports of GUI systems, the reports would
accompanied with images, i.e., the screenshots of the app. We assume the
screenshot to be valuable for replicate crowdtesting report detection because
it reflects the real scenario of the failure and is not affected by the variety
of natural languages.
  In this work, we propose a replicate detection approach, TSDetector, which
combines information from the screenshots and the textual descriptions to
detect replicate crowdtesting reports. We extract four types of features to
characterize the screenshots and the textual descriptions, and design an
algorithm to detect replicates based on four similarity scores derived from the
four different features respectively. We investigate the effectiveness and
advantage of TSDetector on 15 commercial projects with 4,172 reports from one
of the Chinese largest crowdtesting platforms.Results show that TSDetector can
outperform existing state-of-the-art approaches significantly. In addition, we
also evaluate its usefulness using real-world case studies. The feedback from
real-world testers demonstrates its practical value",programmed screenshot
http://arxiv.org/abs/1908.06750v1,"Newly emerging variants of ransomware pose an ever-growing threat to computer
systems governing every aspect of modern life through the handling and analysis
of big data. While various recent security-based approaches have focused on
detecting and classifying ransomware at the network or system level,
easy-to-use post-infection ransomware classification for the lay user has not
been attempted before. In this paper, we investigate the possibility of
classifying the ransomware a system is infected with simply based on a
screenshot of the splash screen or the ransom note captured using a consumer
camera commonly found in any modern mobile device. To train and evaluate our
system, we create a sample dataset of the splash screens of 50 well-known
ransomware variants. In our dataset, only a single training image is available
per ransomware. Instead of creating a large training dataset of ransomware
screenshots, we simulate screenshot capture conditions via carefully designed
data augmentation techniques, enabling simple and efficient one-shot learning.
Moreover, using model uncertainty obtained via Bayesian approximation, we
ensure special input cases such as unrelated non-ransomware images and
previously-unseen ransomware variants are correctly identified for special
handling and not mis-classified. Extensive experimental evaluation demonstrates
the efficacy of our work, with accuracy levels of up to 93.6% for ransomware
classification.",programmed screenshot
http://arxiv.org/abs/1503.03378v1,"Cross-browser compatibility testing is concerned with identifying perceptible
differences in the way a Web page is rendered across different browsers or
configurations thereof. Existing automated cross-browser compatibility testing
methods are generally based on Document Object Model (DOM) analysis, or in some
cases, a combination of DOM analysis with screenshot capture and image
processing. DOM analysis however may miss incompatibilities that arise not
during DOM construction, but rather during rendering. Conversely, DOM analysis
produces false alarms because different DOMs may lead to identical or
sufficiently similar renderings. This paper presents a novel method for
cross-browser testing based purely on image processing. The method relies on
image segmentation to extract regions from a Web page and computer vision
techniques to extract a set of characteristic features from each region.
Regions extracted from a screenshot taken on a baseline browser are compared
against regions extracted from the browser under test based on characteristic
features. A machine learning classifier is used to determine if differences
between two matched regions should be classified as an incompatibility. An
evaluation involving 140 pages shows that the proposed method achieves an
F-score exceeding 0.9, outperforming a state-of-the-art cross-browser testing
tool based on DOM analysis.",programmed screenshot
http://arxiv.org/abs/1801.01316v1,"Daily engagement in life experiences is increasingly interwoven with mobile
device use. Screen capture at the scale of seconds is being used in behavioral
studies and to implement ""just-in-time"" health interventions. The increasing
psychological breadth of digital information will continue to make the actual
screens that people view a preferred if not required source of data about life
experiences. Effective and efficient Information Extraction and Retrieval from
digital screenshots is a crucial prerequisite to successful use of screen data.
In this paper, we present the experimental workflow we exploited to: (i)
pre-process a unique collection of screen captures, (ii) extract unstructured
text embedded in the images, (iii) organize image text and metadata based on a
structured schema, (iv) index the resulting document collection, and (v) allow
for Image Retrieval through a dedicated vertical search engine application. The
adopted procedure integrates different open source libraries for traditional
image processing, Optical Character Recognition (OCR), and Image Retrieval. Our
aim is to assess whether and how state-of-the-art methodologies can be applied
to this novel data set. We show how combining OpenCV-based pre-processing
modules with a Long short-term memory (LSTM) based release of Tesseract OCR,
without ad hoc training, led to a 74% character-level accuracy of the extracted
text. Further, we used the processed repository as baseline for a dedicated
Image Retrieval system, for the immediate use and application for behavioral
and prevention scientists. We discuss issues of Text Information Extraction and
Retrieval that are particular to the screenshot image case and suggest
important future work.",programmed screenshot
http://arxiv.org/abs/0810.3609v1,"VISPA is a novel development environment for high energy physics analyses,
based on a combination of graphical and textual steering. The primary aim of
VISPA is to support physicists in prototyping, performing, and verifying a data
analysis of any complexity. We present example screenshots, and describe the
underlying software concepts.",programmed screenshot
http://arxiv.org/abs/1710.08389v1,"This representative study of German search engine users (N=1,000) focuses on
the ability of users to distinguish between organic results and advertisements
on Google results pages. We combine questions about Google's business with
task-based studies in which users were asked to distinguish between ads and
organic results in screenshots of results pages. We find that only a small
percentage of users is able to reliably distinguish between ads and organic
results, and that user knowledge of Google's business model is very limited. We
conclude that ads are insufficiently labelled as such, and that many users may
click on ads assuming that they are selecting organic results.",programmed screenshot
http://arxiv.org/abs/1908.02449v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
the main resources to develop these tools are datasets, which are introduced
and provided by the present paper, for the specific cases of visual correlation
of phishing and onion websites. CIRCL's Open-Source tools are the sources of
these screenshots, which had been manually verified against personal
information leaks. Usage examples of these datasets are proposed in the current
paper. These researches directions are, however, not the main contribution of
the paper. The main contribution is the availability of the two datasets.",programmed screenshot
http://arxiv.org/abs/1908.04014v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
no open-source and turnkey library was found able to reach this goal. The
present paper introduces an Open-Source modular library for the specific cases
of visual correlation and Image Matching named Douglas-Quaid. The design of the
library, chosen tradeoffs, encountered challenges, envisioned solutions as well
as quality and speed results are presented in this paper. We also explore
researches directions and future potential developments of the library. Our
claim is that even partial automation of screenshots classification would
reduce the burden on security teams and that Douglas-Quaid is a step forward in
this direction.",programmed screenshot
http://arxiv.org/abs/1905.07767v1,"Phishing, a continuously growing cyber threat, aims to obtain innocent users'
credentials by deceiving them via presenting fake web pages which mimic their
legitimate targets. To date, various attempts have been carried out in order to
detect phishing pages. In this study, we treat the problem of phishing web page
identification as an image classification task and propose a machine learning
augmented pure vision based approach which extracts and classifies compact
visual features from web page screenshots. For this purpose, we employed
several MPEG7 and MPEG7-like compact visual descriptors (SCD, CLD, CEDD, FCTH
and JCD) to reveal color and edge based discriminative visual cues. Throughout
the feature extraction process we have followed two different schemes working
on either whole screenshots in a ""holistic"" manner or equal sized ""patches""
constructing a coarse-to-fine ""pyramidal"" representation. Moreover, for the
task of image classification, we have built SVM and Random Forest based machine
learning models. In order to assess the performance and generalization
capability of the proposed approach, we have collected a mid-sized corpus
covering 14 distinct brands and involving 2852 samples. According to the
conducted experiments, our approach reaches up to 90.5% F1 score via SCD. As a
result, compared to other studies, the suggested approach presents a
lightweight schema serving competitive accuracy and superior feature extraction
and inferring speed that enables it to be used as a browser plugin.",programmed screenshot
http://arxiv.org/abs/0908.3022v1,"This paper outlines an approach to manage and quantify the risks associated
with changes made to spreadsheets. The methodology focuses on structural
differences between spreadsheets and suggests a technique by which a risk
analysis can be achieved in an automated environment. The paper offers an
example that demonstrates how contiguous ranges of data can be mapped into a
generic list of formulae, data and metadata. The example then shows that
comparison of these generic lists can establish the structural differences
between spreadsheets and quantify the level of risk that each change has
introduced. Lastly the benefits, drawbacks and limitations of the technique are
discussed in a commercial context.",programmed screenshot
http://arxiv.org/abs/1005.2072v1,"We review a case study of a UI design project for a complete travel search
engine system prototype for regular and corporate users. We discuss various
usage scenarios, guidelines, and so for, and put them into a web-based
prototype with screenshots and the like. We combined into our prototype the
best features found at the time (2002) on most travel-like sites and added more
to them as a part of our research. We conducted feasibility studies, review
common design guidelines and Nelson's heuristics while constructing this work.
The prototype is itself open-source, but has no backend functionality, as the
focus is the user-centered design of such a system. While the prototype is
mostly static, some dynamic activity is present through the use of PHP.",programmed screenshot
http://arxiv.org/abs/1408.5265v2,"An ensemble inference mechanism is proposed on the Angry Birds domain. It is
based on an efficient tree structure for encoding and representing game
screenshots, where it exploits its enhanced modeling capability. This has the
advantage to establish an informative feature space and modify the task of game
playing to a regression analysis problem. To this direction, we assume that
each type of object material and bird pair has its own Bayesian linear
regression model. In this way, a multi-model regression framework is designed
that simultaneously calculates the conditional expectations of several objects
and makes a target decision through an ensemble of regression models. Learning
procedure is performed according to an online estimation strategy for the model
parameters. We provide comparative experimental results on several game levels
that empirically illustrate the efficiency of the proposed methodology.",programmed screenshot
http://arxiv.org/abs/1801.06428v1,"Unique challenges arise when testing mobile applications due to their
prevailing event-driven nature and complex contextual features (e.g. sensors,
notifications). Current automated input generation approaches for Android apps
are typically not practical for developers to use due to required
instrumentation or platform dependence and generally do not effectively
exercise contextual features. To better support developers in mobile testing
tasks, in this demo we present a novel, automated tool called CrashScope. This
tool explores a given Android app using systematic input generation, according
to several strategies informed by static and dynamic analyses, with the
intrinsic goal of triggering crashes. When a crash is detected, CrashScope
generates an augmented crash report containing screenshots, detailed crash
reproduction steps, the captured exception stack trace, and a fully replayable
script that automatically reproduces the crash on a target device(s). Results
of preliminary studies show that CrashScope is able to uncover about as many
crashes as other state of the art tools, while providing detailed useful crash
reports and test scripts to developers. Website:
www.crashscope-android.com/crashscope-home Video url:
https://youtu.be/ii6S1JF6xDw",programmed screenshot
http://arxiv.org/abs/1802.01628v1,"Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are ""transparent"". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.",programmed screenshot
http://arxiv.org/abs/1807.04191v1,"UI design languages, such as Google's Material Design, make applications both
easier to develop and easier to learn by providing a set of standard UI
components. Nonetheless, it is hard to assess the impact of design languages in
the wild. Moreover, designers often get stranded by strong-opinionated debates
around the merit of certain UI components, such as the Floating Action Button
and the Navigation Drawer. To address these challenges, this short paper
introduces a method for measuring the impact of design languages and informing
design debates through analyzing a dataset consisting of view hierarchies,
screenshots, and app metadata for more than 9,000 mobile apps. Our data
analysis shows that use of Material Design is positively correlated to app
ratings, and to some extent, also the number of installs. Furthermore, we show
that use of UI components vary by app category, suggesting a more nuanced view
needed in design debates.",programmed screenshot
http://arxiv.org/abs/1703.02227v1,"In the Google Play store, an introduction page is associated with every
mobile application (app) for users to acquire its details, including
screenshots, description, reviews, etc. However, it remains a challenge to
identify what items influence users most when downloading an app. To explore
users' perspective, we conduct a survey to inquire about this question. The
results of survey suggest that the participants pay most attention to the app
description which gives users a quick overview of the app. Although there exist
some guidelines about how to write a good app description to attract more
downloads, it is hard to define a high quality app description. Meanwhile,
there is no tool to evaluate the quality of app description. In this paper, we
employ the method of crowdsourcing to extract the attributes that affect the
app descriptions' quality. First, we download some app descriptions from Google
Play, then invite some participants to rate their quality with the score from
one (very poor) to five (very good). The participants are also requested to
explain every score's reasons. By analyzing the reasons, we extract the
attributes that the participants consider important during evaluating the
quality of app descriptions. Finally, we train the supervised learning models
on a sample of 100 app descriptions. In our experiments, the support vector
machine model obtains up to 62% accuracy. In addition, we find that the
permission, the number of paragraphs and the average number of words in one
feature play key roles in defining a good app description.",programmed screenshot
http://arxiv.org/abs/1706.01130v1,"Mobile developers face unique challenges when detecting and reporting crashes
in apps due to their prevailing GUI event-driven nature and additional sources
of inputs (e.g., sensor readings). To support developers in these tasks, we
introduce a novel, automated approach called CRASHSCOPE. This tool explores a
given Android app using systematic input generation, according to several
strategies informed by static and dynamic analyses, with the intrinsic goal of
triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented
crash report containing screenshots, detailed crash reproduction steps, the
captured exception stack trace, and a fully replayable script that
automatically reproduces the crash on a target device(s). We evaluated
CRASHSCOPE's effectiveness in discovering crashes as compared to five
state-of-the-art Android input generation tools on 61 applications. The results
demonstrate that CRASHSCOPE performs about as well as current tools for
detecting crashes and provides more detailed fault information. Additionally,
in a study analyzing eight real-world Android app crashes, we found that
CRASHSCOPE's reports are easily readable and allow for reliable reproduction of
crashes by presenting more explicit information than human written reports.",programmed screenshot
http://arxiv.org/abs/1711.04030v1,"Effective machine-aided diagnosis and repair of configuration errors
continues to elude computer systems designers. Most of the literature targets
errors that can be attributed to a single erroneous configuration setting.
However, a recent study found that a significant amount of configuration errors
require fixing more than one setting together. To address this limitation,
Ocasta statistically clusters dependent configuration settings based on the
application's accesses to its configuration settings and utilizes the extracted
clustering of configuration settings to fix configuration errors involving more
than one configuration settings. Ocasta treats applications as black-boxes and
only relies on the ability to observe application accesses to their
configuration settings.
  We collected traces of real application usage from 24 Linux and 5 Windows
desktops computers and found that Ocasta is able to correctly identify clusters
with 88.6% accuracy. To demonstrate the effectiveness of Ocasta, we evaluated
it on 16 real-world configuration errors of 11 Linux and Windows applications.
Ocasta is able to successfully repair all evaluated configuration errors in 11
minutes on average and only requires the user to examine an average of 3
screenshots of the output of the application to confirm that the error is
repaired. A user study we conducted shows that Ocasta is easy to use by both
expert and non-expert users and is more efficient than manual configuration
error troubleshooting.",programmed screenshot
http://arxiv.org/abs/1801.09946v2,"Recent progress in genomics is bringing genetic testing to the masses.
Participatory public initiatives are underway to sequence the genome of
millions of volunteers, and a new market is booming with a number of companies
like 23andMe and AncestryDNA offering affordable tests directly to consumers.
Consequently, news, experiences, and views on genetic testing are increasingly
shared and discussed online and on social networks like Twitter. In this paper,
we present a large-scale analysis of Twitter discourse on genetic testing. We
collect 302K tweets from 113K users, posted over 2.5 years, by using thirteen
keywords related to genetic testing companies and public initiatives as search
keywords. We study both the tweets and the users posting them along several
axes, aiming to understand who tweets about genetic testing, what they talk
about, and how they use Twitter for that. Among other things, we find that
tweets about genetic testing originate from accounts that overall appear to be
interested in digital health and technology. Also, marketing efforts as well as
announcements, such as the FDA's suspension of 23andMe's health reports,
influence the type and the nature of user engagement.Finally, we report on
users who share screenshots of their results, and raise a few ethical and
societal questions as we find evidence of groups associating genetic testing to
racist ideologies.",programmed screenshot
http://arxiv.org/abs/1804.04605v1,"The number of Android smartphone and tablet users has experienced a rapid
growth in the past few years and it raises users' awareness on the privacy and
security of their mobile devices. The features of openness and extensibility
make Android unique, attractive and competitive but meanwhile vulnerable to
malicious attack. There are lots of users rooting their Android devices for
some useful functions, which are not originally provided to developers and
users, such as backup and taking screenshot. However, after observing the
danger of rooting devices, the developers begin to look for other non-root
alternatives to implement those functions. ADB workaround is one of the best
known non-root alternatives to help app gain higher privilege on Android. It
used to be considered as a secure practice until some cases of ADB privilege
leakage have been found. In this project, we design an approach and implement a
couple of tools to detect the privilege leakage in Android apps. We apply them
to analyse three real-world apps with millions of users, and successfully
identify three ADB privilege leaks from them. Moreover, we also conduct an
exploitation of the ADB privilege in one app, and therefore we prove the
existence of vulnerabilities in ADB workaround. Based on out study, we propose
some suggestion to help developers create their apps that could not only
satisfy users' needs but also protect users' privacy from similar attacks in
future.",programmed screenshot
http://arxiv.org/abs/1908.01351v1,"IT support services industry is going through a major transformation with AI
becoming commonplace. There has been a lot of effort in the direction of
automation at every human touchpoint in the IT support processes. Incident
management is one such process which has been a beacon process for AI based
automation. The vision is to automate the process from the time an
incident/ticket arrives till it is resolved and closed. While text is the
primary mode of communicating the incidents, there has been a growing trend of
using alternate modalities like image to communicate the problem. A large
fraction of IT support tickets today contain attached image data in the form of
screenshots, log messages, invoices and so on. These attachments help in better
explanation of the problem which aids in faster resolution. Anybody who aspires
to provide AI based IT support, it is essential to build systems which can
handle multi-modal content. In this paper we present how incident management in
IT support domain can be made much more effective using multi-modal analysis.
The information extracted from different modalities are correlated to enrich
the information in the ticket and used for better ticket routing and
resolution. We evaluate our system using about 25000 real tickets containing
attachments from selected problem areas. Our results demonstrate significant
improvements in both routing and resolution with the use of multi-modal ticket
analysis compared to only text based analysis.",programmed screenshot
http://arxiv.org/abs/1908.00898v1,"Programming is the activity of modifying a program in order to bring about
specific changes in its behaviour. Yet programming language theory almost
exclusively focuses on the meaning of programs. We motivate a ""change-oriented""
viewpoint from which the meaning of a program change is a change to the
program's meaning.",programmed screenshot
http://arxiv.org/abs/1007.3023v2,"The idea of functional programming has played a big role in shaping today's
landscape of mainstream programming languages. Another concept that dominates
the current programming style is Dijkstra's structured programming. Both
concepts have been successfully married, for example in the programming
language Scala. This paper proposes how the same can be achieved for structured
programming and PURELY functional programming via the notion of LINEAR SCOPE.
One advantage of this proposal is that mainstream programmers can reap the
benefits of purely functional programming like easily exploitable parallelism
while using familiar structured programming syntax and without knowing concepts
like monads. A second advantage is that professional purely functional
programmers can often avoid hard to read functional code by using structured
programming syntax that is often easier to parse mentally.",programmed screenshot
http://arxiv.org/abs/1007.4908v1,"Termination is an important and well-studied property for logic programs.
However, almost all approaches for automated termination analysis focus on
definite logic programs, whereas real-world Prolog programs typically use the
cut operator. We introduce a novel pre-processing method which automatically
transforms Prolog programs into logic programs without cuts, where termination
of the cut-free program implies termination of the original program. Hence
after this pre-processing, any technique for proving termination of definite
logic programs can be applied. We implemented this pre-processing in our
termination prover AProVE and evaluated it successfully with extensive
experiments.",programmed screenshot
http://arxiv.org/abs/1807.06051v3,"Processing programs as data is one of the successes of functional and logic
programming. Higher-order functions, as program-processing programs are called
in functional programming, and meta-programs, as they are called in logic
programming, are widespread declarative programming techniques. In logic
programming, there is a gap between the meta-programming practice and its
theory: The formalisations of meta-programming do not explicitly address its
impredicativity and are not fully adequate. This article aims at overcoming
this unsatisfactory situation by discussing the relevance of impredicativity to
meta-programming, by revisiting former formalisations of meta-programming and
by defining Reflective Predicate Logic, a conservative extension of first-order
logic, which provides a simple formalisation of meta-programming.",programmed screenshot
http://arxiv.org/abs/cs/9809032v1,"In this paper we reexamine the place and role of stable model semantics in
logic programming and contrast it with a least Herbrand model approach to Horn
programs. We demonstrate that inherent features of stable model semantics
naturally lead to a logic programming system that offers an interesting
alternative to more traditional logic programming styles of Horn logic
programming, stratified logic programming and logic programming with
well-founded semantics. The proposed approach is based on the interpretation of
program clauses as constraints. In this setting programs do not describe a
single intended model, but a family of stable models. These stable models
encode solutions to the constraint satisfaction problem described by the
program. Our approach imposes restrictions on the syntax of logic programs. In
particular, function symbols are eliminated from the language. We argue that
the resulting logic programming system is well-attuned to problems in the class
NP, has a well-defined domain of applications, and an emerging methodology of
programming. We point out that what makes the whole approach viable is recent
progress in implementations of algorithms to compute stable models of
propositional logic programs.",programmed screenshot
http://arxiv.org/abs/1808.07770v1,"Programming Computable Functions (PCF) is a simplified programming language
which provides the theoretical basis of modern functional programming
languages. Answer set programming (ASP) is a programming paradigm focused on
solving search problems. In this paper we provide a translation from PCF to
ASP. Using this translation it becomes possible to specify search problems
using PCF.",programmed screenshot
http://arxiv.org/abs/1705.07962v2,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",automatic screenshot
http://arxiv.org/abs/1805.02763v2,"Crowdtesting is effective especially when it comes to the feedback on GUI
systems, or subjective opinions about features. Despite of this, we find
crowdtesting reports are highly replicated, i.e., 82% of them are replicates of
others. Hence automatically detecting replicate reports could help reduce
triaging efforts. Most of the existing approaches mainly adopted textual
information for replicate detection, and suffered from low accuracy because of
the expression gap. Our observation on real industrial crowdtesting data found
that when dealing with crowdtesting reports of GUI systems, the reports would
accompanied with images, i.e., the screenshots of the app. We assume the
screenshot to be valuable for replicate crowdtesting report detection because
it reflects the real scenario of the failure and is not affected by the variety
of natural languages.
  In this work, we propose a replicate detection approach, TSDetector, which
combines information from the screenshots and the textual descriptions to
detect replicate crowdtesting reports. We extract four types of features to
characterize the screenshots and the textual descriptions, and design an
algorithm to detect replicates based on four similarity scores derived from the
four different features respectively. We investigate the effectiveness and
advantage of TSDetector on 15 commercial projects with 4,172 reports from one
of the Chinese largest crowdtesting platforms.Results show that TSDetector can
outperform existing state-of-the-art approaches significantly. In addition, we
also evaluate its usefulness using real-world case studies. The feedback from
real-world testers demonstrates its practical value",automatic screenshot
http://arxiv.org/abs/1908.02449v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
the main resources to develop these tools are datasets, which are introduced
and provided by the present paper, for the specific cases of visual correlation
of phishing and onion websites. CIRCL's Open-Source tools are the sources of
these screenshots, which had been manually verified against personal
information leaks. Usage examples of these datasets are proposed in the current
paper. These researches directions are, however, not the main contribution of
the paper. The main contribution is the availability of the two datasets.",automatic screenshot
http://arxiv.org/abs/1908.04014v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
no open-source and turnkey library was found able to reach this goal. The
present paper introduces an Open-Source modular library for the specific cases
of visual correlation and Image Matching named Douglas-Quaid. The design of the
library, chosen tradeoffs, encountered challenges, envisioned solutions as well
as quality and speed results are presented in this paper. We also explore
researches directions and future potential developments of the library. Our
claim is that even partial automation of screenshots classification would
reduce the burden on security teams and that Douglas-Quaid is a step forward in
this direction.",automatic screenshot
http://arxiv.org/abs/1810.11536v1,"Recent progress on deep learning has made it possible to automatically
transform the screenshot of Graphic User Interface (GUI) into code by using the
encoder-decoder framework. While the commonly adopted image encoder (e.g., CNN
network), might be capable of extracting image features to the desired level,
interpreting these abstract image features into hundreds of tokens of code puts
a particular challenge on the decoding power of the RNN-based code generator.
Considering the code used for describing GUI is usually hierarchically
structured, we propose a new attention-based hierarchical code generation
model, which can describe GUI images in a finer level of details, while also
being able to generate hierarchically structured code in consistency with the
hierarchical layout of the graphic elements in the GUI. Our model follows the
encoder-decoder framework, all the components of which can be trained jointly
in an end-to-end manner. The experimental results show that our method
outperforms other current state-of-the-art methods on both a publicly available
GUI-code dataset as well as a dataset established by our own.",automatic screenshot
http://arxiv.org/abs/1707.03908v1,"Game maps are useful for human players, general-game-playing agents, and
data-driven procedural content generation. These maps are generally made by
hand-assembling manually-created screenshots of game levels. Besides being
tedious and error-prone, this approach requires additional effort for each new
game and level to be mapped. The results can still be hard for humans or
computational systems to make use of, privileging visual appearance over
semantic information. We describe a software system, Mappy, that produces a
good approximation of a linked map of rooms given a Nintendo Entertainment
System game program and a sequence of button inputs exploring its world. In
addition to visual maps, Mappy outputs grids of tiles (and how they change over
time), positions of non-tile objects, clusters of similar rooms that might in
fact be the same room, and a set of links between these rooms. We believe this
is a necessary step towards developing larger corpora of high-quality
semantically-annotated maps for PCG via machine learning and other
applications.",automatic screenshot
http://arxiv.org/abs/1801.06428v1,"Unique challenges arise when testing mobile applications due to their
prevailing event-driven nature and complex contextual features (e.g. sensors,
notifications). Current automated input generation approaches for Android apps
are typically not practical for developers to use due to required
instrumentation or platform dependence and generally do not effectively
exercise contextual features. To better support developers in mobile testing
tasks, in this demo we present a novel, automated tool called CrashScope. This
tool explores a given Android app using systematic input generation, according
to several strategies informed by static and dynamic analyses, with the
intrinsic goal of triggering crashes. When a crash is detected, CrashScope
generates an augmented crash report containing screenshots, detailed crash
reproduction steps, the captured exception stack trace, and a fully replayable
script that automatically reproduces the crash on a target device(s). Results
of preliminary studies show that CrashScope is able to uncover about as many
crashes as other state of the art tools, while providing detailed useful crash
reports and test scripts to developers. Website:
www.crashscope-android.com/crashscope-home Video url:
https://youtu.be/ii6S1JF6xDw",automatic screenshot
http://arxiv.org/abs/1901.02701v2,"A significant proportion of individuals' daily activities is experienced
through digital devices. Smartphones in particular have become one of the
preferred interfaces for content consumption and social interaction.
Identifying the content embedded in frequently-captured smartphone screenshots
is thus a crucial prerequisite to studies of media behavior and health
intervention planning that analyze activity interplay and content switching
over time. Screenshot images can depict heterogeneous contents and
applications, making the a priori definition of adequate taxonomies a
cumbersome task, even for humans. Privacy protection of the sensitive data
captured on screens means the costs associated with manual annotation are
large, as the effort cannot be crowd-sourced. Thus, there is need to examine
utility of unsupervised and semi-supervised methods for digital screenshot
classification. This work introduces the implications of applying clustering on
large screenshot sets when only a limited amount of labels is available. In
this paper we develop a framework for combining K-Means clustering with Active
Learning for efficient leveraging of labeled and unlabeled samples, with the
goal of discovering latent classes and describing a large collection of
screenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions
for class probability propagation provide for more well-formed cluster
configurations. Visual and textual vector representations of the screenshot
images are derived and combined to assess the relative contribution of
multi-modal features to the overall performance.",automatic screenshot
http://arxiv.org/abs/1706.01130v1,"Mobile developers face unique challenges when detecting and reporting crashes
in apps due to their prevailing GUI event-driven nature and additional sources
of inputs (e.g., sensor readings). To support developers in these tasks, we
introduce a novel, automated approach called CRASHSCOPE. This tool explores a
given Android app using systematic input generation, according to several
strategies informed by static and dynamic analyses, with the intrinsic goal of
triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented
crash report containing screenshots, detailed crash reproduction steps, the
captured exception stack trace, and a fully replayable script that
automatically reproduces the crash on a target device(s). We evaluated
CRASHSCOPE's effectiveness in discovering crashes as compared to five
state-of-the-art Android input generation tools on 61 applications. The results
demonstrate that CRASHSCOPE performs about as well as current tools for
detecting crashes and provides more detailed fault information. Additionally,
in a study analyzing eight real-world Android app crashes, we found that
CRASHSCOPE's reports are easily readable and allow for reliable reproduction of
crashes by presenting more explicit information than human written reports.",automatic screenshot
http://arxiv.org/abs/cs/0504039v1,"This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later).",automatic screenshot
http://arxiv.org/abs/1908.06750v1,"Newly emerging variants of ransomware pose an ever-growing threat to computer
systems governing every aspect of modern life through the handling and analysis
of big data. While various recent security-based approaches have focused on
detecting and classifying ransomware at the network or system level,
easy-to-use post-infection ransomware classification for the lay user has not
been attempted before. In this paper, we investigate the possibility of
classifying the ransomware a system is infected with simply based on a
screenshot of the splash screen or the ransom note captured using a consumer
camera commonly found in any modern mobile device. To train and evaluate our
system, we create a sample dataset of the splash screens of 50 well-known
ransomware variants. In our dataset, only a single training image is available
per ransomware. Instead of creating a large training dataset of ransomware
screenshots, we simulate screenshot capture conditions via carefully designed
data augmentation techniques, enabling simple and efficient one-shot learning.
Moreover, using model uncertainty obtained via Bayesian approximation, we
ensure special input cases such as unrelated non-ransomware images and
previously-unseen ransomware variants are correctly identified for special
handling and not mis-classified. Extensive experimental evaluation demonstrates
the efficacy of our work, with accuracy levels of up to 93.6% for ransomware
classification.",automatic screenshot
http://arxiv.org/abs/1503.03378v1,"Cross-browser compatibility testing is concerned with identifying perceptible
differences in the way a Web page is rendered across different browsers or
configurations thereof. Existing automated cross-browser compatibility testing
methods are generally based on Document Object Model (DOM) analysis, or in some
cases, a combination of DOM analysis with screenshot capture and image
processing. DOM analysis however may miss incompatibilities that arise not
during DOM construction, but rather during rendering. Conversely, DOM analysis
produces false alarms because different DOMs may lead to identical or
sufficiently similar renderings. This paper presents a novel method for
cross-browser testing based purely on image processing. The method relies on
image segmentation to extract regions from a Web page and computer vision
techniques to extract a set of characteristic features from each region.
Regions extracted from a screenshot taken on a baseline browser are compared
against regions extracted from the browser under test based on characteristic
features. A machine learning classifier is used to determine if differences
between two matched regions should be classified as an incompatibility. An
evaluation involving 140 pages shows that the proposed method achieves an
F-score exceeding 0.9, outperforming a state-of-the-art cross-browser testing
tool based on DOM analysis.",automatic screenshot
http://arxiv.org/abs/1801.01316v1,"Daily engagement in life experiences is increasingly interwoven with mobile
device use. Screen capture at the scale of seconds is being used in behavioral
studies and to implement ""just-in-time"" health interventions. The increasing
psychological breadth of digital information will continue to make the actual
screens that people view a preferred if not required source of data about life
experiences. Effective and efficient Information Extraction and Retrieval from
digital screenshots is a crucial prerequisite to successful use of screen data.
In this paper, we present the experimental workflow we exploited to: (i)
pre-process a unique collection of screen captures, (ii) extract unstructured
text embedded in the images, (iii) organize image text and metadata based on a
structured schema, (iv) index the resulting document collection, and (v) allow
for Image Retrieval through a dedicated vertical search engine application. The
adopted procedure integrates different open source libraries for traditional
image processing, Optical Character Recognition (OCR), and Image Retrieval. Our
aim is to assess whether and how state-of-the-art methodologies can be applied
to this novel data set. We show how combining OpenCV-based pre-processing
modules with a Long short-term memory (LSTM) based release of Tesseract OCR,
without ad hoc training, led to a 74% character-level accuracy of the extracted
text. Further, we used the processed repository as baseline for a dedicated
Image Retrieval system, for the immediate use and application for behavioral
and prevention scientists. We discuss issues of Text Information Extraction and
Retrieval that are particular to the screenshot image case and suggest
important future work.",automatic screenshot
http://arxiv.org/abs/0810.3609v1,"VISPA is a novel development environment for high energy physics analyses,
based on a combination of graphical and textual steering. The primary aim of
VISPA is to support physicists in prototyping, performing, and verifying a data
analysis of any complexity. We present example screenshots, and describe the
underlying software concepts.",automatic screenshot
http://arxiv.org/abs/1710.08389v1,"This representative study of German search engine users (N=1,000) focuses on
the ability of users to distinguish between organic results and advertisements
on Google results pages. We combine questions about Google's business with
task-based studies in which users were asked to distinguish between ads and
organic results in screenshots of results pages. We find that only a small
percentage of users is able to reliably distinguish between ads and organic
results, and that user knowledge of Google's business model is very limited. We
conclude that ads are insufficiently labelled as such, and that many users may
click on ads assuming that they are selecting organic results.",automatic screenshot
http://arxiv.org/abs/1905.07767v1,"Phishing, a continuously growing cyber threat, aims to obtain innocent users'
credentials by deceiving them via presenting fake web pages which mimic their
legitimate targets. To date, various attempts have been carried out in order to
detect phishing pages. In this study, we treat the problem of phishing web page
identification as an image classification task and propose a machine learning
augmented pure vision based approach which extracts and classifies compact
visual features from web page screenshots. For this purpose, we employed
several MPEG7 and MPEG7-like compact visual descriptors (SCD, CLD, CEDD, FCTH
and JCD) to reveal color and edge based discriminative visual cues. Throughout
the feature extraction process we have followed two different schemes working
on either whole screenshots in a ""holistic"" manner or equal sized ""patches""
constructing a coarse-to-fine ""pyramidal"" representation. Moreover, for the
task of image classification, we have built SVM and Random Forest based machine
learning models. In order to assess the performance and generalization
capability of the proposed approach, we have collected a mid-sized corpus
covering 14 distinct brands and involving 2852 samples. According to the
conducted experiments, our approach reaches up to 90.5% F1 score via SCD. As a
result, compared to other studies, the suggested approach presents a
lightweight schema serving competitive accuracy and superior feature extraction
and inferring speed that enables it to be used as a browser plugin.",automatic screenshot
http://arxiv.org/abs/0908.3022v1,"This paper outlines an approach to manage and quantify the risks associated
with changes made to spreadsheets. The methodology focuses on structural
differences between spreadsheets and suggests a technique by which a risk
analysis can be achieved in an automated environment. The paper offers an
example that demonstrates how contiguous ranges of data can be mapped into a
generic list of formulae, data and metadata. The example then shows that
comparison of these generic lists can establish the structural differences
between spreadsheets and quantify the level of risk that each change has
introduced. Lastly the benefits, drawbacks and limitations of the technique are
discussed in a commercial context.",automatic screenshot
http://arxiv.org/abs/1005.2072v1,"We review a case study of a UI design project for a complete travel search
engine system prototype for regular and corporate users. We discuss various
usage scenarios, guidelines, and so for, and put them into a web-based
prototype with screenshots and the like. We combined into our prototype the
best features found at the time (2002) on most travel-like sites and added more
to them as a part of our research. We conducted feasibility studies, review
common design guidelines and Nelson's heuristics while constructing this work.
The prototype is itself open-source, but has no backend functionality, as the
focus is the user-centered design of such a system. While the prototype is
mostly static, some dynamic activity is present through the use of PHP.",automatic screenshot
http://arxiv.org/abs/1408.5265v2,"An ensemble inference mechanism is proposed on the Angry Birds domain. It is
based on an efficient tree structure for encoding and representing game
screenshots, where it exploits its enhanced modeling capability. This has the
advantage to establish an informative feature space and modify the task of game
playing to a regression analysis problem. To this direction, we assume that
each type of object material and bird pair has its own Bayesian linear
regression model. In this way, a multi-model regression framework is designed
that simultaneously calculates the conditional expectations of several objects
and makes a target decision through an ensemble of regression models. Learning
procedure is performed according to an online estimation strategy for the model
parameters. We provide comparative experimental results on several game levels
that empirically illustrate the efficiency of the proposed methodology.",automatic screenshot
http://arxiv.org/abs/1802.01628v1,"Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are ""transparent"". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.",automatic screenshot
http://arxiv.org/abs/1807.04191v1,"UI design languages, such as Google's Material Design, make applications both
easier to develop and easier to learn by providing a set of standard UI
components. Nonetheless, it is hard to assess the impact of design languages in
the wild. Moreover, designers often get stranded by strong-opinionated debates
around the merit of certain UI components, such as the Floating Action Button
and the Navigation Drawer. To address these challenges, this short paper
introduces a method for measuring the impact of design languages and informing
design debates through analyzing a dataset consisting of view hierarchies,
screenshots, and app metadata for more than 9,000 mobile apps. Our data
analysis shows that use of Material Design is positively correlated to app
ratings, and to some extent, also the number of installs. Furthermore, we show
that use of UI components vary by app category, suggesting a more nuanced view
needed in design debates.",automatic screenshot
http://arxiv.org/abs/1611.03906v2,"Non-programming users should be able to create their own customized scripts
to perform computer-based tasks for them, just by demonstrating to the machine
how it's done. To that end, we develop a system prototype which
learns-by-demonstration called HILC (Help, It Looks Confusing). Users train
HILC to synthesize a task script by demonstrating the task, which produces the
needed screenshots and their corresponding mouse-keyboard signals. After the
demonstration, the user answers follow-up questions.
  We propose a user-in-the-loop framework that learns to generate scripts of
actions performed on visible elements of graphical applications. While pure
programming-by-demonstration is still unrealistic, we use quantitative and
qualitative experiments to show that non-programming users are willing and
effective at answering follow-up queries posed by our system. Our models of
events and appearance are surprisingly simple, but are combined effectively to
cope with varying amounts of supervision.
  The best available baseline, Sikuli Slides, struggled with the majority of
the tests in our user study experiments. The prototype with our proposed
approach successfully helped users accomplish simple linear tasks, complicated
tasks (monitoring, looping, and mixed), and tasks that span across multiple
executables. Even when both systems could ultimately perform a task, ours was
trained and refined by the user in less time.",automatic screenshot
http://arxiv.org/abs/1703.02227v1,"In the Google Play store, an introduction page is associated with every
mobile application (app) for users to acquire its details, including
screenshots, description, reviews, etc. However, it remains a challenge to
identify what items influence users most when downloading an app. To explore
users' perspective, we conduct a survey to inquire about this question. The
results of survey suggest that the participants pay most attention to the app
description which gives users a quick overview of the app. Although there exist
some guidelines about how to write a good app description to attract more
downloads, it is hard to define a high quality app description. Meanwhile,
there is no tool to evaluate the quality of app description. In this paper, we
employ the method of crowdsourcing to extract the attributes that affect the
app descriptions' quality. First, we download some app descriptions from Google
Play, then invite some participants to rate their quality with the score from
one (very poor) to five (very good). The participants are also requested to
explain every score's reasons. By analyzing the reasons, we extract the
attributes that the participants consider important during evaluating the
quality of app descriptions. Finally, we train the supervised learning models
on a sample of 100 app descriptions. In our experiments, the support vector
machine model obtains up to 62% accuracy. In addition, we find that the
permission, the number of paragraphs and the average number of words in one
feature play key roles in defining a good app description.",automatic screenshot
http://arxiv.org/abs/1711.04030v1,"Effective machine-aided diagnosis and repair of configuration errors
continues to elude computer systems designers. Most of the literature targets
errors that can be attributed to a single erroneous configuration setting.
However, a recent study found that a significant amount of configuration errors
require fixing more than one setting together. To address this limitation,
Ocasta statistically clusters dependent configuration settings based on the
application's accesses to its configuration settings and utilizes the extracted
clustering of configuration settings to fix configuration errors involving more
than one configuration settings. Ocasta treats applications as black-boxes and
only relies on the ability to observe application accesses to their
configuration settings.
  We collected traces of real application usage from 24 Linux and 5 Windows
desktops computers and found that Ocasta is able to correctly identify clusters
with 88.6% accuracy. To demonstrate the effectiveness of Ocasta, we evaluated
it on 16 real-world configuration errors of 11 Linux and Windows applications.
Ocasta is able to successfully repair all evaluated configuration errors in 11
minutes on average and only requires the user to examine an average of 3
screenshots of the output of the application to confirm that the error is
repaired. A user study we conducted shows that Ocasta is easy to use by both
expert and non-expert users and is more efficient than manual configuration
error troubleshooting.",automatic screenshot
http://arxiv.org/abs/1801.09946v2,"Recent progress in genomics is bringing genetic testing to the masses.
Participatory public initiatives are underway to sequence the genome of
millions of volunteers, and a new market is booming with a number of companies
like 23andMe and AncestryDNA offering affordable tests directly to consumers.
Consequently, news, experiences, and views on genetic testing are increasingly
shared and discussed online and on social networks like Twitter. In this paper,
we present a large-scale analysis of Twitter discourse on genetic testing. We
collect 302K tweets from 113K users, posted over 2.5 years, by using thirteen
keywords related to genetic testing companies and public initiatives as search
keywords. We study both the tweets and the users posting them along several
axes, aiming to understand who tweets about genetic testing, what they talk
about, and how they use Twitter for that. Among other things, we find that
tweets about genetic testing originate from accounts that overall appear to be
interested in digital health and technology. Also, marketing efforts as well as
announcements, such as the FDA's suspension of 23andMe's health reports,
influence the type and the nature of user engagement.Finally, we report on
users who share screenshots of their results, and raise a few ethical and
societal questions as we find evidence of groups associating genetic testing to
racist ideologies.",automatic screenshot
http://arxiv.org/abs/1804.04605v1,"The number of Android smartphone and tablet users has experienced a rapid
growth in the past few years and it raises users' awareness on the privacy and
security of their mobile devices. The features of openness and extensibility
make Android unique, attractive and competitive but meanwhile vulnerable to
malicious attack. There are lots of users rooting their Android devices for
some useful functions, which are not originally provided to developers and
users, such as backup and taking screenshot. However, after observing the
danger of rooting devices, the developers begin to look for other non-root
alternatives to implement those functions. ADB workaround is one of the best
known non-root alternatives to help app gain higher privilege on Android. It
used to be considered as a secure practice until some cases of ADB privilege
leakage have been found. In this project, we design an approach and implement a
couple of tools to detect the privilege leakage in Android apps. We apply them
to analyse three real-world apps with millions of users, and successfully
identify three ADB privilege leaks from them. Moreover, we also conduct an
exploitation of the ADB privilege in one app, and therefore we prove the
existence of vulnerabilities in ADB workaround. Based on out study, we propose
some suggestion to help developers create their apps that could not only
satisfy users' needs but also protect users' privacy from similar attacks in
future.",automatic screenshot
http://arxiv.org/abs/1908.01351v1,"IT support services industry is going through a major transformation with AI
becoming commonplace. There has been a lot of effort in the direction of
automation at every human touchpoint in the IT support processes. Incident
management is one such process which has been a beacon process for AI based
automation. The vision is to automate the process from the time an
incident/ticket arrives till it is resolved and closed. While text is the
primary mode of communicating the incidents, there has been a growing trend of
using alternate modalities like image to communicate the problem. A large
fraction of IT support tickets today contain attached image data in the form of
screenshots, log messages, invoices and so on. These attachments help in better
explanation of the problem which aids in faster resolution. Anybody who aspires
to provide AI based IT support, it is essential to build systems which can
handle multi-modal content. In this paper we present how incident management in
IT support domain can be made much more effective using multi-modal analysis.
The information extracted from different modalities are correlated to enrich
the information in the ticket and used for better ticket routing and
resolution. We evaluate our system using about 25000 real tickets containing
attachments from selected problem areas. Our results demonstrate significant
improvements in both routing and resolution with the use of multi-modal ticket
analysis compared to only text based analysis.",automatic screenshot
http://arxiv.org/abs/1201.5070v1,"A tree automatic structure is a structure whose domain can be encoded by a
regular tree language such that each relation is recognisable by a finite
automaton processing tuples of trees synchronously. Words can be regarded as
specific simple trees and a structure is word automatic if it is encodable
using only these trees. The question naturally arises whether a given tree
automatic structure is already word automatic. We prove that this problem is
decidable for tree automatic scattered linear orderings. Moreover, we show that
in case of a positive answer a word automatic presentation is computable from
the tree automatic presentation.",automatic screenshot
http://arxiv.org/abs/1410.5197v1,"Recently, Schlicht and Stephan lifted the notion of automatic-structures to
the notion of (finite-word) ordinal-automatic structures. These are structures
whose domain and relations can be represented by automata reading finite words
whose shape is some fixed ordinal $\alpha$. We lift Delhomm\'e's
relative-growth-technique from the automatic and tree-automatic setting to the
ordinal-automatic setting. This result implies that the random graph is not
ordinal-automatic and infinite integral domains are not ordinal-automatic with
respect to ordinals below $\omega_1+\omega^\omega$ where $\omega_1$ is the
first uncountable ordinal.",automatic screenshot
http://arxiv.org/abs/1204.3048v1,"We generalise Delhomm\'e's result that each tree-automatic ordinal is
strictly below \omega^\omega^\omega{} by showing that any tree-automatic linear
ordering has FC-rank strictly below \omega^\omega. We further investigate a
restricted form of tree-automaticity and prove that every linear ordering which
admits a tree-automatic presentation of branching complexity at most k has
FC-rank strictly below \omega^k.",automatic screenshot
http://arxiv.org/abs/1901.02701v2,"A significant proportion of individuals' daily activities is experienced
through digital devices. Smartphones in particular have become one of the
preferred interfaces for content consumption and social interaction.
Identifying the content embedded in frequently-captured smartphone screenshots
is thus a crucial prerequisite to studies of media behavior and health
intervention planning that analyze activity interplay and content switching
over time. Screenshot images can depict heterogeneous contents and
applications, making the a priori definition of adequate taxonomies a
cumbersome task, even for humans. Privacy protection of the sensitive data
captured on screens means the costs associated with manual annotation are
large, as the effort cannot be crowd-sourced. Thus, there is need to examine
utility of unsupervised and semi-supervised methods for digital screenshot
classification. This work introduces the implications of applying clustering on
large screenshot sets when only a limited amount of labels is available. In
this paper we develop a framework for combining K-Means clustering with Active
Learning for efficient leveraging of labeled and unlabeled samples, with the
goal of discovering latent classes and describing a large collection of
screenshot data. We tested whether SVM-embedded or XGBoost-embedded solutions
for class probability propagation provide for more well-formed cluster
configurations. Visual and textual vector representations of the screenshot
images are derived and combined to assess the relative contribution of
multi-modal features to the overall performance.",screenshot
http://arxiv.org/abs/1705.07962v2,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",screenshot
http://arxiv.org/abs/cs/0504039v1,"This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later).",screenshot
http://arxiv.org/abs/1805.02763v2,"Crowdtesting is effective especially when it comes to the feedback on GUI
systems, or subjective opinions about features. Despite of this, we find
crowdtesting reports are highly replicated, i.e., 82% of them are replicates of
others. Hence automatically detecting replicate reports could help reduce
triaging efforts. Most of the existing approaches mainly adopted textual
information for replicate detection, and suffered from low accuracy because of
the expression gap. Our observation on real industrial crowdtesting data found
that when dealing with crowdtesting reports of GUI systems, the reports would
accompanied with images, i.e., the screenshots of the app. We assume the
screenshot to be valuable for replicate crowdtesting report detection because
it reflects the real scenario of the failure and is not affected by the variety
of natural languages.
  In this work, we propose a replicate detection approach, TSDetector, which
combines information from the screenshots and the textual descriptions to
detect replicate crowdtesting reports. We extract four types of features to
characterize the screenshots and the textual descriptions, and design an
algorithm to detect replicates based on four similarity scores derived from the
four different features respectively. We investigate the effectiveness and
advantage of TSDetector on 15 commercial projects with 4,172 reports from one
of the Chinese largest crowdtesting platforms.Results show that TSDetector can
outperform existing state-of-the-art approaches significantly. In addition, we
also evaluate its usefulness using real-world case studies. The feedback from
real-world testers demonstrates its practical value",screenshot
http://arxiv.org/abs/1908.06750v1,"Newly emerging variants of ransomware pose an ever-growing threat to computer
systems governing every aspect of modern life through the handling and analysis
of big data. While various recent security-based approaches have focused on
detecting and classifying ransomware at the network or system level,
easy-to-use post-infection ransomware classification for the lay user has not
been attempted before. In this paper, we investigate the possibility of
classifying the ransomware a system is infected with simply based on a
screenshot of the splash screen or the ransom note captured using a consumer
camera commonly found in any modern mobile device. To train and evaluate our
system, we create a sample dataset of the splash screens of 50 well-known
ransomware variants. In our dataset, only a single training image is available
per ransomware. Instead of creating a large training dataset of ransomware
screenshots, we simulate screenshot capture conditions via carefully designed
data augmentation techniques, enabling simple and efficient one-shot learning.
Moreover, using model uncertainty obtained via Bayesian approximation, we
ensure special input cases such as unrelated non-ransomware images and
previously-unseen ransomware variants are correctly identified for special
handling and not mis-classified. Extensive experimental evaluation demonstrates
the efficacy of our work, with accuracy levels of up to 93.6% for ransomware
classification.",screenshot
http://arxiv.org/abs/1503.03378v1,"Cross-browser compatibility testing is concerned with identifying perceptible
differences in the way a Web page is rendered across different browsers or
configurations thereof. Existing automated cross-browser compatibility testing
methods are generally based on Document Object Model (DOM) analysis, or in some
cases, a combination of DOM analysis with screenshot capture and image
processing. DOM analysis however may miss incompatibilities that arise not
during DOM construction, but rather during rendering. Conversely, DOM analysis
produces false alarms because different DOMs may lead to identical or
sufficiently similar renderings. This paper presents a novel method for
cross-browser testing based purely on image processing. The method relies on
image segmentation to extract regions from a Web page and computer vision
techniques to extract a set of characteristic features from each region.
Regions extracted from a screenshot taken on a baseline browser are compared
against regions extracted from the browser under test based on characteristic
features. A machine learning classifier is used to determine if differences
between two matched regions should be classified as an incompatibility. An
evaluation involving 140 pages shows that the proposed method achieves an
F-score exceeding 0.9, outperforming a state-of-the-art cross-browser testing
tool based on DOM analysis.",screenshot
http://arxiv.org/abs/1801.01316v1,"Daily engagement in life experiences is increasingly interwoven with mobile
device use. Screen capture at the scale of seconds is being used in behavioral
studies and to implement ""just-in-time"" health interventions. The increasing
psychological breadth of digital information will continue to make the actual
screens that people view a preferred if not required source of data about life
experiences. Effective and efficient Information Extraction and Retrieval from
digital screenshots is a crucial prerequisite to successful use of screen data.
In this paper, we present the experimental workflow we exploited to: (i)
pre-process a unique collection of screen captures, (ii) extract unstructured
text embedded in the images, (iii) organize image text and metadata based on a
structured schema, (iv) index the resulting document collection, and (v) allow
for Image Retrieval through a dedicated vertical search engine application. The
adopted procedure integrates different open source libraries for traditional
image processing, Optical Character Recognition (OCR), and Image Retrieval. Our
aim is to assess whether and how state-of-the-art methodologies can be applied
to this novel data set. We show how combining OpenCV-based pre-processing
modules with a Long short-term memory (LSTM) based release of Tesseract OCR,
without ad hoc training, led to a 74% character-level accuracy of the extracted
text. Further, we used the processed repository as baseline for a dedicated
Image Retrieval system, for the immediate use and application for behavioral
and prevention scientists. We discuss issues of Text Information Extraction and
Retrieval that are particular to the screenshot image case and suggest
important future work.",screenshot
http://arxiv.org/abs/0810.3609v1,"VISPA is a novel development environment for high energy physics analyses,
based on a combination of graphical and textual steering. The primary aim of
VISPA is to support physicists in prototyping, performing, and verifying a data
analysis of any complexity. We present example screenshots, and describe the
underlying software concepts.",screenshot
http://arxiv.org/abs/1710.08389v1,"This representative study of German search engine users (N=1,000) focuses on
the ability of users to distinguish between organic results and advertisements
on Google results pages. We combine questions about Google's business with
task-based studies in which users were asked to distinguish between ads and
organic results in screenshots of results pages. We find that only a small
percentage of users is able to reliably distinguish between ads and organic
results, and that user knowledge of Google's business model is very limited. We
conclude that ads are insufficiently labelled as such, and that many users may
click on ads assuming that they are selecting organic results.",screenshot
http://arxiv.org/abs/1908.02449v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
the main resources to develop these tools are datasets, which are introduced
and provided by the present paper, for the specific cases of visual correlation
of phishing and onion websites. CIRCL's Open-Source tools are the sources of
these screenshots, which had been manually verified against personal
information leaks. Usage examples of these datasets are proposed in the current
paper. These researches directions are, however, not the main contribution of
the paper. The main contribution is the availability of the two datasets.",screenshot
http://arxiv.org/abs/1908.04014v1,"Security analysts need to classify, search and correlate numerous images.
Automatic classification tools improve the efficiency of such tasks. However,
no open-source and turnkey library was found able to reach this goal. The
present paper introduces an Open-Source modular library for the specific cases
of visual correlation and Image Matching named Douglas-Quaid. The design of the
library, chosen tradeoffs, encountered challenges, envisioned solutions as well
as quality and speed results are presented in this paper. We also explore
researches directions and future potential developments of the library. Our
claim is that even partial automation of screenshots classification would
reduce the burden on security teams and that Douglas-Quaid is a step forward in
this direction.",screenshot
http://arxiv.org/abs/1905.07767v1,"Phishing, a continuously growing cyber threat, aims to obtain innocent users'
credentials by deceiving them via presenting fake web pages which mimic their
legitimate targets. To date, various attempts have been carried out in order to
detect phishing pages. In this study, we treat the problem of phishing web page
identification as an image classification task and propose a machine learning
augmented pure vision based approach which extracts and classifies compact
visual features from web page screenshots. For this purpose, we employed
several MPEG7 and MPEG7-like compact visual descriptors (SCD, CLD, CEDD, FCTH
and JCD) to reveal color and edge based discriminative visual cues. Throughout
the feature extraction process we have followed two different schemes working
on either whole screenshots in a ""holistic"" manner or equal sized ""patches""
constructing a coarse-to-fine ""pyramidal"" representation. Moreover, for the
task of image classification, we have built SVM and Random Forest based machine
learning models. In order to assess the performance and generalization
capability of the proposed approach, we have collected a mid-sized corpus
covering 14 distinct brands and involving 2852 samples. According to the
conducted experiments, our approach reaches up to 90.5% F1 score via SCD. As a
result, compared to other studies, the suggested approach presents a
lightweight schema serving competitive accuracy and superior feature extraction
and inferring speed that enables it to be used as a browser plugin.",screenshot
http://arxiv.org/abs/0908.3022v1,"This paper outlines an approach to manage and quantify the risks associated
with changes made to spreadsheets. The methodology focuses on structural
differences between spreadsheets and suggests a technique by which a risk
analysis can be achieved in an automated environment. The paper offers an
example that demonstrates how contiguous ranges of data can be mapped into a
generic list of formulae, data and metadata. The example then shows that
comparison of these generic lists can establish the structural differences
between spreadsheets and quantify the level of risk that each change has
introduced. Lastly the benefits, drawbacks and limitations of the technique are
discussed in a commercial context.",screenshot
http://arxiv.org/abs/1005.2072v1,"We review a case study of a UI design project for a complete travel search
engine system prototype for regular and corporate users. We discuss various
usage scenarios, guidelines, and so for, and put them into a web-based
prototype with screenshots and the like. We combined into our prototype the
best features found at the time (2002) on most travel-like sites and added more
to them as a part of our research. We conducted feasibility studies, review
common design guidelines and Nelson's heuristics while constructing this work.
The prototype is itself open-source, but has no backend functionality, as the
focus is the user-centered design of such a system. While the prototype is
mostly static, some dynamic activity is present through the use of PHP.",screenshot
http://arxiv.org/abs/1408.5265v2,"An ensemble inference mechanism is proposed on the Angry Birds domain. It is
based on an efficient tree structure for encoding and representing game
screenshots, where it exploits its enhanced modeling capability. This has the
advantage to establish an informative feature space and modify the task of game
playing to a regression analysis problem. To this direction, we assume that
each type of object material and bird pair has its own Bayesian linear
regression model. In this way, a multi-model regression framework is designed
that simultaneously calculates the conditional expectations of several objects
and makes a target decision through an ensemble of regression models. Learning
procedure is performed according to an online estimation strategy for the model
parameters. We provide comparative experimental results on several game levels
that empirically illustrate the efficiency of the proposed methodology.",screenshot
http://arxiv.org/abs/1707.03908v1,"Game maps are useful for human players, general-game-playing agents, and
data-driven procedural content generation. These maps are generally made by
hand-assembling manually-created screenshots of game levels. Besides being
tedious and error-prone, this approach requires additional effort for each new
game and level to be mapped. The results can still be hard for humans or
computational systems to make use of, privileging visual appearance over
semantic information. We describe a software system, Mappy, that produces a
good approximation of a linked map of rooms given a Nintendo Entertainment
System game program and a sequence of button inputs exploring its world. In
addition to visual maps, Mappy outputs grids of tiles (and how they change over
time), positions of non-tile objects, clusters of similar rooms that might in
fact be the same room, and a set of links between these rooms. We believe this
is a necessary step towards developing larger corpora of high-quality
semantically-annotated maps for PCG via machine learning and other
applications.",screenshot
http://arxiv.org/abs/1801.06428v1,"Unique challenges arise when testing mobile applications due to their
prevailing event-driven nature and complex contextual features (e.g. sensors,
notifications). Current automated input generation approaches for Android apps
are typically not practical for developers to use due to required
instrumentation or platform dependence and generally do not effectively
exercise contextual features. To better support developers in mobile testing
tasks, in this demo we present a novel, automated tool called CrashScope. This
tool explores a given Android app using systematic input generation, according
to several strategies informed by static and dynamic analyses, with the
intrinsic goal of triggering crashes. When a crash is detected, CrashScope
generates an augmented crash report containing screenshots, detailed crash
reproduction steps, the captured exception stack trace, and a fully replayable
script that automatically reproduces the crash on a target device(s). Results
of preliminary studies show that CrashScope is able to uncover about as many
crashes as other state of the art tools, while providing detailed useful crash
reports and test scripts to developers. Website:
www.crashscope-android.com/crashscope-home Video url:
https://youtu.be/ii6S1JF6xDw",screenshot
http://arxiv.org/abs/1802.01628v1,"Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are ""transparent"". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.",screenshot
http://arxiv.org/abs/1807.04191v1,"UI design languages, such as Google's Material Design, make applications both
easier to develop and easier to learn by providing a set of standard UI
components. Nonetheless, it is hard to assess the impact of design languages in
the wild. Moreover, designers often get stranded by strong-opinionated debates
around the merit of certain UI components, such as the Floating Action Button
and the Navigation Drawer. To address these challenges, this short paper
introduces a method for measuring the impact of design languages and informing
design debates through analyzing a dataset consisting of view hierarchies,
screenshots, and app metadata for more than 9,000 mobile apps. Our data
analysis shows that use of Material Design is positively correlated to app
ratings, and to some extent, also the number of installs. Furthermore, we show
that use of UI components vary by app category, suggesting a more nuanced view
needed in design debates.",screenshot
http://arxiv.org/abs/1810.11536v1,"Recent progress on deep learning has made it possible to automatically
transform the screenshot of Graphic User Interface (GUI) into code by using the
encoder-decoder framework. While the commonly adopted image encoder (e.g., CNN
network), might be capable of extracting image features to the desired level,
interpreting these abstract image features into hundreds of tokens of code puts
a particular challenge on the decoding power of the RNN-based code generator.
Considering the code used for describing GUI is usually hierarchically
structured, we propose a new attention-based hierarchical code generation
model, which can describe GUI images in a finer level of details, while also
being able to generate hierarchically structured code in consistency with the
hierarchical layout of the graphic elements in the GUI. Our model follows the
encoder-decoder framework, all the components of which can be trained jointly
in an end-to-end manner. The experimental results show that our method
outperforms other current state-of-the-art methods on both a publicly available
GUI-code dataset as well as a dataset established by our own.",screenshot
http://arxiv.org/abs/1611.03906v2,"Non-programming users should be able to create their own customized scripts
to perform computer-based tasks for them, just by demonstrating to the machine
how it's done. To that end, we develop a system prototype which
learns-by-demonstration called HILC (Help, It Looks Confusing). Users train
HILC to synthesize a task script by demonstrating the task, which produces the
needed screenshots and their corresponding mouse-keyboard signals. After the
demonstration, the user answers follow-up questions.
  We propose a user-in-the-loop framework that learns to generate scripts of
actions performed on visible elements of graphical applications. While pure
programming-by-demonstration is still unrealistic, we use quantitative and
qualitative experiments to show that non-programming users are willing and
effective at answering follow-up queries posed by our system. Our models of
events and appearance are surprisingly simple, but are combined effectively to
cope with varying amounts of supervision.
  The best available baseline, Sikuli Slides, struggled with the majority of
the tests in our user study experiments. The prototype with our proposed
approach successfully helped users accomplish simple linear tasks, complicated
tasks (monitoring, looping, and mixed), and tasks that span across multiple
executables. Even when both systems could ultimately perform a task, ours was
trained and refined by the user in less time.",screenshot
http://arxiv.org/abs/1703.02227v1,"In the Google Play store, an introduction page is associated with every
mobile application (app) for users to acquire its details, including
screenshots, description, reviews, etc. However, it remains a challenge to
identify what items influence users most when downloading an app. To explore
users' perspective, we conduct a survey to inquire about this question. The
results of survey suggest that the participants pay most attention to the app
description which gives users a quick overview of the app. Although there exist
some guidelines about how to write a good app description to attract more
downloads, it is hard to define a high quality app description. Meanwhile,
there is no tool to evaluate the quality of app description. In this paper, we
employ the method of crowdsourcing to extract the attributes that affect the
app descriptions' quality. First, we download some app descriptions from Google
Play, then invite some participants to rate their quality with the score from
one (very poor) to five (very good). The participants are also requested to
explain every score's reasons. By analyzing the reasons, we extract the
attributes that the participants consider important during evaluating the
quality of app descriptions. Finally, we train the supervised learning models
on a sample of 100 app descriptions. In our experiments, the support vector
machine model obtains up to 62% accuracy. In addition, we find that the
permission, the number of paragraphs and the average number of words in one
feature play key roles in defining a good app description.",screenshot
http://arxiv.org/abs/1706.01130v1,"Mobile developers face unique challenges when detecting and reporting crashes
in apps due to their prevailing GUI event-driven nature and additional sources
of inputs (e.g., sensor readings). To support developers in these tasks, we
introduce a novel, automated approach called CRASHSCOPE. This tool explores a
given Android app using systematic input generation, according to several
strategies informed by static and dynamic analyses, with the intrinsic goal of
triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented
crash report containing screenshots, detailed crash reproduction steps, the
captured exception stack trace, and a fully replayable script that
automatically reproduces the crash on a target device(s). We evaluated
CRASHSCOPE's effectiveness in discovering crashes as compared to five
state-of-the-art Android input generation tools on 61 applications. The results
demonstrate that CRASHSCOPE performs about as well as current tools for
detecting crashes and provides more detailed fault information. Additionally,
in a study analyzing eight real-world Android app crashes, we found that
CRASHSCOPE's reports are easily readable and allow for reliable reproduction of
crashes by presenting more explicit information than human written reports.",screenshot
http://arxiv.org/abs/1711.04030v1,"Effective machine-aided diagnosis and repair of configuration errors
continues to elude computer systems designers. Most of the literature targets
errors that can be attributed to a single erroneous configuration setting.
However, a recent study found that a significant amount of configuration errors
require fixing more than one setting together. To address this limitation,
Ocasta statistically clusters dependent configuration settings based on the
application's accesses to its configuration settings and utilizes the extracted
clustering of configuration settings to fix configuration errors involving more
than one configuration settings. Ocasta treats applications as black-boxes and
only relies on the ability to observe application accesses to their
configuration settings.
  We collected traces of real application usage from 24 Linux and 5 Windows
desktops computers and found that Ocasta is able to correctly identify clusters
with 88.6% accuracy. To demonstrate the effectiveness of Ocasta, we evaluated
it on 16 real-world configuration errors of 11 Linux and Windows applications.
Ocasta is able to successfully repair all evaluated configuration errors in 11
minutes on average and only requires the user to examine an average of 3
screenshots of the output of the application to confirm that the error is
repaired. A user study we conducted shows that Ocasta is easy to use by both
expert and non-expert users and is more efficient than manual configuration
error troubleshooting.",screenshot
http://arxiv.org/abs/1801.09946v2,"Recent progress in genomics is bringing genetic testing to the masses.
Participatory public initiatives are underway to sequence the genome of
millions of volunteers, and a new market is booming with a number of companies
like 23andMe and AncestryDNA offering affordable tests directly to consumers.
Consequently, news, experiences, and views on genetic testing are increasingly
shared and discussed online and on social networks like Twitter. In this paper,
we present a large-scale analysis of Twitter discourse on genetic testing. We
collect 302K tweets from 113K users, posted over 2.5 years, by using thirteen
keywords related to genetic testing companies and public initiatives as search
keywords. We study both the tweets and the users posting them along several
axes, aiming to understand who tweets about genetic testing, what they talk
about, and how they use Twitter for that. Among other things, we find that
tweets about genetic testing originate from accounts that overall appear to be
interested in digital health and technology. Also, marketing efforts as well as
announcements, such as the FDA's suspension of 23andMe's health reports,
influence the type and the nature of user engagement.Finally, we report on
users who share screenshots of their results, and raise a few ethical and
societal questions as we find evidence of groups associating genetic testing to
racist ideologies.",screenshot
http://arxiv.org/abs/1804.04605v1,"The number of Android smartphone and tablet users has experienced a rapid
growth in the past few years and it raises users' awareness on the privacy and
security of their mobile devices. The features of openness and extensibility
make Android unique, attractive and competitive but meanwhile vulnerable to
malicious attack. There are lots of users rooting their Android devices for
some useful functions, which are not originally provided to developers and
users, such as backup and taking screenshot. However, after observing the
danger of rooting devices, the developers begin to look for other non-root
alternatives to implement those functions. ADB workaround is one of the best
known non-root alternatives to help app gain higher privilege on Android. It
used to be considered as a secure practice until some cases of ADB privilege
leakage have been found. In this project, we design an approach and implement a
couple of tools to detect the privilege leakage in Android apps. We apply them
to analyse three real-world apps with millions of users, and successfully
identify three ADB privilege leaks from them. Moreover, we also conduct an
exploitation of the ADB privilege in one app, and therefore we prove the
existence of vulnerabilities in ADB workaround. Based on out study, we propose
some suggestion to help developers create their apps that could not only
satisfy users' needs but also protect users' privacy from similar attacks in
future.",screenshot
http://arxiv.org/abs/1908.01351v1,"IT support services industry is going through a major transformation with AI
becoming commonplace. There has been a lot of effort in the direction of
automation at every human touchpoint in the IT support processes. Incident
management is one such process which has been a beacon process for AI based
automation. The vision is to automate the process from the time an
incident/ticket arrives till it is resolved and closed. While text is the
primary mode of communicating the incidents, there has been a growing trend of
using alternate modalities like image to communicate the problem. A large
fraction of IT support tickets today contain attached image data in the form of
screenshots, log messages, invoices and so on. These attachments help in better
explanation of the problem which aids in faster resolution. Anybody who aspires
to provide AI based IT support, it is essential to build systems which can
handle multi-modal content. In this paper we present how incident management in
IT support domain can be made much more effective using multi-modal analysis.
The information extracted from different modalities are correlated to enrich
the information in the ticket and used for better ticket routing and
resolution. We evaluate our system using about 25000 real tickets containing
attachments from selected problem areas. Our results demonstrate significant
improvements in both routing and resolution with the use of multi-modal ticket
analysis compared to only text based analysis.",screenshot
http://arxiv.org/abs/1407.2717v1,"Many computers and devices are becoming more connected to the internet in
recent years; the use of the Internet Protocol (IP) has made the connectivity
and identification of these devices possible in large scale. In this paper, we
will discuss the evolution of Internet Protocol version 4 (IPv4), its features,
issues and limitations and how Internet Protocol version 6 (IPv6) tends to
solve some of these issues including the differences and transition between
these two protocols.",internet versions
http://arxiv.org/abs/cs/0309054v1,"Denial of Service (DoS) attacks are one of the most challenging threats to
Internet security. An attacker typically compromises a large number of
vulnerable hosts and uses them to flood the victim's site with malicious
traffic, clogging its tail circuit and interfering with normal traffic. At
present, the network operator of a site under attack has no other resolution
but to respond manually by inserting filters in the appropriate edge routers to
drop attack traffic. However, as DoS attacks become increasingly sophisticated,
manual filter propagation becomes unacceptably slow or even infeasible.
  In this paper, we present Active Internet Traffic Filtering, a new automatic
filter propagation protocol. We argue that this system provides a guaranteed,
significant level of protection against DoS attacks in exchange for a
reasonable, bounded amount of router resources. We also argue that the proposed
system cannot be abused by a malicious node to interfere with normal Internet
operation. Finally, we argue that it retains its efficiency in the face of
continued Internet growth.",internet versions
http://arxiv.org/abs/1902.10910v1,"Internet of Things (IoT) and Network Softwarization are fast becoming core
technologies of information systems and network management for next generation
Internet. The deployment and applications of IoT ranges from smart cities to
urban computing, and from ubiquitous healthcare to tactile Internet. For this
reason the physical infrastructure of heterogeneous network systems has become
more complicated, and thus requires efficient and dynamic solutions for
management, configuration, and flow scheduling. Network softwarization in the
form of Software Defined Networks (SDN) and Network Function Virtualization
(NFV) has been extensively researched for IoT in recent past. In this article
we present a systematic and comprehensive review of virtualization techniques
explicitly designed for IoT networks. We have classified the literature into
software defined networks designed for IoT, function virtualization for IoT
networks, and software defined IoT networks. These categories are further
divided into works which present architectural, security, and management
solutions. In addition, the paper highlights a number of short term and long
term research challenges and open issues related to adoption of software
defined Internet of things.",internet versions
http://arxiv.org/abs/1412.5052v2,"The vulnerability of the Internet has been demonstrated by prominent IP
prefix hijacking events. Major outages such as the China Telecom incident in
2010 stimulate speculations about malicious intentions behind such anomalies.
Surprisingly, almost all discussions in the current literature assume that
hijacking incidents are enabled by the lack of security mechanisms in the
inter-domain routing protocol BGP. In this paper, we discuss an attacker model
that accounts for the hijacking of network ownership information stored in
Regional Internet Registry (RIR) databases. We show that such threats emerge
from abandoned Internet resources (e.g., IP address blocks, AS numbers). When
DNS names expire, attackers gain the opportunity to take resource ownership by
re-registering domain names that are referenced by corresponding RIR database
objects. We argue that this kind of attack is more attractive than conventional
hijacking, since the attacker can act in full anonymity on behalf of a victim.
Despite corresponding incidents have been observed in the past, current
detection techniques are not qualified to deal with these attacks. We show that
they are feasible with very little effort, and analyze the risk potential of
abandoned Internet resources for the European service region: our findings
reveal that currently 73 /24 IP prefixes and 7 ASes are vulnerable to be
stealthily abused. We discuss countermeasures and outline research directions
towards preventive solutions.",internet versions
http://arxiv.org/abs/1112.5760v1,"The paper analyses current versions of top three used Internet browsers and
compare their security levels to a research done in 2006. The security is
measured by analyzing how user data is stored. Data recorded during different
browsing sessions and by different password management functions it is
considered sensitive data. The paper describes how the browser protects the
sensitive data and how an attacker or a forensic analyst can access it.",internet versions
http://arxiv.org/abs/0708.2309v1,"While there exist compact routing schemes designed for grids, trees, and
Internet-like topologies that offer routing tables of sizes that scale
logarithmically with the network size, we demonstrate in this paper that in
view of recent results in compact routing research, such logarithmic scaling on
Internet-like topologies is fundamentally impossible in the presence of
topology dynamics or topology-independent (flat) addressing. We use analytic
arguments to show that the number of routing control messages per topology
change cannot scale better than linearly on Internet-like topologies. We also
employ simulations to confirm that logarithmic routing table size scaling gets
broken by topology-independent addressing, a cornerstone of popular
locator-identifier split proposals aiming at improving routing scaling in the
presence of network topology dynamics or host mobility. These pessimistic
findings lead us to the conclusion that a fundamental re-examination of
assumptions behind routing models and abstractions is needed in order to find a
routing architecture that would be able to scale ``indefinitely.''",internet versions
http://arxiv.org/abs/1307.1650v1,"In this work, using a game-theoretic approach, cost-sensitive mechanisms that
lead to reliable Internet-based computing are designed. In particular, we
consider Internet-based master-worker computations, where a master processor
assigns, across the Internet, a computational task to a set of potentially
untrusted worker processors and collects their responses. Workers may collude
in order to increase their benefit. Several game-theoretic models that capture
the nature of the problem are analyzed, and algorithmic mechanisms that, for
each given set of cost and system parameters, achieve high reliability are
designed. Additionally, two specific realistic system scenarios are studied.
These scenarios are a system of volunteer computing like SETI, and a company
that buys computing cycles from Internet computers and sells them to its
customers in the form of a task- computation service. Notably, under certain
conditions, non redundant allocation yields the best trade-off between cost and
reliability.",internet versions
http://arxiv.org/abs/1807.06724v1,"Internet of Things (IoT), also referred to as the Internet of Objects, is
envisioned as a holistic and transformative approach for providing numerous
services. The rapid development of various communication protocols and
miniaturization of transceivers along with recent advances in sensing
technologies offer the opportunity to transform isolated devices into
communicating smart things. Smart things, that can sense, store, and even
process electrical, thermal, optical, chemical, and other signals to extract
user-/environment-related information, have enabled services only limited by
human imagination.
  Despite picturesque promises of IoT-enabled systems, the integration of smart
things into the standard Internet introduces several security challenges
because the majority of Internet technologies, communication protocols, and
sensors were not designed to support IoT. Several recent research studies have
demonstrated that launching security/privacy attacks against IoT-enabled
systems, in particular wearable medical sensor (WMS)-based systems, may lead to
catastrophic situations and life-threatening conditions. Therefore, security
threats and privacy concerns in the IoT domain need to be proactively studied
and aggressively addressed. In this thesis, we tackle several domain-specific
security/privacy challenges associated with IoT-enabled systems.",internet versions
http://arxiv.org/abs/1809.07836v1,"Internet Protocol (IP) is the narrow waist of multilayered Internet protocol
stack which defines the rules for data sent across networks. IPv4 is the fourth
version of IP and first commercially available for deployment set by ARPANET in
1983 which is a 32 bit long address and can support up to 232 devices. In April
2017, all Regional Internet Registries (RIRs) confirmed that IPv4 addresses are
exhausted and cannot be allocated anymore implying any new organization
requesting a block of Internet addresses will be allocated IPv6. This creates
troubles of interoperability, migration and deployment, and therefore
organizations hesitated to use IPv6 borrowing IPv4 addresses from other big
organizations instead. Currently, when IPv4 is not available, and IPv6 is not
adopted for around 20 years, the question arises whether IPv6 will still be
accepted by the computer society or will it have an end of life soon with
alternate better protocol such as ID based networks taking its place. This
paper claims that IPv6 has lost its deployment window and can be safely skipped
when new ID based protocols are available which not only have simple
interoperability, deployment and migration guidelines but also provide advanced
features as compared to IPv6. The paper provides answers to these questions
with a comprehensive comparison of IPv6 with its available alternatives and
reasons of IPv6 failures in its adoption. Finally, the paper declares IPv6 as a
dead protocol and suggests to use newer available protocols in future.",internet versions
http://arxiv.org/abs/0908.2721v2,"Internet performance is tightly related to the properties of TCP and UDP
protocols, jointly responsible for the delivery of the great majority of
Internet traffic. It is well understood how these protocols behave under FIFO
queuing and what the network congestion effects. However, no comprehensive
analysis is available when flow-aware mechanisms such as per-flow scheduling
and dropping policies are deployed. Previous simulation and experimental
results leave a number of unanswered questions. In the paper, we tackle this
issue by modeling via a set of fluid non-linear ODEs the instantaneous
throughput and the buffer occupancy of N long-lived TCP sources under three
per-flow scheduling disciplines (Fair Queuing, Longest Queue First, Shortest
Queue First) and with longest queue drop buffer management. We study the system
evolution and analytically characterize the stationary regime: closed-form
expressions are derived for the stationary throughput/sending rate and buffer
occupancy which give thorough understanding of short/long-term fairness for TCP
traffic. Similarly, we provide the characterization of the loss rate
experienced by UDP flows in presence of TCP traffic.",internet versions
http://arxiv.org/abs/cs/0109009v1,"Our goal is to distinguish between the following two hypotheses: (A) The
Internet will remain disproportionately in English and will, over time, cause
more people to learn English as second language and thus solidify the role of
English as a global language. This outcome will prevail even though there are
more native Chinese and Spanish speakers than there are native English
speakers. (B) As the Internet matures, it will more accurately reflect the
native languages spoken around the world (perhaps weighted by purchasing power)
and will not promote English as a global language.
  English's ""early lead"" on the web is more likely to persist if those who are
not native English speakers frequently access the large number of English
language web sites that are currently available. In that case, many existing
web sites will have little incentive to develop non-English versions of their
sites, and new sites will tend to gravitate towards English. The key empirical
question, therefore, is whether individuals whose native language is not
English use the Web, or certain types of Web sites, less than do native English
speakers. In order to examine this issue empirically, we employ a unique data
set on Internet use at the individual level in Canada from Media Metrix. Canada
provides an ideal setting to examine this issue because English is one of the
two official languages.
  Our preliminary results suggest that English web sites are not a barrier to
Internet use for French-speaking Quebecois. These preliminary results are
consistent with the scenario in which the Internet will promote English as a
global language.",internet versions
http://arxiv.org/abs/cs/0510007v1,"Internet mapping projects generally consist in sampling the network from a
limited set of sources by using traceroute probes. This methodology, akin to
the merging of spanning trees from the different sources to a set of
destinations, leads necessarily to a partial, incomplete map of the Internet.
Accordingly, determination of Internet topology characteristics from such
sampled maps is in part a problem of statistical inference. Our contribution
begins with the observation that the inference of many of the most basic
topological quantities -- including network size and degree characteristics --
from traceroute measurements is in fact a version of the so-called `species
problem' in statistics. This observation has important implications, since
species problems are often quite challenging. We focus here on the most
fundamental example of a traceroute internet species: the number of nodes in a
network. Specifically, we characterize the difficulty of estimating this
quantity through a set of analytical arguments, we use statistical subsampling
principles to derive two proposed estimators, and we illustrate the performance
of these estimators on networks with various topological characteristics.",internet versions
http://arxiv.org/abs/0710.5006v1,"The fragmented nature and asymmetry of local and remote file access and
network access, combined with the current lack of robust authenticity and
privacy, hamstrings the current internet. The collection of disjoint and often
ad-hoc technologies currently in use are at least partially responsible for the
magnitude and potency of the plagues besetting the information economy, of
which spam and email borne virii are canonical examples. The proposed
replacement for the internet, Internet Protocol Version 6 (IPv6), does little
to tackle these underlying issues, instead concentrating on addressing the
technical issues of a decade ago.
  This paper introduces CANE, a Content Addressed Network Environment, and
compares it against current internet and related technologies. Specifically,
CANE presents a simple computing environment in which location is abstracted
away in favour of identity, and trust is explicitly defined. Identity is
cryptographically verified and yet remains pervasively open in nature. It is
argued that this approach is capable of being generalised such that file
storage and network access can be unified and subsequently combined with human
interfaces to result in a Unified Theory of Access, which addresses many of the
significant problems besetting the internet community of the early 21st
century.",internet versions
http://arxiv.org/abs/1312.5739v1,"We describe a method for remotely detecting intentional packet drops on the
Internet via side channel inferences. That is, given two arbitrary IP addresses
on the Internet that meet some simple requirements, our proposed technique can
discover packet drops (e.g., due to censorship) between the two remote
machines, as well as infer in which direction the packet drops are occurring.
The only major requirements for our approach are a client with a global IP
Identifier (IPID) and a target server with an open port. We require no special
access to the client or server. Our method is robust to noise because we apply
intervention analysis based on an autoregressive-moving-average (ARMA) model.
In a measurement study using our method featuring clients from multiple
continents, we observed that, of all measured client connections to Tor
directory servers that were censored, 98% of those were from China, and only
0.63% of measured client connections from China to Tor directory servers were
not censored. This is congruent with current understandings about global
Internet censorship, leading us to conclude that our method is effective.",internet versions
http://arxiv.org/abs/1811.03353v2,"The next generation of networks must support billions of connected devices in
the Internet-of-Things (IoT). To support IoT applications, sources sense and
send their measurement updates over the Internet to a monitor (control station)
for real-time monitoring and actuation. Ideally, these updates would be
delivered at a high rate, only constrained by the sensing rate supported by the
sources. However, given network constraints, such a rate may lead to delays in
delivery of updates at the monitor that make the freshest update at the monitor
unacceptably old for the application.
  We propose a novel transport layer protocol, namely the Age Control Protocol
(ACP), that enables timely delivery of such updates to monitors, in a
network-transparent manner. ACP allows the source to adapt its rate of updates
to dynamic network conditions such that the average age of the sensed
information at the monitor is minimized. We detail the protocol and the
proposed control algorithm. We demonstrate its efficacy using extensive
simulations and real-world experiments, which have a source send its updates
over the Internet to a monitor on another continent.",internet versions
http://arxiv.org/abs/1305.0245v1,"In this paper, we demonstrate that it is possible to automatically generate
fingerprints for various web server types using multifactor Bayesian inference
on randomly selected servers on the Internet, without building an a priori
catalog of server features or behaviors. This makes it possible to conclusively
study web server distribution without relying on reported (and variable)
version strings. We gather data by sending a collection of specialized requests
to 110,000 live web servers. Using only the server response codes, we then
train an algorithm to successfully predict server types independently of the
server version string. In the process, we note several distinguishing features
of current web infrastructure.",internet versions
http://arxiv.org/abs/cs/0512095v1,"We calculate an extensive set of characteristics for Internet AS topologies
extracted from the three data sources most frequently used by the research
community: traceroutes, BGP, and WHOIS. We discover that traceroute and BGP
topologies are similar to one another but differ substantially from the WHOIS
topology. Among the widely considered metrics, we find that the joint degree
distribution appears to fundamentally characterize Internet AS topologies as
well as narrowly define values for other important metrics. We discuss the
interplay between the specifics of the three data collection mechanisms and the
resulting topology views. In particular, we show how the data collection
peculiarities explain differences in the resulting joint degree distributions
of the respective topologies. Finally, we release to the community the input
topology datasets, along with the scripts and output of our calculations. This
supplement should enable researchers to validate their models against real data
and to make more informed selection of topology data sources for their specific
needs.",internet versions
http://arxiv.org/abs/1805.02751v2,"This paper investigates the security and privacy of Internet-connected
children's smart toys through case studies of three commercially-available
products. We conduct network and application vulnerability analyses of each toy
using static and dynamic analysis techniques, including application binary
decompilation and network monitoring. We discover several publicly undisclosed
vulnerabilities that violate the Children's Online Privacy Protection Rule
(COPPA) as well as the toys' individual privacy policies. These
vulnerabilities, especially security flaws in network communications with
first-party servers, are indicative of a disconnect between many IoT toy
developers and security and privacy best practices despite increased attention
to Internet-connected toy hacking risks.",internet versions
http://arxiv.org/abs/1812.09404v1,"We study a class of distributed optimization problems for multiple shared
resource allocation in Internet-connected devices. We propose a derandomized
version of an existing stochastic additive-increase and multiplicative-decrease
(AIMD) algorithm. The proposed solution uses one bit feedback signal for each
resource between the system and the Internet-connected devices and does not
require inter-device communication. Additionally, the Internet-connected
devices do not compromise their privacy and the solution does not dependent on
the number of participating devices. In the system, each Internet-connected
device has private cost functions which are strictly convex, twice continuously
differentiable and increasing. We show empirically that the long-term average
allocations of multiple shared resources converge to optimal allocations and
the system achieves minimum social cost. Furthermore, we show that the proposed
derandomized AIMD algorithm converges faster than the stochastic AIMD algorithm
and both the approaches provide approximately same solutions.",internet versions
http://arxiv.org/abs/0705.0817v1,"This document describes the QSPN, the routing discovery algorithm used by
Netsukuku. Through a deductive analysis the main proprieties of the QSPN are
shown. Moreover, a second version of the algorithm, is presented.",internet versions
http://arxiv.org/abs/1707.04835v1,"Process migration involves moving the running state of a process from one
physical system to another, as is commonly done for virtual machines. In this
paper, we describe how Content Centric Networking (CCNx) facilitates process
migration through an intuitive naming ontology and version checkpointing.",internet versions
http://arxiv.org/abs/1708.07160v1,"The world had witnessed several generations of the Internet. Starting with
the Fixed Internet, then the Mobile Internet, scientists now focus on many
types of research related to the ""Thing"" Internet (or Internet of Things). The
question is ""what is the next Internet generation after the Thing Internet?""
This paper envisions about the Tactile Internet which could be the next
Internet generation in the near future. The paper will introduce what is the
tactile internet, why it could be the next future Internet, as well as the
impact and its application in the future society. Furthermore, some challenges
and the requirements are presented to guide further research in this near
future field.",internet versions
http://arxiv.org/abs/1806.08246v2,"The multimedia content in the World Wide Web is rapidly growing and contains
valuable information for many applications in different domains. For this
reason, the Internet Archive initiative has been gathering billions of
time-versioned web pages since the mid-nineties. However, the huge amount of
data is rarely labeled with appropriate metadata and automatic approaches are
required to enable semantic search. Normally, the textual content of the
Internet Archive is used to extract entities and their possible relations
across domains such as politics and entertainment, whereas image and video
content is usually neglected. In this paper, we introduce a system for person
recognition in image content of web news stored in the Internet Archive. Thus,
the system complements entity recognition in text and allows researchers and
analysts to track media coverage and relations of persons more precisely. Based
on a deep learning face recognition approach, we suggest a system that
automatically detects persons of interest and gathers sample material, which is
subsequently used to identify them in the image data of the Internet Archive.
We evaluate the performance of the face recognition system on an appropriate
standard benchmark dataset and demonstrate the feasibility of the approach with
two use cases.",internet versions
http://arxiv.org/abs/1904.07960v1,"This document describes the framework of the Softwire ''Hub and Spoke''
solution with the Layer Two Tunneling Protocol version 2 (L2TPv2). The
implementation details specified in this document should be followed to achieve
interoperability among different vendor implementations.",internet versions
http://arxiv.org/abs/1005.4505v1,"Mobile Ad hoc NETworks (MANETs) are leaving the confines of research
laboratories, to find place in real-world deployments. Outside specialized
domains (military, vehicular, etc.), city-wide communitynetworks are emerging,
connecting regular Internet users with each other, and with the Internet, via
MANETs. Growing to encompass more than a handful of ""trusted participants"", the
question of preserving the MANET network connectivity, even when faced with
careless or malicious participants, arises, and must be addressed. A first step
towards protecting a MANET is to analyze the vulnerabilities of the routing
protocol, managing the connectivity. By understanding how the algorithms of the
routing protocol operate, and how these can be exploited by those with ill
intent, countermeasures can be developed, readying MANETs for wider deployment
and use. This paper takes an abstract look at the algorithms that constitute
the Optimized Link State Routing Protocol version 2 (OLSRv2), and identifies
for each protocol element the possible vulnerabilities and attacks -- in a
certain way, provides a ""cookbook"" for how to best attack an operational OLSRv2
network, or for how to proceed with developing protective countermeasures
against these attacks.",internet versions
http://arxiv.org/abs/1303.6841v1,"This paper criticises the notion that long-range dependence is an important
contributor to the queuing behaviour of real Internet traffic. The idea is
questioned in two different ways. Firstly, a class of models used to simulate
Internet traffic is shown to have important theoretical flaws. It is shown that
this behaviour is inconsistent with the behaviour of real traffic traces.
Secondly, the notion that long-range correlations significantly affects the
queuing performance of traffic is investigated by destroying those correlations
in real traffic traces (by reordering). It is shown that the longer ranges of
correlations are not important except in one case with an extremely high load.",internet versions
http://arxiv.org/abs/physics/0505026v1,"Significant obstacles prevent large, university-level, introductory physics
courses from effectively teaching problem-solving skills. We describe our
program for integrating three internet-based ""teaching-while-quizzing"" tools to
address two of these barriers: students' poor math skills and instructors'
insufficient grading recourses. We outline our system of math remediation,
homework and after-homework quizzes, and mini-practice exams, and demonstrate
how it can be incorporated into courses with modest instructor effort.",internet versions
http://arxiv.org/abs/1501.04865v1,"Project Monitomation is a unique implementation which focuses on justifying
the capability of smart wireless networks based on IEEE 802.15.4 standard, for
low power, short range Personal Area Network (PAN) communication. Through this
project wireless text messaging, device control & network monitoring is
implemented to demonstrate the future of Internet of things .",internet versions
http://arxiv.org/abs/1803.11256v2,"Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
paper explores the characteristics, capabilities and intelligence required of
such a network of intelligent things and humans - Internet of Battle Things
(IOBT). It will experience unique challenges that are not yet well addressed by
the current generation of AI and machine learning.",internet versions
http://arxiv.org/abs/1902.10086v1,"Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
chapter explores the characteristics, capabilities and intelli-gence required
of such a network of intelligent things and humans - Internet of Battle Things
(IOBT). The IOBT will experience unique challenges that are not yet well
addressed by the current generation of AI and machine learning.",internet versions
http://arxiv.org/abs/1904.02148v1,"The Internet delivered in excess of forty terabytes per second in 2017
(Cisco, 2018), and over half of today's Internet traffic is encrypted
(Sandvine, 2018); enabling trade worth trillions of dollars (Statista, 2017).
Yet, the underlying encryption technology is only understood by a select few.
This manuscript broadens understanding by exploring TLS, an encryption
technology used to protect application layer communication (including HTTP, FTP
and SMTP traffic), and by examining Oracle's Java implementation. We focus on
the most recent TLS release, namely, version 1.3, which is defined by RFC 8446.",internet versions
http://arxiv.org/abs/1908.09170v1,"The next Moscow City Duma elections will be held on September 8th with an
option of Internet voting. Some source code of the voting system is posted
online for public testing. Pierrick Gaudry recently showed that due to the
relatively small length of the key, the encryption scheme could be easily
broken. This issue has been fixed in the current version of the voting system.
In this note we show that the new implementation of the ElGamal encryption
system is not semantically secure. We also demonstrate how this newly found
security vulnerability can be potentially used for counting the number of votes
cast for a candidate.",internet versions
http://arxiv.org/abs/1504.02842v1,"More and more scientific research shows that there is a close correlation
between the Internet and brain science. This paper presents the idea of
establishing the Internet neurology, which means to make a cross-contrast
between the two in terms of physiology and psychology, so that a complete
infrastructure system of the Internet is established, predicting the
development trend of the Internet in the future as well as the brain structure
and operation mechanism, and providing theoretical support for the generation
principle of intelligence, cognition and emotion. It also proposes the
viewpoint that the Internet can be divided into Internet neurophysiology,
Internet neuropsychology, Brain Internet physiology, Brain Internet psychology
and the Internet in cognitive science.",internet versions
http://arxiv.org/abs/cs/0402019v1,"Most cities in Germany regularly publish a booklet called the {\em
Mietspiegel}. It basically contains a verbal description of an expert system.
It allows the calculation of the estimated fair rent for a flat. By hand, one
may need a weekend to do so. With our computerized version, the {\em Munich
Rent Advisor}, the user just fills in a form in a few minutes and the rent is
calculated immediately. We also extended the functionality and applicability of
the {\em Mietspiegel} so that the user need not answer all questions on the
form. The key to computing with partial information using high-level
programming was to use constraint logic programming. We rely on the internet,
and more specifically the World Wide Web, to provide this service to a broad
user group. More than ten thousand people have used our service in the last
three years. This article describes the experiences in implementing and using
the {\em Munich Rent Advisor}. Our results suggests that logic programming with
constraints can be an important ingredient in intelligent internet systems.",internet versions
http://arxiv.org/abs/cs/0409022v1,"In this paper, we introduce two deterministic models aimed at capturing the
dynamics of congested Internet connections. The first model is a
continuous-time model that combines a system of differential equations with a
sudden change in one of the state variables. The second model is a
discrete-time model with a time step that arises naturally from the system.
Results from these models show good agreement with the well-known ns network
simulator, better than the results of a previous, similar model. This is due in
large part to the use of the sudden change to reflect the impact of lost data
packets. We also discuss the potential use of this model in network traffic
state estimation.",internet versions
http://arxiv.org/abs/1806.00871v1,"Personal and private Web archives are proliferating due to the increase in
the tools to create them and the realization that Internet Archive and other
public Web archives are unable to capture personalized (e.g., Facebook) and
private (e.g., banking) Web pages. We introduce a framework to mitigate issues
of aggregation in private, personal, and public Web archives without
compromising potential sensitive information contained in private captures. We
amend Memento syntax and semantics to allow TimeMap enrichment to account for
additional attributes to be expressed inclusive of the requirements for
dereferencing private Web archive captures. We provide a method to involve the
user further in the negotiation of archival captures in dimensions beyond time.
We introduce a model for archival querying precedence and short-circuiting, as
needed when aggregating private and personal Web archive captures with those
from public Web archives through Memento. Negotiation of this sort is novel to
Web archiving and allows for the more seamless aggregation of various types of
Web archives to convey a more accurate picture of the past Web.",internet archive
http://arxiv.org/abs/1506.06279v1,"A variety of fan-based wikis about episodic fiction (e.g., television shows,
novels, movies) exist on the World Wide Web. These wikis provide a wealth of
information about complex stories, but if readers are behind in their viewing
they run the risk of encountering ""spoilers"" -- information that gives away key
plot points before the intended time of the show's writers. Enterprising
readers might browse the wiki in a web archive so as to view the page prior to
a specific episode date and thereby avoid spoilers. Unfortunately, due to how
web archives choose the ""best"" page, it is still possible to see spoilers
(especially in sparse archives).
  In this paper we discuss how to use Memento to avoid spoilers. Memento uses
TimeGates to determine which best archived page to give back to the user,
currently using a minimum distance heuristic. We quantify how this heuristic is
inadequate for avoiding spoilers, analyzing data collected from fan wikis and
the Internet Archive. We create an algorithm for calculating the probability of
encountering a spoiler in a given wiki article. We conduct an experiment with
16 wiki sites for popular television shows. We find that 38% of those pages are
unavailable in the Internet Archive. We find that when accessing fan wiki pages
in the Internet Archive there is as much as a 66% chance of encountering a
spoiler. Using sample access logs from the Internet Archive, we find that 19%
of actual requests to the Wayback Machine for wikia.com pages ended in
spoilers. We suggest the use of a different minimum distance heuristic,
minpast, for wikis, using the desired datetime as an upper bound.",internet archive
http://arxiv.org/abs/1308.2433v1,"The historical, cultural, and intellectual importance of archiving the web
has been widely recognized. Today, all countries with high Internet penetration
rate have established high-profile archiving initiatives to crawl and archive
the fast-disappearing web content for long-term use. As web technologies
evolve, established web archiving techniques face challenges. This paper
focuses on the potential impact of the relaxed consistency web design on
crawler driven web archiving. Relaxed consistent websites may disseminate,
albeit ephemerally, inaccurate and even contradictory information. If captured
and preserved in the web archives as historical records, such information will
degrade the overall archival quality. To assess the extent of such quality
degradation, we build a simplified feed-following application and simulate its
operation with synthetic workloads. The results indicate that a non-trivial
portion of a relaxed consistency web archive may contain observable
inconsistency, and the inconsistency window may extend significantly longer
than that observed at the data store. We discuss the nature of such quality
degradation and propose a few possible remedies.",internet archive
http://arxiv.org/abs/1212.6177v2,"Although the Internet Archive's Wayback Machine is the largest and most
well-known web archive, there have been a number of public web archives that
have emerged in the last several years. With varying resources, audiences and
collection development policies, these archives have varying levels of overlap
with each other. While individual archives can be measured in terms of number
of URIs, number of copies per URI, and intersection with other archives, to
date there has been no answer to the question ""How much of the Web is
archived?"" We study the question by approximating the Web using sample URIs
from DMOZ, Delicious, Bitly, and search engine indexes; and, counting the
number of copies of the sample URIs exist in various public web archives. Each
sample set provides its own bias. The results from our sample sets indicate
that range from 35%-90% of the Web has at least one archived copy, 17%-49% has
between 2-5 copies, 1%-8% has 6-10 copies, and 8%-63% has more than 10 copies
in public web archives. The number of URI copies varies as a function of time,
but no more than 31.3% of URIs are archived more than once per month.",internet archive
http://arxiv.org/abs/1701.08256v1,"Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.",internet archive
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",internet archive
http://arxiv.org/abs/1909.04404v1,"Web archiving frameworks are commonly assessed by the quality of their
archival records and by their ability to operate at scale. The ubiquity of
dynamic web content poses a significant challenge for crawler-based solutions
such as the Internet Archive that are optimized for scale. Human driven
services such as the Webrecorder tool provide high-quality archival captures
but are not optimized to operate at scale. We introduce the Memento Tracer
framework that aims to balance archival quality and scalability. We outline its
concept and architecture and evaluate its archival quality and operation at
scale. Our findings indicate quality is on par or better compared against
established archiving frameworks and operation at scale comes with a manageable
overhead.",internet archive
http://arxiv.org/abs/0911.1112v2,"The Web is ephemeral. Many resources have representations that change over
time, and many of those representations are lost forever. A lucky few manage to
reappear as archived resources that carry their own URIs. For example, some
content management systems maintain version pages that reflect a frozen prior
state of their changing resources. Archives recurrently crawl the web to obtain
the actual representation of resources, and subsequently make those available
via special-purpose archived resources. In both cases, the archival copies have
URIs that are protocol-wise disconnected from the URI of the resource of which
they represent a prior state. Indeed, the lack of temporal capabilities in the
most common Web protocol, HTTP, prevents getting to an archived resource on the
basis of the URI of its original. This turns accessing archived resources into
a significant discovery challenge for both human and software agents, which
typically involves following a multitude of links from the original to the
archival resource, or of searching archives for the original URI. This paper
proposes the protocol-based Memento solution to address this problem, and
describes a proof-of-concept experiment that includes major servers of archival
content, including Wikipedia and the Internet Archive. The Memento solution is
based on existing HTTP capabilities applied in a novel way to add the temporal
dimension. The result is a framework in which archived resources can seamlessly
be reached via the URI of their original: protocol-based time travel for the
Web.",internet archive
http://arxiv.org/abs/1309.4016v1,"The Internet Archive's (IA) Wayback Machine is the largest and oldest public
web archive and has become a significant repository of our recent history and
cultural heritage. Despite its importance, there has been little research about
how it is discovered and used. Based on web access logs, we analyze what users
are looking for, why they come to IA, where they come from, and how pages link
to IA. We find that users request English pages the most, followed by the
European languages. Most human users come to web archives because they do not
find the requested pages on the live web. About 65% of the requested archived
pages no longer exist on the live web. We find that more than 82% of human
sessions connect to the Wayback Machine via referrals from other web sites,
while only 15% of robots have referrers. Most of the links (86%) from websites
are to individual archived pages at specific points in time, and of those 83%
no longer exist on the live web.",internet archive
http://arxiv.org/abs/1806.08246v2,"The multimedia content in the World Wide Web is rapidly growing and contains
valuable information for many applications in different domains. For this
reason, the Internet Archive initiative has been gathering billions of
time-versioned web pages since the mid-nineties. However, the huge amount of
data is rarely labeled with appropriate metadata and automatic approaches are
required to enable semantic search. Normally, the textual content of the
Internet Archive is used to extract entities and their possible relations
across domains such as politics and entertainment, whereas image and video
content is usually neglected. In this paper, we introduce a system for person
recognition in image content of web news stored in the Internet Archive. Thus,
the system complements entity recognition in text and allows researchers and
analysts to track media coverage and relations of persons more precisely. Based
on a deep learning face recognition approach, we suggest a system that
automatically detects persons of interest and gathers sample material, which is
subsequently used to identify them in the image data of the Internet Archive.
We evaluate the performance of the face recognition system on an appropriate
standard benchmark dataset and demonstrate the feasibility of the approach with
two use cases.",internet archive
http://arxiv.org/abs/1309.5503v1,"When a user views an archived page using the archive's user interface (UI),
the user selects a datetime to view from a list. The archived web page, if
available, is then displayed. From this display, the web archive UI attempts to
simulate the web browsing experience by smoothly transitioning between archived
pages. During this process, the target datetime changes with each link
followed; drifting away from the datetime originally selected. When browsing
sparsely-archived pages, this nearly-silent drift can be many years in just a
few clicks. We conducted 200,000 acyclic walks of archived pages, following up
to 50 links per walk, comparing the results of two target datetime policies.
The Sliding Target policy allows the target datetime to change as it does in
archive UIs such as the Internet Archive's Wayback Machine. The Sticky Target
policy, represented by the Memento API, keeps the target datetime the same
throughout the walk. We found that the Sliding Target policy drift increases
with the number of walk steps, number of domains visited, and choice (number of
links available). However, the Sticky Target policy controls temporal drift,
holding it to less than 30 days on average regardless of walk length or number
of domains visited. The Sticky Target policy shows some increase as choice
increases, but this may be caused by other factors. We conclude that based on
walk length, the Sticky Target policy generally produces at least 30 days less
drift than the Sliding Target policy.",internet archive
http://arxiv.org/abs/1906.07104v1,"We describe challenges related to web archiving, replaying archived web
resources, and verifying their authenticity. We show that Web Packaging has
significant potential to help address these challenges and identify areas in
which changes are needed in order to fully realize that potential.",internet archive
http://arxiv.org/abs/1806.06878v1,"Web archives, a key area of digital preservation, meet the needs of
journalists, social scientists, historians, and government organizations. The
use cases for these groups often require that they guide the archiving process
themselves, selecting their own original resources, or seeds, and creating
their own web archive collections. We focus on the collections within
Archive-It, a subscription service started by the Internet Archive in 2005 for
the purpose of allowing organizations to create their own collections of
archived web pages, or mementos. Understanding these collections could be done
via their user-supplied metadata or via text analysis, but the metadata is
applied inconsistently between collections and some Archive-It collections
consist of hundreds of thousands of seeds, making it costly in terms of time to
download each memento. Our work proposes using structural metadata as an
additional way to understand these collections. We explore structural features
currently existing in these collections that can unveil curation and crawling
behaviors. We adapt the concept of the collection growth curve for
understanding Archive-It collection curation and crawling behavior. We also
introduce several seed features and come to an understanding of the diversity
of resources that make up a collection. Finally, we use the descriptions of
each collection to identify four semantic categories of Archive-It collections.
Using the identified structural features, we reviewed the results of runs with
20 classifiers and are able to predict the semantic category of a collection
using a Random Forest classifier with a weighted average F1 score of 0.720,
thus bridging the structural to the descriptive. Our method is useful because
it saves the researcher time and bandwidth. Identifying collections by their
semantic category allows further downstream processing to be tailored to these
categories.",internet archive
http://arxiv.org/abs/1301.0159v1,"Internet of Things (IoT) will comprise billions of devices that can sense,
communicate, compute and potentially actuate. Data streams coming from these
devices will challenge the traditional approaches to data management and
contribute to the emerging paradigm of big data. This paper discusses emerging
Internet of Things (IoT) architecture, large scale sensor network applications,
federating sensor networks, sensor data and related context capturing
techniques, challenges in cloud-based management, storing, archiving and
processing of sensor data.",internet archive
http://arxiv.org/abs/1306.6413v1,"At present Internet has emerged as a country's predominant and viable data
communication infrastructure. The Autonomous System (AS) resources which are
building blocks of the Internet are AS numbers, IPv4 and IPv6 Prefixes. AS
number growth is one of Internet infrastructure development indicators. Hence
understanding on long term trend and stochastic variation behaviour are
essential to detect significant events during the growth. In this work, time
series based approximation is considered for mathematical modelling and
forecast the yearly AS growth. The AS data of five countries namely India,
China, Japan, South Korea and Taiwan are extracted from APNIC archive. ARIMA
models with different Auto Regressive and Moving Average parameters are
identified for forecasting. Model validation, parameter estimation, point
forecast and prediction intervals with 95 % confidence levels for the five
countries are reported in the paper. The significant level change in
variations, positive growth percentage in Inter Annual Absolute Variations
(IAAV) and higher percentage of advertised ASes when compared to other
countries indicate India's fast growth and wider global reachability of
Internet infrastructure from 2007 onwards. The correlation between IAAV change
point and GDP growth period indicates that service sector industry growth is
the driving force behind significant yearly changes.",internet archive
http://arxiv.org/abs/1305.5959v2,"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.",internet archive
http://arxiv.org/abs/1603.04387v1,"NetMemex explores efficient network traffic archival without any loss of
information. Unlike NetFlow-like aggregation, NetMemex allows retrieving the
entire packet data including full payload, which makes it useful in forensic
analysis, networked and distributed system research, and network
administration. Different from packet trace dumps, NetMemex performs
sophisticated data compression for small storage space use and optimizes the
data layout for fast query processing. NetMemex takes advantage of high-speed
random access of flash drives and inexpensive storage space of hard disk
drives. These efforts lead to a cost-effective yet high-performance full
traffic archival system. We demonstrate that NetMemex can record full-fidelity
traffic at near-Gbps rates using a single commodity machine, handling common
queries at up to 90.1 K queries/second, at a low storage cost comparable to
conventional hard disk-only traffic archival solutions.",internet archive
http://arxiv.org/abs/1702.01151v1,"The Web has been around and maturing for 25 years. The popular websites of
today have undergone vast changes during this period, with a few being there
almost since the beginning and many new ones becoming popular over the years.
This makes it worthwhile to take a look at how these sites have evolved and
what they might tell us about the future of the Web. We therefore embarked on a
longitudinal study spanning almost the whole period of the Web, based on data
collected by the Internet Archive starting in 1996, to retrospectively analyze
how the popular Web as of now has evolved over the past 18 years.
  For our study we focused on the German Web, specifically on the top 100 most
popular websites in 17 categories. This paper presents a selection of the most
interesting findings in terms of volume, size as well as age of the Web. While
related work in the field of Web Dynamics has mainly focused on change rates
and analyzed datasets spanning less than a year, we looked at the evolution of
websites over 18 years. We found that around 70% of the pages we investigated
are younger than a year, with an observed exponential growth in age as well as
in size up to now. If this growth rate continues, the number of pages from the
popular domains will almost double in the next two years. In addition, we give
insights into our data set, provided by the Internet Archive, which hosts the
largest and most complete Web archive as of today.",internet archive
http://arxiv.org/abs/1407.4992v2,"Background: With the ever increasing use of computational models in the
biosciences, the need to share models and reproduce the results of published
studies efficiently and easily is becoming more important. To this end, various
standards have been proposed that can be used to describe models, simulations,
data or other essential information in a consistent fashion. These constitute
various separate components required to reproduce a given published scientific
result.
  Results: We describe the Open Modeling EXchange format (OMEX). Together with
the use of other standard formats from the Computational Modeling in Biology
Network (COMBINE), OMEX is the basis of the COMBINE Archive, a single file that
supports the exchange of all the information necessary for a modeling and
simulation experiment in biology. An OMEX file is a ZIP container that includes
a manifest file, listing the content of the archive, an optional metadata file
adding information about the archive and its content, and the files describing
the model. The content of a COMBINE Archive consists of files encoded in
COMBINE standards whenever possible, but may include additional files defined
by an Internet Media Type. Several tools that support the COMBINE Archive are
available, either as independent libraries or embedded in modeling software.
  Conclusions: The COMBINE Archive facilitates the reproduction of modeling and
simulation experiments in biology by embedding all the relevant information in
one file. Having all the information stored and exchanged at once also helps in
building activity logs and audit trails. We anticipate that the COMBINE Archive
will become a significant help for modellers, as the domain moves to larger,
more complex experiments such as multi-scale models of organs, digital
organisms, and bioengineering.",internet archive
http://arxiv.org/abs/0704.3647v1,"Internet-based personal digital belongings present different vulnerabilities
than locally stored materials. We use responses to a survey of people who have
recovered lost websites, in combination with supplementary interviews, to paint
a fuller picture of current curatorial strategies and practices. We examine the
types of personal, topical, and commercial websites that respondents have lost
and the reasons they have lost this potentially valuable material. We further
explore what they have tried to recover and how the loss influences their
subsequent practices. We found that curation of personal digital materials in
online stores bears some striking similarities to the curation of similar
materials stored locally in that study participants continue to archive
personal assets by relying on a combination of benign neglect, sporadic
backups, and unsystematic file replication. However, we have also identified
issues specific to Internet-based material: how risk is spread by distributing
the files among multiple servers and services; the circular reasoning
participants use when they discuss the safety of their digital assets; and the
types of online material that are particularly vulnerable to loss. The study
reveals ways in which expectations of permanence and notification are violated
and situations in which benign neglect has far greater consequences for the
long-term fate of important digital assets.",internet archive
http://arxiv.org/abs/1802.07285v2,"Nowadays, the Internet is indispensable when it comes to information
dissemination. People rely on the Internet to inform themselves on current news
events, as well as to verify facts. We, as a community, are quickly approaching
an 'electronic information age' where the majority of information will be
distributed electronically and tools to preserve this information will become
essential. While archiving online digital information is a good way to preserve
online information for future generations, it has many disadvantages including
the easy manipulation of archived information, e.g. by the archiving authority.
Online information is also prone to getting hacked or being taken offline.
Therefore, it is necessary that archived online news information is securely
time-stamped with the date and time when it was first archived in a way that
cannot be manipulated. The process of 'trusted timestamping' is an established
approach for claiming that particular digital information existed at a
particular 'point in time' in the past. However, traditional approaches for
trusted timestamping depend on the time-stamping authority's fidelity. Directly
embedding the hash of a digital file into the blockchain of a cryptocurrency is
a more recent method that allows for secure time-stamping, since digital
information is stored as part of the transaction information in, e.g.
Bitcoin's, blockchain, and not stored at a centralized time-stamping authority.
However, there is no system yet available, which uses this approach for
archiving and time-stamping online news articles. Therefore, the aim of this
thesis is to develop a system that 1) enables decentralized trusted
time-stamping of web and news articles as a means of making future manipulation
of online information identifiable, and 2) allows users to determine the
authenticity of articles by checking different versions of the same article
online.",internet archive
http://arxiv.org/abs/cs/0512069v1,"Backup or preservation of websites is often not considered until after a
catastrophic event has occurred. In the face of complete website loss, ""lazy""
webmasters or concerned third parties may be able to recover some of their
website from the Internet Archive. Other pages may also be salvaged from
commercial search engine caches. We introduce the concept of ""lazy
preservation""- digital preservation performed as a result of the normal
operations of the Web infrastructure (search engines and caches). We present
Warrick, a tool to automate the process of website reconstruction from the
Internet Archive, Google, MSN and Yahoo. Using Warrick, we have reconstructed
24 websites of varying sizes and composition to demonstrate the feasibility and
limitations of website reconstruction from the public Web infrastructure. To
measure Warrick's window of opportunity, we have profiled the time required for
new Web resources to enter and leave search engine caches.",internet archive
http://arxiv.org/abs/1701.03277v1,"Social graph construction from various sources has been of interest to
researchers due to its application potential and the broad range of technical
challenges involved. The World Wide Web provides a huge amount of continuously
updated data and information on a wide range of topics created by a variety of
content providers, and makes the study of extracted people networks and their
temporal evolution valuable for social as well as computer scientists. In this
paper we present SocGraph - an extraction and exploration system for social
relations from the content of around 2 billion web pages collected by the
Internet Archive over the 17 years time period between 1996 and 2013. We
describe methods for constructing large social graphs from extracted relations
and introduce an interface to study their temporal evolution.",internet archive
http://arxiv.org/abs/cs/0211023v1,"Traditional science searched for new objects and phenomena that led to
discoveries. Tomorrow's science will combine together the large pool of
information in scientific archives and make discoveries. Scienthists are
currently keen to federate together the existing scientific databases. The
major challenge in building a federation of these autonomous and heterogeneous
databases is system integration. Ineffective integration will result in defunct
federations and under utilized scientific data.
  Astronomy, in particular, has many autonomous archives spread over the
Internet. It is now seeking to federate these, with minimal effort, into a
Virtual Observatory that will solve complex distributed computing tasks such as
answering federated spatial join queries.
  In this paper, we present SkyQuery, a successful prototype of an evolving
federation of astronomy archives. It interoperates using the emerging Web
services standard. We describe the SkyQuery architecture and show how it
efficiently evaluates a probabilistic federated spatial join query.",internet archive
http://arxiv.org/abs/0808.3441v1,"YouTube (http://www.youtube.com) is an online, public-access video-sharing
site that allows users to post short streaming-video submissions for open
viewing. Along with Google, MySpace, Facebook, etc. it is one of the great
success stories of the Internet, and is widely used by many of today's
undergraduate students. The higher education sector has recently realised the
potential of YouTube for presenting teaching resources/material to students,
and publicising research. This article considers another potential use for
online video archiving websites such as YouTube and GoogleVideo in higher
education - as an online video archive providing thousands of hours of video
footage for use in lectures. In this article I will discuss why this might be
useful, present some examples that demonstrate the potential for YouTube as a
teaching resource, and highlight some of the copyright and legal issues that
currently impact on the effective use of new online video websites, such as
YouTube, for use as a teaching resource.",internet archive
http://arxiv.org/abs/1003.3661v1,"Dereferencing a URI returns a representation of the current state of the
resource identified by that URI. But, on the Web representations of prior
states of a resource are also available, for example, as resource versions in
Content Management Systems or archival resources in Web Archives such as the
Internet Archive. This paper introduces a resource versioning mechanism that is
fully based on HTTP and uses datetime as a global version indicator. The
approach allows ""follow your nose"" style navigation both from the current
time-generic resource to associated time-specific version resources as well as
among version resources. The proposed versioning mechanism is congruent with
the Architecture of the World Wide Web, and is based on the Memento framework
that extends HTTP with transparent content negotiation in the datetime
dimension. The paper shows how the versioning approach applies to Linked Data,
and by means of a demonstrator built for DBpedia, it also illustrates how it
can be used to conduct a time-series analysis across versions of Linked Data
descriptions.",internet archive
http://arxiv.org/abs/1703.02005v1,"In the mid-90's, it was shown that the statistics of aggregated time series
from Internet traffic departed from those of traditional short range dependent
models, and were instead characterized by asymptotic self-similarity. Following
this seminal contribution, over the years, many studies have investigated the
existence and form of scaling in Internet traffic. This contribution aims first
at presenting a methodology, combining multiscale analysis (wavelet and wavelet
leaders) and random projections (or sketches), permitting a precise, efficient
and robust characterization of scaling which is capable of seeing through
non-stationary anomalies. Second, we apply the methodology to a data set
spanning an unusually long period: 14 years, from the MAWI traffic archive,
thereby allowing an in-depth longitudinal analysis of the form, nature and
evolutions of scaling in Internet traffic, as well as network mechanisms
producing them. We also study a separate 3-day long trace to obtain
complementary insight into intra-day behavior. We find that a biscaling (two
ranges of independent scaling phenomena) regime is systematically observed:
long-range dependence over the large scales, and multifractal-like scaling over
the fine scales. We quantify the actual scaling ranges precisely, verify to
high accuracy the expected relationship between the long range dependent
parameter and the heavy tail parameter of the flow size distribution, and
relate fine scale multifractal scaling to typical IP packet inter-arrival and
to round-trip time distributions.",internet archive
http://arxiv.org/abs/cs/0701145v1,"We present a concept to achieve non-repudiation for natural language
conversations over the Internet. The method rests on chained electronic
signatures applied to pieces of packet-based, digital, voice communication. It
establishes the integrity and authenticity of the bidirectional data stream and
its temporal sequence and thus the security context of a conversation. The
concept is close to the protocols for Voice over the Internet (VoIP), provides
a high level of inherent security, and extends naturally to multilateral
non-repudiation, e.g., for conferences. Signatures over conversations can
become true declarations of will in analogy to electronically signed, digital
documents. This enables binding verbal contracts, in principle between
unacquainted speakers, and in particular without witnesses. A reference
implementation of a secure VoIP archive is exhibited.",internet archive
http://arxiv.org/abs/1605.01401v1,"This paper proposes a defense scheme against malicious use of DNS tunnel. A
tunnel validator is designed to provide trustworthy tunnel-aware defensive
recursive service. In addition to the detection algorithm of malicious tunnel
domains, the tunnel validation relies on registered tunnel domains as whitelist
and identified malicious tunnel domains as blacklist. A benign tunnel user is
thus motivated to register its tunnel domain before using it. Through the
tunnel validation, the secure domains are allowed to the recursive service
provided by the tunnel validator and the insecure domains are blocked. All
inbound suspicious DNS queries are recorded and stored for forensics and future
malicious tunnel detection by the tunnel validator.",domain validity
http://arxiv.org/abs/0704.1394v1,"In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.",domain validity
http://arxiv.org/abs/1710.06924v2,"We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a
large-scale testbed for unsupervised domain adaptation across visual domains.
Unsupervised domain adaptation aims to solve the real-world problem of domain
shift, where machine learning models trained on one domain must be transferred
and adapted to a novel visual domain without additional supervision. The
VisDA2017 challenge is focused on the simulation-to-reality shift and has two
associated tasks: image classification and image segmentation. The goal in both
tracks is to first train a model on simulated, synthetic data in the source
domain and then adapt it to perform well on real image data in the unlabeled
test domain. Our dataset is the largest one to date for cross-domain object
classification, with over 280K images across 12 categories in the combined
training, validation and testing domains. The image segmentation dataset is
also large-scale with over 30K images across 18 categories in the three
domains. We compare VisDA to existing cross-domain adaptation datasets and
provide a baseline performance analysis using various domain adaptation models
that are currently popular in the field.",domain validity
http://arxiv.org/abs/1906.02238v1,"We tackle an unsupervised domain adaptation problem for which the domain
discrepancy between labeled source and unlabeled target domains is large, due
to many factors of inter and intra-domain variation. While deep domain
adaptation methods have been realized by reducing the domain discrepancy, these
are difficult to apply when domains are significantly unalike. In this work, we
propose to decompose domain discrepancy into multiple but smaller, and thus
easier to minimize, discrepancies by introducing unlabeled bridging domains
that connect the source and target domains. We realize our proposal through an
extension of the domain adversarial neural network with multiple
discriminators, each of which accounts for reducing discrepancies between
unlabeled (bridge, target) domains and a mix of all precedent domains including
source. We validate the effectiveness of our method on several adaptation tasks
including object recognition and semantic segmentation.",domain validity
http://arxiv.org/abs/1804.07013v1,"In order to make the task, description of planning domains and problems, more
comprehensive for non-experts in planning, the visual representation has been
used in planning domain modeling in recent years. However, current knowledge
engineering tools with visual modeling, like itSIMPLE (Vaquero et al. 2012) and
VIZ (Vodr\'a\v{z}ka and Chrpa 2010), are less efficient than the traditional
method of hand-coding by a PDDL expert using a text editor, and rarely involved
in finetuning planning domains depending on the plan validation. Aim at this,
we present an integrated development environment KAVI for planning domain
modeling inspired by itSIMPLE and VIZ. KAVI using an abstract domain knowledge
base to improve the efficiency of planning domain visual modeling. By
integrating planners and a plan validator, KAVI proposes a method to fine-tune
planning domains based on the plan validation.",domain validity
http://arxiv.org/abs/1605.01886v1,"Normann proved that the domains of the game model of PCF (the domains of
sequential functionals) need not be dcpos. Sazonov has defined natural domains
for a theory of such incomplete domains. This paper further develops that
theory. It defines lub-rules that infer natural lubs from existing natural
lubs, and lub-rule classes that describe axiom systems like that of natural
domains. There is a canonical proper subcategory of the natural domains, the
closed directed lub partial orders (cdlubpo), that corresponds to the complete
lub-rule class of all valid lub-rules. Cdlubpos can be completed to restricted
dcpos, which are dcpos that retain the data of the incomplete cdlubpo as a
subset.",domain validity
http://arxiv.org/abs/1904.01353v2,"In this paper, we propose a framework to perform verification and validation
of semantically annotated data. The annotations, extracted from websites, are
verified against the schema.org vocabulary and Domain Specifications to ensure
the syntactic correctness and completeness of the annotations. The Domain
Specifications allow checking the compliance of annotations against
corresponding domain-specific constraints. The validation mechanism will detect
errors and inconsistencies between the content of the analyzed schema.org
annotations and the content of the web pages where the annotations were found.",domain validity
http://arxiv.org/abs/1803.09210v2,"This paper proposes an importance weighted adversarial nets-based method for
unsupervised domain adaptation, specific for partial domain adaptation where
the target domain has less number of classes compared to the source domain.
Previous domain adaptation methods generally assume the identical label spaces,
such that reducing the distribution divergence leads to feasible knowledge
transfer. However, such an assumption is no longer valid in a more realistic
scenario that requires adaptation from a larger and more diverse source domain
to a smaller target domain with less number of classes. This paper extends the
adversarial nets-based domain adaptation and proposes a novel adversarial
nets-based partial domain adaptation method to identify the source samples that
are potentially from the outlier classes and, at the same time, reduce the
shift of shared classes between domains.",domain validity
http://arxiv.org/abs/1904.12543v2,"Learning domain-invariant representation is a dominant approach for domain
generalization (DG), where we need to build a classifier that is robust toward
domain shifts. However, previous domain-invariance-based methods overlooked the
underlying dependency of classes on domains, which is responsible for the
trade-off between classification accuracy and domain invariance. Because the
primary purpose of DG is to classify unseen domains rather than the invariance
itself, the improvement of the invariance can negatively affect DG performance
under this trade-off. To overcome the problem, this study first expands the
analysis of the trade-off by Xie et. al., and provides the notion of
accuracy-constrained domain invariance, which means the maximum domain
invariance within a range that does not interfere with accuracy. We then
propose a novel method adversarial feature learning with accuracy constraint
(AFLAC), which explicitly leads to that invariance on adversarial training.
Empirical validations show that the performance of AFLAC is superior to that of
domain-invariance-based methods on both synthetic and three real-world
datasets, supporting the importance of considering the dependency and the
efficacy of the proposed method.",domain validity
http://arxiv.org/abs/1206.6438v1,"We study the problem of unsupervised domain adaptation, which aims to adapt
classifiers trained on a labeled source domain to an unlabeled target domain.
Many existing approaches first learn domain-invariant features and then
construct classifiers with them. We propose a novel approach that jointly learn
the both. Specifically, while the method identifies a feature space where data
in the source and the target domains are similarly distributed, it also learns
the feature space discriminatively, optimizing an information-theoretic metric
as an proxy to the expected misclassification error on the target domain. We
show how this optimization can be effectively carried out with simple
gradient-based methods and how hyperparameters can be cross-validated without
demanding any labeled data from the target domain. Empirical studies on
benchmark tasks of object recognition and sentiment analysis validated our
modeling assumptions and demonstrated significant improvement of our method
over competing ones in classification accuracies.",domain validity
http://arxiv.org/abs/1706.07527v1,"Domain adaptation deals with adapting classifiers trained on data from a
source distribution, to work effectively on data from a target distribution. In
this paper, we introduce the Nonlinear Embedding Transform (NET) for
unsupervised domain adaptation. The NET reduces cross-domain disparity through
nonlinear domain alignment. It also embeds the domain-aligned data such that
similar data points are clustered together. This results in enhanced
classification. To determine the parameters in the NET model (and in other
unsupervised domain adaptation models), we introduce a validation procedure by
sampling source data points that are similar in distribution to the target
data. We test the NET and the validation procedure using popular image datasets
and compare the classification results across competitive procedures for
unsupervised domain adaptation.",domain validity
http://arxiv.org/abs/1406.7497v1,"Domain theory is `a mathematical theory that serves as a foundation for the
semantics of programming languages'. Domains form the basis of a theory of
partial information, which extends the familiar notion of partial function to
encompass a whole spectrum of ""degrees of definedness"", so as to model
incremental higher-order computation (i.e., computing with infinite data
values, such as functions defined over an infinite domain like the domain of
integers, infinite trees, and such as objects of object-oriented programming).
General considerations from recursion theory dictate that partial functions are
unavoidable in any discussion of computability. Domain theory provides an
appropriately abstract setting in which the notion of a partial function can be
lifted and used to give meaning to higher types, recursive types, etc. NOOP is
a domain-theoretic model of nominally-typed OOP. NOOP was used to prove the
identification of inheritance and subtyping in mainstream nominally-typed OO
programming languages and the validity of this identification. In this report
we first present the definitions of basic domain theoretic notions and domain
constructors used in the construction of NOOP, then we present the construction
of a simple structural model of OOP called COOP as a step towards the
construction of NOOP. Like the construction of NOOP, the construction of COOP
uses earlier presented domain constructors.",domain validity
http://arxiv.org/abs/1806.00804v2,"Several methods were recently proposed for the task of translating images
between domains without prior knowledge in the form of correspondences. The
existing methods apply adversarial learning to ensure that the distribution of
the mapped source domain is indistinguishable from the target domain, which
suffers from known stability issues. In addition, most methods rely heavily on
`cycle' relationships between the domains, which enforce a one-to-one mapping.
In this work, we introduce an alternative method: Non-Adversarial Mapping
(NAM), which separates the task of target domain generative modeling from the
cross-domain mapping task. NAM relies on a pre-trained generative model of the
target domain, and aligns each source image with an image synthesized from the
target domain, while jointly optimizing the domain mapping function. It has
several key advantages: higher quality and resolution image translations,
simpler and more stable training and reusable target models. Extensive
experiments are presented validating the advantages of our method.",domain validity
http://arxiv.org/abs/1302.4888v2,"One of the most challenging problems in recommender systems based on the
collaborative filtering (CF) concept is data sparseness, i.e., limited user
preference data is available for making recommendations. Cross-domain
collaborative filtering (CDCF) has been studied as an effective mechanism to
alleviate data sparseness of one domain using the knowledge about user
preferences from other domains. A key question to be answered in the context of
CDCF is what common characteristics can be deployed to link different domains
for effective knowledge transfer. In this paper, we assess the usefulness of
user-contributed (social) tags in this respect. We do so by means of the
Generalized Tag-induced Cross-domain Collaborative Filtering (GTagCDCF)
approach that we propose in this paper and that we developed based on the
general collective matrix factorization framework. Assessment is done by a
series of experiments, using publicly available CF datasets that represent
three cross-domain cases, i.e., two two-domain cases and one three-domain case.
A comparative analysis on two-domain cases involving GTagCDCF and several
state-of-the-art CDCF approaches indicates the increased benefit of using
social tags as representatives of explicit links between domains for CDCF as
compared to the implicit links deployed by the existing CDCF methods. In
addition, we show that users from different domains can already benefit from
GTagCDCF if they only share a few common tags. Finally, we use the three-domain
case to validate the robustness of GTagCDCF with respect to the scale of
datasets and the varying number of domains.",domain validity
http://arxiv.org/abs/1202.5690v1,"The paper presents a Stateflow based network test-bed to validate real-time
optimal control algorithms. Genetic Algorithm (GA) based time domain
performance index minimization is attempted for tuning of PI controller to
handle a balanced lag and delay type First Order Plus Time Delay (FOPTD)
process over network. The tuning performance is validated on a real-time
communication network with artificially simulated stochastic delay, packet loss
and out-of order packets characterizing the network.",domain validity
http://arxiv.org/abs/1906.09179v1,"Many fields of science rely on software systems to answer different research
questions. For valid results researchers need to trust the results scientific
software produces, and consequently quality assurance is of utmost importance.
In this paper we are investigating the impact of quality assurance in the
domain of computational materials science (CMS). Based on our experience in
this domain we formulate challenges for validation and verification of
scientific software and their results. Furthermore, we describe directions for
future research that can potentially help dealing with these challenges.",domain validity
http://arxiv.org/abs/1903.10211v1,"In many practical transfer learning scenarios, the feature distribution is
different across the source and target domains (i.e. non-i.i.d.). Maximum mean
discrepancy (MMD), as a domain discrepancy metric, has achieved promising
performance in unsupervised domain adaptation (DA). We argue that MMD-based DA
methods ignore the data locality structure, which, to some extent, would cause
the negative transfer effect. The locality plays an important role in
minimizing the nonlinear local domain discrepancy underlying the marginal
distributions. For better exploiting the domain locality, a novel local
generative discrepancy metric (LGDM) based intermediate domain generation
learning called Manifold Criterion guided Transfer Learning (MCTL) is proposed
in this paper. The merits of the proposed MCTL are four-fold: 1) the concept of
manifold criterion (MC) is first proposed as a measure validating the
distribution matching across domains, and domain adaptation is achieved if the
MC is satisfied; 2) the proposed MC can well guide the generation of the
intermediate domain sharing similar distribution with the target domain, by
minimizing the local domain discrepancy; 3) a global generative discrepancy
metric (GGDM) is presented, such that both the global and local discrepancy can
be effectively and positively reduced; 4) a simplified version of MCTL called
MCTL-S is presented under a perfect domain generation assumption for more
generic learning scenario. Experiments on a number of benchmark visual transfer
tasks demonstrate the superiority of the proposed manifold criterion guided
generative transfer method, by comparing with other state-of-the-art methods.
The source code is available in https://github.com/wangshanshanCQU/MCTL.",domain validity
http://arxiv.org/abs/0901.3620v1,"This article presents a Verification and Validation approach which is used
here in order to complete the classical tool box the industrial user may
utilize in Enterprise Modeling and Integration domain. This approach, which has
been defined independently from any application domain is based on several
formal concepts and tools presented in this paper. These concepts are property
concepts, property reference matrix, properties graphs, enterprise modeling
domain ontology, conceptual graphs and formal reasoning mechanisms.",domain validity
http://arxiv.org/abs/1804.10916v2,"Convolutional networks (ConvNets) have achieved great successes in various
challenging vision tasks. However, the performance of ConvNets would degrade
when encountering the domain shift. The domain adaptation is more significant
while challenging in the field of biomedical image analysis, where
cross-modality data have largely different distributions. Given that annotating
the medical data is especially expensive, the supervised transfer learning
approaches are not quite optimal. In this paper, we propose an unsupervised
domain adaptation framework with adversarial learning for cross-modality
biomedical image segmentations. Specifically, our model is based on a dilated
fully convolutional network for pixel-wise prediction. Moreover, we build a
plug-and-play domain adaptation module (DAM) to map the target input to
features which are aligned with source domain feature space. A domain critic
module (DCM) is set up for discriminating the feature space of both domains. We
optimize the DAM and DCM via an adversarial loss without using any target
domain label. Our proposed method is validated by adapting a ConvNet trained
with MRI images to unpaired CT data for cardiac structures segmentations, and
achieved very promising results.",domain validity
http://arxiv.org/abs/1906.03249v3,"Word embeddings are traditionally trained on a large corpus in an
unsupervised setting, with no specific design for incorporating domain
knowledge. This can lead to unsatisfactory performances when training data
originate from heterogeneous domains. In this paper, we propose two novel
mechanisms for domain-aware word embedding training, namely domain indicator
and domain attention, which integrate domain-specific knowledge into the widely
used SG and CBOW models, respectively. The two methods are based on a joint
learning paradigm and ensure that words in a target domain are intensively
focused when trained on a source domain corpus. Qualitative and quantitative
evaluation confirm the validity and effectiveness of our models. Compared to
baseline methods, our method is particularly effective in near-cold-start
scenarios.",domain validity
http://arxiv.org/abs/1402.0140v1,"This paper presents a probabilistic model validation methodology for
nonlinear systems in time-domain. The proposed formulation is simple,
intuitive, and accounts both deterministic and stochastic nonlinear systems
with parametric and nonparametric uncertainties. Instead of hard invalidation
methods available in the literature, a relaxed notion of validation in
probability is introduced. To guarantee provably correct inference, algorithm
for constructing probabilistically robust validation certificate is given along
with computational complexities. Several examples are worked out to illustrate
its use.",domain validity
http://arxiv.org/abs/1404.6603v1,"Over the years, ProB has moved from a tool that complemented proving, to a
development environment that is now sometimes used instead of proving for
applications, such as exhaustive model checking or data validation. This has
led to much more stringent requirements on the integrity of ProB. In this paper
we present a summary of our validation efforts for ProB, in particular within
the context of the norm EN 50128 and safety critical applications in the
railway domain.",domain validity
http://arxiv.org/abs/1608.00250v1,"This paper identifies a problem with the usual procedure for
L2-regularization parameter estimation in a domain adaptation setting. In such
a setting, there are differences between the distributions generating the
training data (source domain) and the test data (target domain). The usual
cross-validation procedure requires validation data, which can not be obtained
from the unlabeled target data. The problem is that if one decides to use
source validation data, the regularization parameter is underestimated. One
possible solution is to scale the source validation data through importance
weighting, but we show that this correction is not sufficient. We conclude the
paper with an empirical analysis of the effect of several importance weight
estimators on the estimation of the regularization parameter.",domain validity
http://arxiv.org/abs/1809.01756v1,"Token curated registries (TCRs) have been proposed recently as an approach to
create and maintain high quality lists of resources or recommendations in a
decentralized manner. Applications range from maintaining registries of web
domains for advertising purposes (e.g., adChain) or restaurants, consumer
products, etc. The registry is maintained through a combination of candidate
applications requiring a token deposit, challenges based on token staking and
token-weighted votes with a redistribution of tokens occurring as a consequence
of the vote. We present a simplified mathematical model of a TCR and its
challenge and voting process analyze it from a game-theoretic perspective. We
derive some insights into conditions with respect to the quality of a candidate
under which challenges occur, and under which the outcome is reject or accept.
We also show that there are conditions under which the outcome may not be
entirely predictable in the sense that everyone voting for accept and everyone
voting for reject could both be Nash Equilibria outcomes. For such conditions,
we also explore when a particular strategy profile may be payoff dominant. We
identify ways in which our modeling can be extended and also some implications
of our model with respect to the composition of TCRs.",domain registries
http://arxiv.org/abs/cs/0212051v1,"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.",domain registries
http://arxiv.org/abs/1007.3631v1,"The advanced features of today's smart phones and hand held devices, like the
increased memory and processing capabilities, allowed them to act even as
information providers. Thus a smart phone hosting web services is not a fancy
anymore. But the relevant discovery of these services provided by the smart
phones has became quite complex, because of the volume of services possible
with each Mobile Host providing some services. Centralized registries have
severe drawbacks in such a scenario and alternate means of service discovery
are to be addressed. P2P domain with it resource sharing capabilities comes
quite handy and here in this paper we provide an alternate approach to UDDI
registry for discovering mobile web services. The services are published into
the P2P network as JXTA modules and the discovery issues of these module
advertisements are addressed. The approach also provides alternate means of
identifying the Mobile Host.",domain registries
http://arxiv.org/abs/1007.3589v1,"Registries play a key role in service-oriented applications. Originally, they
were neutral players between service providers and clients. The UDDI Business
Registry (UBR) was meant to foster these concepts and provide a common
reference for companies interested in Web services. The more Web services were
used, the more companies started create their own local registries: more
efficient discovery processes, better control over the quality of published
information, and also more sophisticated publication policies motivated the
creation of private repositories. The number and heterogeneity of the different
registries - besides the decision to close the UBR are pushing for new and
sophisticated means to make different registries cooperate. This paper proposes
DIRE (DIstributed REgistry), a novel approach based on a publish and subscribe
(P/S) infrastructure to federate different heterogeneous registries and make
them exchange information about published services. The paper discusses the
main motivations for the P/S-based infrastructure, proposes an integrated
service model, introduces the main components of the framework, and exemplifies
them on a simple case study.",domain registries
http://arxiv.org/abs/cs/0605111v1,"The NSDL Metadata Registry is designed to provide humans and machines with
the means to discover, create, access and manage metadata schemes, schemas,
application profiles, crosswalks and concept mappings. This paper describes the
general goals and architecture of the NSDL Metadata Registry as well as issues
encountered during the first year of the project's implementation.",domain registries
http://arxiv.org/abs/1609.09211v1,"Advancements in technology have transformed mobile devices from being mere
communication widgets to versatile computing devices. Proliferation of these
hand held devices has made them a common means to access and process digital
information. Most web based applications are today available in a form that can
conveniently be accessed over mobile devices. However, webservices
(applications meant for consumption by other applications rather than humans)
are not as commonly provided and consumed over mobile devices. Facilitating
this and in effect realizing a service-oriented system over mobile devices has
the potential to further enhance the potential of mobile devices. One of the
major challenges in this integration is the lack of an efficient service
registry system that caters to issues associated with the dynamic and volatile
mobile environments. Existing service registry technologies designed for
traditional systems fall short of accommodating such issues. In this paper, we
propose a novel approach to manage service registry systems provided 'solely'
over mobile devices, and thus realising an SOA without the need for high-end
computing systems. The approach manages a dynamic service registry system in
the form of light weight and distributed registries. We assess the feasibility
of our approach by engineering and deploying a working prototype of the
proposed registry system over actual mobile devices. A comparative study of the
proposed approach and the traditional UDDI (Universal Description, Discovery,
and Integration) registry is also included. The evaluation of our framework has
shown propitious results in terms of battery cost, scalability, hindrance with
native applications.",domain registries
http://arxiv.org/abs/cs/0212052v1,"In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.",domain registries
http://arxiv.org/abs/1603.01774v2,"Scientific full text papers are usually stored in separate places than their
underlying research datasets. Authors typically make references to datasets by
mentioning them for example by using their titles and the year of publication.
However, in most cases explicit links that would provide readers with direct
access to referenced datasets are missing. Manually detecting references to
datasets in papers is time consuming and requires an expert in the domain of
the paper. In order to make explicit all links to datasets in papers that have
been published already, we suggest and evaluate a semi-automatic approach for
finding references to datasets in social sciences papers. Our approach does not
need a corpus of papers (no cold start problem) and it performs well on a small
test corpus (gold standard). Our approach achieved an F-measure of 0.84 for
identifying references in full texts and an F-measure of 0.83 for finding
correct matches of detected references in the da|ra dataset registry.",domain registries
http://arxiv.org/abs/1901.01185v1,"Using runtime execution artifacts to identify malware and its associated
family is an established technique in the security domain. Many papers in the
literature rely on explicit features derived from network, file system, or
registry interaction. While effective, the use of these fine-granularity data
points makes these techniques computationally expensive. Moreover, the
signatures and heuristics are often circumvented by subsequent malware authors.
In this work, we propose Chatter, a system that is concerned only with the
order in which high-level system events take place. Individual events are
mapped onto an alphabet and execution traces are captured via terse
concatenations of those letters. Then, leveraging an analyst labeled corpus of
malware, n-gram document classification techniques are applied to produce a
classifier predicting malware family. This paper describes that technique and
its proof-of-concept evaluation. In its prototype form, only network events are
considered and eleven malware families are used. We show the technique achieves
83%-94% accuracy in isolation and makes non-trivial performance improvements
when integrated with a baseline classifier of combined order features to reach
an accuracy of up to 98.8%.",domain registries
http://arxiv.org/abs/1309.4853v1,"A question of considerable interest to cell membrane biology is whether phase
segregated domains across an asymmetric bilayer are strongly correlated with
each other and whether phase segregation in one leaflet can induce segregation
in the other. We answer both these questions in the affirmative, using an
atomistic molecular dynamics simulation to study the equilibrium statistical
properties of a 3-component {\em asymmetric} lipid bilayer comprising an
unsaturated POPC (palmitoyl-oleoyl-phosphatidyl-choline), a saturated SM
(sphingomyelin) and cholesterol with different composition ratios. Our
simulations are done by fixing the composition of the upper leaflet to be at
the coexistence of the liquid ordered ($l_o$) - liquid disordered ($l_d$)
phases, while the composition of the lower leaflet is varied from the phase
coexistence regime to the mixed $l_d$ phase, across a first-order phase
boundary. In the regime of phase coexistence in each leaflet, we find strong
transbilayer correlations of the $l_o$ domains across the two leaflets,
resulting in {\it bilayer registry}. This transbilayer correlation depends
sensitively upon the chain length of the participating lipids and possibly
other features of lipid chemistry, such as degree of saturation. We find that
the $l_o$ domains in the upper leaflet can {\em induce} phase segregation in
the lower leaflet, when the latter is nominally in the mixed ($l_d$) phase.",domain registries
http://arxiv.org/abs/1412.5052v2,"The vulnerability of the Internet has been demonstrated by prominent IP
prefix hijacking events. Major outages such as the China Telecom incident in
2010 stimulate speculations about malicious intentions behind such anomalies.
Surprisingly, almost all discussions in the current literature assume that
hijacking incidents are enabled by the lack of security mechanisms in the
inter-domain routing protocol BGP. In this paper, we discuss an attacker model
that accounts for the hijacking of network ownership information stored in
Regional Internet Registry (RIR) databases. We show that such threats emerge
from abandoned Internet resources (e.g., IP address blocks, AS numbers). When
DNS names expire, attackers gain the opportunity to take resource ownership by
re-registering domain names that are referenced by corresponding RIR database
objects. We argue that this kind of attack is more attractive than conventional
hijacking, since the attacker can act in full anonymity on behalf of a victim.
Despite corresponding incidents have been observed in the past, current
detection techniques are not qualified to deal with these attacks. We show that
they are feasible with very little effort, and analyze the risk potential of
abandoned Internet resources for the European service region: our findings
reveal that currently 73 /24 IP prefixes and 7 ASes are vulnerable to be
stealthily abused. We discuss countermeasures and outline research directions
towards preventive solutions.",domain registries
http://arxiv.org/abs/1909.07539v1,"The internationalized domain name (IDN) is a mechanism that enables us to use
Unicode characters in domain names. The set of Unicode characters contains
several pairs of characters that are visually identical with each other; e.g.,
the Latin character 'a' (U+0061) and Cyrillic character 'a' (U+0430). Visually
identical characters such as these are generally known as homoglyphs. IDN
homograph attacks, which are widely known, abuse Unicode homoglyphs to create
lookalike URLs. Although the threat posed by IDN homograph attacks is not new,
the recent rise of IDN adoption in both domain name registries and web browsers
has resulted in the threat of these attacks becoming increasingly widespread,
leading to large-scale phishing attacks such as those targeting cryptocurrency
exchange companies. In this work, we developed a framework named ""ShamFinder,""
which is an automated scheme to detect IDN homographs. Our key contribution is
the automatic construction of a homoglyph database, which can be used for
direct countermeasures against the attack and to inform users about the context
of an IDN homograph. Using the ShamFinder framework, we perform a large-scale
measurement study that aims to understand the IDN homographs that exist in the
wild. On the basis of our approach, we provide insights into an effective
counter-measure against the threats caused by the IDN homograph attack.",domain registries
http://arxiv.org/abs/1608.01019v1,"The entity registry system (ERS) is a decentralized entity registry that can
be used to replace the Web as a platform for publishing linked data when the
latter is not available. In developing countries, where off-line is the default
mode of operation, centralized linked data solutions fail to address the needs
of the communities. Although the features are mostly completed, the system is
not yet ready for deployment. This project aims to provide extensive tests and
scalability investigations that would make it ready for a real scenario.",domain registries
http://arxiv.org/abs/1612.03101v1,"This documents presents the final report of a two-year project to evaluate
the impact of AbuseHUB, a Dutch clearinghouse for acquiring and processing
abuse data on infected machines. The report was commissioned by the Netherlands
Ministry of Economic Affairs, a co-funder of the development of AbuseHUB.
AbuseHUB is the initiative of 9 Internet Service Providers, SIDN (the registry
for the .nl top-level domain) and Surfnet (the national research and education
network operator). The key objective of AbuseHUB is to improve the mitigation
of botnets by its members.
  We set out to assess whether this objective is being reached by analyzing
malware infection levels in the networks of AbuseHUB members and comparing them
to those of other Internet Service Providers (ISPs). Since AbuseHUB members
together comprise over 90 percent of the broadband market in the Netherlands,
it also makes sense to compare how the country as a whole has performed
compared to other countries. This report complements the baseline measurement
report produced in December 2013 and the interim report from March 2015. We are
using the same data sources as in the interim report, which is an expanded set
compared to the earlier baseline report and to our 2011 study into botnet
mitigation in the Netherlands.",domain registries
http://arxiv.org/abs/1111.5733v1,"The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.",domain registries
http://arxiv.org/abs/1811.04169v2,"The blockchain technology has potential applications in various areas such as
smart-contracts, Internet of Things (IoT), land registry, supply chain
management, storing medical data, and identity management. Although the Github
currently hosts more than six thousand active Blockchain software (BCS)
projects, few software engineering research has investigated these projects and
its' contributors. Although the number of BCS projects is growing rapidly, the
motivations, challenges, and needs of BCS developers remain a puzzle.
Therefore, the primary objective of this study is to understand the
motivations, challenges, and needs of BCS developers and analyze the
differences between BCS and non-BCS development. On this goal, we sent an
online survey to 1,604 active BCS developers identified via mining the Github
repositories of 145 popular BCS projects. The survey received 156 responses
that met our criteria for analysis.
  The results suggest that the majority of the BCS developers are experienced
in non-BCS development and are primarily motivated by the ideology of creating
a decentralized financial system. Although most of the BCS projects are Open
Source Software (OSS) projects by nature, more than 93% of our respondents
found BCS development somewhat different from a non-BCS development as BCS
projects have higher emphasis on security and reliability than most of the
non-BCS projects. Other differences include: higher costs of defects,
decentralized and hostile environment, technological complexity, and difficulty
in upgrading the software after release. Software development tools that are
tuned for non-BCS development are inadequate for BCS and the ecosystem needs an
array of new or improved tools, such as: customized IDE for BCS development
tasks, debuggers for smart-contracts, testing support, easily deployable
simulators, and BCS domain specific design notations.",domain registries
http://arxiv.org/abs/1602.03681v1,"The public package registry npm is one of the biggest software registry. With
its 216 911 software packages, it forms a big network of software dependencies.
In this paper we evaluate various methods for finding similar packages in the
npm network, using only the structure of the graph. Namely, we want to find a
way of categorizing similar packages, which would be useful for recommendation
systems. This size enables us to compute meaningful results, as it softened the
particularities of the graph. Npm is also quite famous as it is the default
package repository of Node.js. We believe that it will make our results
interesting for more people than a less used package repository. This makes it
a good subject of analysis of software networks.",domain registries
http://arxiv.org/abs/1512.08612v1,"At dry and clean material junctions of rigid materials the corrugation of the
sliding energy landscape is dominated by variations of Pauli repulsions. These
occur when electron clouds centered around atoms in adjacent layers overlap as
they slide across each other. In such cases there exists a direct relation
between interfacial surface (in)commensurability and superlubricity, a
frictionless and wearless tribological state. The Registry Index is a purely
geometrical parameter that quantifies the degree of interlayer
commensurability, thus providing a simple and intuitive method for the
prediction of sliding energy landscapes at rigid material interfaces. In the
present study, we extend the applicability of the Registry Index to
non-parallel surfaces, using a model system of nanotubes motion on flat
hexagonal materials. Our method successfully reproduces sliding energy
landscapes of carbon nanotubes on Graphene calculated using a Lennard-Jones
type and the Kolmogorov-Crespi interlayer potentials. Furthermore, it captures
the sliding energy corrugation of a boron nitride nanotube on hexagonal boron
nitride calculated using the h-BN ILP. Finally, we use the Registry Index to
predict the sliding energy landscapes of the heterogeneous junctions of a
carbon nanotubes on hexagonal boron nitride and of boron nitride nanotubes on
graphene that are shown to exhibit a significantly reduced corrugation. For
such rigid interfaces this is expected to be manifested by superlubric motion.",domain registries
http://arxiv.org/abs/1811.09680v1,"Token Curated Registries (TCR) are decentralized recommendation systems that
can be implemented using Blockchain smart contracts. They allow participants to
vote for or against adding items to a list through a process that involves
staking tokens intrinsic to the registry, with winners receiving the staked
tokens for each vote. A TCR aims to provide incentives to create a well-curated
list. In this work, we consider a challenge for these systems - incentivizing
token-holders to actually engage and participate in the voting process. We
propose a novel token-inflation mechanism for enhancing engagement, whereby
only voting participants see their token supply increased by a pre-defined
multiple after each round of voting. To evaluate this proposal, we propose a
simple 4-class model of voters that captures all possible combinations of two
key dimensions: whether they are engaged (likely to vote at all for a given
item) or disengaged, and whether they are informed (likely to vote in a way
that increases the quality of the list) or uninformed, and a simple metric to
evaluate the quality of the list as a function of the vote outcomes. We conduct
simulations using this model of voters and show that implementing
token-inflation results in greater wealth accumulation for engaged voters. In
particular, when the number of informed voters is sufficiently high, our
simulations show that voters that are both informed and engaged see the
greatest benefits from participating in the registry when our proposed
token-inflation mechanism is employed. We further validate this finding using a
simplified mathematical analysis.",domain registries
http://arxiv.org/abs/1308.3357v1,"Linked Data applications often assume that connectivity to data repositories
and entity resolution services are always available. This may not be a valid
assumption in many cases. Indeed, there are about 4.5 billion people in the
world who have no or limited Web access. Many data-driven applications may have
a critical impact on the life of those people, but are inaccessible to those
populations due to the architecture of today's data registries. In this paper,
we propose and evaluate a new open-source system that can be used as a
general-purpose entity registry suitable for deployment in poorly-connected or
ad-hoc environments.",domain registries
http://arxiv.org/abs/1903.03061v1,"This paper presents DIALOG (Digital Investigation Ontology); a framework for
the management, reuse, and analysis of Digital Investigation knowledge. DIALOG
provides a general, application independent vocabulary that can be used to
describe an investigation at different levels of detail. DIALOG is defined to
encapsulate all concepts of the digital forensics field and the relationships
between them. In particular, we concentrate on the Windows Registry, where
registry keys are modeled in terms of both their structure and function.
Registry analysis software tools are modeled in a similar manner and we
illustrate how the interpretation of their results can be done using the
reasoning capabilities of ontology",domain registries
http://arxiv.org/abs/1906.03300v1,"In this study, we aim to incorporate the expertise of anonymous curators into
a token-curated registry (TCR), a decentralized recommender system for
collecting a list of high-quality content. This registry is important, because
previous studies on TCRs have not specifically focused on technical content,
such as academic papers and patents, whose effective curation requires
expertise in relevant fields. To measure expertise, curation in our model
focuses on both the content and its citation relationships, for which curator
assignment uses the Personalized PageRank (PPR) algorithm while reward
computation uses a multi-task peer-prediction mechanism. Our proposed CitedTCR
bridges the literature on network-based and token-based recommender systems and
contributes to the autonomous development of an evolving citation graph for
high-quality content. Moreover, we experimentally confirm the incentive for
registration and curation in CitedTCR using the simplification of a one-to-one
correspondence between users and content (nodes).",domain registries
http://arxiv.org/abs/1611.01820v1,"Today, full-texts of scientific articles are often stored in different
locations than the used datasets. Dataset registries aim at a closer
integration by making datasets citable but authors typically refer to datasets
using inconsistent abbreviations and heterogeneous metadata (e.g. title,
publication year). It is thus hard to reproduce research results, to access
datasets for further analysis, and to determine the impact of a dataset.
Manually detecting references to datasets in scientific articles is
time-consuming and requires expert knowledge in the underlying research
domain.We propose and evaluate a semi-automatic three-step approach for finding
explicit references to datasets in social sciences articles.We first extract
pre-defined special features from dataset titles in the da|ra registry, then
detect references to datasets using the extracted features, and finally match
the references found with corresponding dataset titles. The approach does not
require a corpus of articles (avoiding the cold start problem) and performs
well on a test corpus. We achieved an F-measure of 0.84 for detecting
references in full-texts and an F-measure of 0.83 for finding correct matches
of detected references in the da|ra dataset registry.",domain registries
http://arxiv.org/abs/1106.4577v1,"There is an increasing need for automated support for humans monitoring the
activity of distributed teams of cooperating agents, both human and machine. We
characterize the domain-independent challenges posed by this problem, and
describe how properties of domains influence the challenges and their
solutions. We will concentrate on dynamic, data-rich domains where humans are
ultimately responsible for team behavior. Thus, the automated aid should
interactively support effective and timely decision making by the human. We
present a domain-independent categorization of the types of alerts a plan-based
monitoring system might issue to a user, where each type generally requires
different monitoring techniques. We describe a monitoring framework for
integrating many domain-specific and task-specific monitoring techniques and
then using the concept of value of an alert to avoid operator overload. We use
this framework to describe an execution monitoring approach we have used to
implement Execution Assistants (EAs) in two different dynamic, data-rich,
real-world domains to assist a human in monitoring team behavior. One domain
(Army small unit operations) has hundreds of mobile, geographically distributed
agents, a combination of humans, robots, and vehicles. The other domain (teams
of unmanned ground and air vehicles) has a handful of cooperating robots. Both
domains involve unpredictable adversaries in the vicinity. Our approach
customizes monitoring behavior for each specific task, plan, and situation, as
well as for user preferences. Our EAs alert the human controller when reported
events threaten plan execution or physically threaten team members. Alerts were
generated in a timely manner without inundating the user with too many alerts
(less than 10 percent of alerts are unwanted, as judged by domain experts).",domain monitoring
http://arxiv.org/abs/1106.0235v1,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",domain monitoring
http://arxiv.org/abs/1711.03952v2,"Trust in publicly verifiable Certificate Transparency (CT) logs is reduced
through cryptography, gossip, auditing, and monitoring. The role of a monitor
is to observe each and every log entry, looking for suspicious certificates
that interest the entity running the monitor. While anyone can run a monitor,
it requires continuous operation and copies of the logs to be inspected. This
has lead to the emergence of monitoring-as-a-service: a trusted party runs the
monitor and provides registered subjects with selective certificate
notifications, e.g., ""notify me of all foo.com certificates"". We present a
CT/bis extension for verifiable light-weight monitoring that enables subjects
to verify the correctness of such notifications, reducing the trust that is
placed in these monitors. Our extension supports verifiable monitoring of
wild-card domains and piggybacks on CT's existing gossip-audit security model.",domain monitoring
http://arxiv.org/abs/1809.00711v1,"Context: Adaptive monitoring is a method used in a variety of domains for
responding to changing conditions. It has been applied in different ways, from
monitoring systems' customization to re-composition, in different application
domains. However, to the best of our knowledge, there are no studies analyzing
how adaptive monitoring differs or resembles among the existing approaches.
Method: We have conducted a systematic mapping study of adaptive monitoring
approaches following recommended practices. We have applied automatic search
and snowballing sampling on different sources and used rigorous selection
criteria to retrieve the final set of papers. Moreover, we have used an
existing qualitative analysis method for extracting relevant data from studies.
Finally, we have applied data mining techniques for identifying patterns in the
solutions. Conclusions: This cross-domain overview of the current state of the
art on adaptive monitoring may be a solid and comprehensive baseline for
researchers and practitioners in the field. Especially, it may help in
identifying opportunities of research, for instance, the need of proposing
generic and flexible software engineering solutions for supporting adaptive
monitoring in a variety of systems.",domain monitoring
http://arxiv.org/abs/1611.00739v1,"There is need for several software systems within the energy domain and
corresponding systems are being developed to satisfy these needs. These systems
include energy monitoring, information, wide area monitoring and control
systems, and SCADA systems. Energy monitoring systems are one of the most
important and common systems among them. In this study, after briefly reviewing
several of the software systems within the energy domain, a centralized and
generic software architecture for energy monitoring systems is presented. Next,
sample projects are described in which energy monitoring systems based on this
architecture have been implemented. We envisage that this study will be an
important resource for software projects in the energy domain.",domain monitoring
http://arxiv.org/abs/1101.5087v1,"Central to understanding membrane bound cell signaling is to quantify how the
membrane ultra-structure consisting of transient spatial domains modulates
signaling and how the signaling influences this ultra-structure. Yet, measuring
the association of membrane proteins with domains in living, intact cells poses
considerable challenges. Here, we describe a non-destructive method to quantify
protein-lipid domain and protein cytoskeleton interactions in single, intact
cells enabling continuous monitoring of the protein domains interaction over
time during signaling.",domain monitoring
http://arxiv.org/abs/1305.7403v1,"Monitoring is an essential aspect of maintaining and developing computer
systems that increases in difficulty proportional to the size of the system.
The need for robust monitoring tools has become more evident with the advent of
cloud computing. Infrastructure as a Service (IaaS) clouds allow end users to
deploy vast numbers of virtual machines as part of dynamic and transient
architectures. Current monitoring solutions, including many of those in the
open-source domain rely on outdated concepts including manual deployment and
configuration, centralised data collection and adapt poorly to membership
churn.
  In this paper we propose the development of a cloud monitoring suite to
provide scalable and robust lookup, data collection and analysis services for
large-scale cloud systems. In lieu of centrally managed monitoring we propose a
multi-tier architecture using a layered gossip protocol to aggregate monitoring
information and facilitate lookup, information collection and the
identification of redundant capacity. This allows for a resource aware data
collection and storage architecture that operates over the system being
monitored. This in turn enables monitoring to be done in-situ without the need
for significant additional infrastructure to facilitate monitoring services. We
evaluate this approach against alternative monitoring paradigms and demonstrate
how our solution is well adapted to usage in a cloud-computing context.",domain monitoring
http://arxiv.org/abs/1905.04486v1,"Monitoring consists in deciding whether a log meets a given specification. In
this work, we propose an automata-based formalism to monitor logs in the form
of actions associated with time stamps and arbitrarily data values over
infinite domains. Our formalism uses both timing parameters and data
parameters, and is able to output answers symbolic in these parameters and in
the log segments where the property is satisfied or violated. We implemented
our approach in an ad-hoc prototype SyMon, and experiments show that its high
expressive power still allows for efficient online monitoring.",domain monitoring
http://arxiv.org/abs/1809.06573v2,"For using neural networks in safety critical domains, it is important to know
if a decision made by a neural network is supported by prior similarities in
training. We propose runtime neuron activation pattern monitoring - after the
standard training process, one creates a monitor by feeding the training data
to the network again in order to store the neuron activation patterns in
abstract form. In operation, a classification decision over an input is further
supplemented by examining if a pattern similar (measured by Hamming distance)
to the generated pattern is contained in the monitor. If the monitor does not
contain any pattern similar to the generated pattern, it raises a warning that
the decision is not based on the training data. Our experiments show that, by
adjusting the similarity-threshold for activation patterns, the monitors can
report a significant portion of misclassfications to be not supported by
training with a small false-positive rate, when evaluated on a test set.",domain monitoring
http://arxiv.org/abs/1610.05788v1,"Monitoring the execution of business processes and activities composing them
is an essential capability of Business Process Management (BPM) Suites. Human
tasks are a particular type of business activities, and the understanding of
their execution is essential in effectively managing both the processes and
human resources. This paper proposes a monitoring framework with a capability
to monitor and analyze the human tasks in a domain specific setting and
contextually correlate the task execution patterns to the workload distribution
on human users. The framework uses the notion of concept probes that match the
business concepts used in definition of business processes. The proposed human
task monitoring and contextual analysis (HTMCA) component considers multiple
artifacts involved in the execution of a human task, rather than focusing only
on classic activity/task metrics retrieved from BPM engines.This approach
aspires to provide two main advantages to organizations using it. Firstly, it
enhances the understanding of the workload of human users that participate in
people-intensive business processes under various roles. Secondly, it gives
organizations tools and insight for fine-tuning their user performance taking
into account the specific context of their business various artifacts domains.
The proposed framework builds on previous work that lays the basis of
vendor-independent, concept-centric BPM monitoring, and provides the critical
missing element of human task understanding. This has the potential to
significantly benefit any BPM deployment and the validation work is in advanced
stages of building a full prototype that demonstrates this value in a realistic
industrial setting.",domain monitoring
http://arxiv.org/abs/1606.04287v1,"Business process design and monitoring are essential elements of Business
Process Management (BPM), often relying on Service Oriented Architectures
(SOA). However the current BPM approaches and standards have not sufficiently
reduced the Business-IT gap. Today's solutions are mostly domain-independent
and platform-dependent, which limits the ability of business matter experts to
express business intent and enact process change. In contrast, the approach
presented in this paper focuses on BPM and SOA environments in a
domain-dependent and platform-independent way. We propose to add a domain
specific-layer on top of current solutions so business stakeholders can design
and understand their processes in a more intuitive way. We rely on previously
proposed technical solutions and integrate them in an end-to-end methodology
(from design to monitoring and back). The appropriateness and the feasibility
of the approach is justified through a use case and a complete prototype
implementation.",domain monitoring
http://arxiv.org/abs/1806.02290v2,"The need for countering Advanced Persistent Threat (APT) attacks has led to
the solutions that ubiquitously monitor system activities in each host, and
perform timely attack investigation over the monitoring data for analyzing
attack provenance. However, existing query systems based on relational
databases and graph databases lack language constructs to express key
properties of major attack behaviors, and often execute queries inefficiently
since their semantics-agnostic design cannot exploit the properties of system
monitoring data to speed up query execution.
  To address this problem, we propose a novel query system built on top of
existing monitoring tools and databases, which is designed with novel types of
optimizations to support timely attack investigation. Our system provides (1)
domain-specific data model and storage for scaling the storage, (2) a
domain-specific query language, Attack Investigation Query Language (AIQL) that
integrates critical primitives for attack investigation, and (3) an optimized
query engine based on the characteristics of the data and the semantics of the
queries to efficiently schedule the query execution. We deployed our system in
NEC Labs America comprising 150 hosts and evaluated it using 857 GB of real
system monitoring data (containing 2.5 billion events). Our evaluations on a
real-world APT attack and a broad set of attack behaviors show that our system
surpasses existing systems in both efficiency (124x over PostgreSQL, 157x over
Neo4j, and 16x over Greenplum) and conciseness (SQL, Neo4j Cypher, and Splunk
SPL contain at least 2.4x more constraints than AIQL).",domain monitoring
http://arxiv.org/abs/1805.01894v1,"Citizen science often requires volunteers to perform low-skill tasks such as
counting and documenting en- vironmental features. In this work, we contend
that these tasks do not adequately meet the needs of citizen scientists
motivated by scientific learning. We propose to provide intrinsic motivation by
asking them to no- tice, compare, and synthesize qualitative observations. We
describe the process of learning and performing qualitative assessments in the
domain of water quality monitoring, which appraises the impact of land use on
habitat quality and biological diversity. We use the example of water
monitoring because qualitative wa- tershed assessments are exclusively
performed by professionals, yet do not require specialized tools, making it an
excellent fit for volunteers. Within this domain, we observe and report on
differences in background and training between professional and volunteer
monitors, using these experiences to synthesize findings about volunteer
training needs. Our findings reveal that to successfully make qualitative
stream assessments, vol- unteers need to: (1) experience a diverse range of
streams, (2) discuss judgments with other monitors, and (3) construct internal
narratives about water quality. We use our findings to describe how different
technologies may support these needs and generalize our findings to the larger
citizen science community.",domain monitoring
http://arxiv.org/abs/0710.4698v1,"Automated synthesis of monitors from high-level properties plays a
significant role in assertion-based verification. We present here a methodology
to synthesize assertion monitors from visual specifications given in CESC
(Clocked Event Sequence Chart). CESC is a visual language designed for
specifying system level interactions involving single and multiple clock
domains. It has well-defined graphical and textual syntax and formal semantics
based on synchronous language paradigm enabling formal analysis of
specifications. In this paper we provide an overview of CESC language with few
illustrative examples. The algorithm for automated synthesis of assertion
monitors from CESC specifications is described. A few examples from standard
bus protocols (OCP-IP and AMBA) are presented to demonstrate the application of
monitor synthesis algorithm.",domain monitoring
http://arxiv.org/abs/1301.7496v1,"Passive monitoring utilizing distributed wireless sniffers is an effective
technique to monitor activities in wireless infrastructure networks for fault
diagnosis, resource management and critical path analysis. In this paper, we
introduce a quality of monitoring (QoM) metric defined by the expected number
of active users monitored, and investigate the problem of maximizing QoM by
judiciously assigning sniffers to channels based on the knowledge of user
activities in a multi-channel wireless network. Two types of capture models are
considered. The user-centric model assumes frame-level capturing capability of
sniffers such that the activities of different users can be distinguished while
the sniffer-centric model only utilizes the binary channel information (active
or not) at a sniffer. For the user-centric model, we show that the implied
optimization problem is NP-hard, but a constant approximation ratio can be
attained via polynomial complexity algorithms. For the sniffer-centric model,
we devise stochastic inference schemes to transform the problem into the
user-centric domain, where we are able to apply our polynomial approximation
algorithms. The effectiveness of our proposed schemes and algorithms is further
evaluated using both synthetic data as well as real-world traces from an
operational WLAN.",domain monitoring
http://arxiv.org/abs/1802.06270v1,"Distributed monitoring plays a crucial role in managing the activities of
cloud-based datacenters. System administrators have long relied on monitoring
systems such as Nagios and Ganglia to obtain status alerts on their
desktop-class machines. However, the popularity of mobile devices is pushing
the community to develop datacenter monitoring solutions for smartphone-class
devices. Here we lay out desirable characteristics of such smartphone-based
monitoring and identify quantitatively the shortcomings from directly applying
existing solutions to this domain. Then we introduce a possible design that
addresses some of these shortcomings and provide results from an early
prototype, called MAVIS, using one month of monitoring data from approximately
3,000 machines hosted by Purdue's central IT organization.",domain monitoring
http://arxiv.org/abs/1804.02422v1,"Predictive process monitoring has recently gained traction in academia and is
maturing also in companies. However, with the growing body of research, it
might be daunting for companies to navigate in this domain in order to find,
provided certain data, what can be predicted and what methods to use. The main
objective of this paper is developing a value-driven framework for classifying
existing work on predictive process monitoring. This objective is achieved by
systematically identifying, categorizing, and analyzing existing approaches for
predictive process monitoring. The review is then used to develop a
value-driven framework that can support organizations to navigate in the
predictive process monitoring field and help them to find value and exploit the
opportunities enabled by these analysis techniques.",domain monitoring
http://arxiv.org/abs/1803.01598v1,"Ransomware, a type of malicious software that encrypts a victim's files and
only releases the cryptographic key once a ransom is paid, has emerged as a
potentially devastating class of cybercrimes in the past few years. In this
paper, we present RAPTOR, a promising line of defense against ransomware
attacks. RAPTOR fingerprints attackers' operations to forecast ransomware
activity. More specifically, our method learns features of malicious domains by
looking at examples of domains involved in known ransomware attacks, and then
monitors newly registered domains to identify potentially malicious ones. In
addition, RAPTOR uses time series forecasting techniques to learn models of
historical ransomware activity and then leverages malicious domain
registrations as an external signal to forecast future ransomware activity. We
illustrate RAPTOR's effectiveness by forecasting all activity stages of Cerber,
a popular ransomware family. By monitoring zone files of the top-level domain
.top starting from August 30, 2016 through May 31, 2017, RAPTOR predicted 2,126
newly registered domains to be potential Cerber domains. Of these, 378 later
actually appeared in blacklists. Our empirical evaluation results show that
using predicted domain registrations helped improve forecasts of future Cerber
activity. Most importantly, our approach demonstrates the value of fusing
different signals in forecasting applications in the cyber domain.",domain monitoring
http://arxiv.org/abs/cs/0109111v2,"We propose a quality of service (QoS) monitoring program for broadband access
to measure the impact of proprietary network spaces. Our paper surveys other
QoS policy initiatives, including those in the airline, and wireless and
wireline telephone industries, to situate broadband in the context of other
markets undergoing regulatory devolution. We illustrate how network
architecture can create impediments to open communications, and how QoS
monitoring can detect such effects. We present data from a field test of
QoS-monitoring software now in development. We suggest QoS metrics to gauge
whether information ""walled gardens"" represent a real threat for dividing the
Internet into proprietary spaces.
  To demonstrate our proposal, we are placing our software on the computers of
a sample of broadband subscribers. The software periodically conducts a battery
of tests that assess the quality of connections from the subscriber's computer
to various content sites. Any systematic differences in connection quality
between affiliated and non-affiliated content sites would warrant research into
the behavioral implications of those differences.
  QoS monitoring is timely because the potential for the Internet to break into
a loose network of proprietary content domains appears stronger than ever.
Recent court rulings and policy statements suggest a growing trend towards
relaxed scrutiny of mergers and the easing or elimination of content ownership
rules. This policy environment could lead to a market with a small number of
large, vertically integrated network operators, each pushing its proprietary
content on subscribers.",domain monitoring
http://arxiv.org/abs/0901.2685v1,"Soft real-time applications require timely delivery of messages conforming to
the soft real-time constraints. Satisfying such requirements is a complex task
both due to the volatile nature of distributed environments, as well as due to
numerous domain-specific factors that affect message latency. Prompt detection
of the root-cause of excessive message delay allows a distributed system to
react accordingly. This may significantly improve compliance with the required
timeliness constraints.
  In this work, we present a novel approach for distributed performance
monitoring of soft-real time distributed systems. We propose to employ recent
distributed algorithms from the statistical signal processing and learning
domains, and to utilize them in a different context of online performance
monitoring and root-cause analysis, for pinpointing the reasons for violation
of performance requirements. Our approach is general and can be used for
monitoring of any distributed system, and is not limited to the soft real-time
domain.
  We have implemented the proposed framework in TransFab, an IBM prototype of
soft real-time messaging fabric. In addition to root-cause analysis, the
framework includes facilities to resolve resource allocation problems, such as
memory and bandwidth deficiency. The experiments demonstrate that the system
can identify and resolve latency problems in a timely fashion.",domain monitoring
http://arxiv.org/abs/1409.0400v1,"A lot of current buildings are operated energy inefficient and offer a great
potential to reduce the overall energy consumption and CO2 emission. Detecting
these inefficiencies is a complicated task and needs domain experts that are
able to identify them. Most approaches try to support detection by focussing on
monitoring the building's operation and visualizing data. Instead our approach
focuses on using techniques taken from the cyber-physical systems' modeling
domain. We create a model of the building and show how we constrain the model
by OCL-like rules to support a sound specification which can be matched against
monitoring results afterwards. The paper presents our domain-specific language
for modeling buildings and technical facilities that is implemented in a
software-based tool used by domain experts and thus hopefully providing a
suitable contribution to modeling the cyber-physical world.",domain monitoring
http://arxiv.org/abs/1810.00647v2,"Talaia is a platform for monitoring social media and digital press. A
configurable crawler gathers content with respect to user defined domains or
topics. Crawled data is processed by means of the EliXa Sentiment Analysis
system. A Django powered interface provides data visualization for a user-based
analysis of the data. This paper presents the architecture of the system and
describes in detail its different components. To prove the validity of the
approach, two real use cases are accounted for: one in the cultural domain and
one in the political domain. Evaluation for the sentiment analysis task in both
scenarios is also provided, showing the capacity for domain adaptation.",domain monitoring
http://arxiv.org/abs/1810.09425v2,"Across numerous applications, forecasting relies on numerical solvers for
partial differential equations (PDEs). Although the use of deep-learning
techniques has been proposed, actual applications have been restricted by the
fact the training data are obtained using traditional PDE solvers. Thereby, the
uses of deep-learning techniques were limited to domains, where the PDE solver
was applicable.
  We demonstrate a deep-learning framework for air-pollution monitoring and
forecasting that provides the ability to train across different model domains,
as well as a reduction in the run-time by two orders of magnitude. It presents
a first-of-a-kind implementation that combines deep-learning and
domain-decomposition techniques to allow model deployments extend beyond the
domain(s) on which the model has been trained.",domain monitoring
http://arxiv.org/abs/cs/0407024v1,"Fairly rapid environmental changes call for continuous surveillance and
on-line decision making. There are two main areas where IT technologies can be
valuable. In this paper we present a multi-agent system for monitoring and
assessing air-quality attributes, which uses data coming from a meteorological
station. A community of software agents is assigned to monitor and validate
measurements coming from several sensors, to assess air-quality, and, finally,
to fire alarms to appropriate recipients, when needed. Data mining techniques
have been used for adding data-driven, customized intelligence into agents. The
architecture of the developed system, its domain ontology, and typical agent
interactions are presented. Finally, the deployment of a real-world test case
is demonstrated.",domain monitoring
http://arxiv.org/abs/1504.04757v2,"In runtime verification, the central problem is to decide if a given program
execution violates a given property. In online runtime verification, a monitor
observes a program's execution as it happens. If the program being observed has
hard real-time constraints, then the monitor inherits them. In the presence of
hard real-time constraints it becomes a challenge to maintain enough
information to produce error traces, should a property violation be observed.
In this paper we introduce a data structure, called tree buffer, that solves
this problem in the context of automata-based monitors: If the monitor itself
respects hard real-time constraints, then enriching it by tree buffers makes it
possible to provide error traces, which are essential for diagnosing defects.
We show that tree buffers are also useful in other application domains. For
example, they can be used to implement functionality of capturing groups in
regular expressions. We prove optimal asymptotic bounds for our data structure,
and validate them using empirical data from two sources: regular expression
searching through Wikipedia, and runtime verification of execution traces
obtained from the DaCapo test suite.",domain monitoring
http://arxiv.org/abs/1504.06836v1,"We discuss the design and ongoing development of the Monitoring Extreme-scale
Lustre Toolkit (MELT), a unified Lustre performance monitoring and analysis
infrastructure that provides continuous, low-overhead summary information on
the health and performance of Lustre, as well as on-demand, in- depth problem
diagnosis and root-cause analysis. The MELT infrastructure leverages a
distributed overlay network to enable monitoring of center-wide Lustre
filesystems where clients are located across many network domains. We preview
interactive command-line utilities that help administrators and users to
observe Lustre performance at various levels of resolution, from individual
servers or clients to whole filesystems, including job-level reporting.
Finally, we discuss our future plans for automating the root-cause analysis of
common Lustre performance problems.",domain monitoring
http://arxiv.org/abs/1407.2394v1,"Wireless tomography is a technique for inferring a physical environment
within a monitored region by analyzing RF signals traversed across the region.
In this paper, we consider wireless tomography in a two and higher
dimensionally structured monitored region, and propose a multi-dimensional
wireless tomography scheme based on compressed sensing to estimate a spatial
distribution of shadowing loss in the monitored region. In order to estimate
the spatial distribution, we consider two compressed sensing frameworks:
vector-based compressed sensing and tensor-based compressed sensing. When the
shadowing loss has a high spatial correlation in the monitored region, the
spatial distribution has a sparsity in its frequency domain. Existing wireless
tomography schemes are based on the vector-based compressed sensing and
estimates the distribution by utilizing the sparsity. On the other hand, the
proposed scheme is based on the tensor-based compressed sensing, which
estimates the distribution by utilizing its low-rank property. We reveal that
the tensor-based compressed sensing has a potential for highly accurate
estimation as compared with the vector-based compressed sensing.",domain monitoring
http://arxiv.org/abs/1904.04896v1,"Measuring performance of an automatic speech recognition (ASR) system without
ground-truth could be beneficial in many scenarios, especially with data from
unseen domains, where performance can be highly inconsistent. In conventional
ASR systems, several performance monitoring (PM) techniques have been
well-developed to monitor performance by looking at tri-phone posteriors or
pre-softmax activations from neural network acoustic modeling. However,
strategies for monitoring more recently developed end-to-end ASR systems have
not yet been explored, and so that is the focus of this paper. We adapt
previous PM measures (Entropy, M-measure and Auto-encoder) and apply our
proposed RNN predictor in the end-to-end setting. These measures utilize the
decoder output layer and attention probability vectors, and their predictive
power is measured with simple linear models. Our findings suggest that
decoder-level features are more feasible and informative than attention-level
probabilities for PM measures, and that M-measure on the decoder posteriors
achieves the best overall predictive performance with an average prediction
error 8.8%. Entropy measures and RNN-based prediction also show competitive
predictability, especially for unseen conditions.",domain monitoring
http://arxiv.org/abs/1908.02366v2,"With the increasing number of smart services implemented in smart cities, it
is important yet challenging to dynamically detect service conflicts with
respect to safety and performance requirements. In this paper, we propose a
framework for monitoring the operation of smart cities and services at runtime.
We formalize a set of typical safety and performance requirements from
different domains in smart cities (e.g., transportation, emergency, and
environment) using Signal Temporal Logic. We present a case study based on a
smart city simulator, in which actions of smart services and their predicted
effects on city states are converted into signal traces over time and monitored
continuously using formal specifications. The experimental results demonstrate
the feasibility of using runtime monitoring to detect various conflicts of
smart services.",domain monitoring
http://arxiv.org/abs/1005.3148v1,"In the current Internet, there is no clean way for affected parties to react
to poor forwarding performance: when a domain violates its Service Level
Agreement (SLA) with a contractual partner, the partner must resort to ad-hoc
probing-based monitoring to determine the existence and extent of the
violation. Instead, we propose a new, systematic approach to the problem of
forwarding-performance verification. Our mechanism relies on voluntary
reporting, allowing each domain to disclose its loss and delay performance to
its neighbors; it does not disclose any information regarding the participating
domains' topology or routing policies beyond what is already publicly
available. Most importantly, it enables verifiable performance measurements,
i.e., domains cannot abuse it to significantly exaggerate their performance.
Finally, our mechanism is tunable, allowing each participating domain to
determine how many resources to devote to it independently (i.e., without any
inter-domain coordination), exposing a controllable trade-off between
performance-verification quality and resource consumption. Our mechanism comes
at the cost of deploying modest functionality at the participating domains'
border routers; we show that it requires reasonable processing and memory
resources within modern network capabilities.",domain monitoring
http://arxiv.org/abs/1703.00446v2,"The development of a system that would ease the diagnosis of heart diseases
would also fasten the work of the cardiologic department in hospitals and
facilitate the monitoring of patients with portable devices. This paper
presents a tool for ECG signal analysis which is designed in Matlab. The
Hermite transform domain is exploited for the analysis. The proposed transform
domain is very convenient for ECG signal analysis and classification. Parts of
the ECG signals, i.e. QRS complexes, show shape similarity with the Hermite
basis functions, which is one of the reasons for choosing this domain. Also,
the information about the signal can be represented using a small set of
coefficients in this domain, which makes data transmission and analysis faster.
The signal concentration in the Hermite domain and consequently, the number of
samples required for signal representation, can additionally be reduced by
performing the parametization of the Hermite transform. For the comparison
purpose, the Fourier transform domain is also implemented within the software,
in order to compare the signal concentration in two transform domains.",domain monitoring
http://arxiv.org/abs/1701.00728v1,"In this work, we present the results of a systematic study to investigate the
(commercial) benefits of automatic text summarization systems in a real world
scenario. More specifically, we define a use case in the context of media
monitoring and media response analysis and claim that even using a simple
query-based extractive approach can dramatically save the processing time of
the employees without significantly reducing the quality of their work.",domain monitoring
http://arxiv.org/abs/1709.09302v3,"We consider a market in which capacity-constrained generators compete in
scalar-parameterized supply functions to serve an inelastic demand spread
throughout a transmission constrained power network. The market clears
according to a locational marginal pricing mechanism, in which the independent
system operator (ISO) determines the generators' production quantities to
minimize the revealed cost of meeting demand, while ensuring that network
transmission and generator capacity constraints are met. Under the stylizing
assumption that both the ISO and generators choose their strategies
simultaneously, we establish the existence of Nash equilibria for the
underlying market, and derive an upper bound on the allocative efficiency loss
at Nash equilibrium relative to the socially optimal level. We also
characterize an upper bound on the markup of locational marginal prices at Nash
equilibrium above their perfectly competitive levels. Of particular relevance
to ex ante market power monitoring, these bounds reveal the role of certain
market structures---specifically, the \emph{market share} and \emph{residual
supply index} of a producer---in predicting the degree to which that producer
is able to exercise market power to influence the market outcome to its
advantage. Finally, restricting our attention to the simpler setting of a
two-node power network, we provide a characterization of market structures
under which a Braess-like paradox occurs due to the exercise of market
power---that is to say, we provide a necessary and sufficient condition on
market structure under which the strengthening of the network's transmission
line capacity results in the (counterintuitive) increase in the total cost of
generation at Nash equilibrium.",market monitoring
http://arxiv.org/abs/cs/0306024v1,"The DESY Computer Center is the home of O(1000) computers supplying a wide
range of different services Monitoring such a large installation is a
challenge. After a long time running a SNMP based commercial Network Management
System, the evaluation of a new System was started. There are a lot of
different commercial and freeware products on the market, but none of them
fully satisfied all our requirements. After re-valuating our original
requirements we selected NAGIOS as our monitoring and alarming tool. After a
successful test we are in production since autumn 2002 and are extending the
service to fully support a distributed monitoring and alarming.",market monitoring
http://arxiv.org/abs/1607.03583v1,"This paper analyzes repeated multimarket contact with observation errors
where two players operate in multiple markets simultaneously. Multimarket
contact has received much attention in economics, management, and so on.
Despite vast empirical studies that examine whether multimarket contact fosters
cooperation or collusion, little is theoretically known as to how players
behave in an equilibrium when each player receives a noisy and different
observation or signal indicating other firms' actions (private monitoring). To
the best of our knowledge, we are the first to construct a strategy designed
for multiple markets whose per-market equilibrium payoffs exceed one for a
single market, in our setting. We first construct an entirely novel strategy
whose behavior is specified by a non-linear function of the signal
configurations. We then show that the per-market equilibrium payoff improves
when the number of markets is sufficiently large.",market monitoring
http://arxiv.org/abs/1809.01640v1,"The paper presents web based information system for heat supply monitoring.
The proposed model and information system for gathering data from heating
station heat-flow meters and regulators is software realized. The novel system
with proved functionality can be commercialized at the cost of minimal
investments, finding wildly use on Bulgarian market as cheap and quality
alternative of the western systems.",market monitoring
http://arxiv.org/abs/1602.07063v2,"This article investigates graph analysis for intelligent marketing in smart
cities, where metatrails are crowdsourced by mobile sensing for marketing
strategies. Unlike most works that focused on client sides, this study is
intended for market planning, from the perspective of enterprises. Several
novel crowdsourced features based on metatrails, including hotspot networks,
crowd transitions, affinity subnetworks, and sequential visiting patterns, are
discussed in the article. These smart footprints can reflect crowd preferences
and the topology of a site of interest. Marketers can utilize such information
for commercial resource planning and deployment. Simulations were conducted to
demonstrate the performance. At the end, this study also discusses different
scenarios for practical geo-conquesting applications.",market monitoring
http://arxiv.org/abs/1608.04143v2,"We investigate the problem of market mechanism design for wind energy. We
consider a dynamic two-step model with one strategic seller with wind
generation and one buyer, who trade energy through a mechanism determined by a
designer. The seller has private information about his technology and wind
condition, which he learns dynamically over time. We consider (static) forward
and real-time mechanisms that take place at time T=1 and T=2, respectively. We
also propose a dynamic mechanism that provides a coupling between the outcomes
of the forward and real-time markets. We show that the dynamic mechanism
outperforms the forward and real-time mechanisms for a general objective of the
designer. Therefore, we demonstrate the advantage of adopting dynamic market
mechanisms over static market mechanisms for wind energy. The dynamic mechanism
reveals information about wind generation in advance, and also provides
flexibility for incorporation of new information arriving over time. We discuss
how our results generalize to environments with many strategic sellers. We also
study two variants of the dynamic mechanism that guarantee no penalty risk for
the seller, and/or monitor the wind condition. We illustrate our results with a
numerical example.",market monitoring
http://arxiv.org/abs/cs/0405028v1,"In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.",market monitoring
http://arxiv.org/abs/cs/0109111v2,"We propose a quality of service (QoS) monitoring program for broadband access
to measure the impact of proprietary network spaces. Our paper surveys other
QoS policy initiatives, including those in the airline, and wireless and
wireline telephone industries, to situate broadband in the context of other
markets undergoing regulatory devolution. We illustrate how network
architecture can create impediments to open communications, and how QoS
monitoring can detect such effects. We present data from a field test of
QoS-monitoring software now in development. We suggest QoS metrics to gauge
whether information ""walled gardens"" represent a real threat for dividing the
Internet into proprietary spaces.
  To demonstrate our proposal, we are placing our software on the computers of
a sample of broadband subscribers. The software periodically conducts a battery
of tests that assess the quality of connections from the subscriber's computer
to various content sites. Any systematic differences in connection quality
between affiliated and non-affiliated content sites would warrant research into
the behavioral implications of those differences.
  QoS monitoring is timely because the potential for the Internet to break into
a loose network of proprietary content domains appears stronger than ever.
Recent court rulings and policy statements suggest a growing trend towards
relaxed scrutiny of mergers and the easing or elimination of content ownership
rules. This policy environment could lead to a market with a small number of
large, vertically integrated network operators, each pushing its proprietary
content on subscribers.",market monitoring
http://arxiv.org/abs/1806.05914v1,"Cloud services are becoming increasingly popular: 60\% of information
technology spending in 2016 was Cloud-based, and the size of the public Cloud
service market will reach \$236B by 2020. To ensure reliable operation of the
Cloud services, one must monitor their health.
  While a number of research challenges in the area of Cloud monitoring have
been solved, problems are remaining. This prompted us to highlight three areas,
which cause problems to practitioners and require further research. These three
areas are as follows: A) defining health states of Cloud systems, B) creating
unified monitoring environments, and C) establishing high availability
strategies.
  In this paper we provide details of these areas and suggest a number of
potential solutions to the challenges. We also show that Cloud monitoring
presents exciting opportunities for novel research and practice.",market monitoring
http://arxiv.org/abs/1201.2207v1,"We consider the problem of information fusion from multiple sensors of
different types with the objective of improving the confidence of inference
tasks, such as object classification, performed from the data collected by the
sensors. We propose a novel technique based on distributed belief aggregation
using a multi-agent prediction market to solve this information fusion problem.
To monitor the improvement in the confidence of the object classification as
well as to dis-incentivize agents from misreporting information, we have
introduced a market maker that rewards the agents instantaneously as well as at
the end of the inference task, based on the quality of the submitted reports.
We have implemented the market maker's reward calculation in the form of a
scoring rule and have shown analytically that it incentivizes truthful
revelation or accurate reporting by each agent. We have experimentally verified
our technique for multi-sensor information fusion for an automated landmine
detection scenario. Our experimental results show that, for identical data
distributions and settings, using our information aggregation technique
increases the accuracy of object classification favorably as compared to two
other commonly used techniques for information fusion for landmine detection.",market monitoring
http://arxiv.org/abs/1606.06510v1,"Aggregators are playing an increasingly crucial role in the integration of
renewable generation in power systems. However, the intermittent nature of
renewable generation makes market interactions of aggregators difficult to
monitor and regulate, raising concerns about potential market manipulation by
aggregators. In this paper, we study this issue by quantifying the profit an
aggregator can obtain through strategic curtailment of generation in an
electricity market. We show that, while the problem of maximizing the benefit
from curtailment is hard in general, efficient algorithms exist when the
topology of the network is radial (acyclic). Further, we highlight that
significant increases in profit are possible via strategic curtailment in
practical settings.",market monitoring
http://arxiv.org/abs/1507.02043v1,"Today's cellular telecommunications markets require continuous monitoring and
intervention by regulators in order to balance the interests of various
stakeholders. In order to reduce the extent of regulatory involvements in the
day-to-day business of cellular operators, the present paper proposes a
""self-regulating"" spectrum market regime named ""society spectrum"". This regime
provides a market-inherent and automatic self-balancing of stakeholder powers,
which at the same time provides a series of coordination and fairness assurance
functions that clearly distinguish it from ""spectrum as a commons"" solutions.
The present paper will introduce the fundamental regulatory design and will
elaborate on mechanisms to assure fairness among stakeholders and individuals.
This work further puts the society spectrum into the context of contemporary
radio access technologies and cognitive radio approaches.",market monitoring
http://arxiv.org/abs/1705.03233v4,"Managing the prediction of metrics in high-frequency financial markets is a
challenging task. An efficient way is by monitoring the dynamics of a limit
order book to identify the information edge. This paper describes the first
publicly available benchmark dataset of high-frequency limit order markets for
mid-price prediction. We extracted normalized data representations of time
series data for five stocks from the NASDAQ Nordic stock market for a time
period of ten consecutive days, leading to a dataset of ~4,000,000 time series
samples in total. A day-based anchored cross-validation experimental protocol
is also provided that can be used as a benchmark for comparing the performance
of state-of-the-art methodologies. Performance of baseline approaches are also
provided to facilitate experimental comparisons. We expect that such a
large-scale dataset can serve as a testbed for devising novel solutions of
expert systems for high-frequency limit order book data analysis.",market monitoring
http://arxiv.org/abs/1802.02996v1,"The difficulty of large scale monitoring of app markets affects our
understanding of their dynamics. This is particularly true for dimensions such
as app update frequency, control and pricing, the impact of developer actions
on app popularity, as well as coveted membership in top app lists. In this
paper we perform a detailed temporal analysis on two datasets we have collected
from the Google Play Store, one consisting of 160,000 apps and the other of
87,223 newly released apps. We have monitored and collected data about these
apps over more than 6 months. Our results show that a high number of these apps
have not been updated over the monitoring interval. Moreover, these apps are
controlled by a few developers that dominate the total number of app downloads.
We observe that infrequently updated apps significantly impact the median app
price. However, a changing app price does not correlate with the download
count. Furthermore, we show that apps that attain higher ranks have better
stability in top app lists. We show that app market analytics can help detect
emerging threat vectors, and identify search rank fraud and even malware.
Further, we discuss the research implications of app market analytics on
improving developer and user experiences.",market monitoring
http://arxiv.org/abs/1510.07385v1,"Twitter is now a gold marketing tool for entities concerned with online
reputation. To automatically monitor online reputation of entities , systems
have to deal with ambiguous entity names, polarity detection and topic
detection. We propose three approaches to tackle the first issue: monitoring
Twitter in order to find relevant tweets about a given entity. Evaluated within
the framework of the RepLab-2013 Filtering task, each of them has been shown
competitive with state-of-the-art approaches. Mainly we investigate on how much
merging strategies may impact performances on a filtering task according to the
evaluation measure.",market monitoring
http://arxiv.org/abs/1211.6512v1,"Recent research has focused on the monitoring of global-scale online data for
improved detection of epidemics, mood patterns, movements in the stock market,
political revolutions, box-office revenues, consumer behaviour and many other
important phenomena. However, privacy considerations and the sheer scale of
data available online are quickly making global monitoring infeasible, and
existing methods do not take full advantage of local network structure to
identify key nodes for monitoring. Here, we develop a model of the contagious
spread of information in a global-scale, publicly-articulated social network
and show that a simple method can yield not just early detection, but advance
warning of contagious outbreaks. In this method, we randomly choose a small
fraction of nodes in the network and then we randomly choose a ""friend"" of each
node to include in a group for local monitoring. Using six months of data from
most of the full Twittersphere, we show that this friend group is more central
in the network and it helps us to detect viral outbreaks of the use of novel
hashtags about 7 days earlier than we could with an equal-sized randomly chosen
group. Moreover, the method actually works better than expected due to network
structure alone because highly central actors are both more active and exhibit
increased diversity in the information they transmit to others. These results
suggest that local monitoring is not just more efficient, it is more effective,
and it is possible that other contagious processes in global-scale networks may
be similarly monitored.",market monitoring
http://arxiv.org/abs/1502.00206v2,"Cloud computing provides on-demand access to affordable hardware (multi-core
CPUs, GPUs, disks, and networking equipment) and software (databases,
application servers and data processing frameworks) platforms with features
such as elasticity, pay-per-use, low upfront investment and low time to market.
This has led to the proliferation of business critical applications that
leverage various cloud platforms. Such applications hosted on single or
multiple cloud provider platforms have diverse characteristics requiring
extensive monitoring and benchmarking mechanisms to ensure run-time Quality of
Service (QoS) (e.g., latency and throughput). This paper proposes, develops and
validates CLAMBS:Cross-Layer Multi-Cloud Application Monitoring and
Benchmarking as-a-Service for efficient QoS monitoring and benchmarking of
cloud applications hosted on multi-clouds environments. The major highlight of
CLAMBS is its capability of monitoring and benchmarking individual application
components such as databases and web servers, distributed across cloud layers,
spread among multiple cloud providers. We validate CLAMBS using prototype
implementation and extensive experimentation and show that CLAMBS efficiently
monitors and benchmarks application components on multi-cloud platforms
including Amazon EC2 and Microsoft Azure.",market monitoring
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",market monitoring
http://arxiv.org/abs/1301.1444v2,"We study an economic decision problem where the actors are two firms and the
Antitrust Authority whose main task is to monitor and prevent firms' potential
anti-competitive behaviour and its effect on the market. The Antitrust
Authority's decision process is modelled using a Bayesian network where both
the relational structure and the parameters of the model are estimated from a
data set provided by the Authority itself. A number of economic variables that
influence this decision process are also included in the model. We analyse how
monitoring by the Antitrust Authority affects firms' strategies about
cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma
using object-oriented Bayesian networks. We show how the integration of firms'
decision process and external market information can be modelled in this way.
Various decision scenarios and strategies are illustrated.",market monitoring
http://arxiv.org/abs/1503.01061v4,"We investigate a hierarchically organized cloud infrastructure and compare
distributed hierarchical control based on resource monitoring with market
mechanisms for resource management. The latter do not require a model of the
system, incur a low overhead, are robust, and satisfy several other desiderates
of autonomic computing. We introduce several performance measures and report on
simulation studies which show that a straightforward bidding scheme supports an
effective admission control mechanism, while reducing the communication
complexity by several orders of magnitude and also increasing the acceptance
rate compared to hierarchical control and monitoring mechanisms. Resource
management based on market-based mechanisms can be seen as an intermediate step
towards cloud self-organization, an ideal alternative to current mechanisms for
cloud resource management.",market monitoring
http://arxiv.org/abs/physics/0603196v1,"We construct a correlation matrix based financial network for a set of New
York Stock Exchange (NYSE) traded stocks with stocks corresponding to nodes and
the links between them added one after the other, according to the strength of
the correlation between the nodes. The eigenvalue spectrum of the correlation
matrix reflects the structure of the market, which also shows in the cluster
structure of the emergent network. The stronger and more compact a cluster is,
the earlier the eigenvalue representing the corresponding business sector
occurs in the spectrum. On the other hand, if groups of stocks belonging to a
given business sector are considered as a fully connected subgraph of the final
network, their intensity and coherence can be monitored as a function of time.
This approach indicates to what extent the business sector classifications are
visible in market prices, which in turn enables us to gauge the extent of
group-behaviour exhibited by stocks belonging to a given business sector.",market monitoring
http://arxiv.org/abs/1003.5438v1,"Mobile data services are penetrating mobile markets rapidly. The mobile
industry relies heavily on data service to replace the traditional voice
services with the evolution of the wireless technology and market. A reliable
packet service network is critical to the mobile operators to maintain their
core competence in data service market. Furthermore, mobile operators need to
develop effective operational models to manage the varying mix of voice, data
and video traffic on a single network. Application of statistical models could
prove to be an effective approach. This paper first introduces the architecture
of Universal Mobile Telecommunications System (UMTS) packet switched (PS)
network and then applies multivariate statistical analysis to Key Performance
Indicators (KPI) monitored from network entities in UMTS PS network to guide
the long term capacity planning for the network. The approach proposed in this
paper could be helpful to mobile operators in operating and maintaining their
3G packet switched networks for the long run.",market monitoring
http://arxiv.org/abs/1703.10279v1,"As affordability pressures and tight rental markets in global cities mount,
online shared accommodation sites proliferate. Home sharing arrangements
present dilemmas for planning that aims to improve health and safety standards,
while supporting positives such as the usage of dormant stock and the relieving
of rental pressures on middle/lower income earners. Currently, no formal data
exists on this internationally growing trend. Here, we present a first
quantitative glance on shared accommodation practices across all major urban
centers of Australia enabled via collection and analysis of thousands of online
listings. We examine, countrywide, the spatial and short time scale temporal
characteristics of this market, along with preliminary analysis on rents,
dwelling types and other characteristics. Findings have implications for
housing policy makers and planning practitioners seeking to monitor and respond
to housing policy and affordability pressures in formal and informal housing
markets.",market monitoring
http://arxiv.org/abs/1805.03513v1,"The mobile ad-hoc networks (MANETs) represent a broad area of study and
market interest. They provide a wide set of applications in multiple domains.
In that context, the functional and non-functional monitoring of these networks
is crucial. For that purpose, monitoring techniques have been deeply studied in
wired networks using gossip-based or hierarchical-based approaches. However,
when applied to a MANET, several problematics arise mainly due to the absence
of a centralized administration, the inherent MANETs constraints and the nodes
mobility. In this paper, we present a hybrid distributed monitoring
architecture for mobile adhoc networks in context of mobility pattern. We get
inspired of gossip-based and hierarchical-based algorithms for query
dissemination and data aggregation. We define gossip-based mechanisms that help
our virtual hierarchical topology to complete the data aggregation, and then
ensure the stability and robustness of our approach in dynamic environments.
Further, we propose a fully distributed monitoring protocol that ease the nodes
communications. We evaluate our approach through a simulated testbed by using
NS3 and Docker, and illustrate the efficiency of our mechanisms.",market monitoring
http://arxiv.org/abs/1407.3077v1,"A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.",consumer profiling algorithm
http://arxiv.org/abs/1411.3961v2,"Loyalty programs are promoted by vendors to incentivize loyalty in buyers.
Although such programs have become widespread, they have been criticized by
business experts and consumer associations: loyalty results in profiling and
hence in loss of privacy of consumers. We propose a protocol for
privacy-preserving loyalty programs that allows vendors and consumers to enjoy
the benefits of loyalty (returning customers and discounts, respectively),
while allowing consumers to stay anonymous and empowering them to decide how
much of their profile they reveal to the vendor. The vendor must offer
additional reward if he wants to learn more details on the consumer's profile.
Our protocol is based on partially blind signatures and generalization
techniques, and provides anonymity to consumers and their purchases, while
still allowing negotiated consumer profiling.",consumer profiling algorithm
http://arxiv.org/abs/1604.08330v1,"Server consolidation based on virtualization technology simplifies system
administration and improves energy efficiency by improving resource
utilizations and reducing the physical machine (PM) number in contemporary
service-oriented data centers. The elasticity of Internet applications changes
the consolidation technologies from addressing virtual machines (VMs) to PMs
mapping schemes which must know the VMs statuses, i.e. the number of VMs and
the profiling data of each VM, into providing the application-to-VM-to-PM
mapping. In this paper, we study on the consolidation of multiple Internet
applications, minimizing the number of PMs with required performance. We first
model the consolidation providing the application-to-VM-to-PM mapping to
minimize the number of PMs as an integer linear programming problem, and then
present a heuristic algorithm to solve the problem in polynomial time.
Extensive experimental results show that our heuristic algorithm consumes less
than 4.3% more resources than the optimal amounts with few overheads. Existing
consolidation technologies using the input of the VM statuses output by our
heuristic algorithm consume 1.06% more PMs.",consumer profiling algorithm
http://arxiv.org/abs/1208.4651v1,"In wireless networks, energy consumed for communication includes both the
transmission and the processing energy. In this paper, point-to-point
communication over a fading channel with an energy harvesting transmitter is
studied considering jointly the energy costs of transmission and processing.
Under the assumption of known energy arrival and fading profiles, optimal
transmission policy for throughput maximization is investigated. Assuming that
the transmitter has sufficient amount of data in its buffer at the beginning of
the transmission period, the average throughput by a given deadline is
maximized. Furthermore, a ""directional glue pouring algorithm"" that computes
the optimal transmission policy is described.",consumer profiling algorithm
http://arxiv.org/abs/1304.5197v1,"Identifying the hottest paths in the control flow graph of a routine can
direct optimizations to portions of the code where most resources are consumed.
This powerful methodology, called path profiling, was introduced by Ball and
Larus in the mid 90s and has received considerable attention in the last 15
years for its practical relevance. A shortcoming of Ball-Larus path profiling
was the inability to profile cyclic paths, making it difficult to mine
interesting execution patterns that span multiple loop iterations. Previous
results, based on rather complex algorithms, have attempted to circumvent this
limitation at the price of significant performance losses already for a small
number of iterations. In this paper, we present a new approach to multiple
iterations path profiling, based on data structures built on top of the
original Ball-Larus numbering technique. Our approach allows it to profile all
executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths,
where k is a user-defined parameter. An extensive experimental investigation on
a large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,
our approach can be even faster than Ball-Larus due to fewer operations on
smaller hash tables, producing compact representations of cyclic paths even for
large values of k.",consumer profiling algorithm
http://arxiv.org/abs/1907.12219v1,"JPEG is one of the popular image compression algorithms that provide
efficient storage and transmission capabilities in consumer electronics, and
hence it is the most preferred image format over the internet world. In the
present digital and Big-data era, a huge volume of JPEG compressed document
images are being archived and communicated through consumer electronics on
daily basis. Though it is advantageous to have data in the compressed form on
one side, however, on the other side processing with off-the-shelf methods
becomes computationally expensive because it requires decompression and
recompression operations. Therefore, it would be novel and efficient, if the
compressed data are processed directly in their respective compressed domains
of consumer electronics. In the present research paper, we propose to
demonstrate this idea taking the case study of printed text line segmentation.
Since, JPEG achieves compression by dividing the image into non overlapping 8x8
blocks in the pixel domain and using Discrete Cosine Transform (DCT); it is
very likely that the partitioned 8x8 DCT blocks overlap the contents of two
adjacent text-lines without leaving any clue for the line separator, thus
making text-line segmentation a challenging problem. Two approaches of
segmentation have been proposed here using the DC projection profile and AC
coefficients of each 8x8 DCT block. The first approach is based on the strategy
of partial decompression of selected DCT blocks, and the second approach is
with intelligent analysis of F10 and F11 AC coefficients and without using any
type of decompression. The proposed methods have been tested with variable font
sizes, font style and spacing between lines, and a good performance is
reported.",consumer profiling algorithm
http://arxiv.org/abs/1401.2440v1,"Future e-business models will rely on electronic contracts which are agreed
dynamically and adaptively by web services. Thus, the automatic negotiation of
Service Level Agreements (SLAs) between consumers and providers is key for
enabling service-based value chains.
  The process of finding appropriate providers for web services seems to be
simple. Consumers contact several providers and take the provider which offers
the best matching SLA. However, currently consumers are not able forecasting
the probability of finding a matching provider for their requested SLA. So
consumers contact several providers and check if their offers are matching. In
case of continuing faults, on the one hand consumers may adapt their Service
Level Objects (SLOs) of the required SLA or on the other hand simply accept
offered SLAs of the contacted providers.
  By forecasting the probability of finding a matching provider, consumers
could assess their chances of finding a provider offering the requested SLA. If
a low probability is predicted, consumers can immediately adapt their SLOs or
increase the numbers of providers to be contacted.
  Thus, this paper proposes an analytical forecast model, which allows
consumers to get a realistic assessment of the probability to find matching
providers. Additionally, we present an optimization algorithm based on the
forecast results, which allows adapting the SLO parameter ranges in order to
find at least one matching provider. Not only consumers, but also providers can
use this forecast model to predict the prospective demand. So providers are
able to assess the number of potential consumers based on their offers too.
  Justification of our approach is done by simulation of practical examples
checking our theoretical findings.",consumer profiling algorithm
http://arxiv.org/abs/1110.5351v2,"We report on the implementation of a dynamically configurable, servomotor-
controlled, permanent magnet Zeeman slower for quantum optics experiments with
ultracold atoms and molecules. This atom slower allows for switching between
magnetic field profiles that are designed for different atomic species.
Additionally, through feedback on the atom trapping rate, we demonstrate that
computer-controlled genetic optimization algorithms applied to the magnet
positions can be used in situ to obtain field profiles that maximize the
trapping rate for any given experimental conditions. The device is lightweight,
remotely controlled, and consumes no power in steady state; it is a step toward
automated control of quantum optics experiments.",consumer profiling algorithm
http://arxiv.org/abs/1502.02125v2,"The last decade has witnessed a tremendous growth in the volume as well as
the diversity of multimedia content generated by a multitude of sources (news
agencies, social media, etc.). Faced with a variety of content choices,
consumers are exhibiting diverse preferences for content; their preferences
often depend on the context in which they consume content as well as various
exogenous events. To satisfy the consumers' demand for such diverse content,
multimedia content aggregators (CAs) have emerged which gather content from
numerous multimedia sources. A key challenge for such systems is to accurately
predict what type of content each of its consumers prefers in a certain
context, and adapt these predictions to the evolving consumers' preferences,
contexts and content characteristics. We propose a novel, distributed, online
multimedia content aggregation framework, which gathers content generated by
multiple heterogeneous producers to fulfill its consumers' demand for content.
Since both the multimedia content characteristics and the consumers'
preferences and contexts are unknown, the optimal content aggregation strategy
is unknown a priori. Our proposed content aggregation algorithm is able to
learn online what content to gather and how to match content and users by
exploiting similarities between consumer types. We prove bounds for our
proposed learning algorithms that guarantee both the accuracy of the
predictions as well as the learning speed. Importantly, our algorithms operate
efficiently even when feedback from consumers is missing or content and
preferences evolve over time. Illustrative results highlight the merits of the
proposed content aggregation system in a variety of settings.",consumer profiling algorithm
http://arxiv.org/abs/1609.04053v1,"The arrival of small-scale distributed energy generation in the future smart
grid has led to the emergence of so-called prosumers, who can both consume as
well as produce energy. By using local generation from renewable energy
resources, the stress on power generation and supply system can be
significantly reduced during high demand periods. However, this also creates a
significant challenge for conventional power plants that suddenly need to ramp
up quickly when the renewable energy drops off. In this paper, we propose an
energy consumption scheduling problem for prosumers to minimize the peak ramp
of the system. The optimal schedule of prosumers can be obtained by solving the
centralized optimization problem. However, due to the privacy concerns and the
distributed topology of the power system, the centralized design is difficult
to implement in practice. Therefore, we propose the distributed algorithms to
efficiently solve the centralized problem using the alternating direction
method of multiplier (ADMM), in which each prosumer independently schedules its
energy consumption profile. The simulation results demonstrate the convergence
performance of the proposed algorithms as well as the capability of our model
in reducing the peak ramp of the system.",consumer profiling algorithm
http://arxiv.org/abs/1507.03328v1,"In this paper, we propose the amphibious influence maximization (AIM) model
that combines traditional marketing via content providers and viral marketing
to consumers in social networks in a single framework. In AIM, a set of content
providers and consumers form a bipartite network while consumers also form
their social network, and influence propagates from the content providers to
consumers and among consumers in the social network following the independent
cascade model. An advertiser needs to select a subset of seed content providers
and a subset of seed consumers, such that the influence from the seed providers
passing through the seed consumers could reach a large number of consumers in
the social network in expectation.
  We prove that the AIM problem is NP-hard to approximate to within any
constant factor via a reduction from Feige's k-prover proof system for 3-SAT5.
We also give evidence that even when the social network graph is trivial (i.e.
has no edges), a polynomial time constant factor approximation for AIM is
unlikely. However, when we assume that the weighted bi-adjacency matrix that
describes the influence of content providers on consumers is of constant rank,
a common assumption often used in recommender systems, we provide a
polynomial-time algorithm that achieves approximation ratio of
$(1-1/e-\epsilon)^3$ for any (polynomially small) $\epsilon > 0$. Our
algorithmic results still hold for a more general model where cascades in
social network follow a general monotone and submodular function.",consumer profiling algorithm
http://arxiv.org/abs/1809.05245v1,"Achieving a balance of supply and demand in a multi-agent system with many
individual self-interested and rational agents that act as suppliers and
consumers is a natural problem in a variety of real-life domains---smart power
grids, data centers, and others. In this paper, we address the
profit-maximization problem for a group of distributed supplier and consumer
agents, with no inter-agent communication. We simulate a scenario of a market
with $S$ suppliers and $C$ consumers such that at every instant, each supplier
agent supplies a certain quantity and simultaneously, each consumer agent
consumes a certain quantity. The information about the total amount supplied
and consumed is only kept with the center. The proposed algorithm is a
combination of the classical additive-increase multiplicative-decrease (AIMD)
algorithm in conjunction with a probabilistic rule for the agents to respond to
a capacity signal. This leads to a nonhomogeneous Markov chain and we show
almost sure convergence of this chain to the social optimum, for our market of
distributed supplier and consumer agents. Employing this AIMD-type algorithm,
the center sends a feedback message to the agents in the supplier side if there
is a scenario of excess supply, or to the consumer agents if there is excess
consumption. Each agent has a concave utility function whose derivative tends
to 0 when an optimum quantity is supplied/consumed. Hence when social
convergence is reached, each agent supplies or consumes a quantity which leads
to its individual maximum profit, without the need of any communication. So
eventually, each agent supplies or consumes a quantity which leads to its
individual maximum profit, without communicating with any other agents. Our
simulations show the efficacy of this approach.",consumer profiling algorithm
http://arxiv.org/abs/1908.10713v1,"Non-intrusive load monitoring (NILM) has been extensively researched over the
last decade. The objective of NILM is to identify the power consumption of
individual appliances and to detect when particular devices are on or off from
measuring the power consumption of an entire house. This information allows
households to receive customized advice on how to better manage their
electrical consumption. In this paper, we present an alternative NILM method
that breaks down the aggregated power signal into categories of appliances. The
ultimate goal is to use this approach for demand-side management to estimate
potential flexibility within the electricity consumption of households. Our
method is implemented as an algorithm combining NILM and load profile
simulation. This algorithm, based on a Markov model, allocates an activity
chain to each inhabitant of the household, deduces from the whole-house power
measurement and statistical data the appliance usage, generate the power
profile accordingly and finally returns the share of energy consumed by each
appliance category over time. To analyze its performance, the algorithm was
benchmarked against several state-of-the-art NILM algorithms and tested on
three public datasets. The proposed algorithm is unsupervised; hence it does
not require any labeled data, which are expensive to acquire. Although better
performance is shown for the supervised algorithms, our proposed unsupervised
algorithm achieves a similar range of uncertainty while saving on the cost of
acquiring labeled data. Additionally, our method requires lower computational
power compared to most of the tested NILM algorithms. It was designed for
low-sampling-rate power measurement (every 15 min), which corresponds to the
frequency range of most common smart meters.",consumer profiling algorithm
http://arxiv.org/abs/1803.03560v2,"In this paper, we propose a distributed control strategy for the design of an
energy market. The method relies on a hierarchical structure of aggregators for
the coordination of prosumers (agents which can produce and consume energy).
The hierarchy reflects the voltage level separations of the electrical grid and
allows aggregating prosumers in pools, while taking into account the grid
operational constraints. To reach optimal coordination, the prosumers
communicate their forecasted power profile to the upper level of the hierarchy.
Each time the information crosses upwards a level of the hierarchy, it is first
aggregated, both to strongly reduce the data flow and to preserve the privacy.
In the first part of the paper, the decomposition algorithm, which is based on
the alternating direction method of multipliers (ADMM), is presented. In the
second part, we explore how the proposed algorithm scales with increasing
number of prosumers and hierarchical levels, through extensive simulations
based on randomly generated scenarios.",consumer profiling algorithm
http://arxiv.org/abs/1805.11646v1,"Gradient index (GRIN) acoustic devices have spatially inhomogeneous
refractive index profile and allow flexible control of the propagation of
acoustic waves. Previous GRIN acoustic lenses are mostly inherently
two-dimensional designs that are difficult to be extended to all three
dimensions. Besides, manually designing the spatially inhomogeneous structure
is both time-consuming and error-prone. In this work, we proposed and
numerically verified an automated computer-aided design tool: GRadient Index
Pick-and-Place (GRIPP) algorithm, for generating three-dimensional GRIN
acoustic wave controlling devices with scalable and 3D printable structures.
The algorithm receives as inputs a spatial distribution of refractive index and
a pre-defined library of gradient index unit cells, and outputs a 3D model of
GRIN device that is ready to be 3D printed. The tool enables rapid design and
realization of a large variety of 3D GRIN acoustic devices, which can be useful
in areas such as speaker system design, airborne ultrasonic sensing, as well as
therapeutic ultrasound.",consumer profiling algorithm
http://arxiv.org/abs/1806.09542v1,"Mapping and translating professional but arcane clinical jargons to consumer
language is essential to improve the patient-clinician communication.
Researchers have used the existing biomedical ontologies and consumer health
vocabulary dictionary to translate between the languages. However, such
approaches are limited by expert efforts to manually build the dictionary,
which is hard to be generalized and scalable. In this work, we utilized the
embeddings alignment method for the word mapping between unparalleled clinical
professional and consumer language embeddings. To map semantically similar
words in two different word embeddings, we first independently trained word
embeddings on both the corpus with abundant clinical professional terms and the
other with mainly healthcare consumer terms. Then, we aligned the embeddings by
the Procrustes algorithm. We also investigated the approach with the
adversarial training with refinement. We evaluated the quality of the alignment
through the similar words retrieval both by computing the model precision and
as well as judging qualitatively by human. We show that the Procrustes
algorithm can be performant for the professional consumer language embeddings
alignment, whereas adversarial training with refinement may find some relations
between two languages.",consumer profiling algorithm
http://arxiv.org/abs/1802.08112v1,"A rational behavior of a consumer is analyzed when the user participates in a
Peak Time Rebate (PTR) mechanism, which is a demand response (DR) incentive
program based on a baseline. A multi-stage stochastic programming is proposed
from the demand side in order to understand the rational decisions. The
consumer preferences are modeled as a risk-averse function under additive
uncertainty. The user chooses the optimal consumption profile to maximize his
economic benefits for each period. The stochastic optimization problem is
solved backward in time. A particular situation is developed when the System
Operator (SO) uses consumption of the previous interval as the
household-specific baseline for the DR program. It is found that a rational
consumer alters the baseline in order to increase the well-being when there is
an economic incentive. As results, whether the incentive is lower than the
retail price, the user shifts his load requirement to the baseline setting
period. On the other hand, if the incentive is greater than the regular energy
price, the optimal decision is that the user spends the maximum possible energy
in the baseline setting period and reduces the consumption at the PTR time.
This consumer behavior produces more energy consumption in total considering
all periods. In addition, the user with high uncertainty level in his energy
pattern should spend less energy than a predictable consumer when the incentive
is lower than the retail price.",consumer profiling algorithm
http://arxiv.org/abs/1608.01244v1,"This paper introduces a new scheme for autonomous electricity cooperatives,
called predictive cooperative (PCP), which aggregates commercial and
residential electricity consumers and participates in the electricity market on
behalf of its members. An axiomatic approach is proposed to calculate the
day-ahead bid and to disaggregate the collective cost among participating
consumers. The resulting formulation is shown to keep the members incentivized
to both participate in the cooperative and remain truthful in reporting their
expected loads. The scheme is implemented using PJM (world's largest wholesale
electricity market) real-time and day-ahead price data for 2015 and a
collection of residential and commercial load profiles. The model performance
of this framework is compared to that of real-time pricing (RTP) scheme, in
which wholesale market prices are directly applied to individual consumers. The
results show truthful load announcement by consumers, reduction in electricity
price variation for all consumers, and comparative benefits for participants.",consumer profiling algorithm
http://arxiv.org/abs/1608.01658v1,"Metastatic presence in lymph nodes is one of the most important prognostic
variables of breast cancer. The current diagnostic procedure for manually
reviewing sentinel lymph nodes, however, is very time-consuming and subjective.
Pathologists have to manually scan an entire digital whole-slide image (WSI)
for regions of metastasis that are sometimes only detectable under high
resolution or entirely hidden from the human visual cortex. From October 2015
to April 2016, the International Symposium on Biomedical Imaging (ISBI) held
the Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for
automatic detection of lymph node metastasis. Using a generalizable stain
normalization technique and the Proscia Pathology Cloud computing platform, we
trained a deep convolutional neural network on millions of tissue and tumor
image tiles to perform slide-based evaluation on our testing set of whole-slide
images images, with a sensitivity of 0.96, specificity of 0.89, and AUC score
of 0.90. Our results indicate that our platform can automatically scan any WSI
for metastatic regions without institutional calibration to respective stain
profiles.",consumer profiling algorithm
http://arxiv.org/abs/1803.11560v1,"Learning through experience is time-consuming, inefficient and often bad for
your cortisol levels. To address this problem, a number of recently proposed
teacher-student methods have demonstrated the benefits of private tuition, in
which a single model learns from an ensemble of more experienced tutors.
Unfortunately, the cost of such supervision restricts good representations to a
privileged minority. Unsupervised learning can be used to lower tuition fees,
but runs the risk of producing networks that require extracurriculum learning
to strengthen their CVs and create their own LinkedIn profiles. Inspired by the
logo on a promotional stress ball at a local recruitment fair, we make the
following three contributions. First, we propose a novel almost no supervision
training algorithm that is effective, yet highly scalable in the number of
student networks being supervised, ensuring that education remains affordable.
Second, we demonstrate our approach on a typical use case: learning to bake,
developing a method that tastily surpasses the current state of the art.
Finally, we provide a rigorous quantitive analysis of our method, proving that
we have access to a calculator. Our work calls into question the long-held
dogma that life is the best teacher.",consumer profiling algorithm
http://arxiv.org/abs/1811.11272v1,"In recent years, service-oriented-based Internet of Things (IoT) has received
massive attention from research and industry. Integrating and composing smart
objects functionalities or their services is required to create and promote
more complex IoT applications with advanced features. When many smart objects
are deployed, selecting the most appropriate set of smart objects to compose a
service by considering both energy and quality of service (QoS) is an essential
and challenging task. In this paper, we reduced the problem of finding an
optimal balance between QoS level and the consumed energy of the IoT service
composition to a bi-objective shortest path optimization (BSPO) problem and
used an exact algorithm named pulse to solve the problem. The BSPO has two
objectives, minimizing the QoS including execution time, network latency, and
service price, and minimize the energy consumption of the composite service.
Experimental evaluations show that the proposed approach has short execution
time in various complex service profiles. Meanwhile, it can obtain good
performance in energy consumption and thus network lifetime while maintaining a
reasonable QoS level.",consumer profiling algorithm
http://arxiv.org/abs/1906.07840v1,"Porting code from CPU to GPU is costly and time-consuming; Unless much time
is invested in development and optimization, it is not obvious, a priori, how
much speed-up is achievable or how much room is left for improvement. Knowing
the potential speed-up a priori can be very useful: It can save hundreds of
engineering hours, help programmers with prioritization and algorithm
selection. We aim to address this problem using machine learning in a
supervised setting, using solely the single-threaded source code of the
program, without having to run or profile the code. We propose a static
analysis-based cross-architecture performance prediction framework (Static
XAPP) which relies solely on program properties collected using static analysis
of the CPU source code and predicts whether the potential speed-up is above or
below a given threshold. We offer preliminary results that show we can achieve
94% accuracy in binary classification, in average, across different thresholds",consumer profiling algorithm
http://arxiv.org/abs/1206.3634v1,"We explore a novel theoretical model for studying the performance of
distributed storage management systems where the data-centers have limited
capacities (as compared to storage space requested by the users). Prior schemes
such as Balls-into-bins (used for load balancing) neither consider bin
(consumer) capacities (multiple balls into a bin) nor the future performance of
the system after, balls (producer requests) are allocated to bins and restrict
number of balls as a function of the number of bins. Our problem consists of
finding an optimal assignment of the online producer requests to consumers (via
weighted edges) in a complete bipartite graph while ensuring that the total
size of request assigned on a consumer is limited by its capacity. The metric
used to measure the performance in this model is the (minimization of) weighted
sum of the requests assigned on the edges (loads) and their corresponding
weights. We first explore the optimal offline algorithms followed by
competitive analysis of different online techniques. Using oblivious adversary.
LP and Primal-Dual algorithms are used for calculating the optimal offline
solution in O(r*n) time (where r and n are the number of requests and consumers
respectively) while randomized algorithms are used for the online case.
  For the simplified model with equal consumer capacities an average-case
competitive ratio of AVG(d) / MIN(d) (where d is the edge weight / distance) is
achieved using an algorithm that has equal probability for selecting any of the
available edges with a running time of $O(r)$. In the extending the model to
arbitrary consumer capacities we show an average case competitive ratio of
AVG(d*c) / (AVG(c) *MIN(d)).",consumer profiling algorithm
http://arxiv.org/abs/0908.1789v2,"There is an increasing need for high density data storage devices driven by
the increased demand of consumer electronics. In this work, we consider a data
storage system that operates by encoding information as topographic profiles on
a polymer medium. A cantilever probe with a sharp tip (few nm radius) is used
to create and sense the presence of topographic profiles, resulting in a
density of few Tb per in.2. The prevalent mode of using the cantilever probe is
the static mode that is harsh on the probe and the media. In this article, the
high quality factor dynamic mode operation, that is less harsh on the media and
the probe, is analyzed. The read operation is modeled as a communication
channel which incorporates system memory due to inter-symbol interference and
the cantilever state. We demonstrate an appropriate level of abstraction of
this complex nanoscale system that obviates the need for an involved physical
model. Next, a solution to the maximum likelihood sequence detection problem
based on the Viterbi algorithm is devised. Experimental and simulation results
demonstrate that the performance of this detector is several orders of
magnitude better than the performance of other existing schemes.",consumer profiling algorithm
http://arxiv.org/abs/1909.13345v1,"Given n jobs with release dates, deadlines and processing times we consider
the problem of scheduling them on m parallel machines so as to minimize the
total energy consumed. Machines can enter a sleep state and they consume no
energy in this state. Each machine requires Q units of energy to awaken from
the sleep state and in its active state the machine can process jobs and
consumes a unit of energy per unit time. We allow for preemption and migration
of jobs and provide the first constant approximation algorithm for this
problem.",consumer profiling algorithm
http://arxiv.org/abs/1810.02895v1,"Consumer genetic testing has become immensely popular in recent years and has
lead to the creation of large scale genetic databases containing millions of
dense autosomal genotype profiles. One of the most used features offered by
genetic databases is the ability to find distant relatives using a technique
called relative matching (or DNA matching). Recently, novel uses of relative
matching were discovered that combined matching results with genealogical
information to solve criminal cold cases. New estimates suggest that relative
matching, combined with simple demographic information, could be used to
re-identify a significant percentage of US Caucasian individuals. In this work
we attempt to systematize computer security and privacy risks from relative
matching and describe new security problems that can occur if an attacker
uploads manipulated or forged genetic profiles. For example, forged profiles
can be used by criminals to misdirect investigations, con-artists to defraud
victims, or political operatives to blackmail opponents. We discuss solutions
to mitigate these threats, including existing proposals to use digital
signatures, and encourage the consumer genetics community to consider the
broader security implications of relative matching now that it is becoming so
prominent.",consumer profiling algorithm
http://arxiv.org/abs/1701.08757v1,"In coming years residential consumers will face real-time electricity tariffs
with energy prices varying day to day, and effective energy saving will require
automation - a recommender system, which learns consumer's preferences from her
actions. A consumer chooses a scenario of home appliance use to balance her
comfort level and the energy bill. We propose a Bayesian learning algorithm to
estimate the comfort level function from the history of appliance use. In
numeric experiments with datasets generated from a simulation model of a
consumer interacting with small home appliances the algorithm outperforms
popular regression analysis tools. Our approach can be extended to control an
air heating and conditioning system, which is responsible for up to half of a
household's energy bill.",consumer profiling algorithm
http://arxiv.org/abs/1501.04850v1,"With the rapid development of applications in open distributed environments
such as eCommerce, privacy of information is becoming a critical issue. Today,
many online companies are gathering information and have assembled
sophisticated databases that know a great deal about many people, generally
without the knowledge of those people. Such information changes hands or
ownership as a normal part of eCommerce transactions, or through strategic
decisions that often includes the sale of users' information to other firms.
The key commercial value of users' personal information derives from the
ability of firms to identify consumers and charge them personalized prices for
goods and services they have previously used or may wish to use in the future.
A look at present-day practices reveals that consumers' profile data is now
considered as one of the most valuable assets owned by online businesses. In
this thesis, we argue the following: if consumers' private data is such a
valuable asset, should they not be entitled to commercially benefit from their
asset as well? The scope of this thesis is on developing architecture for
privacy payoff as a means of rewarding consumers for sharing their personal
information with online businesses. The architecture is a multi-agent system in
which several agents employ various requirements for personal information
valuation and interaction capabilities that most users cannot do on their own.
The agents in the system bear the responsibility of working on behalf of
consumers to categorize their personal data objects, report to consumers on
online businesses' trustworthiness and reputation, determine the value of their
compensation using risk-based financial models, and, finally, negotiate for a
payoff value in return for the dissemination of users' information.",consumer profiling algorithm
http://arxiv.org/abs/1606.01403v1,"Mass-market mobile security threats have increased recently due to the growth
of mobile technologies and the popularity of mobile devices. Accordingly,
techniques have been introduced for identifying, classifying, and defending
against mobile threats utilizing static, dynamic, on-device, off-device, and
hybrid approaches. In this paper, we contribute to the mobile security defense
posture by introducing Andro-profiler, a hybrid behavior based analysis and
classification system for mobile malware. Andro-profiler classifies malware by
exploiting the behavior profiling extracted from the integrated system logs
including system calls, which are implicitly equivalent to distinct behavior
characteristics. Andro-profiler executes a malicious application on an emulator
in order to generate the integrated system logs, and creates human-readable
behavior profiles by analyzing the integrated system logs. By comparing the
behavior profile of malicious application with representative behavior profile
for each malware family, Andro-profiler detects and classifies it into malware
families. The experiment results demonstrate that Andro-profiler is scalable,
performs well in detecting and classifying malware with accuracy greater than
$98\%$, outperforms the existing state-of-the-art work, and is capable of
identifying zero-day mobile malware samples.",behavioral profiling
http://arxiv.org/abs/1506.01675v1,"Data aggregators collect large amount of information about individual users
and create detailed online behavioral profiles of individuals. Behavioral
profiles benefit users by improving products and services. However, they have
also raised concerns regarding user privacy, transparency of collection
practices and accuracy of data in the profiles. To improve transparency, some
companies are allowing users to access their behavioral profiles. In this work,
we investigated behavioral profiles of users by utilizing these access
mechanisms. Using in-person interviews (n=8), we analyzed the data shown in the
profiles, elicited user concerns, and estimated accuracy of profiles. We
confirmed our interview findings via an online survey (n=100). To assess the
claim of improving transparency, we compared data shown in profiles with the
data that companies have about users. More than 70% of the participants
expressed concerns about collection of sensitive data such as credit and health
information, level of detail and how their data may be used. We found a large
gap between the data shown in profiles and the data possessed by companies. A
large number of profiles were inaccurate with as much as 80% inaccuracy. We
discuss implications for public policy management.",behavioral profiling
http://arxiv.org/abs/1909.10012v1,"Users on Twitter are identified with the help of their profile attributes
that consists of username, display name, profile image, to name a few. The
profile attributes that users adopt can reflect their interests, belief, or
thematic inclinations. Literature has proposed the implications and
significance of profile attribute change for a random population of users.
However, the use of profile attribute for endorsements and to start a movement
have been under-explored. In this work, we consider #LokSabhaElections2019 as a
movement and perform a large-scale study of the profile of users who actively
made changes to profile attributes centered around #LokSabhaElections2019. We
collect the profile metadata for 49.4M users for a period of 2 months from
April 5, 2019 to June 5, 2019 amid #LokSabhaElections2019. We investigate how
the profile changes vary for the influential leaders and their followers over
the social movement. We further differentiate the organic and inorganic ways to
show the political inclination from the prism of profile changes. We report how
the addition of election campaign related keywords lead to spread of behavior
contagion and further investigate it with respect to ""Chowkidar Movement"" in
detail.",behavioral profiling
http://arxiv.org/abs/0807.1153v1,"We propose behavior-oriented services as a new paradigm of communication in
mobile human networks. Our study is motivated by the tight user-network
coupling in future mobile societies. In such a paradigm, messages are sent to
inferred behavioral profiles, instead of explicit IDs. Our paper provides a
systematic framework in providing such services. First, user behavioral
profiles are constructed based on traces collected from two large wireless
networks, and their spatio-temporal stability is analyzed. The implicit
relationship discovered between mobile users could be utilized to provide a
service for message delivery and discovery in various network environments. As
an example application, we provide a detailed design of such a service in
challenged opportunistic network architecture, named CSI. We provide a fully
distributed solution using behavioral profile space gradients and small world
structures.
  Our analysis shows that user behavioral profiles are surprisingly stable,
i.e., the similarity of the behavioral profile of a user to its future
behavioral profile is above 0.8 for two days and 0.75 for one week, and remains
above 0.6 for five weeks. The correlation coefficient of the similarity metrics
between a user pair at different time instants is above 0.7 for four days, 0.62
for a week, and remains above 0.5 for two weeks. Leveraging such a stability in
user behaviors, the CSI service achieves delivery rate very close to the
delay-optimal strategy (above 94%), with minimal overhead (less than 84% of the
optimal). We believe that this new paradigm will act as an enabler of multiple
new services in mobile societies, and is potentially applicable in
server-based, heterogeneous or infrastructure-less wireless environments.",behavioral profiling
http://arxiv.org/abs/1703.09745v2,"Users of electronic devices, e.g., laptop, smartphone, etc. have
characteristic behaviors while surfing the Web. Profiling this behavior can
help identify the person using a given device. In this paper, we introduce a
technique to profile users based on their web transactions. We compute several
features extracted from a sequence of web transactions and use them with
one-class classification techniques to profile a user. We assess the efficacy
and speed of our method at differentiating 25 users on a dataset representing 6
months of web traffic monitoring from a small company network.",behavioral profiling
http://arxiv.org/abs/1003.0466v1,"Analysing Online Social Networks (OSN), voluntarily maintained and
automatically exploitable databases of electronic personal information,
promises a wealth of insight into their users' behavior, interest, and
utilization of these currently predominant services on the Internet. To
understand popularity in OSN, we monitored a large sample of profiles from a
highly popular network for three months, and analysed the relation between
profile properties and their impression frequency. Evaluating the data
indicates a strong relation between both the number of accepted contacts and
the diligence of updating contacts versus the frequency of requests for a
profile. Counter intuitively, the overall activity, gender, as well as
participation span of users have no remarkable impact on their profile's
popularity.",behavioral profiling
http://arxiv.org/abs/1407.3950v1,"The analysis of user behavior in digital games has been aided by the
introduction of user telemetry in game development, which provides
unprecedented access to quantitative data on user behavior from the installed
game clients of the entire population of players. Player behavior telemetry
datasets can be exceptionally complex, with features recorded for a varying
population of users over a temporal segment that can reach years in duration.
Categorization of behaviors, whether through descriptive methods (e.g.
segmention) or unsupervised/supervised learning techniques, is valuable for
finding patterns in the behavioral data, and developing profiles that are
actionable to game developers. There are numerous methods for unsupervised
clustering of user behavior, e.g. k-means/c-means, Non-negative Matrix
Factorization, or Principal Component Analysis. Although all yield behavior
categorizations, interpretation of the resulting categories in terms of actual
play behavior can be difficult if not impossible. In this paper, a range of
unsupervised techniques are applied together with Archetypal Analysis to
develop behavioral clusters from playtime data of 70,014 World of Warcraft
players, covering a five year interval. The techniques are evaluated with
respect to their ability to develop actionable behavioral profiles from the
dataset.",behavioral profiling
http://arxiv.org/abs/1802.03500v1,"Electricity users are the major players of the electric systems, and
electricity consumption is growing at an extraordinary rate. The research on
electricity consumption behaviors is becoming increasingly important to design
and deployment of the electric systems. Unfortunately, electricity load
profiles are difficult to acquire. Data synthesis is one of the best approaches
to solving the lack of data, and the key is the model that preserves the real
electricity consumption behaviors. In this paper, we propose a hierarchical
multi-matrices Markov Chain (HMMC) model to synthesize scalable electricity
load profiles that preserve the real consumption behavior on three time scales:
per day, per week, and per year. To promote the research on the electricity
consumption behavior, we use the HMMC approach to model two distinctive raw
electricity load profiles. One is collected from the resident sector, and the
other is collected from the non-resident sectors, including different
industries such as education, finance, and manufacturing. The experiments show
our model performs much better than the classical Markov Chain model. We
publish two trained models online, and researchers can directly use these
trained models to synthesize scalable electricity load profiles for further
researches.",behavioral profiling
http://arxiv.org/abs/1908.06869v1,"The world sees a proliferation of machine learning/deep learning (ML) models
and their wide adoption in different application domains recently. This has
made the profiling and characterization of ML models an increasingly pressing
task for both hardware designers and system providers, as they would like to
offer the best possible computing system to serve ML models with the desired
latency, throughput, and energy requirements while maximizing resource
utilization. Such an endeavor is challenging as the characteristics of an ML
model depend on the interplay between the model, framework, system libraries,
and the hardware (or the HW/SW stack). A thorough characterization requires
understanding the behavior of the model execution across the HW/SW stack
levels. Existing profiling tools are disjoint, however, and only focus on
profiling within a particular level of the stack. This paper proposes a leveled
profiling design that leverages existing profiling tools to perform
across-stack profiling. The design does so in spite of the profiling overheads
incurred from the profiling providers. We coupled the profiling capability with
an automatic analysis pipeline to systematically characterize 65
state-of-the-art ML models. Through this characterization, we show that our
across-stack profiling solution provides insights (which are difficult to
discern otherwise) on the characteristics of ML models, ML frameworks, and GPU
hardware.",behavioral profiling
http://arxiv.org/abs/1603.07728v1,"Wang and Castillo have developed empirical parameters for scaling the
temperature profile of the turbulent boundary layer flowing over a heated wall
in the paper X. Wang and L. Castillo, J. Turbul., 4, 1(2003). They presented
experimental data plots that showed similarity type behavior when scaled with
their new scaling parameters. However, what was actually plotted, and what
actually showed similarity type behavior, was not the temperature profile but
the defect profile formed by subtracting the temperature in the boundary layer
from the temperature in the bulk flow. We show that if the same data and same
scaling is replotted as just the scaled temperature profile, similarity is no
longer prevalent. This failure to show both defect profile similarity and
temperature profile similarity is indicative of false similarity. The nature of
this false similarity problem is discussed in detail.",behavioral profiling
http://arxiv.org/abs/1507.06951v4,"Zagarola and Smits developed an empirical velocity parameter for scaling the
outer region of the turbulent boundary layer velocity profile that has been
widely applied to experimental datasets. Plots of the scaled defect profiles
indicate that most datasets display similar-like behavior using the Zagarola
and Smits scaling parameter. In the work herein, it is shown that the common
practice of finding similarity behavior using the defect profile is often
incomplete in the sense that not all of the criteria for similarity have been
checked for compliance. When full compliance is checked, it is found that most
of the datasets which display defect similarity do not satisfy all the criteria
required for similarity. The nature of this contradiction and noncompliance is
described in detail. It is shown that the original datasets used by Zagarola
and Smits display this flawed similarity behavior. Hence, a careful
reassessment of any claims in the literature is required for those groups that
attempted to use the defect profile and the Zagarola and Smits type of velocity
scaling parameter to assert similarity of the velocity profile.",behavioral profiling
http://arxiv.org/abs/1109.1421v1,"The behavior of parallel programs is even harder to understand than the
behavior of sequential programs. Parallel programs may suffer from any of the
performance problems affecting sequential programs, as well as from several
problems unique to parallel systems. Many of these problems are quite hard (or
even practically impossible) to diagnose without help from specialized tools.
We present a proposal for a tool for profiling the parallel execution of
Mercury programs, a proposal whose implementation we have already started. This
tool is an adaptation and extension of the ThreadScope profiler that was first
built to help programmers visualize the execution of parallel Haskell programs.",behavioral profiling
http://arxiv.org/abs/1002.2202v1,"Currently, criminals profile (CP) is obtained from investigators or forensic
psychologists interpretation, linking crime scene characteristics and an
offenders behavior to his or her characteristics and psychological profile.
This paper seeks an efficient and systematic discovery of nonobvious and
valuable patterns between variables from a large database of solved cases via a
probabilistic network (PN) modeling approach. The PN structure can be used to
extract behavioral patterns and to gain insight into what factors influence
these behaviors. Thus, when a new case is being investigated and the profile
variables are unknown because the offender has yet to be identified, the
observed crime scene variables are used to infer the unknown variables based on
their connections in the structure and the corresponding numerical
(probabilistic) weights. The objective is to produce a more systematic and
empirical approach to profiling, and to use the resulting PN model as a
decision tool.",behavioral profiling
http://arxiv.org/abs/1705.09444v1,"Sequential allocation is a simple mechanism for sharing multiple indivisible
items. We study strategic behavior in sequential allocation. In particular, we
consider Nash dynamics, as well as the computation and Pareto optimality of
pure equilibria, and Stackelberg strategies. We first demonstrate that, even
for two agents, better responses can cycle. We then present a linear-time
algorithm that returns a profile (which we call the ""bluff profile"") that is in
pure Nash equilibrium. Interestingly, the outcome of the bluff profile is the
same as that of the truthful profile and the profile is in pure Nash
equilibrium for \emph{all} cardinal utilities consistent with the ordinal
preferences. We show that the outcome of the bluff profile is Pareto optimal
with respect to pairwise comparisons. In contrast, we show that an assignment
may not be Pareto optimal with respect to pairwise comparisons even if it is a
result of a preference profile that is in pure Nash equilibrium for all
utilities consistent with ordinal preferences. Finally, we present a dynamic
program to compute an optimal Stackelberg strategy for two agents, where the
second agent has a constant number of distinct values for the items.",behavioral profiling
http://arxiv.org/abs/1902.02484v1,"IoT devices are increasingly being implicated in cyber-attacks, raising
community concern about the risks they pose to critical infrastructure,
corporations, and citizens. In order to reduce this risk, the IETF is pushing
IoT vendors to develop formal specifications of the intended purpose of their
IoT devices, in the form of a Manufacturer Usage Description (MUD), so that
their network behavior in any operating environment can be locked down and
verified rigorously. This paper aims to assist IoT manufacturers in developing
and verifying MUD profiles, while also helping adopters of these devices to
ensure they are compatible with their organizational policies and track devices
network behavior based on their MUD profile. Our first contribution is to
develop a tool that takes the traffic trace of an arbitrary IoT device as input
and automatically generates the MUD profile for it. We contribute our tool as
open source, apply it to 28 consumer IoT devices, and highlight insights and
challenges encountered in the process. Our second contribution is to apply a
formal semantic framework that not only validates a given MUD profile for
consistency, but also checks its compatibility with a given organizational
policy. We apply our framework to representative organizations and selected
devices, to demonstrate how MUD can reduce the effort needed for IoT acceptance
testing. Finally, we show how operators can dynamically identify IoT devices
using known MUD profiles and monitor their behavioral changes on their network.",behavioral profiling
http://arxiv.org/abs/1310.4399v1,"In this work we present an in-depth analysis of the user behaviors on
different Social Sharing systems. We consider three popular platforms, Flickr,
Delicious and StumbleUpon, and, by combining techniques from social network
analysis with techniques from semantic analysis, we characterize the tagging
behavior as well as the tendency to create friendship relationships of the
users of these platforms. The aim of our investigation is to see if (and how)
the features and goals of a given Social Sharing system reflect on the behavior
of its users and, moreover, if there exists a correlation between the social
and tagging behavior of the users. We report our findings in terms of the
characteristics of user profiles according to three different dimensions: (i)
intensity of user activities, (ii) tag-based characteristics of user profiles,
and (iii) semantic characteristics of user profiles.",behavioral profiling
http://arxiv.org/abs/1506.02289v1,"Matching the profiles of a user across multiple online social networks brings
opportunities for new services and applications as well as new insights on user
online behavior, yet it raises serious privacy concerns. Prior literature has
proposed methods to match profiles and showed that it is possible to do it
accurately, but using evaluations that focused on sampled datasets only. In
this paper, we study the extent to which we can reliably match profiles in
practice, across real-world social networks, by exploiting public attributes,
i.e., information users publicly provide about themselves. Today's social
networks have hundreds of millions of users, which brings completely new
challenges as a reliable matching scheme must identify the correct matching
profile out of the millions of possible profiles. We first define a set of
properties for profile attributes--Availability, Consistency,
non-Impersonability, and Discriminability (ACID)--that are both necessary and
sufficient to determine the reliability of a matching scheme. Using these
properties, we propose a method to evaluate the accuracy of matching schemes in
real practical cases. Our results show that the accuracy in practice is
significantly lower than the one reported in prior literature. When considering
entire social networks, there is a non-negligible number of profiles that
belong to different users but have similar attributes, which leads to many
false matches. Our paper sheds light on the limits of matching profiles in the
real world and illustrates the correct methodology to evaluate matching schemes
in realistic scenarios.",behavioral profiling
http://arxiv.org/abs/1711.04036v1,"Pain is a subjective experience commonly measured through patient's self
report. While there exist numerous situations in which automatic pain
estimation methods may be preferred, inter-subject variability in physiological
and behavioral pain responses has hindered the development of such methods. In
this work, we address this problem by introducing a novel personalized
multitask machine learning method for pain estimation based on individual
physiological and behavioral pain response profiles, and show its advantages in
a dataset containing multimodal responses to nociceptive heat pain.",behavioral profiling
http://arxiv.org/abs/0903.1897v1,"The relation between the shape of the force driving a turbulent flow and the
upper bound on the dimensionless dissipation factor $\beta$ is presented. We
are interested in non-trivial (more than two wave numbers) forcing functions in
a three dimensional domain periodic in all directions. A comparative analysis
between results given by the optimization problem and the results of Direct
Numerical Simulations is performed. We report that the bound on the dissipation
factor in the case of infinite Reynolds numbers have the same qualitative
behavior as for the dissipation factor at finite Reynolds number. As predicted
by the analysis, the dissipation factor depends strongly on the force shape.
However, the optimization problem does not predict accurately the quantitative
behavior. We complete our study by analyzing the mean flow profile in relation
to the Stokes flow profile and the optimal multiplier profile shape for
different force-shapes. We observe that in our 3D-periodic domain, the mean
velocity profile and the Stokes flow profile reproduce all the characteristic
features of the force-shape. The optimal multiplier proves to be linked to the
intensity of the wave numbers of the forcing function.",behavioral profiling
http://arxiv.org/abs/1703.08206v1,"Allocating resources to virtualized network functions and services to meet
service level agreements is a challenging task for NFV management and
orchestration systems. This becomes even more challenging when agile
development methodologies, like DevOps, are applied. In such scenarios,
management and orchestration systems are continuously facing new versions of
functions and services which makes it hard to decide how much resources have to
be allocated to them to provide the expected service performance. One solution
for this problem is to support resource allocation decisions with performance
behavior information obtained by profiling techniques applied to such network
functions and services.
  In this position paper, we analyze and discuss the components needed to
generate such performance behavior information within the NFV DevOps workflow.
We also outline research questions that identify open issues and missing pieces
for a fully integrated NFV profiling solution. Further, we introduce a novel
profiling mechanism that is able to profile virtualized network functions and
entire network service chains under different resource constraints before they
are deployed on production infrastructure.",behavioral profiling
http://arxiv.org/abs/1705.01697v1,"The proliferation of malwares have been attributed to the alternations of a
handful of original malware source codes. The malwares alternated from the same
origin share some intrinsic behaviors and form a malware family. Expediently,
identifying its malware family when a malware is first seen on the Internet can
provide useful clues to mitigate the threat. In this paper, a malware profiler
(VMP) is proposed to profile the execution behaviors of a malware by leveraging
virtual machine introspection (VMI) technique. The VMP inserts plug-ins inside
the virtual machine monitor (VMM) to record the invoked API calls with their
input parameters and return values as the profile of malware. In this paper, a
popular similarity measurement Jaccard distance and a phylogenetic tree
construction method are adopted to discover malware families. The studies of
malware profiles show the malwares from a malware family are very similar to
each others and distinct from other malware families as well as benign
software. This paper also examines VMP against existing anti-malware detection
engines and some well-known malware grouping methods to compare the goodness in
their malware family constructions. A peer voting approach is proposed and the
results show VMP is better than almost all of the compared anti-malware
engines, and compatible with the fine tuned text-mining approach and high order
N-gram approaches. We also establish a malware profiling website based on VMP
for malware research.",behavioral profiling
http://arxiv.org/abs/1506.05752v3,"Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to ""shilling"" attacks or ""profile injection"" attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
""shilling"" attacks or ""profile injection"" attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN.",behavioral profiling
http://arxiv.org/abs/1902.09154v1,"With the increasing variety of services that e-commerce platforms provide,
criteria for evaluating their success become also increasingly multi-targeting.
This work introduces a multi-target optimization framework with Bayesian
modeling of the target events, called Deep Bayesian Multi-Target Learning
(DBMTL). In this framework, target events are modeled as forming a Bayesian
network, in which directed links are parameterized by hidden layers, and
learned from training samples. The structure of Bayesian network is determined
by model selection. We applied the framework to Taobao live-streaming
recommendation, to simultaneously optimize (and strike a balance) on targets
including click-through rate, user stay time in live room, purchasing behaviors
and interactions. Significant improvement has been observed for the proposed
method over other MTL frameworks and the non-MTL model. Our practice shows that
with an integrated causality structure, we can effectively make the learning of
a target benefit from other targets, creating significant synergy effects that
improve all targets. The neural network construction guided by DBMTL fits in
with the general probabilistic model connecting features and multiple targets,
taking weaker assumption than the other methods discussed in this paper. This
theoretical generality brings about practical generalization power over various
targets distributions, including sparse targets and continuous-value ones.",behavioral targeting
http://arxiv.org/abs/1805.09436v1,"Dyadic interactions among humans are marked by speakers continuously
influencing and reacting to each other in terms of responses and behaviors,
among others. Understanding how interpersonal dynamics affect behavior is
important for successful treatment in psychotherapy domains. Traditional
schemes that automatically identify behavior for this purpose have often looked
at only the target speaker. In this work, we propose a Markov model of how a
target speaker's behavior is influenced by their own past behavior as well as
their perception of their partner's behavior, based on lexical features. Apart
from incorporating additional potentially useful information, our model can
also control the degree to which the partner affects the target speaker. We
evaluate our proposed model on the task of classifying Negative behavior in
Couples Therapy and show that it is more accurate than the single-speaker
model. Furthermore, we investigate the degree to which the optimal influence
relates to how well a couple does on the long-term, via relating to
relationship outcomes",behavioral targeting
http://arxiv.org/abs/cs/0607143v1,"In this paper we consider and analyze the behavior of two combinational rules
for temporal (sequential) attribute data fusion for target type estimation. Our
comparative analysis is based on Dempster's fusion rule proposed in
Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution
rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We
show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a
very efficient Target Type Tracking and reduces drastically the latency delay
for correct Target Type decision with respect to Demspter's rule. For cases
presenting some short Target Type switches, Demspter's rule is proved to be
unable to detect the switches and thus to track correctly the Target Type
changes. The approach proposed here is totally new, efficient and promising to
be incorporated in real-time Generalized Data Association - Multi Target
Tracking systems (GDA-MTT) and provides an important result on the behavior of
PCR5 with respect to Dempster's rule. The MatLab source code is provided in",behavioral targeting
http://arxiv.org/abs/1303.5903v1,"We identify influential early adopters that achieve a target behavior
distribution for a resource constrained social network with multiple costly
behaviors. This problem is important for applications ranging from collective
behavior change to corporate viral marketing campaigns. In this paper, we
propose a model of diffusion of multiple behaviors when individual participants
have resource constraints. Individuals adopt the set of behaviors that maximize
their utility subject to available resources. We show that the problem of
influence maximization for multiple behaviors is NP-complete. Thus we propose
heuristics, which are based on node degree and expected immediate adoption, to
select early adopters. We evaluate the effectiveness under three metrics:
unique number of participants, total number of active behaviors and network
resource utilization. We also propose heuristics to distribute the behaviors
amongst the early adopters to achieve a target distribution in the population.
We test our approach on synthetic and real-world topologies with excellent
results. Our heuristics produce 15-51\% increase in resource utilization over
the na\""ive approach.",behavioral targeting
http://arxiv.org/abs/1305.3384v1,"In this paper we present a new approach to content-based transfer learning
for solving the data sparsity problem in cases when the users' preferences in
the target domain are either scarce or unavailable, but the necessary
information on the preferences exists in another domain. We show that training
a system to use such information across domains can produce better performance.
Specifically, we represent users' behavior patterns based on topological graph
structures. Each behavior pattern represents the behavior of a set of users,
when the users' behavior is defined as the items they rated and the items'
rating values. In the next step we find a correlation between behavior patterns
in the source domain and behavior patterns in the target domain. This mapping
is considered a bridge between the two domains. Based on the correlation and
content-attributes of the items, we train a machine learning model to predict
users' ratings in the target domain. When we compare our approach to the
popularity approach and KNN-cross-domain on a real world dataset, the results
show that on an average of 83$%$ of the cases our approach outperforms both
methods.",behavioral targeting
http://arxiv.org/abs/1809.00289v1,"In Twitter, there is a rising trend in abusive behavior which often leads to
incivility. This trend is affecting users mentally and as a result they tend to
leave Twitter and other such social networking sites thus depleting the active
user base. In this paper, we study factors associated with incivility. We
observe that the act of incivility is highly correlated with the opinion
differences between the account holder (i.e., the user writing the incivil
tweet) and the target (i.e., the user for whom the incivil tweet is meant for
or targeted), toward a named entity. We introduce a character level CNN model
and incorporate the entity-specific sentiment information for efficient
incivility detection which significantly outperforms multiple baseline methods
achieving an impressive accuracy of 93.3% (4.9% improvement over the best
baseline). In a post-hoc analysis, we also study the behavioral aspects of the
targets and account holders and try to understand the reasons behind the
incivility incidents. Interestingly, we observe that there are strong signals
of repetitions in incivil behavior. In particular, we find that there are a
significant fraction of account holders who act as repeat offenders - attacking
the targets even more than 10 times. Similarly, there are also targets who get
targeted multiple times. In general, the targets are found to have higher
reputation scores than the account holders.",behavioral targeting
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",behavioral targeting
http://arxiv.org/abs/1906.08025v1,"This paper describes a software-based tool that tracks mobile node roaming
and infers the time-to-handover as well as the preferential handover target,
based on behavior inference solely derived from regular usage data captured in
visited wireless networks. The paper presents the tool architecture;
computational background for mobility estimation; operational guidelines
concerning how the tool is being used to track several aspects of roaming
behavior in the context of wireless networks. Target selection accuracy is
validated having as baseline traces obtained in realistic scenarios.",behavioral targeting
http://arxiv.org/abs/1808.01075v1,"In the modern e-commerce, the behaviors of customers contain rich
information, e.g., consumption habits, the dynamics of preferences. Recently,
session-based recommendations are becoming popular to explore the temporal
characteristics of customers' interactive behaviors. However, existing works
mainly exploit the short-term behaviors without fully taking the customers'
long-term stable preferences and evolutions into account. In this paper, we
propose a novel Behavior-Intensive Neural Network (BINN) for next-item
recommendation by incorporating both users' historical stable preferences and
present consumption motivations. Specifically, BINN contains two main
components, i.e., Neural Item Embedding, and Discriminative Behaviors Learning.
Firstly, a novel item embedding method based on user interactions is developed
for obtaining an unified representation for each item. Then, with the embedded
items and the interactive behaviors over item sequences, BINN discriminatively
learns the historical preferences and present motivations of the target users.
Thus, BINN could better perform recommendations of the next items for the
target users. Finally, for evaluating the performances of BINN, we conduct
extensive experiments on two real-world datasets, i.e., Tianchi and JD. The
experimental results clearly demonstrate the effectiveness of BINN compared
with several state-of-the-art methods.",behavioral targeting
http://arxiv.org/abs/1809.08793v1,"This paper addresses a novel architecture for person-following robots using
active search. The proposed system can be applied in real-time to general
mobile robots for learning features of a human, detecting and tracking, and
finally navigating towards that person. To succeed at person-following,
perception, planning, and robot behavior need to be integrated properly. Toward
this end, an active target searching capability, including prediction and
navigation toward vantage locations for finding human targets, is proposed. The
proposed capability aims at improving the robustness and efficiency for
tracking and following people under dynamic conditions such as crowded
environments. A multi-modal sensor information approach including fusing an
RGB-D sensor and a laser scanner, is pursued to robustly track and identify
human targets. Bayesian filtering for keeping track of human and a regression
algorithm to predict the trajectory of people are investigated. In order to
make the robot autonomous, the proposed framework relies on a behavior-tree
structure. Using Toyota Human Support Robot (HSR), real-time experiments
demonstrate that the proposed architecture can generate fast, efficient
person-following behaviors.",behavioral targeting
http://arxiv.org/abs/1503.08048v1,"We consider the effect of inducement to vaccinate during the spread of an
infectious disease on complex networks. Suppose that public resources are
finite and that only a small proportion of individuals can be vaccinated freely
(complete subsidy), for the remainder of the population vaccination is a
voluntary behavior --- and each vaccinated individual carries a perceived cost.
We ask whether the classical targeted subsidy strategy is definitely better
than the random strategy: does targeting subsidy at individuals perceived to be
with the greatest risk actually help? With these questions, we propose a model
to investigate the \emph{interaction effects} of the subsidy policies and
individuals responses when facing subsidy policies on the epidemic dynamics on
complex networks. In the model, a small proportion of individuals are freely
vaccinated according to either the targeted or random subsidy policy, the
remainder choose to vaccinate (or not) based on voluntary principle and update
their vaccination decision via an imitation rule. Our findings show that the
targeted strategy is only advantageous when individuals prefer to imitate the
subsidized individuals' strategy. Otherwise, the effect of the targeted policy
is worse than the random immunization, since individuals preferentially select
non-subsidized individuals as the imitation objects. More importantly, we find
that under the targeted subsidy policy, increasing the proportion of subsidized
individuals may increase the final epidemic size. We further define social cost
as the sum of the costs of vaccination and infection, and study how each of the
two policies affect the social cost. Our result shows that there exist some
optimal intermediate regions leading to the minimal social cost.",behavioral targeting
http://arxiv.org/abs/1607.07647v1,"We propose a method for tracking an unknown number of targets based on
measurements provided by multiple sensors. Our method achieves low
computational complexity and excellent scalability by running belief
propagation on a suitably devised factor graph. A redundant formulation of data
association uncertainty and the use of ""augmented target states"" including
binary target indicators make it possible to exploit statistical independencies
for a drastic reduction of complexity. An increase in the number of targets,
sensors, or measurements leads to additional variable nodes in the factor graph
but not to higher dimensions of the messages. As a consequence, the complexity
of our method scales only quadratically in the number of targets, linearly in
the number of sensors, and linearly in the number of measurements per sensors.
The performance of the method compares well with that of previously proposed
methods, including methods with a less favorable scaling behavior. In
particular, our method can outperform multisensor versions of the probability
hypothesis density (PHD) filter, the cardinalized PHD filter, and the
multi-Bernoulli filter.",behavioral targeting
http://arxiv.org/abs/1212.4305v1,"The two-frequency problem of synchronization of the pulse train of a
passively mode locked soliton laser to an externally injected pulse train is
solved in the weak injection regime. The source and target frequency combs are
distinguished by the spacing and offset frequency mismatches. Locking diagrams
map the domain in the mismatch parameter space where stable locking of the
combs is possible. We analyze the dependence of the locking behavior on the
relative frequency and chirp of the source and target pulses, and the
conditions where the relative offset frequency has to be actively stabilized.
Locked steady states are characterized by a fixed source-target time and phase
shifts that map the locking domain.",behavioral targeting
http://arxiv.org/abs/1908.03597v1,"Gene regulation is one of the most important fundamental biological processes
in living cells. It involves multiple protein molecules that locate specific
sites on DNA and assemble gene initiation or gene repression multi-molecular
complexes. While the protein search dynamics for DNA targets has been
intensively investigated, the role of inter-molecular interactions during the
genetic activation or repression remains not well quantified. Here we present a
simple one-dimensional model of target search for two interacting molecules
that can reversibly form a dimer molecular complex, which also participates in
the search process. In addition, the proteins have finite residence times on
specific target sites, and the gene is activated or repressed when both
proteins are simultaneously present at the target. The model is analyzed using
first-passage analytical calculations and Monte Carlo computer simulations. It
is shown that the search dynamics exhibits a complex behavior depending on the
strength of inter-molecular interactions and on the target residence times. We
also found that the search time shows a non-monotonic behavior as a function of
the dissociation rate for the molecular complex. Physical-chemical arguments to
explain these observations are presented. Our theoretical approach highlights
the importance of molecular interactions in the complex process of gene
activation/repression by multiple transcription factor proteins.",behavioral targeting
http://arxiv.org/abs/1603.09495v1,"We describe a representation in a high-level transition system for policies
that express a reactive behavior for the agent. We consider a target decision
component that figures out what to do next and an (online) planning capability
to compute the plans needed to reach these targets. Our representation allows
one to analyze the flow of executing the given reactive policy, and to
determine whether it works as expected. Additionally, the flexibility of the
representation opens a range of possibilities for designing behaviors.",behavioral targeting
http://arxiv.org/abs/physics/0503054v1,"A pellet target produces micro-spheres of different materials, which are used
as an internal target for nuclear and particle physics studies. We will
describe the pellet hydrogen behavior by means of fluid dynamics and
thermodynamics. In particular one aim is to theoretically understand the
cooling effect in order to find an effective method to optimize the working
conditions of a pellet target. During the droplet formation the evaporative
cooling is best described by a multi-droplet diffusion-controlled model, while
in vacuum, the evaporation follows the (revised) Hertz-Knudsen formula.
Experimental observations compared with calculations clearly indicated the
presence of supercooling, the effect of which is discussed as well.",behavioral targeting
http://arxiv.org/abs/1811.09699v1,"Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.",behavioral targeting
http://arxiv.org/abs/1903.00958v1,"Stackelberg security games are a critical tool for maximizing the utility of
limited defense resources to protect important targets from an intelligent
adversary. Motivated by green security, where the defender may only observe an
adversary's response to defense on a limited set of targets, we study the
problem of defending against the same adversary on a larger set of targets from
the same distribution. We give a theoretical justification for why standard
two-stage learning approaches, where a model of the adversary is trained for
predictive accuracy and then optimized against, may fail to maximize the
defender's expected utility in this setting. We develop a decision-focused
learning approach, where the adversary behavior model is optimized for decision
quality, and show empirically that it achieves higher defender expected utility
than the two-stage approach when there is limited training data and a large
number of target features.",behavioral targeting
http://arxiv.org/abs/1304.7718v1,"The first-price auction is popular in practice for its simplicity and
transparency. Moreover, its potential virtues grow in complex settings where
incentive compatible auctions may generate little or no revenue. Unfortunately,
the first-price auction is poorly understood in theory because equilibrium is
not {\em a priori} a credible predictor of bidder behavior.
  We take a dynamic approach to studying first-price auctions: rather than
basing performance guarantees solely on static equilibria, we study the
repeated setting and show that robust performance guarantees may be derived
from simple axioms of bidder behavior. For example, as long as a loser raises
her bid quickly, a standard first-price auction will generate at least as much
revenue as a second-price auction. We generalize this dynamic technique to
complex pay-your-bid auction settings and show that progressively stronger
assumptions about bidder behavior imply progressively stronger guarantees about
the auction's performance.
  Along the way, we find that the auctioneer's choice of bidding language is
critical when generalizing beyond the single-item setting, and we propose a
specific construction called the {\em utility-target auction} that performs
well. The utility-target auction includes a bidder's final utility as an
additional parameter, identifying the single dimension along which she wishes
to compete. This auction is closely related to profit-target bidding in
first-price and ascending proxy package auctions and gives strong revenue
guarantees for a variety of complex auction environments. Of particular
interest, the guaranteed existence of a pure-strategy equilibrium in the
utility-target auction shows how Overture might have eliminated the cyclic
behavior in their generalized first-price sponsored search auction if bidders
could have placed more sophisticated bids.",behavioral targeting
http://arxiv.org/abs/1604.08768v1,"We relate behavior composition, a synthesis task studied in AI, to
supervisory control theory from the discrete event systems field. In
particular, we show that realizing (i.e., implementing) a target behavior
module (e.g., a house surveillance system) by suitably coordinating a
collection of available behaviors (e.g., automatic blinds, doors, lights,
cameras, etc.) amounts to imposing a supervisor onto a special discrete event
system. Such a link allows us to leverage on the solid foundations and
extensive work on discrete event systems, including borrowing tools and ideas
from that field. As evidence of that we show how simple it is to introduce
preferences in the mapped framework.",behavioral targeting
http://arxiv.org/abs/1101.3400v1,"We present a new algorithm for behavioral targeting of banner advertisements.
We record different user's actions such as clicks, search queries and page
views. We use the collected information on the user to estimate in real time
the probability of a click on a banner. A banner is displayed if it either has
the highest probability of being clicked or if it is the one that generates the
highest average profit.",behavioral targeting
http://arxiv.org/abs/1610.00787v2,"This paper develops a distributed resource allocation game to study
countries' pursuit of targets such as self-survival in the networked
international environment. The contributions are two. First, the game
formalizes countries' power allocation behaviors which fall into the broad
category of humans resource allocation behaviors. Second, the game presents a
new technical problem, and establishes pure strategy Nash equilibrium
existence.",behavioral targeting
http://arxiv.org/abs/0705.3694v1,"The problems of stripper target behavior in the nonstationary intense
particle beams are considered. The historical sketch of studying of radiation
damage failure of carbon targets under ion bombardment is presented. The simple
model of evaporation of a target by an intensive pulsing beam is supposed.
Stripper foils lifetimes in the nonstationary intense particle can be described
by two failure mechanisms: radiation damage accumulation and evaporation of
target. At the maximal temperatures less than 2500K the radiation damage are
dominated; at temperatures above 2500K the mechanism of evaporation of a foil
prevails. The proposed approach has been applied to the discription of
behaviour of stripper foils in the BNL linac and SNS conditions.",behavioral targeting
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",targeted advertising
http://arxiv.org/abs/1907.01862v2,"Being able to check whether an online advertisement has been targeted is
essential for resolving privacy controversies and implementing in practice data
protection regulations like GDPR, CCPA, and COPPA. In this paper we describe
the design, implementation, and deployment of an advertisement auditing system
called iWnder that uses crowdsourcing to reveal in real time whether a display
advertisement has been targeted or not. Crowdsourcing simplifies the detection
of targeted advertising, but requires reporting to a central repository the
impressions seen by different users, thereby jeopardising their privacy. We
break this deadlock with a privacy preserving data sharing protocol that allows
iWnder to compute global statistics required to detect targeting, while keeping
the advertisements seen by individual users and their browsing history private.
We conduct a simulation study to explore the effect of different parameters and
a live validation to demonstrate the accuracy of our approach. Unlike previous
solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns
that promote a product or service whose description bears no semantic overlap
with its targeted audience.",targeted advertising
http://arxiv.org/abs/1407.3338v1,"We undertake a formal study of the value of targeting data to an advertiser.
As expected, this value is increasing in the utility difference between
realizations of the targeting data and the accuracy of the data, and depends on
the distribution of competing bids. However, this value may vary
non-monotonically with an advertiser's budget. Similarly, modeling the values
as either private or correlated, or allowing other advertisers to also make use
of the data, leads to unpredictable changes in the value of data. We address
questions related to multiple data sources, show that utility of additional
data may be non-monotonic, and provide tradeoffs between the quality and the
price of data sources. In a game-theoretic setting, we show that advertisers
may be worse off than if the data had not been available at all. We also ask
whether a publisher can infer the value an advertiser would place on targeting
data from the advertiser's bidding behavior and illustrate that this is
impossible.",targeted advertising
http://arxiv.org/abs/1510.04031v1,"In the last decade, the advertisement market spread significantly in the web
and mobile app system. Its effectiveness is also due thanks to the possibility
to target the advertisement on the specific interests of the actual user, other
than on the content of the website hosting the advertisement. In this scenario,
became of great value services that collect and hence can provide information
about the browsing user, like Facebook and Google. In this paper, we show how
to maliciously exploit the Google Targeted Advertising system to infer personal
information in Google user profiles. In particular, the attack we consider is
external from Google and relies on combining data from Google AdWords with
other data collected from a website of the Google Display Network. We validate
the effectiveness of our proposed attack, also discussing possible application
scenarios. The result of our research shows a significant practical privacy
issue behind such type of targeted advertising service, and call for further
investigation and the design of more privacy-aware solutions, possibly without
impeding the current business model involved in online advertisement.",targeted advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",targeted advertising
http://arxiv.org/abs/1508.03080v1,"We study how privacy technologies affect user and advertiser behavior in a
simple economic model of targeted advertising. In our model, a consumer first
decides whether or not to buy a good, and then an advertiser chooses an
advertisement to show the consumer. The consumer's value for the good is
correlated with her type, which determines which ad the advertiser would prefer
to show to her---and hence, the advertiser would like to use information about
the consumer's purchase decision to target the ad that he shows.
  In our model, the advertiser is given only a differentially private signal
about the consumer's behavior---which can range from no signal at all to a
perfect signal, as we vary the differential privacy parameter. This allows us
to study equilibrium behavior as a function of the level of privacy provided to
the consumer. We show that this behavior can be highly counter-intuitive, and
that the effect of adding privacy in equilibrium can be completely different
from what we would expect if we ignored equilibrium incentives. Specifically,
we show that increasing the level of privacy can actually increase the amount
of information about the consumer's type contained in the signal the advertiser
receives, lead to decreased utility for the consumer, and increased profit for
the advertiser, and that generally these quantities can be non-monotonic and
even discontinuous in the privacy level of the signal.",targeted advertising
http://arxiv.org/abs/1703.02091v4,"Taobao, as the largest online retail platform in the world, provides billions
of online display advertising impressions for millions of advertisers every
day. For commercial purposes, the advertisers bid for specific spots and target
crowds to compete for business traffic. The platform chooses the most suitable
ads to display in tens of milliseconds. Common pricing methods include cost per
mille (CPM) and cost per click (CPC). Traditional advertising systems target
certain traits of users and ad placements with fixed bids, essentially regarded
as coarse-grained matching of bid and traffic quality. However, the fixed bids
set by the advertisers competing for different quality requests cannot fully
optimize the advertisers' key requirements. Moreover, the platform has to be
responsible for the business revenue and user experience. Thus, we proposed a
bid optimizing strategy called optimized cost per click (OCPC) which
automatically adjusts the bid to achieve finer matching of bid and traffic
quality of page view (PV) request granularity. Our approach optimizes
advertisers' demands, platform business revenue and user experience and as a
whole improves traffic allocation efficiency. We have validated our approach in
Taobao display advertising system in production. The online A/B test shows our
algorithm yields substantially better results than previous fixed bid manner.",targeted advertising
http://arxiv.org/abs/1411.5281v3,"Online Behavioural targeted Advertising (OBA) has risen in prominence as a
method to increase the effectiveness of online advertising. OBA operates by
associating tags or labels to users based on their online activity and then
using these labels to target them. This rise has been accompanied by privacy
concerns from researchers, regulators and the press. In this paper, we present
a novel methodology for measuring and understanding OBA in the online
advertising market. We rely on training artificial online personas representing
behavioural traits like 'cooking', 'movies', 'motor sports', etc. and build a
measurement system that is automated, scalable and supports testing of multiple
configurations. We observe that OBA is a frequent practice and notice that
categories valued more by advertisers are more intensely targeted. In addition,
we provide evidences showing that the advertising market targets sensitive
topics (e.g, religion or health) despite the existence of regulation that bans
such practices. We also compare the volume of OBA advertising for our personas
in two different geographical locations (US and Spain) and see little
geographic bias in terms of intensity of OBA targeting. Finally, we check for
targeting with do-not-track (DNT) enabled and discovered that DNT is not yet
enforced in the web.",targeted advertising
http://arxiv.org/abs/1909.02156v1,"Interactions between bids to show ads online can lead to an advertiser's ad
being shown to more men than women even when the advertiser does not target
towards men. We design bidding strategies that advertisers can use to avoid
such emergent discrimination without having to modify the auction mechanism. We
mathematically analyze the strategies to determine the additional cost to the
advertiser for avoiding discrimination, proving our strategies to be optimal in
some settings. We use simulations to understand other settings.",targeted advertising
http://arxiv.org/abs/1603.07768v1,"We study the online budgeted allocation (also called ADWORDS) problem, where
a set of impressions arriving online are allocated to a set of
budget-constrained advertisers to maximize revenue. Motivated by connections to
Internet advertising, several variants of this problem have been studied since
the seminal work of Mehta, Saberi, Vazirani, and Vazirani (FOCS 2005). However,
this entire body of work focuses on a single budget for every advertising
campaign, whereas in order to fully represent the actual agenda of an
advertiser, an advertising budget should be expressible over multiple tiers of
user-attribute granularity. A simple example is an advertising campaign that is
constrained by an overall budget but is also accompanied by a set of
sub-budgets for each target demographic. In such a contract scheme, an
advertiser can specify their true user-targeting goals, allowing the publisher
to fulfill them through relevant allocations.
  In this paper, we give a complete characterization of the ADWORDS problem for
general advertising budgets. In the most general setting, we show that, unlike
in the single-budget ADWORDS problem, obtaining a constant competitive ratio is
impossible and give asymptotically tight upper and lower bounds. However for
our main result, we observe that in many real-world scenarios (as in the above
example), multi-tier budgets have a laminar structure, since most relevant
consumer or product classifications are hierarchical. For laminar budgets, we
obtain a competitive ratio of e/(e-1) in the small bids case, which matches the
best known ADWORDS result for single budgets. Our algorithm has a primal-dual
structure and generalizes the primal-dual analysis for single- budget ADWORDS
first given by Buchbinder, Jain, and Naor (ESA 2007).",targeted advertising
http://arxiv.org/abs/1907.12118v1,"Sponsored search has more than 20 years of history, and it has been proven to
be a successful business model for online advertising. Based on the
pay-per-click pricing model and the keyword targeting technology, the sponsored
system runs online auctions to determine the allocations and prices of search
advertisements. In the traditional setting, advertisers should manually create
lots of ad creatives and bid on some relevant keywords to target their
audience. Due to the huge amount of search traffic and a wide variety of ad
creations, the limits of manual optimizations from advertisers become the main
bottleneck for improving the efficiency of this market. Moreover, as many
emerging advertising forms and supplies are growing, it's crucial for sponsored
search platform to pay more attention to the ROI metrics of ads for getting the
marketing budgets of advertisers. In this paper, we present the AiAds system
developed at Baidu, which use machine learning techniques to build an automated
and intelligent advertising system. By designing and implementing the automated
bidding strategy, the intelligent targeting and the intelligent creation
models, the AiAds system can transform the manual optimizations into multiple
automated tasks and optimize these tasks in advanced methods. AiAds is a
brand-new architecture of sponsored search system which changes the bidding
language and allocation mechanism, breaks the limit of keyword targeting with
end-to-end ad retrieval framework and provides global optimization of ad
creation. This system can increase the advertiser's campaign performance, the
user experience and the revenue of the advertising platform simultaneously and
significantly. We present the overall architecture and modeling techniques for
each module of the system and share our lessons learned in solving several key
challenges.",targeted advertising
http://arxiv.org/abs/1808.09218v4,"Targeted advertising is meant to improve the efficiency of matching
advertisers to their customers. However, targeted advertising can also be
abused by malicious advertisers to efficiently reach people susceptible to
false stories, stoke grievances, and incite social conflict. Since targeted ads
are not seen by non-targeted and non-vulnerable people, malicious ads are
likely to go unreported and their effects undetected. This work examines a
specific case of malicious advertising, exploring the extent to which political
ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
elections exploited Facebook's targeted advertising infrastructure to
efficiently target ads on divisive or polarizing topics (e.g., immigration,
race-based policing) at vulnerable sub-populations. In particular, we do the
following: (a) We conduct U.S. census-representative surveys to characterize
how users with different political ideologies report, approve, and perceive
truth in the content of the IRA ads. Our surveys show that many ads are
""divisive"": they elicit very different reactions from people belonging to
different socially salient groups. (b) We characterize how these divisive ads
are targeted to sub-populations that feel particularly aggrieved by the status
quo. Our findings support existing calls for greater transparency of content
and targeting of political ads. (c) We particularly focus on how the Facebook
ad API facilitates such targeting. We show how the enormous amount of personal
data Facebook aggregates about users and makes available to advertisers enables
such malicious targeting.",targeted advertising
http://arxiv.org/abs/1907.02178v3,"Firms implementing digital advertising campaigns face a complex problem in
determining the right match between their advertising creatives and target
audiences. Typical solutions to the problem have leveraged non-experimental
methods, or used ""split-testing"" strategies that have not explicitly addressed
the complexities induced by targeted audiences that can potentially overlap
with one another. This paper presents an adaptive algorithm that addresses the
problem via online experimentation. The algorithm is set up as a contextual
bandit and addresses the overlap issue by partitioning the target audiences
into disjoint, non-overlapping sub-populations. It learns an optimal creative
display policy in the disjoint space, while assessing in parallel which
creative has the best match in the space of possibly overlapping target
audiences. Experiments show that the proposed method is more efficient compared
to naive ""split-testing"" or non-adaptive ""A/B/n"" testing based methods. We also
describe a testing product we built that uses the algorithm. The product is
currently deployed on the advertising platform of JD.com, an eCommerce company
and a publisher of digital ads in China.",targeted advertising
http://arxiv.org/abs/1206.1754v2,"Internet advertising is a fast growing business which has proved to be
significantly important in digital economics. It is vitally important for both
web search engines and online content providers and publishers because web
advertising provides them with major sources of revenue. Its presence is
increasingly important for the whole media industry due to the influence of the
Web. For advertisers, it is a smarter alternative to traditional marketing
media such as TVs and newspapers. As the web evolves and data collection
continues, the design of methods for more targeted, interactive, and friendly
advertising may have a major impact on the way our digital economy evolves, and
to aid societal development.
  Towards this goal mathematically well-grounded Computational Advertising
methods are becoming necessary and will continue to develop as a fundamental
tool towards the Web. As a vibrant new discipline, Internet advertising
requires effort from different research domains including Information
Retrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,
and even Psychology to predict and understand user behaviours. In this paper,
we provide a comprehensive survey on Internet advertising, discussing and
classifying the research issues, identifying the recent technologies, and
suggesting its future directions. To have a comprehensive picture, we first
start with a brief history, introduction, and classification of the industry
and present a schematic view of the new advertising ecosystem. We then
introduce four major participants, namely advertisers, online publishers, ad
exchanges and web users; and through analysing and discussing the major
research problems and existing solutions from their perspectives respectively,
we discover and aggregate the fundamental problems that characterise the
newly-formed research field and capture its potential future prospects.",targeted advertising
http://arxiv.org/abs/1902.02429v1,"Vehicle service providers can display commercial ads in their vehicles based
on passengers' origins and destinations to create a new revenue stream. In this
work, we study a vehicle service provider who can generate different ad
revenues when displaying ads on different arcs (i.e., origin-destination
pairs). The provider needs to ensure the vehicle flow balance at each location,
which makes it challenging to analyze the provider's vehicle assignment and
pricing decisions for different arcs. For example, the provider's price for its
service on an arc depends on the ad revenues on other arcs as well as on the
arc in question. To tackle the problem, we show that the traffic network
corresponds to an electrical network. When the effective resistance between two
locations is small, there are many paths between the two locations and the
provider can easily route vehicles between them. We characterize the dependence
of an arc's optimal price on any other arc's ad revenue using the effective
resistances between these two arcs' origins and destinations. Furthermore, we
study the provider's optimal selection of advertisers when it can only display
ads for a limited number of advertisers. If each advertiser has one target arc
for advertising, the provider should display ads for the advertiser whose
target arc has a small effective resistance. We investigate the performance of
our advertiser selection strategy based on a real-world dataset.",targeted advertising
http://arxiv.org/abs/1711.11175v1,"In online advertising, our aim is to match the advertisers with the most
relevant users to optimize the campaign performance. In the pursuit of
achieving this goal, multiple data sources provided by the advertisers or
third-party data providers are utilized to choose the set of users according to
the advertisers' targeting criteria. In this paper, we present a framework that
can be applied to assess the quality of such data sources in large scale. This
framework efficiently evaluates the similarity of a specific data source
categorization to that of the ground truth, especially for those cases when the
ground truth is accessible only in aggregate, and the user-level information is
anonymized or unavailable due to privacy reasons. We propose multiple
methodologies within this framework, present some preliminary assessment
results, and evaluate how the methodologies compare to each other. We also
present two use cases where we can utilize the data quality assessment results:
the first use case is targeting specific user categories, and the second one is
forecasting the desirable audiences we can reach for an online advertising
campaign with pre-set targeting criteria.",targeted advertising
http://arxiv.org/abs/1101.3400v1,"We present a new algorithm for behavioral targeting of banner advertisements.
We record different user's actions such as clicks, search queries and page
views. We use the collected information on the user to estimate in real time
the probability of a click on a banner. A banner is displayed if it either has
the highest probability of being clicked or if it is the one that generates the
highest average profit.",targeted advertising
http://arxiv.org/abs/1112.5396v2,"With more than four billion usage of cellular phones worldwide, mobile
advertising has become an attractive alternative to online advertisements. In
this paper, we propose a new targeted advertising policy for Wireless Service
Providers (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSP
charges the advertisers for showing their ads. Each advertiser has a valuation
for specific types of customers in various times and locations and has a limit
on the maximum available budget. Each query is in the form of time and location
and is associated with one individual customer. In order to achieve a
non-intrusive delivery, only a limited number of ads can be sent to each
customer. Recently, new services have been introduced that offer location-based
advertising over cellular network that fit in our model (e.g., ShopAlerts by
AT&T) .
  We consider both online and offline version of the AdCell problem and develop
approximation algorithms with constant competitive ratio. For the online
version, we assume that the appearances of the queries follow a stochastic
distribution and thus consider a Bayesian setting. Furthermore, queries may
come from different distributions on different times. This model generalizes
several previous advertising models such as online secretary problem
\cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords
\cite{saberi05}. ...",targeted advertising
http://arxiv.org/abs/1305.3014v1,"Online advertising has been introduced as one of the most efficient methods
of advertising throughout the recent years. Yet, advertisers are concerned
about the efficiency of their online advertising campaigns and consequently,
would like to restrict their ad impressions to certain websites and/or certain
groups of audience. These restrictions, known as targeting criteria, limit the
reachability for better performance. This trade-off between reachability and
performance illustrates a need for a forecasting system that can quickly
predict/estimate (with good accuracy) this trade-off. Designing such a system
is challenging due to (a) the huge amount of data to process, and, (b) the need
for fast and accurate estimates. In this paper, we propose a distributed fault
tolerant system that can generate such estimates fast with good accuracy. The
main idea is to keep a small representative sample in memory across multiple
machines and formulate the forecasting problem as queries against the sample.
The key challenge is to find the best strata across the past data, perform
multivariate stratified sampling while ensuring fuzzy fall-back to cover the
small minorities. Our results show a significant improvement over the uniform
and simple stratified sampling strategies which are currently widely used in
the industry.",targeted advertising
http://arxiv.org/abs/1502.06657v1,"Budget allocation in online advertising deals with distributing the campaign
(insertion order) level budgets to different sub-campaigns which employ
different targeting criteria and may perform differently in terms of
return-on-investment (ROI). In this paper, we present the efforts at Turn on
how to best allocate campaign budget so that the advertiser or campaign-level
ROI is maximized. To do this, it is crucial to be able to correctly determine
the performance of sub-campaigns. This determination is highly related to the
action-attribution problem, i.e. to be able to find out the set of ads, and
hence the sub-campaigns that provided them to a user, that an action should be
attributed to. For this purpose, we employ both last-touch (last ad gets all
credit) and multi-touch (many ads share the credit) attribution methodologies.
We present the algorithms deployed at Turn for the attribution problem, as well
as their parallel implementation on the large advertiser performance datasets.
We conclude the paper with our empirical comparison of last-touch and
multi-touch attribution-based budget allocation in a real online advertising
setting.",targeted advertising
http://arxiv.org/abs/1909.13221v2,"Online advertising in E-commerce platforms provides sellers an opportunity to
achieve potential audiences with different target goals. Ad serving systems
(like display and search advertising systems) that assign ads to pages should
satisfy objectives such as plenty of audience for branding advertisers, clicks
or conversions for performance-based advertisers, at the same time try to
maximize overall revenue of the platform. In this paper, we propose an approach
based on linear programming subjects to constraints in order to optimize the
revenue and improve different performance goals simultaneously. We have
validated our algorithm by implementing an offline simulation system in Alibaba
E-commerce platform and running the auctions from online requests which takes
system performance, ranking and pricing schemas into account. We have also
compared our algorithm with related work, and the results show that our
algorithm can effectively improve campaign performance and revenue of the
platform.",targeted advertising
http://arxiv.org/abs/1910.02358v1,"Assessing aesthetic preference is a fundamental task related to human
cognition. It can also contribute to various practical applications such as
image creation for online advertisements. Despite crucial influences of image
quality, auxiliary information of ad images such as tags and target subjects
can also determine image preference. Existing studies mainly focus on images
and thus are less useful for advertisement scenarios where rich auxiliary data
are available. Here we propose a modality fusion-based neural network that
evaluates the aesthetic preference of images with auxiliary information. Our
method fully utilizes auxiliary data by introducing multi-step modality fusion
using both conditional batch normalization-based low-level and attention-based
high-level fusion mechanisms, inspired by the findings from statistical
analyses on real advertisement data. Our approach achieved state-of-the-art
performance on the AVA dataset, a widely used dataset for aesthetic assessment.
Besides, the proposed method is evaluated on large-scale real-world
advertisement image data with rich auxiliary attributes, providing promising
preference prediction results. Through extensive experiments, we investigate
how image and auxiliary information together influence click-through rate.",targeted advertising
http://arxiv.org/abs/1904.02095v5,"The enormous financial success of online advertising platforms is partially
due to the precise targeting features they offer. Although researchers and
journalists have found many ways that advertisers can target---or
exclude---particular groups of users seeing their ads, comparatively little
attention has been paid to the implications of the platform's ad delivery
process, comprised of the platform's choices about which users see which ads.
  It has been hypothesized that this process can ""skew"" ad delivery in ways
that the advertisers do not intend, making some users less likely than others
to see particular ads based on their demographic characteristics. In this
paper, we demonstrate that such skewed delivery occurs on Facebook, due to
market and financial optimization effects as well as the platform's own
predictions about the ""relevance"" of ads to different groups of users. We find
that both the advertiser's budget and the content of the ad each significantly
contribute to the skew of Facebook's ad delivery. Critically, we observe
significant skew in delivery along gender and racial lines for ""real"" ads for
employment and housing opportunities despite neutral targeting parameters.
  Our results demonstrate previously unknown mechanisms that can lead to
potentially discriminatory ad delivery, even when advertisers set their
targeting parameters to be highly inclusive. This underscores the need for
policymakers and platforms to carefully consider the role of the ad delivery
optimization run by ad platforms themselves---and not just the targeting
choices of advertisers---in preventing discrimination in digital advertising.",targeted advertising
http://arxiv.org/abs/1606.07189v1,"As one of the leading platforms for creative content, Tumblr offers
advertisers a unique way of creating brand identity. Advertisers can tell their
story through images, animation, text, music, video, and more, and promote that
content by sponsoring it to appear as an advertisement in the streams of Tumblr
users. In this paper we present a framework that enabled one of the key
targeted advertising components for Tumblr, specifically gender and interest
targeting. We describe the main challenges involved in development of the
framework, which include creating the ground truth for training gender
prediction models, as well as mapping Tumblr content to an interest taxonomy.
For purposes of inferring user interests we propose a novel semi-supervised
neural language model for categorization of Tumblr content (i.e., post tags and
post keywords). The model was trained on a large-scale data set consisting of
6.8 billion user posts, with very limited amount of categorized keywords, and
was shown to have superior performance over the bag-of-words model. We
successfully deployed gender and interest targeting capability in Yahoo
production systems, delivering inference for users that cover more than 90% of
daily activities at Tumblr. Online performance results indicate advantages of
the proposed approach, where we observed 20% lift in user engagement with
sponsored posts as compared to untargeted campaigns.",targeted advertising
http://arxiv.org/abs/1001.2735v4,"Internet advertising is a sophisticated game in which the many advertisers
""play"" to optimize their return on investment. There are many ""targets"" for the
advertisements, and each ""target"" has a collection of games with a potentially
different set of players involved. In this paper, we study the problem of how
advertisers allocate their budget across these ""targets"". In particular, we
focus on formulating their best response strategy as an optimization problem.
Advertisers have a set of keywords (""targets"") and some stochastic information
about the future, namely a probability distribution over scenarios of cost vs
click combinations. This summarizes the potential states of the world assuming
that the strategies of other players are fixed. Then, the best response can be
abstracted as stochastic budget optimization problems to figure out how to
spread a given budget across these keywords to maximize the expected number of
clicks.
  We present the first known non-trivial poly-logarithmic approximation for
these problems as well as the first known hardness results of getting better
than logarithmic approximation ratios in the various parameters involved. We
also identify several special cases of these problems of practical interest,
such as with fixed number of scenarios or with polynomial-sized parameters
related to cost, which are solvable either in polynomial time or with improved
approximation ratios. Stochastic budget optimization with scenarios has
sophisticated technical structure. Our approximation and hardness results come
from relating these problems to a special type of (0/1, bipartite) quadratic
programs inherent in them. Our research answers some open problems raised by
the authors in (Stochastic Models for Budget Optimization in Search-Based
Advertising, Algorithmica, 58 (4), 1022-1044, 2010).",targeted advertising
http://arxiv.org/abs/1601.02377v1,"User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.",targeted advertising
http://arxiv.org/abs/1610.03013v2,"The most significant progress in recent years in online display advertising
is what is known as the Real-Time Bidding (RTB) mechanism to buy and sell ads.
RTB essentially facilitates buying an individual ad impression in real time
while it is still being generated from a user's visit. RTB not only scales up
the buying process by aggregating a large amount of available inventories
across publishers but, most importantly, enables direct targeting of individual
users. As such, RTB has fundamentally changed the landscape of digital
marketing. Scientifically, the demand for automation, integration and
optimisation in RTB also brings new research opportunities in information
retrieval, data mining, machine learning and other related fields. In this
monograph, an overview is given of the fundamental infrastructure, algorithms,
and technical solutions of this new frontier of computational advertising. The
covered topics include user response prediction, bid landscape forecasting,
bidding algorithms, revenue optimisation, statistical arbitrage, dynamic
pricing, and ad fraud detection.",targeted advertising
http://arxiv.org/abs/1602.02046v1,"The intrusiveness and the increasing invasiveness of online advertising have,
in the last few years, raised serious concerns regarding user privacy and Web
usability. As a reaction to these concerns, we have witnessed the emergence of
a myriad of ad-blocking and anti-tracking tools, whose aim is to return control
to users over advertising. The problem with these technologies, however, is
that they are extremely limited and radical in their approach: users can only
choose either to block or allow all ads. With around 200 million people
regularly using these tools, the economic model of the Web ---in which users
get content free in return for allowing advertisers to show them ads--- is at
serious peril. In this paper, we propose a smart Web technology that aims at
bringing transparency to online advertising, so that users can make an informed
and equitable decision regarding ad blocking. The proposed technology is
implemented as a Web-browser extension and enables users to exert fine-grained
control over advertising, thus providing them with certain guarantees in terms
of privacy and browsing experience, while preserving the Internet economic
model. Experimental results in a real environment demonstrate the suitability
and feasibility of our approach, and provide preliminary findings on behavioral
targeting from real user browsing profiles.",targeted advertising
http://arxiv.org/abs/1903.04554v1,"Vehicular ad-hoc networks (VANETs) have recently attracted a lot of attention
due to their immense potentials and applications. Wide range of coverage and
accessibility to end users make VANETs a good target for commercial companies.
In this paper, we consider a scenario in which advertising companies aim to
disseminate their advertisements in different areas of a city by utilizing
VANETs infrastructure. These companies compete for renting the VANETs
infrastructure to spread their advertisements. We partition the city map into
different blocks, and consider a manager for all the blocks who is in charge of
splitting the time between interested advertising companies. Each advertising
company (AdC) is charged proportional to the allocated time. In order to find
the best time splitting between AdCs, we propose a Stackelberg game scheme in
which the block manager assigns the companies to the blocks and imposes the
renting prices to different companies in order to maximize its own profit.
Based on this, AdCs request the amount of time they desire to rent the
infrastructure in order to maximize their utilities. To obtain the Stackelberg
equilibrium of the game, a mixed integer nonlinear optimization problem is
solved using the proposed optimal and sub-optimal algorithms. The simulation
results demonstrate that the sub-optimal algorithm approaches the optimal one
in performance with lower complexity.",targeted advertising
http://arxiv.org/abs/1803.05990v1,"Nowadays social media has become one of the largest gatherings of people in
online. There are many ways for the industries to promote their products to the
public through advertising. The variety of advertisement is increasing
dramatically. Businessmen are so much dependent on the advertisement that
significantly it really brought out success in the market and hence practiced
by major industries. Thus, companies are trying hard to draw the attention of
customers on social networks through online advertisement. One of the most
popular social media is Twitter which is popular for short text sharing named
Tweet. People here create their profile with basic information. To ensure the
advertisements are shown to relative people, Twitter targets people based on
language, gender, interest, follower, device, behavior, tailored audiences,
keyword, and geography targeting. Twitter generates interest sets based on
their activities on Twitter. What our framework does is that it determines the
topic of interest from a given list of Tweets if it has any. This process is
called Entity Intersect Categorizing Value (EICV). Each category topic
generates a set of words or phrases related to that topic. An entity set is
created from processing tweets by keyword generation and Twitters data using
Twitter API. Value of entities is matched with the set of categories. If they
cross a threshold value, it results in the category which matched the desired
interest category. For smaller amounts of data sizes, the results show that our
framework performs with higher accuracy rate.",targeted advertising
http://arxiv.org/abs/1506.05851v1,"In targeted online advertising, advertisers look for maximizing campaign
performance under delivery constraint within budget schedule. Most of the
advertisers typically prefer to impose the delivery constraint to spend budget
smoothly over the time in order to reach a wider range of audiences and have a
sustainable impact. Since lots of impressions are traded through public
auctions for online advertising today, the liquidity makes price elasticity and
bid landscape between demand and supply change quite dynamically. Therefore, it
is challenging to perform smooth pacing control and maximize campaign
performance simultaneously. In this paper, we propose a smart pacing approach
in which the delivery pace of each campaign is learned from both offline and
online data to achieve smooth delivery and optimal performance goals. The
implementation of the proposed approach in a real DSP system is also presented.
Experimental evaluations on both real online ad campaigns and offline
simulations show that our approach can effectively improve campaign performance
and achieve delivery goals.",targeted advertising
http://arxiv.org/abs/1404.4106v1,"Mobile geo-location advertising, where mobile ads are targeted based on a
user's location, has been identified as a key growth factor for the mobile
market. As with online advertising, a crucial ingredient for their success is
the development of effective economic mechanisms. An important difference is
that mobile ads are shown sequentially over time and information about the user
can be learned based on their movements. Furthermore, ads need to be shown
selectively to prevent ad fatigue. To this end, we introduce, for the first
time, a user model and suitable economic mechanisms which take these factors
into account. Specifically, we design two truthful mechanisms which produce an
advertisement plan based on the user's movements. One mechanism is allocatively
efficient, but requires exponential compute time in the worst case. The other
requires polynomial time, but is not allocatively efficient. Finally, we
experimentally evaluate the tradeoff between compute time and efficiency of our
mechanisms.",targeted advertising
http://arxiv.org/abs/1804.05183v1,"Vehicular users are emerging as a prime market for targeted advertisement,
where advertisements (ads) are sent from network points of access to vehicles,
and displayed to passengers only if they are relevant to them. In this study,
we take the viewpoint of a broker managing the advertisement system, and
getting paid every time a relevant ad is displayed to an interested user. The
broker selects the ads to broadcast at each point of access so as to maximize
its revenue. In this context, we observe that choosing the ads that best fit
the users' interest could actually hurt the broker's revenue. In light of this
conflict, we present Volfied, an algorithm allowing for conflict-free,
near-optimal ad selection with very low computational complexity. Our
performance evaluation, carried out through real-world vehicular traces, shows
that Volfied increases the broker revenue by up to 70% with provably low
computational complexity, compared to state-of-the-art alternatives.",targeted advertising
http://arxiv.org/abs/1905.02106v1,"Online videos have witnessed an unprecedented growth over the last decade,
owing to wide range of content creation. This provides the advertisement and
marketing agencies plethora of opportunities for targeted advertisements. Such
techniques involve replacing an existing advertisement in a video frame, with a
new advertisement. However, such post-processing of online videos is mostly
done manually by video editors. This is cumbersome and time-consuming. In this
paper, we propose DeepAds -- a deep neural network, based on the simple
encoder-decoder architecture, that can accurately localize the position of an
advert in a video frame. Our approach of localizing billboards in outdoor
scenes using neural nets, is the first of its kind, and achieves the best
performance. We benchmark our proposed method with other semantic segmentation
algorithms, on a public dataset of outdoor scenes with manually annotated
billboard binary maps.",targeted advertising
http://arxiv.org/abs/1404.4533v1,"Retargeting ads are increasingly prevalent on the Internet as their
effectiveness has been shown to outperform conventional targeted ads.
Retargeting ads are not only based on users' interests, but also on their
intents, i.e. commercial products users have shown interest in. Existing
retargeting systems heavily rely on tracking, as retargeting companies need to
know not only the websites a user has visited but also the exact products on
these sites. They are therefore very intrusive, and privacy threatening.
Furthermore, these schemes are still sub-optimal since tracking is partial, and
they often deliver ads that are obsolete (because, for example, the targeted
user has already bought the advertised product).
  This paper presents the first privacy-preserving retargeting ads system. In
the proposed scheme, the retargeting algorithm is distributed between the user
and the advertiser such that no systematic tracking is necessary, more control
and transparency is provided to users, but still a lot of targeting flexibility
is provided to advertisers. We show that our scheme, that relies on homomorphic
encryption, can be efficiently implemented and trivially solves many problems
of existing schemes, such as frequency capping and ads freshness.",targeted advertising
http://arxiv.org/abs/0901.3754v1,"Ad auctions in sponsored search support ``broad match'' that allows an
advertiser to target a large number of queries while bidding only on a limited
number. While giving more expressiveness to advertisers, this feature makes it
challenging to optimize bids to maximize their returns: choosing to bid on a
query as a broad match because it provides high profit results in one bidding
for related queries which may yield low or even negative profits.
  We abstract and study the complexity of the {\em bid optimization problem}
which is to determine an advertiser's bids on a subset of keywords (possibly
using broad match) so that her profit is maximized. In the query language model
when the advertiser is allowed to bid on all queries as broad match, we present
an linear programming (LP)-based polynomial-time algorithm that gets the
optimal profit. In the model in which an advertiser can only bid on keywords,
ie., a subset of keywords as an exact or broad match, we show that this problem
is not approximable within any reasonable approximation factor unless P=NP. To
deal with this hardness result, we present a constant-factor approximation when
the optimal profit significantly exceeds the cost. This algorithm is based on
rounding a natural LP formulation of the problem. Finally, we study a budgeted
variant of the problem, and show that in the query language model, one can find
two budget constrained ad campaigns in polynomial time that implement the
optimal bidding strategy. Our results are the first to address bid optimization
under the broad match feature which is common in ad auctions.",targeted advertising
http://arxiv.org/abs/1202.2097v4,"Motivated by applications to word-of-mouth advertising, we consider a
game-theoretic scenario in which competing advertisers want to target initial
adopters in a social network. Each advertiser wishes to maximize the resulting
cascade of influence, modeled by a general network diffusion process. However,
competition between products may adversely impact the rate of adoption for any
given firm. The resulting framework gives rise to complex preferences that
depend on the specifics of the stochastic diffusion model and the network
topology.
  We study this model from the perspective of a central mechanism, such as a
social networking platform, that can optimize seed placement as a service for
the advertisers. We ask: given the reported demands of the competing firms, how
should a mechanism choose seeds to maximize overall efficiency? Beyond the
algorithmic problem, competition raises issues of strategic behaviour: rational
agents should not be incentivized to underreport their budget demands.
  We show that when there are two players, the social welfare can be
$2$-approximated by a polynomial-time strategyproof mechanism. Our mechanism is
defined recursively, randomizing the order in which advertisers are allocated
seeds according to a particular greedy method. For three or more players, we
demonstrate that under additional assumptions (satisfied by many existing
models of influence spread) there exists a simpler strategyproof
$\frac{e}{e-1}$-approximation mechanism; notably, this second mechanism is not
necessarily strategyproof when there are only two players.",targeted advertising
http://arxiv.org/abs/1109.0097v1,"Recent work in traffic analysis has shown that traffic patterns leaked
through side channels can be used to recover important semantic information.
For instance, attackers can find out which website, or which page on a website,
a user is accessing simply by monitoring the packet size distribution. We show
that traffic analysis is even a greater threat to privacy than previously
thought by introducing a new attack that can be carried out remotely. In
particular, we show that, to perform traffic analysis, adversaries do not need
to directly observe the traffic patterns. Instead, they can gain sufficient
information by sending probes from a far-off vantage point that exploits a
queuing side channel in routers. To demonstrate the threat of such remote
traffic analysis, we study a remote website detection attack that works against
home broadband users. Because the remotely observed traffic patterns are more
noisy than those obtained using previous schemes based on direct local traffic
monitoring, we take a dynamic time warping (DTW) based approach to detecting
fingerprints from the same website. As a new twist on website fingerprinting,
we consider a website detection attack, where the attacker aims to find out
whether a user browses a particular web site, and its privacy implications. We
show experimentally that, although the success of the attack is highly
variable, depending on the target site, for some sites very low error rates. We
also show how such website detection can be used to deanonymize message board
users.",website monitoring
http://arxiv.org/abs/1802.05409v1,"Traffic analysis attacks to identify which web page a client is browsing,
using only her packet metadata --- known as website fingerprinting --- has been
proven effective in closed-world experiments against privacy technologies like
Tor. However, due to the base rate fallacy, these attacks have failed in large
open-world settings against clients that visit sensitive pages with a low base
rate. We find that this is because they have poor precision as they were
designed to maximize recall.
  In this work, we argue that precision is more important than recall for
open-world website fingerprinting. For this reason, we develop three classes of
{\em precision optimizers}, based on confidence, distance, and ensemble
learning, that can be applied to any classifier to increase precision. We test
them on known website fingerprinting attacks and show significant improvements
in precision. Against a difficult scenario, where the attacker wants to monitor
and distinguish 100 sensitive pages each with a low mean base rate of 0.00001,
our best optimized classifier can achieve a precision of 0.78; the highest
precision of any known attack before optimization was 0.014. We use precise
classifiers to tackle realistic objectives in website fingerprinting, including
selection, identification, and defeating website fingerprinting defenses.",website monitoring
http://arxiv.org/abs/1509.00789v3,"Website fingerprinting enables an attacker to infer which web page a client
is browsing through encrypted or anonymized network connections. We present a
new website fingerprinting technique based on random decision forests and
evaluate performance over standard web pages as well as Tor hidden services, on
a larger scale than previous works. Our technique, k-fingerprinting, performs
better than current state-of-the-art attacks even against website
fingerprinting defenses, and we show that it is possible to launch a website
fingerprinting attack in the face of a large amount of noisy data. We can
correctly determine which of 30 monitored hidden services a client is visiting
with 85% true positive rate (TPR), a false positive rate (FPR) as low as 0.02%,
from a world size of 100,000 unmonitored web pages. We further show that error
rates vary widely between web resources, and thus some patterns of use will be
predictably more vulnerable to attack than others.",website monitoring
http://arxiv.org/abs/1908.02548v1,"The automated detection of corrosion from images (i.e., photographs) or video
(i.e., drone footage) presents significant advantages in terms of corrosion
monitoring. Such advantages include access to remote locations, mitigation of
risk to inspectors, cost savings and monitoring speed. The automated detection
of corrosion requires deep learning to approach human level artificial
intelligence (A.I.). The training of a deep learning model requires intensive
image labelling, and in order to generate a large database of labelled images,
crowd sourced labelling via a dedicated website was sought. The website
(corrosiondetector.com) permits any user to label images, with such labelling
then contributing to the training of a cloud based A.I. model - with such a
cloud-based model then capable of assessing any fresh (or uploaded) image for
the presence of corrosion. In other words, the website includes both the crowd
sourced training process, but also the end use of the evolving model. Herein,
the results and findings from the website (corrosiondetector.com) over the
period of approximately one month, are reported.",website monitoring
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website monitoring
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website monitoring
http://arxiv.org/abs/1601.07077v1,"Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.",website monitoring
http://arxiv.org/abs/1705.04437v1,"The browser history reveals highly sensitive information about users, such as
financial status, health conditions, or political views. Private browsing modes
and anonymity networks are consequently important tools to preserve the privacy
not only of regular users but in particular of whistleblowers and dissidents.
Yet, in this work we show how a malicious application can infer opened websites
from Google Chrome in Incognito mode and from Tor Browser by exploiting
hardware performance events (HPEs). In particular, we analyze the browsers'
microarchitectural footprint with the help of advanced Machine Learning
techniques: k-th Nearest Neighbors, Decision Trees, Support Vector Machines,
and in contrast to previous literature also Convolutional Neural Networks. We
profile 40 different websites, 30 of the top Alexa sites and 10 whistleblowing
portals, on two machines featuring an Intel and an ARM processor. By monitoring
retired instructions, cache accesses, and bus cycles for at most 5 seconds, we
manage to classify the selected websites with a success rate of up to 86.3%.
The results show that hardware performance events can clearly undermine the
privacy of web users. We therefore propose mitigation strategies that impede
our attacks and still allow legitimate use of HPEs.",website monitoring
http://arxiv.org/abs/1612.05318v1,"CUORE is a cryogenic experiment searching primarily for neutrinoless double
decay in $^{130}$Te. It will begin data-taking operations in 2016. To monitor
the cryostat and detector during commissioning and data taking, we have
designed and developed Slow Monitoring systems. In addition to real-time
systems using LabVIEW, we have an alarm, analysis, and archiving website that
uses MongoDB, AngularJS, and Bootstrap software. These modern, state of the art
software packages make the monitoring system transparent, easily maintainable,
and accessible on many platforms including mobile devices.",website monitoring
http://arxiv.org/abs/1507.06562v1,"As of February, 2015, HTTP/2, the update to the 16-year-old HTTP 1.1, is
officially complete. HTTP/2 aims to improve the Web experience by solving
well-known problems (e.g., head of line blocking and redundant headers), while
introducing new features (e.g., server push and content priority). On paper
HTTP/2 represents the future of the Web. Yet, it is unclear whether the Web
itself will, and should, hop on board. To shed some light on these questions,
we built a measurement platform that monitors HTTP/2 adoption and performance
across the Alexa top 1 million websites on a daily basis. Our system is live
and up-to-date results can be viewed at http://isthewebhttp2yet.com/. In this
paper, we report our initial findings from a 6 month measurement campaign
(November 2014 - May 2015). We find 13,000 websites reporting HTTP/2 support,
but only 600, mostly hosted by Google and Twitter, actually serving content. In
terms of speed, we find no significant benefits from HTTP/2 under stable
network conditions. More benefits appear in a 3G network where current Web
development practices make HTTP/2 more resilient to losses and delay variation
than previously believed.",website monitoring
http://arxiv.org/abs/1704.04937v2,"Browsers can detect malicious websites that are provisioned with forged or
fake TLS/SSL certificates. However, they are not so good at detecting malicious
websites if they are provisioned with mistakenly issued certificates or
certificates that have been issued by a compromised certificate authority.
Google proposed certificate transparency which is an open framework to monitor
and audit certificates in real time. Thereafter, a few other certificate
transparency schemes have been proposed which can even handle revocation. All
currently known constructions use Merkle hash trees and have proof size
logarithmic in the number of certificates/domain owners.
  We present a new certificate transparency scheme with short (constant size)
proofs. Our construction makes use of dynamic bilinear-map accumulators. The
scheme has many desirable properties like efficient revocation, low
verification cost and update costs comparable to the existing schemes. We
provide proofs of security and evaluate the performance of our scheme.",website monitoring
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",website monitoring
http://arxiv.org/abs/1908.02900v1,"Viral diseases are major sources of poor yields for cassava, the 2nd largest
provider of carbohydrates in Africa.At least 80% of small-holder farmer
households in Sub-Saharan Africa grow cassava. Since many of these farmers have
smart phones, they can easily obtain photos of dis-eased and healthy cassava
leaves in their farms, allowing the opportunity to use computer vision
techniques to monitor the disease type and severity and increase yields.
How-ever, annotating these images is extremely difficult as ex-perts who are
able to distinguish between highly similar dis-eases need to be employed. We
provide a dataset of labeled and unlabeled cassava leaves and formulate a
Kaggle challenge to encourage participants to improve the performance of their
algorithms using semi-supervised approaches. This paper describes our dataset
and challenge which is part of the Fine-Grained Visual Categorization workshop
at CVPR2019.",website monitoring
http://arxiv.org/abs/1804.01237v1,"With the rapid growth of mobile internet, mobile application, like website
navigation, searching, e-Shopping and app download, etc. are all popular in
worldwide. Meanwhile, it become more and more popular that traditional HTTP
protocol, which is also applying in not only web browsing but also
communication between mobile application clients and servers. Besides, it has
made HTTP Hijacking profitable. Furthermore, it has brought a lot of troubles
for users, network operators and ISP. We analyze the principle of HTTP spectral
Hijacking and present a mechanism of collaboratively detecting and locating
called Co HijackingMonitor. Experimental result shows that, Co HijackingMonitor
can solve the hijacking problem effectively.",website monitoring
http://arxiv.org/abs/1807.06373v1,"Predicting the popularity of online content has attracted much attention in
the past few years. In news rooms, for instance, journalists and editors are
keen to know, as soon as possible, the articles that will bring the most
traffic into their website. The relevant literature includes a number of
approaches and algorithms to perform this forecasting. Most of the proposed
methods require monitoring the popularity of content during some time after it
is posted, before making any longer-term prediction. In this paper, we propose
a new approach for predicting the popularity of news articles before they go
online. Our approach complements existing content-based methods, and is based
on a number of observations regarding article similarity and topicality. First,
the popularity of a new article is correlated with the popularity of similar
articles of recent publication. Second, the popularity of the new article is
related to the recent historical popularity of its main topic. Based on these
observations, we use time series forecasting to predict the number of visits an
article will receive. Our experiments, conducted on a real data collection of
articles in an international news website, demonstrate the effectiveness and
efficiency of the proposed method.",website monitoring
http://arxiv.org/abs/1811.11218v1,"Over the past years, literature has shown that attacks exploiting the
microarchitecture of modern processors pose a serious threat to the privacy of
mobile phone users. This is because applications leave distinct footprints in
the processor, which can be used by malware to infer user activities. In this
work, we show that these inference attacks are considerably more practical when
combined with advanced AI techniques. In particular, we focus on profiling the
activity in the last-level cache (LLC) of ARM processors. We employ a simple
Prime+Probe based monitoring technique to obtain cache traces, which we
classify with Deep Learning methods including Convolutional Neural Networks. We
demonstrate our approach on an off-the-shelf Android phone by launching a
successful attack from an unprivileged, zeropermission App in well under a
minute. The App thereby detects running applications with an accuracy of 98%
and reveals opened websites and streaming videos by monitoring the LLC for at
most 6 seconds. This is possible, since Deep Learning compensates measurement
disturbances stemming from the inherently noisy LLC monitoring and unfavorable
cache characteristics such as random line replacement policies. In summary, our
results show that thanks to advanced AI techniques, inference attacks are
becoming alarmingly easy to implement and execute in practice. This once more
calls for countermeasures that confine microarchitectural leakage and protect
mobile phone applications, especially those valuing the privacy of their users.",website monitoring
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",website monitoring
http://arxiv.org/abs/1811.09126v2,"Online monitoring user cardinalities (or degrees) in graph streams is
fundamental for many applications. For example in a bipartite graph
representing user-website visiting activities, user cardinalities (the number
of distinct visited websites) are monitored to report network anomalies. These
real-world graph streams may contain user-item duplicates and have a huge
number of distinct user-item pairs, therefore, it is infeasible to exactly
compute user cardinalities when memory and computation resources are
limited.Existing methods are designed to approximately estimate user
cardinalities, whose accuracy highly depends on parameters that are not easy to
set. Moreover, these methods cannot provide anytime-available estimation, as
the user cardinalities are computed at the end of the data stream. Real-time
applications such as anomaly detection require that user cardinalities are
estimated on the fly. To address these problems, we develop novel bit and
register sharing algorithms, which use a bit array and a register array to
build a compact sketch of all users' connected items respectively. Compared
with previous bit and register sharing methods, our algorithms exploit the
dynamic properties of the bit and register arrays (e.g., the fraction of zero
bits in the bit array at each time) to significantly improve the estimation
accuracy, and have low time complexity (O(1)) to update the estimations each
time they observe a new user-item pair. In addition, our algorithms are simple
and easy to use, without requirements to tune any parameter. We evaluate the
performance of our methods on real-world datasets. The experimental results
demonstrate that our methods are several times more accurate and faster than
state-of-the-art methods using the same amount of memory.",website monitoring
http://arxiv.org/abs/1105.1234v2,"Trojan virus attacks pose one of the most serious threats to computer
security. A Trojan horse is typically separated into two parts - a server and a
client. It is the client that is cleverly disguised as significant software and
positioned in peer-to-peer file sharing networks, or unauthorized download
websites. The most common means of infection is through email attachments. The
developer of the virus usually uses various spamming techniques in order to
distribute the virus to unsuspecting users. Malware developers use chat
software as another method to spread their Trojan horse viruses such as Yahoo
Messenger and Skype. The objective of this paper is to explore the network
packet information and detect the behavior of Trojan attacks to monitoring
operating systems such as Windows and Linux. This is accomplished by detecting
and analyzing the Trojan infected packet from a network segment -which passes
through email attachment- before attacking a host computer. The results that
have been obtained to detect information and to store infected packets through
monitoring when using the web browser also compare the behaviors of Linux and
Windows using the payload size after implementing the Wireshark sniffer packet
results. Conclusions of the figures analysis from the packet captured data to
analyze the control bit, and check the behavior of the control bits, and the
usability of the operating systems Linux and Windows.",website monitoring
http://arxiv.org/abs/1203.4099v1,"The aim of the Karlsruhe Tritium Neutrino experiment (KATRIN) is the direct
(model-independent) measurement of the neutrino mass. For that purpose a
windowless gaseous tritium source is used, with a tritium throughput of 40
g/day. In order to reach the design sensitivity of 0.2 eV/c^{2} (90% C.L.) the
key parameters of the tritium source, i.e. the gas inlet rate and the gas
composition, have to be stabilized and monitored at the 0.1% level (1 sigma).
Any small change of the tritium gas composition will manifest itself in
non-negligible effects on the KATRIN measurements; therefore, Laser Raman
spectroscopy (LARA) is the method of choice for the monitoring of the gas
composition because it is a non-invasive and fast in-line measurement
technique. In these proceedings, the requirements of KATRIN for statistical and
systematical uncertainties of this method are discussed. An overview of the
current performance of the LARA system in regard to precision will be given. In
addition, two complementary approaches of intensity calibration are presented.",website monitoring
http://arxiv.org/abs/1610.02065v1,"After carefully considering the scalability problem in Tor and exhaustively
evaluating related works on AS-level adversaries, the author proposes
ASmoniTor, which is an autonomous system monitor for mitigating correlation
attacks in the Tor network. In contrast to prior works, which often released
offline packets, including the source code of a modified Tor client and a
snapshot of the Internet topology, ASmoniTor is an online system that assists
end users with mitigating the threat of AS-level adversaries in a near
real-time fashion. For Tor clients proposed in previous works, users need to
compile the source code on their machine and continually update the snapshot of
the Internet topology in order to obtain accurate AS-path inferences. On the
contrary, ASmoniTor is an online platform that can be utilized easily by not
only technical users, but also by users without a technical background, because
they only need to access it via Tor and input two parameters to execute an
AS-aware path selection algorithm. With ASmoniTor, the author makes three key
technical contributions to the research against AS-level adversaries in the Tor
network. First, ASmoniTor does not require the users to initiate complicated
source code compilations. Second, it helps to reduce errors in AS-path
inferences by letting users input a set of suspected ASes obtained directly
from their own traceroute measurements. Third, the Internet topology database
at the back-end of ASmoniTor is periodically updated to assure near real-time
AS-path inferences between Tor exit nodes and the most likely visited websites.
Finally, in addition to its convenience, ASmoniTor gives users full control
over the information they want to input, thus preserving their privacy.",website monitoring
http://arxiv.org/abs/1709.05628v1,"Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.",website monitoring
http://arxiv.org/abs/1902.03937v2,"Identifying and monitoring Open Access (OA) publications might seem a trivial
task while practical efforts prove otherwise. Contradictory information arise
often depending on metadata employed. We strive to assign OA status to
publications in Web of Science (WOS) and Scopus while complementing it with
different sources of OA information to resolve contradicting cases. We linked
publications from WOS and Scopus via DOIs and ISSNs to Unpaywall, Crossref,
DOAJ and ROAD. Only about 50% of articles and reviews from WOS and Scopus could
be matched via a DOI to Unpaywall. Matching with Crossref brought 56 distinct
licences, which define in many cases the legally binding access status of
publications. But only 44% of publications hold only a single licence on
Crossref, while more than 50% have no licence information submitted to
Crossref. Contrasting OA information from Crossref licences with Unpaywall we
found contradictory cases overall amounting to more than 25%, which might be
partially explained by (ex-)including green OA. A further manual check found
about 17% of OA publications that are not accessible and 15% non-OA
publications that are accessible through publishers' websites. These
preliminary results suggest that identification of OA state of publications
denotes a difficult and currently unfulfilled task.",website monitoring
http://arxiv.org/abs/1409.8171v1,"Peer-to-Peer (P2P) file-sharing is becoming increasingly popular in recent
years. In 2012, it was reported that P2P traffic consumed over 5,374 petabytes
per month, which accounted for approximately 20.5% of consumer internet
traffic. TV is the popular content type on The Pirate Bay (the world's largest
BitTorrent indexing website). In this paper, an analysis of the swarms of the
most popular pirated TV shows is conducted. The purpose of this data gathering
exercise is to enumerate the peer distribution at different geolocational
levels, to measure the temporal trend of the swarm and to discover the amount
of cross-swarm peer participation. Snapshots containing peer related
information involved in the unauthorised distribution of this content were
collected at a high frequency resulting in a more accurate landscape of the
total involvement. The volume of data collected throughout the monitoring of
the network exceeded 2 terabytes. The presented analysis and the results
presented can aid in network usage prediction, bandwidth provisioning and
future network design.",website monitoring
http://arxiv.org/abs/1504.06093v2,"There are over 1.2 million applications on the Google Play store today with a
large number of competing applications for any given use or function. This
creates challenges for users in selecting the right application. Moreover, some
of the applications being of dubious origin, there are no mechanisms for users
to understand who the applications are talking to, and to what extent. In our
work, we first develop a lightweight characterization methodology that can
automatically extract descriptions of application network behavior, and apply
this to a large selection of applications from the Google App Store. We find
several instances of overly aggressive communication with tracking websites, of
excessive communication with ad related sites, and of communication with sites
previously associated with malware activity. Our results underscore the need
for a tool to provide users more visibility into the communication of apps
installed on their mobile devices. To this end, we develop an Android
application to do just this; our application monitors outgoing traffic,
associates it with particular applications, and then identifies destinations in
particular categories that we believe suspicious or else important to reveal to
the end-user.",website monitoring
http://arxiv.org/abs/1902.06306v1,"Anonymous channels allow users to connect to websites or communicate with one
another privately. Assume that either Alice or Allison is communicating with (a
possibly corrupt) Bob. To protect the sender, we seek a protocol that provably
guarantees that these two scenarios are indistinguishable to an adversary that
can monitor the traffic on all channels of the network and control the internal
operations in a constant fraction of the nodes.
  Onion routing is the method of choice for achieving anonymous communication,
used for example by Tor. In an onion routing protocol, messages travel through
several intermediaries before arriving at their destinations; they are wrapped
in layers of encryption (hence they are called ``onions''). In this paper, we
give the first rigorous characterization of the complexity of onion routing
protocols for anonymous communication through public networks. We show that in
order to provide anonymity, an onion routing scheme requires each participant
to transmit, on average, a superlogarithmic number of onions. We match these
negative results with a protocol in which every participant creates a
polylogarithmic number of onions and participates in a polylogarithmic number
of transmissions.",website monitoring
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",website tracking
http://arxiv.org/abs/1703.07578v1,"Third party tracking is the practice by which third parties recognize users
accross different websites as they browse the web. Recent studies show that 90%
of websites contain third party content that is tracking its users across the
web. Website developers often need to include third party content in order to
provide basic functionality. However, when a developer includes a third party
content, she cannot know whether the third party contains tracking mechanisms.
If a website developer wants to protect her users from being tracked, the only
solution is to exclude any third-party content, thus trading functionality for
privacy. We describe and implement a privacy-preserving web architecture that
gives website developers a control over third party tracking: developers are
able to include functionally useful third party content, the same time ensuring
that the end users are not tracked by the third parties.",website tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",website tracking
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",website tracking
http://arxiv.org/abs/1805.01392v1,"Web tracking technologies are pervasive and operated by a few large
technology companies. This technology, and the use of the collected data has
been implicated in influencing elections, fake news, discrimination, and even
health decisions. Little is known about how this technology is deployed on
hospital or other health related websites. The websites of the 210 public
hospitals in the state of Illinois, USA were evaluated with a web tracker
identification tool. Web trackers were identified on 94% of hospital webs
sites, with an average of 3.5 trackers on the websites of general hospitals.
The websites of smaller critical access hospitals used an average of 2 web
trackers. The most common web tracker identified was Google Analytics, found on
74% of Illinois hospital websites. Of the web trackers discovered, 88% were
operated by Google and 26% by Facebook. In light of revelations about how web
browsing profiles have been used and misused, search bubbles, and the potential
for algorithmic discrimination hospital leadership and policy makers must
carefully consider if it is appropriate to use third party tracking technology
on hospital web sites.",website tracking
http://arxiv.org/abs/1801.04829v2,"This paper presents a pilot study on developing an instrument to predict the
quality of e-commerce websites. The 8C model was adopted as the reference model
of the heuristic evaluation. Each dimension of the 8C was mapped into a set of
quantitative website elements, selected websites were scraped to get the
quantitative website elements, and the score of each dimension was calculated.
A software was developed in PHP for the experiments. In the training process,
10 experiments were conducted and quantitative analyses were regressively
conducted between the experiments. The conversion rate was used to verify the
heuristic evaluation of an e-commerce website after each experiment. The
results showed that the mapping revisions between the experiments improved the
performance of the evaluation instrument, therefore the experiment process and
the quantitative mapping revision guideline proposed was on the right track.
The software resulted from the experiment 10 can serve as the aimed e-commerce
website evaluation instrument. The experiment results and the future work have
been discussed.",website tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",website tracking
http://arxiv.org/abs/1607.07403v2,"We perform a large-scale analysis of third-party trackers on the World Wide
Web from more than 3.5 billion web pages of the CommonCrawl 2012 corpus. We
extract a dataset containing more than 140 million third-party embeddings in
over 41 million domains. To the best of our knowledge, this constitutes the
largest web tracking dataset collected so far, and exceeds related studies by
more than an order of magnitude in the number of domains and web pages
analyzed. We perform a large-scale study of online tracking, on three levels:
(1) On a global level, we give a precise figure for the extent of tracking,
give insights into the structure of the `online tracking sphere' and analyse
which trackers are used by how many websites. (2) On a country-specific level,
we analyse which trackers are used by websites in different countries, and
identify the countries in which websites choose significantly different
trackers than in the rest of the world. (3) We answer the question whether the
content of websites influences the choice of trackers they use, leveraging more
than 90 thousand categorized domains. In particular, we analyse whether highly
privacy-critical websites make different choices of trackers than other
websites. Based on the performed analyses, we confirm that trackers are
widespread (as expected), and that a small number of trackers dominates the web
(Google, Facebook and Twitter). In particular, the three tracking domains with
the highest PageRank are all owned by Google. The only exception to this
pattern are a few countries such as China and Russia. Our results suggest that
this dominance is strongly associated with country-specific political factors
such as freedom of the press. We also confirm that websites with highly
privacy-critical content are less likely to contain trackers (60% vs 90% for
other websites), even though the majority of them still do contain trackers.",website tracking
http://arxiv.org/abs/1511.00619v1,"This article provides a quantitative analysis of privacy-compromising
mechanisms on 1 million popular websites. Findings indicate that nearly 9 in 10
websites leak user data to parties of which the user is likely unaware; more
than 6 in 10 websites spawn third- party cookies; and more than 8 in 10
websites load Javascript code from external parties onto users' computers.
Sites that leak user data contact an average of nine external domains,
indicating that users may be tracked by multiple entities in tandem. By tracing
the unintended disclosure of personal browsing histories on the Web, it is
revealed that a handful of U.S. companies receive the vast bulk of user data.
Finally, roughly 1 in 5 websites are potentially vulnerable to known National
Security Agency spying techniques at the time of analysis.",website tracking
http://arxiv.org/abs/1502.00317v2,"Cursor tracking data contains information about website visitors which may
provide new ways to understand visitors and their needs. This paper presents an
Amazon Mechanical Turk study where participants were tracked as they used
modified variants of the Wikipedia and BBC News websites. Participants were
asked to complete reading and information-finding tasks. The results showed
that it was possible to differentiate between users reading content and users
looking for information based on cursor data. The effects of website
aesthetics, user interest and cursor hardware were also analysed which showed
it was possible to identify hardware from cursor data, but no relationship
between cursor data and engagement was found. The implications of these
results, from the impact on web analytics to the design of experiments to
assess user engagement, are discussed.",website tracking
http://arxiv.org/abs/1907.06520v1,"This paper explores tracking and privacy risks on pornography websites. Our
analysis of 22,484 pornography websites indicated that 93% leak user data to a
third party. Tracking on these sites is highly concentrated by a handful of
major companies, which we identify. We successfully extracted privacy policies
for 3,856 sites, 17% of the total. The policies were written such that one
might need a two-year college education to understand them. Our content
analysis of the sample's domains indicated 44.97% of them expose or suggest a
specific gender/sexual identity or interest likely to be linked to the user. We
identify three core implications of the quantitative results: 1) the
unique/elevated risks of porn data leakage versus other types of data, 2) the
particular risks/impact for vulnerable populations, and 3) the complications of
providing consent for porn site users and the need for affirmative consent in
these online sexual interactions.",website tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",website tracking
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",website tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",website tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",website tracking
http://arxiv.org/abs/1805.01187v1,"A dominant regulatory model for web privacy is ""notice and choice"". In this
model, users are notified of data collection and provided with options to
control it. To examine the efficacy of this approach, this study presents the
first large-scale audit of disclosure of third-party data collection in website
privacy policies. Data flows on one million websites are analyzed and over
200,000 websites' privacy policies are audited to determine if users are
notified of the names of the companies which collect their data. Policies from
25 prominent third-party data collectors are also examined to provide deeper
insights into the totality of the policy environment. Policies are additionally
audited to determine if the choice expressed by the ""Do Not Track"" browser
setting is respected.
  Third-party data collection is wide-spread, but fewer than 15% of attributed
data flows are disclosed. The third-parties most likely to be disclosed are
those with consumer services users may be aware of, those without consumer
services are less likely to be mentioned. Policies are difficult to understand
and the average time requirement to read both a given site{\guillemotright}s
policy and the associated third-party policies exceeds 84 minutes. Only 7% of
first-party site policies mention the Do Not Track signal, and the majority of
such mentions are to specify that the signal is ignored. Among third-party
policies examined, none offer unqualified support for the Do Not Track signal.
Findings indicate that current implementations of ""notice and choice"" fail to
provide notice or respect choice.",website tracking
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",website tracking
http://arxiv.org/abs/1812.01514v3,"Web tracking has been extensively studied over the last decade. To detect
tracking, previous studies and user tools rely on filter lists. However, it has
been shown that filter lists miss trackers. In this paper, we propose an
alternative method to detect trackers inspired by analyzing behavior of
invisible pixels. By crawling 84,658 webpages from 8,744 domains, we detect
that third-party invisible pixels are widely deployed: they are present on more
than 94.51% of domains and constitute 35.66% of all third-party images. We
propose a fine-grained behavioral classification of tracking based on the
analysis of invisible pixels. We use this classification to detect new
categories of tracking and uncover new collaborations between domains on the
full dataset of 4,216,454 third-party requests. We demonstrate that two popular
methods to detect tracking, based on EasyList&EasyPrivacy and on Disconnect
lists respectively miss 25.22% and 30.34% of the trackers that we detect.
Moreover, we find that if we combine all three lists 379,245 requests
originated from 8,744 domains still track users on 68.70% of websites.",website tracking
http://arxiv.org/abs/1802.02507v1,"Third-party networks collect vast amounts of data about users via web sites
and mobile applications. Consolidations among tracker companies can
significantly increase their individual tracking capabilities, prompting
scrutiny by competition regulators. Traditional measures of market share, based
on revenue or sales, fail to represent the tracking capability of a tracker,
especially if it spans both web and mobile. This paper proposes a new approach
to measure the concentration of tracking capability, based on the reach of a
tracker on popular websites and apps. Our results reveal that tracker
prominence and parent-subsidiary relationships have significant impact on
accurately measuring concentration.",website tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",website tracking
http://arxiv.org/abs/1201.3783v1,"As the popularity of content sharing websites such as YouTube and Flickr has
increased, they have become targets for spam, phishing and the distribution of
malware. On YouTube, the facility for users to post comments can be used by
spam campaigns to direct unsuspecting users to bogus e-commerce websites. In
this paper, we demonstrate how such campaigns can be tracked over time using
network motif profiling, i.e. by tracking counts of indicative network motifs.
By considering all motifs of up to five nodes, we identify discriminating
motifs that reveal two distinctly different spam campaign strategies. One of
these strategies uses a small number of spam user accounts to comment on a
large number of videos, whereas a larger number of accounts is used with the
other. We present an evaluation that uses motif profiling to track two active
campaigns matching these strategies, and identify some of the associated user
accounts.",website tracking
http://arxiv.org/abs/1805.09155v2,"User demand for blocking advertising and tracking online is large and
growing. Existing tools, both deployed and described in research, have proven
useful, but lack either the completeness or robustness needed for a general
solution. Existing detection approaches generally focus on only one aspect of
advertising or tracking (e.g. URL patterns, code structure), making existing
approaches susceptible to evasion.
  In this work we present AdGraph, a novel graph-based machine learning
approach for detecting advertising and tracking resources on the web. AdGraph
differs from existing approaches by building a graph representation of the HTML
structure, network requests, and JavaScript behavior of a webpage, and using
this unique representation to train a classifier for identifying advertising
and tracking resources. Because AdGraph considers many aspects of the context a
network request takes place in, it is less susceptible to the single-factor
evasion techniques that flummox existing approaches.
  We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly
accurate, able to replicate the labels of human-generated filter lists with
95.33% accuracy, and can even identify many mistakes in filter lists. We
implement AdGraph as a modification to Chromium. AdGraph adds only minor
overhead to page loading and execution, and is actually faster than stock
Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we
conclude that AdGraph is both accurate enough and performant enough for online
use, breaking comparable or fewer websites than popular filter list based
approaches.",website tracking
http://arxiv.org/abs/1806.09111v1,"We present WPSE, a browser-side security monitor for web protocols designed
to ensure compliance with the intended protocol flow, as well as
confidentiality and integrity properties of messages. We formally prove that
WPSE is expressive enough to protect web applications from a wide range of
protocol implementation bugs and web attacks. We discuss concrete examples of
attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a
novel attack on the Google implementation of SAML 2.0 which we discovered by
formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry
out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90
tested websites, we identify security flaws in 55 websites (61.1%), including
new critical vulnerabilities introduced by tracking libraries such as Facebook
Pixel, all of which fixable by WPSE. Finally, we show that WPSE works
flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused
by custom implementations deviating from the OAuth 2.0 specification, one of
which introducing a critical vulnerability.",website tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",website tracking
http://arxiv.org/abs/1409.1066v1,"The presence of third-party tracking on websites has become customary.
However, our understanding of the third-party ecosystem is still very
rudimentary. We examine third-party trackers from a geographical perspective,
observing the third-party tracking ecosystem from 29 countries across the
globe. When examining the data by region (North America, South America, Europe,
East Asia, Middle East, and Oceania), we observe significant geographical
variation between regions and countries within regions. We find trackers that
focus on specific regions and countries, and some that are hosted in countries
outside their expected target tracking domain. Given the differences in
regulatory regimes between jurisdictions, we believe this analysis sheds light
on the geographical properties of this ecosystem and on the problems that these
may pose to our ability to track and manage the different data silos that now
store personal data about us all.",website tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",website tracking
http://arxiv.org/abs/1908.02261v1,"We turn our attention to the elephant in the room of data protection, which
is none other than the simple and obvious question: ""Who's tracking sensitive
domains?"". Despite a fast-growing amount of work on more complex facets of the
interplay between privacy and the business models of the Web, the obvious
question of who collects data on domains where most people would prefer not be
seen, has received rather limited attention. First, we develop a methodology
for automatically annotating websites that belong to a sensitive category, e.g.
as defined by the General Data Protection Regulation (GDPR). Then, we extract
the third party tracking services included directly, or via recursive
inclusions, by the above mentioned sites. Having analyzed around 30k sensitive
domains, we show that such domains are tracked, albeit less intensely than the
mainstream ones. Looking in detail at the tracking services operating on them,
we find well known names, as well as some less known ones, including some
specializing on specific sensitive categories.",website tracking
http://arxiv.org/abs/1603.06289v1,"Numerous tools have been developed to aggressively block the execution of
popular JavaScript programs (JS) in Web browsers. Such blocking also affects
functionality of webpages and impairs user experience. As a consequence, many
privacy preserving tools (PP-Tools) that have been developed to limit online
tracking, often executed via JS, may suffer from poor performance and limited
uptake. A mechanism that can isolate JS necessary for proper functioning of the
website from tracking JS would thus be useful. Through the use of a manually
labelled dataset composed of 2,612 JS, we show how current PP-Tools are
ineffective in finding the right balance between blocking tracking JS and
allowing functional JS. To the best of our knowledge, this is the first study
to assess the performance of current web PP-Tools.
  To improve this balance, we examine the two classes of JS and hypothesize
that tracking JS share structural similarities that can be used to
differentiate them from functional JS. The rationale of our approach is that
web developers often borrow and customize existing pieces of code in order to
embed tracking (resp. functional) JS into their webpages. We then propose
one-class machine learning classifiers using syntactic and semantic features
extracted from JS. When trained only on samples of tracking JS, our classifiers
achieve an accuracy of 99%, where the best of the PP-Tools achieved an accuracy
of 78%.
  We further test our classifiers and several popular PP-Tools on a corpus of
4K websites with 135K JS. The output of our best classifier on this data is
between 20 to 64% different from the PP-Tools. We manually analyse a sample of
the JS for which our classifier is in disagreement with all other PP-Tools, and
show that our approach is not only able to enhance user web experience by
correctly classifying more functional JS, but also discovers previously unknown
tracking services.",website tracking
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",website tracking
http://arxiv.org/abs/1811.00918v1,"Web developers routinely rely on third-party Java-Script libraries such as
jQuery to enhance the functionality of their sites. However, if not properly
maintained, such dependencies can create attack vectors allowing a site to be
compromised.
  In this paper, we conduct the first comprehensive study of client-side
JavaScript library usage and the resulting security implications across the
Web. Using data from over 133 k websites, we show that 37% of them include at
least one library with a known vulnerability; the time lag behind the newest
release of a library is measured in the order of years. In order to better
understand why websites use so many vulnerable or outdated libraries, we track
causal inclusion relationships and quantify different scenarios. We observe
sites including libraries in ad hoc and often transitive ways, which can lead
to different versions of the same library being loaded into the same document
at the same time. Furthermore, we find that libraries included transitively, or
via ad and tracking code, are more likely to be vulnerable. This demonstrates
that not only website administrators, but also the dynamic architecture and
developers of third-party services are to blame for the Web's poor state of
library management.
  The results of our work underline the need for more thorough approaches to
dependency management, code maintenance and third-party code inclusion on the
Web.",website tracking
http://arxiv.org/abs/1404.4533v1,"Retargeting ads are increasingly prevalent on the Internet as their
effectiveness has been shown to outperform conventional targeted ads.
Retargeting ads are not only based on users' interests, but also on their
intents, i.e. commercial products users have shown interest in. Existing
retargeting systems heavily rely on tracking, as retargeting companies need to
know not only the websites a user has visited but also the exact products on
these sites. They are therefore very intrusive, and privacy threatening.
Furthermore, these schemes are still sub-optimal since tracking is partial, and
they often deliver ads that are obsolete (because, for example, the targeted
user has already bought the advertised product).
  This paper presents the first privacy-preserving retargeting ads system. In
the proposed scheme, the retargeting algorithm is distributed between the user
and the advertiser such that no systematic tracking is necessary, more control
and transparency is provided to users, but still a lot of targeting flexibility
is provided to advertisers. We show that our scheme, that relies on homomorphic
encryption, can be efficiently implemented and trivially solves many problems
of existing schemes, such as frequency capping and ads freshness.",website tracking
http://arxiv.org/abs/1407.0697v1,"SLA (Service level agreement) is defined by an organization to fulfil its
client requirements, the time within which the deliverables should be turned
over to the clients. Tracking of SLA can be done manually by checking the
status, priority of any particular task. Manual SLA tracking takes time as one
has to go over each and every task that needs to be completed. For instance,
you ordered a product from a website and you are not happy with the quality of
the product and want to replace the same on urgent basis, You send mail to the
customer support department, the query/complaint will be submitted in a queue
and will be processed basis of its priority and urgency (The SLA for responding
back to customers concern are listed in the policy). This online SLA tracking
system will ensure that no queries/complaints are missed and are processed in
an organized manner as per their priority and the date by when it should be
handled. The portal will provide the status of the complaints for that
particular day and the ones which have been pending since last week. The
information can be refreshed as per the client need (within what time frame the
complaint should be addressed).",website tracking
http://arxiv.org/abs/1703.09771v2,"We present a temporal 6-DOF tracking method which leverages deep learning to
achieve state-of-the-art performance on challenging datasets of real world
capture. Our method is both more accurate and more robust to occlusions than
the existing best performing approaches while maintaining real-time
performance. To assess its efficacy, we evaluate our approach on several
challenging RGBD sequences of real objects in a variety of conditions. Notably,
we systematically evaluate robustness to occlusions through a series of
sequences where the object to be tracked is increasingly occluded. Finally, our
approach is purely data-driven and does not require any hand-designed features:
robust tracking is automatically learned from data.",website tracking
http://arxiv.org/abs/1905.03518v1,"Small TCP flows make up the majority of web flows. For them, the TCP
three-way handshake represents a significant delay overhead. The TCP Fast Open
(TFO) protocol provides zero round-trip time (0-RTT) handshakes for subsequent
TCP connections to the same host. In this paper, we present real-world privacy
and performance limitations of TFO. We investigated its deployment on popular
websites and browsers. We found that a client revisiting a web site for the
first time fails to use an abbreviated TFO handshake about 40% of the time due
to web server load-balancing. Our analysis further reveals significant privacy
problems in the protocol design and implementation. Network-based attackers and
online trackers can exploit these shortcomings to track the online activities
of users. As a countermeasure, we introduce a novel protocol called TCP Fast
Open Privacy (FOP). It overcomes the performance and privacy limitations of TLS
over TFO by utilizing a custom TLS extension. TCP FOP prevents tracking by
network attackers and impedes third-party tracking, while still allowing for
0-RTT handshakes as in TFO. As a proof-of-concept, we have implemented the
proposed protocol. Our measurements indicate that TCP FOP outperforms TLS over
TFO when websites are served from multiple IP addresses.",website tracking
http://arxiv.org/abs/1803.09502v3,"We introduce the OxUvA dataset and benchmark for evaluating single-object
tracking algorithms. Benchmarks have enabled great strides in the field of
object tracking by defining standardized evaluations on large sets of diverse
videos. However, these works have focused exclusively on sequences that are
just tens of seconds in length and in which the target is always visible.
Consequently, most researchers have designed methods tailored to this
""short-term"" scenario, which is poorly representative of practitioners' needs.
Aiming to address this disparity, we compile a long-term, large-scale tracking
dataset of sequences with average length greater than two minutes and with
frequent target object disappearance. The OxUvA dataset is much larger than the
object tracking datasets of recent years: it comprises 366 sequences spanning
14 hours of video. We assess the performance of several algorithms, considering
both the ability to locate the target and to determine whether it is present or
absent. Our goal is to offer the community a large and diverse benchmark to
enable the design and evaluation of tracking methods ready to be used ""in the
wild"". The project website is http://oxuva.net",website tracking
http://arxiv.org/abs/1811.10742v3,"Vehicle 3D extents and trajectories are critical cues for predicting the
future location of vehicles and planning future agent ego-motion based on those
predictions. In this paper, we propose a novel online framework for 3D vehicle
detection and tracking from monocular videos. The framework can not only
associate detections of vehicles in motion over time, but also estimate their
complete 3D bounding box information from a sequence of 2D images captured on a
moving platform. Our method leverages 3D box depth-ordering matching for robust
instance association and utilizes 3D trajectory prediction for
re-identification of occluded vehicles. We also design a motion learning module
based on an LSTM for more accurate long-term motion extrapolation. Our
experiments on simulation, KITTI, and Argoverse datasets show that our 3D
tracking pipeline offers robust data association and tracking. On Argoverse,
our image-based method is significantly better for tracking 3D vehicles within
30 meters than the LiDAR-centric baseline methods.",website tracking
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",website tracking
http://arxiv.org/abs/1310.2864v1,"Location-based services gained much popularity through providing users with
helpful information with respect to their current location. The search and
recommendation of nearby locations or places, and the navigation to a specific
location are some of the most prominent location-based services. As a recent
trend, virtual location-based services consider webpages or sites associated
with a location as 'virtual locations' that online users can visit in spite of
not being physically present at the location. The presence of links between
virtual locations and the corresponding physical locations (e.g., geo-location
information of a restaurant linked to its website), allows for novel types of
services and applications which constitute virtual location-based services
(VLBS). The quality and potential benefits of such services largely depends on
the existence of websites referring to physical locations. In this paper, we
investigate the usefulness of linking virtual and physical locations. For this,
we analyze the presence and distribution of virtual locations, i.e., websites
referring to places, for two Irish cities. Using simulated tracks based on a
user movement model, we investigate how mobile users move through the Web as
virtual space. Our results show that virtual locations are omnipresent in urban
areas, and that the situation that a user is close to even several such
locations at any time is rather the normal case instead of the exception.",website tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie monitoring
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie monitoring
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie monitoring
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie monitoring
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie monitoring
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie monitoring
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie monitoring
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie monitoring
http://arxiv.org/abs/1803.10450v1,"Over the last decade, the number of devices per person has increased
substantially. This poses a challenge for cookie-based personalization
applications, such as online search and advertising, as it narrows the
personalization signal to a single device environment. A key task is to find
which cookies belong to the same person to recover a complete cross-device user
journey. Recent work on the topic has shown the benefits of using unsupervised
embeddings learned on user event sequences. In this paper, we extend this
approach to a supervised setting and introduce the Siamese Cookie Embedding
Network (SCEmNet), a siamese convolutional architecture that leverages the
multi-modal aspect of sequences, and show significant improvement over the
state-of-the-art.",cookie monitoring
http://arxiv.org/abs/1510.01175v1,"The number of computers, tablets and smartphones is increasing rapidly, which
entails the ownership and use of multiple devices to perform online tasks. As
people move across devices to complete these tasks, their identities becomes
fragmented. Understanding the usage and transition between those devices is
essential to develop efficient applications in a multi-device world. In this
paper we present a solution to deal with the cross-device identification of
users based on semi-supervised machine learning methods to identify which
cookies belong to an individual using a device. The method proposed in this
paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections
challenge proving its good performance.",cookie monitoring
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",cookie monitoring
http://arxiv.org/abs/1108.5864v1,"With the success of Web applications, most of our data is now stored on
various third-party servers where they are processed to deliver personalized
services. Naturally we must be authenticated to access this personal
information, but the use of personalized services only restricted by
identification could indirectly and silently leak sensitive data. We analyzed
Google Web Search access mechanisms and found that the current policy applied
to session cookies could be used to retrieve users' personal data. We describe
an attack scheme leveraging the search personalization (based on the same SID
cookie) to retrieve a part of the victim's click history and even some of her
contacts. We implemented a proof of concept of this attack on Firefox and
Chrome Web browsers and conducted an experiment with ten volunteers. Thanks to
this prototype we were able to recover up to 80% of the user's search click
history.",cookie monitoring
http://arxiv.org/abs/1711.02703v2,"With the rapid growth in smartphone usage, more organizations begin to focus
on providing better services for mobile users. User identification can help
these organizations to identify their customers and then cater services that
have been customized for them. Currently, the use of cookies is the most common
form to identify users. However, cookies are not easily transportable (e.g.,
when a user uses a different login account, cookies do not follow the user).
This limitation motivates the need to use behavior biometric for user
identification. In this paper, we propose DEEPSERVICE, a new technique that can
identify mobile users based on user's keystroke information captured by a
special keyboard or web browser. Our evaluation results indicate that
DEEPSERVICE is highly accurate in identifying mobile users (over 93% accuracy).
The technique is also efficient and only takes less than 1 ms to perform
identification.",cookie monitoring
http://arxiv.org/abs/1305.2306v1,"Privacy has been a major concern for everybody over the internet. Governments
across the globe have given their views on how the internet space can be
managed effectively so that there is some control on the flow of confidential
information and privacy to users and as well as to data is achieved. Taking
advantage of the lack of one unified body that could govern the online space
with its strict and stringent rules certain websites use the text files called
cookies in collecting data from users and using them for marketing them in
advertisements networks and third party websites with the help of JavaScript
and flash technologies. Even before many of us think what is happening around
us in the online usage we are being invaded by the cookies and are targeted
with their specific information that could tempt us to buy a product or service
to which we were longing for in the recent past. Though it may be argued its a
kind of marketing strategy to give the customer what he wants but it can no way
be something like the user is just being targeted because he has already shown
interest in something and he should be disturbed until he gets hold of
something. Its purely a security breach as most of the websites dont ask
permission for the usage of cookies and setting them into the users browser and
the most important thing is no privacy statement is issued that the information
is used for targeted marketing. This analysis paper has views from different
sources, an example of such an activity that is a potential security breach and
various other information of what these sites do to use the deadly cookies for
commercial tricks.",cookie monitoring
http://arxiv.org/abs/1407.0803v1,"The popularity of mobile device has made people's lives more convenient, but
threatened people's privacy at the same time. As end users are becoming more
and more concerned on the protection of their private information, it is even
harder to track a specific user using conventional technologies. For example,
cookies might be cleared by users regularly. Apple has stopped apps accessing
UDIDs, and Android phones use some special permission to protect IMEI code. To
address this challenge, some recent studies have worked on tracing smart phones
using the hardware features resulted from the imperfect manufacturing process.
These works have demonstrated that different devices can be differentiated to
each other. However, it still has a long way to go in order to replace cookie
and be deployed in real world scenarios, especially in terms of properties like
uniqueness, robustness, etc. In this paper, we presented a novel method to
generate stable and unique device ID stealthy for smartphones by exploiting the
frequency response of the speaker. With carefully selected audio frequencies
and special sound wave patterns, we can reduce the impacts of non-linear
effects and noises, and keep our feature extraction process un-noticeable to
users. The extracted feature is not only very stable for a given smart phone
speaker, but also unique to that phone. The feature contains rich information
that is equivalent to around 40 bits of entropy, which is enough to identify
billions of different smart phones of the same model. We have built a prototype
to evaluate our method, and the results show that the generated device ID can
be used as a replacement of cookie.",cookie monitoring
http://arxiv.org/abs/1805.10505v2,"User data is the primary input of digital advertising, fueling the free
Internet as we know it. As a result, web companies invest a lot in elaborate
tracking mechanisms to acquire user data that can sell to data markets and
advertisers. However, with same-origin policy, and cookies as a primary
identification mechanism on the web, each tracker knows the same user with a
different ID. To mitigate this, Cookie Synchronization (CSync) came to the
rescue, facilitating an information sharing channel between third parties that
may or not have direct access to the website the user visits. In the
background, with CSync, they merge user data they own, but also reconstruct a
user's browsing history, bypassing the same origin policy. In this paper, we
perform a first to our knowledge in-depth study of CSync in the wild, using a
year-long weblog from 850 real mobile users. Through our study, we aim to
understand the characteristics of the CSync protocol and the impact it has on
web users' privacy. For this, we design and implement CONRAD, a holistic
mechanism to detect CSync events at real time, and the privacy loss on the user
side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97%
of the regular web users are exposed to CSync: most of them within the first
week of their browsing, and the median userID gets leaked, on average, to 3.5
different domains. Finally, we see that CSync increases the number of domains
that track the user by a factor of 6.75.",cookie monitoring
http://arxiv.org/abs/1905.01267v1,"The recently introduced General Data Protection Regulation (GDPR) requires
that when obtaining information online that could be used to identify
individuals, their consents must be obtained. Among other things, this affects
many common forms of cookies, and users in the EU have been presented with
notices asking their approvals for data collection. This paper examines the
prevalence of third party cookies before and after GDPR by using two datasets:
accesses to top 500 websites according to Alexa.com, and weekly data of cookies
placed in users' browsers by websites accessed by 16 UK and China users across
one year.
  We find that on average the number of third parties dropped by more than 10%
after GDPR, but when we examine real users' browsing histories over a year, we
find that there is no material reduction in long-term numbers of third party
cookies, suggesting that users are not making use of the choices offered by
GDPR for increased privacy. Also, among websites which offer users a choice in
whether and how they are tracked, accepting the default choices typically ends
up storing more cookies on average than on websites which provide a notice of
cookies stored but without giving users a choice of which cookies, or those
that do not provide a cookie notice at all. We also find that top non-EU
websites have fewer cookie notices, suggesting higher levels of tracking when
visiting international sites. Our findings have deep implications both for
understanding compliance with GDPR as well as understanding the evolution of
tracking on the web.",cookie tracking
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",cookie tracking
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",cookie tracking
http://arxiv.org/abs/1705.08884v2,"In 2002, the European Union (EU) introduced the ePrivacy Directive to
regulate the usage of online tracking technologies. Its aim is to make tracking
mechanisms explicit while increasing privacy awareness in users. It mandates
websites to ask for explicit consent before using any kind of profiling
methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and
now most of European websites embed a ""Cookie Bar"" to explicitly ask user's
consent. To the best of our knowledge, no study focused in checking whether a
website respects the Directive. For this, we engineer CookieCheck, a simple
tool that makes this check automatic. We use it to run a measurement campaign
on more than 35,000 websites. Results depict a dramatic picture: 65% of
websites do not respect the Directive and install tracking cookies before the
user is even offered the accept button. In few words, we testify the failure of
the ePrivacy Directive. Among motivations, we identify the absence of rules
enabling systematic auditing procedures, the lack of tools to verify its
implementation by the deputed agencies, and the technical difficulties of
webmasters in implementing it.",cookie tracking
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",cookie tracking
http://arxiv.org/abs/1510.01175v1,"The number of computers, tablets and smartphones is increasing rapidly, which
entails the ownership and use of multiple devices to perform online tasks. As
people move across devices to complete these tasks, their identities becomes
fragmented. Understanding the usage and transition between those devices is
essential to develop efficient applications in a multi-device world. In this
paper we present a solution to deal with the cross-device identification of
users based on semi-supervised machine learning methods to identify which
cookies belong to an individual using a device. The method proposed in this
paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections
challenge proving its good performance.",cookie tracking
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",cookie tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",cookie tracking
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",cookie tracking
http://arxiv.org/abs/1805.10505v2,"User data is the primary input of digital advertising, fueling the free
Internet as we know it. As a result, web companies invest a lot in elaborate
tracking mechanisms to acquire user data that can sell to data markets and
advertisers. However, with same-origin policy, and cookies as a primary
identification mechanism on the web, each tracker knows the same user with a
different ID. To mitigate this, Cookie Synchronization (CSync) came to the
rescue, facilitating an information sharing channel between third parties that
may or not have direct access to the website the user visits. In the
background, with CSync, they merge user data they own, but also reconstruct a
user's browsing history, bypassing the same origin policy. In this paper, we
perform a first to our knowledge in-depth study of CSync in the wild, using a
year-long weblog from 850 real mobile users. Through our study, we aim to
understand the characteristics of the CSync protocol and the impact it has on
web users' privacy. For this, we design and implement CONRAD, a holistic
mechanism to detect CSync events at real time, and the privacy loss on the user
side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97%
of the regular web users are exposed to CSync: most of them within the first
week of their browsing, and the median userID gets leaked, on average, to 3.5
different domains. Finally, we see that CSync increases the number of domains
that track the user by a factor of 6.75.",cookie tracking
http://arxiv.org/abs/1407.0803v1,"The popularity of mobile device has made people's lives more convenient, but
threatened people's privacy at the same time. As end users are becoming more
and more concerned on the protection of their private information, it is even
harder to track a specific user using conventional technologies. For example,
cookies might be cleared by users regularly. Apple has stopped apps accessing
UDIDs, and Android phones use some special permission to protect IMEI code. To
address this challenge, some recent studies have worked on tracing smart phones
using the hardware features resulted from the imperfect manufacturing process.
These works have demonstrated that different devices can be differentiated to
each other. However, it still has a long way to go in order to replace cookie
and be deployed in real world scenarios, especially in terms of properties like
uniqueness, robustness, etc. In this paper, we presented a novel method to
generate stable and unique device ID stealthy for smartphones by exploiting the
frequency response of the speaker. With carefully selected audio frequencies
and special sound wave patterns, we can reduce the impacts of non-linear
effects and noises, and keep our feature extraction process un-noticeable to
users. The extracted feature is not only very stable for a given smart phone
speaker, but also unique to that phone. The feature contains rich information
that is equivalent to around 40 bits of entropy, which is enough to identify
billions of different smart phones of the same model. We have built a prototype
to evaluate our method, and the results show that the generated device ID can
be used as a replacement of cookie.",cookie tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",cookie tracking
http://arxiv.org/abs/1808.07540v2,"Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming.",cookie tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",cookie tracking
http://arxiv.org/abs/1804.08491v1,"Internet users today are constantly giving away their personal information
and privacy through social media, tracking cookies, 'free' email, and single
sign-on authentication in order to access convenient online services.
Unfortunately, the elected officials who are supposed to be regulating these
technologies often know less about informed consent and data ownership than the
users themselves. This is why without changes, internet users may continue to
be exploited by companies offering free and convenient online services.",cookie tracking
http://arxiv.org/abs/1810.07304v1,"User tracking on the Internet can come in various forms, e.g., via cookies or
by fingerprinting web browsers. A technique that got less attention so far is
user tracking based on TLS and specifically based on the TLS session resumption
mechanism. To the best of our knowledge, we are the first that investigate the
applicability of TLS session resumption for user tracking. For that, we
evaluated the configuration of 48 popular browsers and one million of the most
popular websites. Moreover, we present a so-called prolongation attack, which
allows extending the tracking period beyond the lifetime of the session
resumption mechanism. To show that under the observed browser configurations
tracking via TLS session resumptions is feasible, we also looked into DNS data
to understand the longest consecutive tracking period for a user by a
particular website. Our results indicate that with the standard setting of the
session resumption lifetime in many current browsers, the average user can be
tracked for up to eight days. With a session resumption lifetime of seven days,
as recommended upper limit in the draft for TLS version 1.3, 65% of all users
in our dataset can be tracked permanently.",cookie tracking
http://arxiv.org/abs/1811.00920v1,"Numerous surveys have shown that Web users are concerned about the loss of
privacy associated with online tracking. Alarmingly, these surveys also reveal
that people are also unaware of the amount of data sharing that occurs between
ad exchanges, and thus underestimate the privacy risks associated with online
tracking.
  In reality, the modern ad ecosystem is fueled by a flow of user data between
trackers and ad exchanges. Although recent work has shown that ad exchanges
routinely perform cookie matching with other exchanges, these studies are based
on brittle heuristics that cannot detect all forms of information sharing,
especially under adversarial conditions.
  In this study, we develop a methodology that is able to detect client- and
server-side flows of information between arbitrary ad exchanges. Our key
insight is to leverage retargeted ads as a tool for identifying information
flows. Intuitively, our methodology works because it relies on the semantics of
how exchanges serve ads, rather than focusing on specific cookie matching
mechanisms. Using crawled data on 35,448 ad impressions, we show that our
methodology can successfully categorize four different kinds of information
sharing behavior between ad exchanges, including cases where existing heuristic
methods fail.
  We conclude with a discussion of how our findings and methodologies can be
leveraged to give users more control over what kind of ads they see and how
their information is shared between ad exchanges.",cookie tracking
http://arxiv.org/abs/1811.08660v1,"The European General Data Protection Regulation (GDPR), which went into
effect in May 2018, leads to important changes in this area: companies are now
required to ask for users' consent before collecting and sharing personal data
and by law users now have the right to gain access to the personal information
collected about them.
  In this paper, we study and evaluate the effect of the GDPR on the online
advertising ecosystem. In a first step, we measure the impact of the
legislation on the connections (regarding cookie syncing) between third-parties
and show that the general structure how the entities are arranged is not
affected by the GDPR. However, we find that the new regulation has a
statistically significant impact on the number of connections, which shrinks by
around 40%. Furthermore, we analyze the right to data portability by evaluating
the subject access right process of popular companies in this ecosystem and
observe differences between the processes implemented by the companies and how
they interpret the new legislation. We exercised our right of access under GDPR
with 36 companies that had tracked us online. Although 32 companies (89%) we
inquired replied within the period defined by law, only 21 (58%) finished the
process by the deadline set in the GDPR. Our work has implications regarding
the implementation of privacy law as well as what online tracking companies
should do to be more compliant with the new regulation.",cookie tracking
http://arxiv.org/abs/1906.07141v1,"Certain HTTP Cookies on certain sites can be a source of content bias in
archival crawls. Accommodating Cookies at crawl time, but not utilizing them at
replay time may cause cookie violations, resulting in defaced composite
mementos that never existed on the live web. To address these issues, we
propose that crawlers store Cookies with short expiration time and archival
replay systems account for values in the Vary header along with URIs.",cookie tracking
http://arxiv.org/abs/1807.08026v1,"TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured
that the server did not have to store any information for half-open
connections. A SYN cookie contains all information required by the server to
know the request is valid. However, the usage of these cookies introduces a
vulnerability that allows an attacker to guess the initial sequence number and
use that to spoof a connection or plant false logs.",cookie tracking
http://arxiv.org/abs/1909.02638v1,"Since the adoption of the General Data Protection Regulation (GDPR) in May
2018 more than 60 % of popular websites in Europe display cookie consent
notices to their visitors. This has quickly led to users becoming fatigued with
privacy notifications and contributed to the rise of both browser extensions
that block these banners and demands for a solution that bundles consent across
multiple websites or in the browser.
  In this work, we identify common properties of the graphical user interface
of consent notices and conduct three experiments with more than 80,000 unique
users on a German website to investigate the influence of notice position, type
of choice, and content framing on consent. We find that users are more likely
to interact with a notice shown in the lower (left) part of the screen. Given a
binary choice, more users are willing to accept tracking compared to mechanisms
that require them to allow cookie use for each category or company
individually. We also show that the wide-spread practice of nudging has a large
effect on the choices users make. Our experiments show that seemingly small
implementation decisions can substantially impact whether and how people
interact with consent notices. Our findings demonstrate the importance for
regulation to not just require consent, but also provide clear requirements or
guidance for how this consent has to be obtained in order to ensure that users
can make free and informed choices.",cookie tracking
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",cookie tracking
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",cookie tracking
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",cookie tracking
http://arxiv.org/abs/cs/0105018v1,"How did we get from a world where cookies were something you ate and where
""non-techies"" were unaware of ""Netscape cookies"" to a world where cookies are a
hot-button privacy issue for many computer users? This paper will describe how
HTTP ""cookies"" work, and how Netscape's original specification evolved into an
IETF Proposed Standard. I will also offer a personal perspective on how what
began as a straightforward technical specification turned into a political
flashpoint when it tried to address non-technical issues such as privacy.",cookie tracking
http://arxiv.org/abs/1606.06771v2,"Data is the new oil; this refrain is repeated extensively in the age of
internet tracking, machine learning, and data analytics. Social network
analysis, cookie-based advertising, and government surveillance are all
evidence of the use of data for commercial and national interests. Public
pressure, however, is mounting for the protection of privacy. Frameworks such
as differential privacy offer machine learning algorithms methods to guarantee
limits to information disclosure, but they are seldom implemented. Recently,
however, developers have made significant efforts to undermine tracking through
obfuscation tools that hide user characteristics in a sea of noise. These
services highlight an emerging clash between tracking and data obfuscation. In
this paper, we conceptualize this conflict through a dynamic game between users
and a machine learning algorithm that uses empirical risk minimization. First,
a machine learner declares a privacy protection level, and then users respond
by choosing their own perturbation amounts. We study the interaction between
the users and the learner using a Stackelberg game. The utility functions
quantify accuracy using expected loss and privacy in terms of the bounds of
differential privacy. In equilibrium, we find selfish users tend to cause
significant utility loss to trackers by perturbing heavily, in a phenomenon
reminiscent of public good games. Trackers, however, can improve the balance by
proactively perturbing the data themselves. While other work in this area has
studied privacy markets and mechanism design for truthful reporting of user
information, we take a different viewpoint by considering both user and learner
perturbation.",cookie tracking
http://arxiv.org/abs/1910.00515v1,"Cognitive decline is a sign of Alzheimer's disease (AD), and there is
evidence that tracking a person's eye movement, using eye tracking devices, can
be used for the automatic identification of early signs of cognitive decline.
However, such devices are expensive and may not be easy-to-use for people with
cognitive problems. In this paper, we present a new way of capturing similar
visual features, by using the speech of people describing the Cookie Theft
picture - a common cognitive testing task - to identify regions in the picture
prompt that will have caught the speaker's attention and elicited their speech.
After aligning the automatically recognised words with different regions of the
picture prompt, we extract information inspired by eye tracking metrics such as
coordinates of the area of interests (AOI)s, time spent in AOI, time to reach
the AOI, and the number of AOI visits. Using the DementiaBank dataset we train
a binary classifier (AD vs. healthy control) using 10-fold cross-validation and
achieve an 80% F1-score using the timing information from the forced alignments
of the automatic speech recogniser (ASR); this achieved around 72% using the
timing information from the ASR outputs.",cookie tracking
http://arxiv.org/abs/1511.00619v1,"This article provides a quantitative analysis of privacy-compromising
mechanisms on 1 million popular websites. Findings indicate that nearly 9 in 10
websites leak user data to parties of which the user is likely unaware; more
than 6 in 10 websites spawn third- party cookies; and more than 8 in 10
websites load Javascript code from external parties onto users' computers.
Sites that leak user data contact an average of nine external domains,
indicating that users may be tracked by multiple entities in tandem. By tracing
the unintended disclosure of personal browsing histories on the Web, it is
revealed that a handful of U.S. companies receive the vast bulk of user data.
Finally, roughly 1 in 5 websites are potentially vulnerable to known National
Security Agency spying techniques at the time of analysis.",cookie tracking
http://arxiv.org/abs/1611.03780v2,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.",cookie tracking
http://arxiv.org/abs/1703.05066v2,"Browsers and their users can be tracked even in the absence of a persistent
IP address or cookie. Unique and hence identifying pieces of information,
making up what is known as a fingerprint, can be collected from browsers by a
visited website, e.g. using JavaScript. However, browsers vary in precisely
what information they make available, and hence their fingerprintability may
also vary. In this paper, we report on the results of experiments examining the
fingerprintable attributes made available by a range of modern browsers. We
tested the most widely used browsers for both desktop and mobile platforms. The
results reveal significant differences between browsers in terms of their
fingerprinting potential, meaning that the choice of browser has significant
privacy implications.",cookie tracking
http://arxiv.org/abs/1802.10523v1,"Web browsers are the most common tool to perform various activities over the
internet. Along with normal mode, all modern browsers have private browsing
mode. The name of the mode varies from browser to browser but the purpose of
the private mode remains same in every browser. In normal browsing mode, the
browser keeps track of users' activity and related data such as browsing
histories, cookies, auto-filled fields, temporary internet files, etc. In
private mode, it is said that no information is stored while browsing or all
information is destroyed after closing the current private session. However,
some researchers have already disproved this claim by performing various tests
in most popular browsers. I have also some personal experience where private
mode browsing fails to keep all browsing information as private. In this
position paper, I take the position against private browsing. By examining
various facts, it is proved that the private browsing mode is not really
private as it is claimed; it does not keep everything private. In following
sections, I will present the proof to justify my argument. Along with some
other already performed research work, I will show my personal case studies and
experimental data as well.",cookie tracking
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",cookie tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",ad blocking
http://arxiv.org/abs/1605.05841v1,"The rise of ad-blockers is viewed as an economic threat by online publishers,
especially those who primarily rely on ad- vertising to support their services.
To address this threat, publishers have started retaliating by employing
ad-block detectors, which scout for ad-blocker users and react to them by
restricting their content access and pushing them to whitelist the website or
disabling ad-blockers altogether. The clash between ad-blockers and ad-block
detectors has resulted in a new arms race on the web. In this paper, we present
the first systematic measurement and analysis of ad-block detection on the web.
We have designed and implemented a machine learning based tech- nique to
automatically detect ad-block detection, and use it to study the deployment of
ad-block detectors on Alexa top- 100K websites. The approach is promising with
precision of 94.8% and recall of 93.1%. We characterize the spectrum of
different strategies used by websites for ad-block detection. We find that most
of publishers use fairly simple passive ap- proaches for ad-block detection.
However, we also note that a few websites use third-party services, e.g.
PageFair, for ad-block detection and response. The third-party services use
active deception and other sophisticated tactics to de- tect ad-blockers. We
also find that the third-party services can successfully circumvent ad-blockers
and display ads on publisher websites.",ad blocking
http://arxiv.org/abs/1905.07444v2,"Online advertising has been a long-standing concern for user privacy and
overall web experience. Several techniques have been proposed to block ads,
mostly based on filter-lists and manually-written rules. While a typical ad
blocker relies on manually-curated block lists, these inevitably get
out-of-date, thus compromising the ultimate utility of this ad blocking
approach. In this paper we present Percival, a browser-embedded, lightweight,
deep learning-powered ad blocker. Percival embeds itself within the browser's
image rendering pipeline, which makes it possible to intercept every image
obtained during page execution and to perform blocking based on applying
machine learning for image classification to flag potential ads. Our
implementation inside both Chromium and Brave browsers shows only a minor
rendering performance overhead of 4.55%, demonstrating the feasibility of
deploying traditionally heavy models (i.e. deep neural networks) inside the
critical path of the rendering engine of a browser. We show that our
image-based ad blocker can replicate EasyList rules with an accuracy of 96.76%.
To show the versatility of the Percival's approach we present case studies that
demonstrate that Percival 1) does surprisingly well on ads in languages other
than English; 2) Percival also performs well on blocking first-party Facebook
ads, which have presented issues for other ad blockers. Percival proves that
image-based perceptual ad blocking is an attractive complement to today's
dominant approach of block lists",ad blocking
http://arxiv.org/abs/1811.03194v3,"Perceptual ad-blocking is a novel approach that detects online advertisements
based on their visual content. Compared to traditional filter lists, the use of
perceptual signals is believed to be less prone to an arms race with web
publishers and ad networks. We demonstrate that this may not be the case. We
describe attacks on multiple perceptual ad-blocking techniques, and unveil a
new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual
ad-blocking can also introduce new vulnerabilities that let an attacker bypass
web security boundaries and mount DDoS attacks.
  We first analyze the design space of perceptual ad-blockers and present a
unified architecture that incorporates prior academic and commercial work. We
then explore a variety of attacks on the ad-blocker's detection pipeline, that
enable publishers or ad networks to evade or detect ad-blocking, and at times
even abuse its high privilege level to bypass web security boundaries.
  On one hand, we show that perceptual ad-blocking must visually classify
rendered web content to escape an arms race centered on obfuscation of page
markup. On the other, we present a concrete set of attacks on visual
ad-blockers by constructing adversarial examples in a real web page context.
For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and
native web content that misleads perceptual ad-blocking with 100% success
rates. In one of our attacks, we demonstrate how a malicious user can upload
adversarial content, such as a perturbed image in a Facebook post, that fools
the ad-blocker into removing another users' non-ad content.
  Moving beyond the Web and visual domain, we also build adversarial examples
for AdblockRadio, an open source radio client that uses machine learning to
detects ads in raw audio streams.",ad blocking
http://arxiv.org/abs/1705.08568v1,"We present a systematic study of ad blocking - and the associated ""arms race""
- as a security problem. We model ad blocking as a state space with four states
and six state transitions, which correspond to techniques that can be deployed
by either publishers or ad blockers. We argue that this is a complete model of
the system. We propose several new ad blocking techniques, including ones that
borrow ideas from rootkits to prevent detection by anti-ad blocking scripts.
Another technique uses the insight that ads must be recognizable by humans to
comply with laws and industry self-regulation. We have built prototype
implementations of three of these techniques, successfully blocking ads and
evading detection. We systematically evaluate our proposed techniques, along
with existing ones, in terms of security, practicality, and legality. We
characterize the order of growth of the development effort required to
create/maintain ad blockers as a function of the growth of the web. Based on
our state-space model, our new techniques, and this systematization, we offer
insights into the likely ""end game"" of the arms race. We challenge the
widespread assumption that the arms race will escalate indefinitely, and
instead identify a combination of evolving technical and legal factors that
will determine the outcome.",ad blocking
http://arxiv.org/abs/0807.3383v4,"we will present an estimation for the upper-bound of the amount of 16-bytes
plaintexts for English texts, which indicates that the block ciphers with block
length no more than 16-bytes will be subject to recover plaintext attacks in
the occasions of plaintext -known or plaintext-chosen attacks.",ad blocking
http://arxiv.org/abs/1701.07184v1,"We define multi-block interleaved codes as codes that allow reading
information from either a small sub-block or from a larger full block. The
former offers faster access, while the latter provides better reliability. We
specify the correction capability of the sub-block code through its gap $t$
from optimal minimum distance, and look to have full-block minimum distance
that grows with the parameter $t$. We construct two families of such codes when
the number of sub-blocks is $3$. The codes match the distance properties of
known integrated-interleaving codes, but with the added feature of mapping the
same number of information symbols to each sub-block. As such, they are the
first codes that provide read access in multiple size granularities and
correction capabilities.",ad blocking
http://arxiv.org/abs/1507.05753v2,"Wall-clock-time is minimized for a solution to a linear-program with
block-diagonal-structure, by decomposing the linear-program into as many
small-sized subproblems as possible, each block resulting in a separate
subproblem, when the number of available parallel-processing-units is at least
equal to the number of blocks. This is not necessarily the case when the
parallel processing capability is limited, causing multiple subproblems to be
serially solved on the same processing-unit. In such a situation, it might be
better to aggregate blocks into larger sized subproblems. The optimal
aggregation strategy depends on the computing-platform used, and minimizes the
average-case running time for the set of subproblems. We show that optimal
aggregation is NP-hard when blocks are of unequal size, and that optimal
aggregation can be achieved within polynomial-time when blocks are of equal
size.",ad blocking
http://arxiv.org/abs/1903.11434v2,"We present a general consensus framework that allows to easily introduce a
customizable Byzantine fault tolerant consensus algorithm to an existing
(delegated) proof-of-stake blockchain. We prove the safety of the protocol
under the assumption that less than 1/3 of the block proposers are Byzantine.
The framework further allows for consensus participants to choose subjective
decision thresholds in order to obtain safety even in the case of a larger
proportion of Byzantine block proposers. Moreover, the liveness of the protocol
is shown if less than 1/3 of the block proposers crash.
  Based on the framework, we introduce Lisk-BFT, a Byzantine fault tolerant
consensus algorithm for the Lisk blockchain. Lisk-BFT integrates with the
existing block proposal, requires only two additional integers in block headers
and no additional messages. The protocol is simple and provides safety in the
case of static proposers if less than 1/3 of the block proposers are Byzantine.
For the case of dynamically changing proposers, we proof the safety of the
protocol assuming a bound on the number of Byzantine proposers and the number
of honest proposers that can change at one time. We further show the liveness
of the Lisk-BFT protocol for less than 1/3 crashing block proposers.",ad blocking
http://arxiv.org/abs/1604.04495v1,"Free content and services on the Web are often supported by ads. However,
with the proliferation of intrusive and privacy-invasive ads, a significant
proportion of users have started to use ad blockers. As existing ad blockers
are radical (they block all ads) and are not designed taking into account their
economic impact, ad-based economic model of the Web is in danger today. In this
paper, we target privacy-sensitive users and provide them with fine-grained
control over tracking. Our working assumption is that some categories of web
pages (for example, related to health, religion, etc.) are more
privacy-sensitive to users than others (education, science, etc.). Therefore,
our proposed approach consists in providing users with an option to specify the
categories of web pages that are privacy-sensitive to them and block trackers
present on such web pages only. As tracking is prevented by blocking network
connections of third-party domains, we avoid not only tracking but also
third-party ads. Since users will continue receiving ads on web pages belonging
to non-sensitive categories, our approach essentially provides a trade-off
between privacy and economy. To test the viability of our solution, we
implemented it as a Google Chrome extension, named MyTrackingChoices (available
on Chrome Web Store). Our real-world experiments with MyTrackingChoices show
that the economic impact of ad blocking exerted by privacy-sensitive users can
be significantly reduced.",ad blocking
http://arxiv.org/abs/1704.07114v5,"A method for compression of large graphs and non-negative matrices to a block
structure is proposed. Szemer\'edi's regularity lemma is used as heuristic
motivation of the significance of stochastic block models. Another ingredient
of the method is Rissanen's minimum description length principle (MDL). We
propose practical algorithms and provide theoretical results on the accuracy of
the method.",ad blocking
http://arxiv.org/abs/1406.5039v1,"A deep-water approximation to the Stokes drift velocity profile is explored
as an alternative to the monochromatic profile. The alternative profile
investigated relies on the same two quantities required for the monochromatic
profile, viz the Stokes transport and the surface Stokes drift velocity.
Comparisons with parametric spectra and profiles under wave spectra from the
ERA-Interim reanalysis and buoy observations reveal much better agreement than
the monochromatic profile even for complex sea states. That the profile gives a
closer match and a more correct shear has implications for ocean circulation
models since the Coriolis-Stokes force depends on the magnitude and direction
of the Stokes drift profile and Langmuir turbulence parameterizations depend
sensitively on the shear of the profile. The alternative profile comes at no
added numerical cost compared to the monochromatic profile.",ad profiling
http://arxiv.org/abs/1407.0788v2,"Over the past decade, advertising has emerged as the primary source of
revenue for many web sites and apps. In this paper we report a
first-of-its-kind study that seeks to broadly understand the features,
mechanisms and dynamics of display advertising on the web - i.e., the Adscape.
Our study takes the perspective of users who are the targets of display ads
shown on web sites. We develop a scalable crawling capability that enables us
to gather the details of display ads including creatives and landing pages. Our
crawling strategy is focused on maximizing the number of unique ads harvested.
Of critical importance to our study is the recognition that a user's profile
(i.e. browser profile and cookies) can have a significant impact on which ads
are shown. We deploy our crawler over a variety of websites and profiles and
this yields over 175K distinct display ads.
  We find that while targeting is widely used, there remain many instances in
which delivered ads do not depend on user profile; further, ads vary more over
user profiles than over websites. We also assess the population of advertisers
seen and identify over 3.7K distinct entities from a variety of business
segments. Finally, we find that when targeting is used, the specific types of
ads delivered generally correspond with the details of user profiles, and also
on users' patterns of visit.",ad profiling
http://arxiv.org/abs/1601.02377v1,"User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.",ad profiling
http://arxiv.org/abs/1611.04175v1,"The domain of single crossing preference profiles is a widely studied domain
in social choice theory. It has been generalized to the domain of single
crossing preference profiles with respect to trees which inherits many
desirable properties from the single crossing domain, for example, transitivity
of majority relation, existence of polynomial time algorithms for finding
winners of Kemeny voting rule, etc. In this paper, we consider a further
generalization of the domain of single crossing profiles on trees to the domain
consisting of all preference profiles which can be extended to single crossing
preference profiles with respect to some tree by adding more preferences to it.
We call this domain the weakly single crossing domain on trees. We present a
polynomial time algorithm for recognizing weakly single crossing profiles on
trees. We then move on to develop a polynomial time algorithm with low query
complexity for eliciting weakly single crossing profiles on trees even when we
do not know any tree with respect to which the closure of the input profile is
single crossing and the preferences can be queried only sequentially; moreover,
the sequential order is also unknown. We complement the performance of our
preference elicitation algorithm by proving that our algorithm makes an optimal
number of queries up to constant factors when the number of preferences is
large compared to the number of candidates, even if the input profile is known
to be single crossing with respect to some given tree and the preferences can
be accessed randomly.",ad profiling
http://arxiv.org/abs/physics/0607219v2,"Evolutionally conserved quantity that specifies folding nuclei is pursued by
a case study for a small protein (PDB code: 1ten). First it is demonstrated
that the sequences of amino acids at folding nuclei are not conserved. Then 3D
(3-dimensional) information of the structure is considered and it is found that
a 3D hydrophobicity profile is essential to specify the folding nuclei and
evolutionally conserved. This profile is maintained by the interaction,
including entropic effect, among amino acids realized in the native state
structure. Experimentally observed phi-value is correlated to this 3D
hydrophobicity profile after taking into account the effect of the contact
distance in amino-acid sequence.",ad profiling
http://arxiv.org/abs/1402.4303v2,"Condorcet winning sets are a set-valued generalization of the well-known
concept of a Condorcet winner. As supersets of Condorcet winning sets are
always Condorcet winning sets themselves, an interesting property of preference
profiles is the size of the smallest Condorcet winning set they admit. This
smallest size is called the Condorcet dimension of a preference profile. Since
little is known about profiles that have a certain Condorcet dimension, we show
in this paper how the problem of finding a preference profile that has a given
Condorcet dimension can be encoded as a satisfiability problem and solved by a
SAT solver. Initial results include a minimal example of a preference profile
of Condorcet dimension 3, improving previously known examples both in terms of
the number of agents as well as alternatives. Due to the high complexity of
such problems it remains open whether a preference profile of Condorcet
dimension 4 exists.",ad profiling
http://arxiv.org/abs/1202.1691v1,"Quality of Service(QoS) in Mobile Ad Hoc Networks (MANETs) though a
challenge, becomes a necessity because of its applications in critical
scenarios. Providing QoS for users belonging to various profiles and playing
different roles, becomes the need of the hour. In this paper, we propose
proportional share scheduling and MAC protocol (PS2-MAC) model. It classifies
users based on their profile as High Profiled users (HP), Medium Profiled users
(MP) and Low profiled users (LP) and assigns proportional weights. Service
Differentiation for these three service classes is achieved through, rationed
dequeuing algorithm, variable inter frame space, proportionate prioritized
backoff timers and enhanced RTS/CTS control packets. Differentiated services is
simulated in ns2 and results show that 9.5% control overhead is reduced in our
proposed scheme than the existing scheme and results also justify that,
differentiated services have been achieved for the different profiles of users
with proportionate shares and thereby reducing starvation.",ad profiling
http://arxiv.org/abs/1803.00839v1,"Face recognition achieves exceptional success thanks to the emergence of deep
learning. However, many contemporary face recognition models still perform
relatively poor in processing profile faces compared to frontal faces. A key
reason is that the number of frontal and profile training faces are highly
imbalanced - there are extensively more frontal training samples compared to
profile ones. In addition, it is intrinsically hard to learn a deep
representation that is geometrically invariant to large pose variations. In
this study, we hypothesize that there is an inherent mapping between frontal
and profile faces, and consequently, their discrepancy in the deep
representation space can be bridged by an equivariant mapping. To exploit this
mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,
which is capable of adaptively adding residuals to the input deep
representation to transform a profile face representation to a canonical pose
that simplifies recognition. The DREAM block consistently enhances the
performance of profile face recognition for many strong deep networks,
including ResNet models, without deliberately augmenting training data of
profile faces. The block is easy to use, light-weight, and can be implemented
with a negligible computational overhead.",ad profiling
http://arxiv.org/abs/1502.06577v1,"Advertising, long the financial mainstay of the web ecosystem, has become
nearly ubiquitous in the world of mobile apps. While ad targeting on the web is
fairly well understood, mobile ad targeting is much less studied. In this
paper, we use empirical methods to collect a database of over 225,000 ads on 32
simulated devices hosting one of three distinct user profiles. We then analyze
how the ads are targeted by correlating ads to potential targeting profiles
using Bayes' rule and Pearson's chi squared test. This enables us to measure
the prevalence of different forms of targeting. We find that nearly all ads
show the effects of application- and time-based targeting, while we are able to
identify location-based targeting in 43% of the ads and user-based targeting in
39%.",ad profiling
http://arxiv.org/abs/1706.06944v3,"A profile describes a set of properties, e.g. a set of skills a person may
have, a set of skills required for a particular job, or a set of abilities a
football player may have with respect to a particular team strategy. Profile
matching aims to determine how well a given profile fits to a requested
profile. The approach taken in this article is grounded in a matching theory
that uses filters in lattices to represent profiles, and matching values in the
interval [0,1]: the higher the matching value the better is the fit. Such
lattices can be derived from knowledge bases exploiting description logics to
represent the knowledge about profiles. An interesting first question is, how
human expertise concerning the matching can be exploited to obtain most
accurate matchings. It will be shown that if a set of filters together with
matching values by some human expert is given, then under some mild
plausibility assumptions a matching measure can be determined such that the
computed matching values preserve the rankings given by the expert. A second
question concerns the efficient querying of databases of profile instances. For
matching queries that result in a ranked list of profile instances matching a
given one it will be shown how corresponding top-k queries can be evaluated on
grounds of pre-computed matching values, which in turn allows the maintenance
of the knowledge base to be decoupled from the maintenance of profile
instances. In addition, it will be shown how the matching queries can be
exploited for gap queries that determine how profile instances need to be
extended in order to improve in the rankings. Finally, the theory of matching
will be extended beyond the filters, which lead to a matching theory that
exploits fuzzy sets or probabilistic logic with maximum entropy semantics. It
will be shown that added fuzzy links can be captured by extending the
underlying lattice.",ad profiling
http://arxiv.org/abs/1810.07886v1,"Ad-hoc Social Networks have become popular to support novel applications
related to location-based mobile services that are of great importance to users
and businesses. Unlike traditional social services using a centralized server
to fetch location, ad-hoc social network services support infrastructure less
real-time social networking. It allows users to collaborate and share views
anytime anywhere. However, current ad-hoc social network applications are
either not available without rooting the mobile phones or don't filter the
nearby users based on common interests without a centralized server. This paper
presents an architecture and implementation of social networks on commercially
available mobile devices that allow broadcasting name and a limited number of
keywords representing users' interests without any connection in a nearby
region to facilitate matching of interests. The broadcasting region creates a
digital aura and is limited by WiFi region that is around 200 meters. The
application connects users to form a group based on their profile or interests
using peer-to-peer communication mode without using any centralized networking
or profile matching infrastructure. The peer-to-peer group can be used for
private communication when the network is not available.",ad profiling
http://arxiv.org/abs/cs/0411056v1,"This paper proposes a software architecture for dynamical service adaptation.
The services are constituted by reusable software components. The adaptation's
goal is to optimize the service function of their execution context. For a
first step, the context will take into account just the user needs but other
elements will be added. A particular feature in our proposition is the profiles
that are used not only to describe the context's elements but also the
components itself. An Adapter analyzes the compatibility between all these
profiles and detects the points where the profiles are not compatibles. The
same Adapter search and apply the possible adaptation solutions: component
customization, insertion, extraction or replacement.",ad profiling
http://arxiv.org/abs/1408.6491v2,"To partly address people's concerns over web tracking, Google has created the
Ad Settings webpage to provide information about and some choice over the
profiles Google creates on users. We present AdFisher, an automated tool that
explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher
can run browser-based experiments and analyze data using machine learning and
significance tests. Our tool uses a rigorous experimental design and
statistical analysis to ensure the statistical soundness of our results. We use
AdFisher to find that the Ad Settings was opaque about some features of a
user's profile, that it does provide some choice on ads, and that these choices
can lead to seemingly discriminatory ads. In particular, we found that visiting
webpages associated with substance abuse changed the ads shown but not the
settings page. We also found that setting the gender to female resulted in
getting fewer instances of an ad related to high paying jobs than setting it to
male. We cannot determine who caused these findings due to our limited
visibility into the ad ecosystem, which includes Google, advertisers, websites,
and users. Nevertheless, these results can form the starting point for deeper
investigations by either the companies themselves or by regulatory bodies.",ad profiling
http://arxiv.org/abs/1111.3394v1,"The new beam profile measurement for the Antiproton Decelerator (AD) at CERN
is based on a single Gas Electron Multiplier (GEM) with a 2D readout structure.
This detector is very light, ~0.4% X0, as required by the low energy of the
antiprotons, 5.3 MeV. This overcomes the problems previously encountered with
multi-wire proportional chambers (MWPC) for the same purpose, where beam
interactions with the detector severely affect the obtained profiles. A
prototype was installed and successfully tested in late 2010, with another five
detectors now installed in the ASACUSA and AEGIS beam lines. We will provide a
detailed description of the detector and discuss the results obtained.
  The success of these detectors in the AD makes GEM-based detectors likely
candidates for upgrade of the beam profile monitors in all experimental areas
at CERN. The various types of MWPC currently in use are aging and becoming
increasingly difficult to maintain.",ad profiling
http://arxiv.org/abs/1111.4150v1,"The new beam profile measurement for the Antiproton Decelerator (AD) at CERN
is based on a single Gas Electron Multiplier (GEM) with a 2D readout structure.
This detector is very light, ~0.4% X_0, as required by the low energy of the
antiprotons, 5.3 MeV. This overcomes the problems previously encountered with
multi-wire proportional chambers (MWPC) for the same purpose, where beam
interactions with the detector severely affect the obtained profiles. A
prototype was installed and successfully tested in late 2010, with another five
detectors now installed in the ASACUSA and AEgIS beam lines. We will provide a
detailed description of the detector and discuss the results obtained.
  The success of these detectors in the AD makes GEM-based detectors likely
candidates for upgrade of the beam profile monitors in all experimental areas
at CERN. The various types of MWPC currently in use are aging and becoming
increasingly difficult to maintain.",ad profiling
http://arxiv.org/abs/1111.3052v2,"When existing, cumulants can provide valuable information about a given
distribution and can in principle be used to either fully reconstruct or
approximate the parent distribution function. A previously reported cumulant
expansion approach for Franck-Condon profiles [Faraday Discuss., 150, 363
(2011)] is extended to describe also the profiles of vibronic transitions that
are weakly allowed or forbidden in the Franck-Condon approximation (non-Condon
profiles). In the harmonic approximation the cumulants of the vibronic spectral
profile can be evaluated analytically and numerically with a coherent
state-based generating function that accounts for the Duschinsky effect. As
illustration, the one-photon $1 ^{1}\mathrm{A_{g}}\rightarrow1
^{1}\mathrm{B_{2u}}$ UV absorption spectrum of benzene in the electric dipole
and (linear) Herzberg-Teller approximation is presented herein for zero Kelvin
and finite temperatures.",ad profiling
http://arxiv.org/abs/1809.10948v1,"In Named Data Networking (NDN), there is a need for routing protocols to
populate Forwarding Information Base (FIB) tables so that the Interest messages
can be forwarded. To populate FIBs, clients and routers require some routing
information. One method to obtain this information is that network nodes
exchange routing information by each node advertising the available content
objects. Bloom Filter-based Routing approaches like BFR [1], use Bloom Filters
(BFs) to advertise all provided content objects, which consumes valuable
bandwidth and storage resources. This strategy is inefficient as clients
request only a small number of the provided content objects and they do not
need the content advertisement information for all provided content objects. In
this paper, we propose a novel routing algorithm for NDN called pull-based BFR
in which servers only advertise the demanded file names. We compare the
performance of pull-based BFR with original BFR and with a flooding-assisted
routing protocol. Our experimental evaluations show that pull-based BFR
outperforms original BFR in terms of communication overhead needed for content
advertisements, average roundtrip delay, memory resources needed for storing
content advertisements at clients and routers, and the impact of false positive
reports on routing. The comparisons also show that pull-based BFR outperforms
flooding-assisted routing in terms of average round-trip delay.",false advertising
http://arxiv.org/abs/1808.09218v4,"Targeted advertising is meant to improve the efficiency of matching
advertisers to their customers. However, targeted advertising can also be
abused by malicious advertisers to efficiently reach people susceptible to
false stories, stoke grievances, and incite social conflict. Since targeted ads
are not seen by non-targeted and non-vulnerable people, malicious ads are
likely to go unreported and their effects undetected. This work examines a
specific case of malicious advertising, exploring the extent to which political
ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
elections exploited Facebook's targeted advertising infrastructure to
efficiently target ads on divisive or polarizing topics (e.g., immigration,
race-based policing) at vulnerable sub-populations. In particular, we do the
following: (a) We conduct U.S. census-representative surveys to characterize
how users with different political ideologies report, approve, and perceive
truth in the content of the IRA ads. Our surveys show that many ads are
""divisive"": they elicit very different reactions from people belonging to
different socially salient groups. (b) We characterize how these divisive ads
are targeted to sub-populations that feel particularly aggrieved by the status
quo. Our findings support existing calls for greater transparency of content
and targeting of political ads. (c) We particularly focus on how the Facebook
ad API facilitates such targeting. We show how the enormous amount of personal
data Facebook aggregates about users and makes available to advertisers enables
such malicious targeting.",false advertising
http://arxiv.org/abs/1702.00340v1,"Locating the demanded content is one of the major challenges in
Information-Centric Networking (ICN). This process is known as content
discovery. To facilitate content discovery, in this paper we focus on Named
Data Networking (NDN) and propose a novel routing scheme for content discovery,
called Bloom Filter-based Routing (BFR), which is fully distributed, content
oriented, and topology agnostic at the intra-domain level. In BFR, origin
servers advertise their content objects using Bloom filters. We compare the
performance of the proposed BFR with flooding and shortest path content
discovery approaches. BFR outperforms its counterparts in terms of the average
round-trip delay, while it is shown to be very robust to false positive reports
from Bloom filters. Also, BFR is much more robust than shortest path routing to
topology changes. BFR strongly outperforms flooding and performs almost equal
with shortest path routing with respect to the normalized communication costs
for data retrieval and total communication overhead for forwarding Interests.
All the three approaches achieve similar mean hit distance. The signalling
overhead for content advertisement in BFR is much lower than the signalling
overhead for calculating shortest paths in the shortest path approach. Finally,
BFR requires small storage overhead for maintaining content advertisements.",false advertising
http://arxiv.org/abs/1305.4045v1,"In recent years, there has been rapid growth in mobile devices such as
smartphones, and a number of applications are developed specifically for the
smartphone market. In particular, there are many applications that are ``free''
to the user, but depend on advertisement services for their revenue. Such
applications include an advertisement module - a library provided by the
advertisement service - that can collect a user's sensitive information and
transmit it across the network. Users accept this business model, but in most
cases the applications do not require the user's acknowledgment in order to
transmit sensitive information. Therefore, such applications' behavior becomes
an invasion of privacy. In our analysis of 1,188 Android applications' network
traffic and permissions, 93% of the applications we analyzed connected to
multiple destinations when using the network. 61% required a permission
combination that included both access to sensitive information and use of
networking services. These applications have the potential to leak the user's
sensitive information. In an effort to enable users to control the transmission
of their private information, we propose a system which, using a novel
clustering method based on the HTTP packet destination and content distances,
generates signatures from the clustering result and uses them to detect
sensitive information leakage from Android applications. Our system does not
require an Android framework modification or any special privileges. Thus users
can easily introduce our system to their devices, and manage suspicious
applications' network behavior in a fine grained manner. Our system accurately
detected 94% of the sensitive information leakage from the applications
evaluated and produced only 5% false negative results, and less than 3% false
positive results.",false advertising
http://arxiv.org/abs/1903.00733v2,"Advertising is a primary means for revenue generation for millions of
websites and smartphone apps (publishers). Naturally, a fraction of publishers
abuse the ad-network to systematically defraud advertisers of their money.
Defenses have matured to overcome some forms of click fraud but are inadequate
against the threat of organic click fraud attacks. Malware detection systems
including honeypots fail to stop click fraud apps; ad-network filters are
better but measurement studies have reported that a third of the clicks
supplied by ad-networks are fake; collaborations between ad-networks and app
stores that bad-lists malicious apps works better still, but fails to prevent
criminals from writing fraudulent apps which they monetise until they get
banned and start over again. This work develops novel inference techniques that
can isolate click fraud attacks using their fundamental properties. In the {\em
mimicry defence}, we leverage the observation that organic click fraud involves
the re-use of legitimate clicks. Thus we can isolate fake-clicks by detecting
patterns of click-reuse within ad-network clickstreams with historical
behaviour serving as a baseline. Second, in {\em bait-click defence}. we
leverage the vantage point of an ad-network to inject a pattern of bait clicks
into the user's device, to trigger click fraud-apps that are gated on
user-behaviour. Our experiments show that the mimicry defence detects around
81\% of fake-clicks in stealthy (low rate) attacks with a false-positive rate
of 110110 per hundred thousand clicks. Bait-click defence enables further
improvements in detection rates of 95\% and reduction in false-positive rates
of between 0 and 30 clicks per million, a substantial improvement over current
approaches.",false advertising
http://arxiv.org/abs/1602.07128v2,"It is known that some network operators inject false content into users'
network traffic. Yet all previous works that investigate this practice focus on
edge ISPs (Internet Service Providers), namely, those that provide Internet
access to end users. Edge ISPs that inject false content affect their customers
only. However, in this work we show that not only edge ISPs may inject false
content, but also core network operators. These operators can potentially alter
the traffic of \emph{all} Internet users who visit predetermined websites. We
expose this practice by inspecting a large amount of traffic originating from
several networks. Our study is based on the observation that the forged traffic
is injected in an out-of-band manner: the network operators do not update the
network packets in-path, but rather send the forged packets \emph{without}
dropping the legitimate ones. This creates a race between the forged and the
legitimate packets as they arrive to the end user. This race can be identified
and analyzed. Our analysis shows that the main purpose of content injection is
to increase the network operators' revenue by inserting advertisements to
websites. Nonetheless, surprisingly, we have also observed numerous cases of
injected malicious content. We publish representative samples of the injections
to facilitate continued analysis of this practice by the security community.",false advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",false advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",false advertising
http://arxiv.org/abs/1209.2557v1,"A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.",false advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",false advertising
http://arxiv.org/abs/1508.01843v2,"Background: Twitter has become the ""wild-west"" of marketing and promotional
strategies for advertisement agencies. Electronic cigarettes have been heavily
marketed across Twitter feeds, offering discounts, ""kid-friendly"" flavors,
algorithmically generated false testimonials, and free samples. Methods:All
electronic cigarette keyword related tweets from a 10% sample of Twitter
spanning January 2012 through December 2014 (approximately 850,000 total
tweets) were identified and categorized as Automated or Organic by combining a
keyword classification and a machine trained Human Detection algorithm. A
sentiment analysis using Hedonometrics was performed on Organic tweets to
quantify the change in consumer sentiments over time. Commercialized tweets
were topically categorized with key phrasal pattern matching. Results:The
overwhelming majority (80%) of tweets were classified as automated or
promotional in nature. The majority of these tweets were coded as
commercialized (83.65% in 2013), up to 33% of which offered discounts or free
samples and appeared on over a billion twitter feeds as impressions. The
positivity of Organic (human) classified tweets has decreased over time (5.84
in 2013 to 5.77 in 2014) due to a relative increase in the negative words
ban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the
positive words like haha,good,cool. Automated tweets are more positive than
organic (6.17 versus 5.84) due to a relative increase in the marketing words
best,win,buy,sale,health,discount and a relative decrease in negative words
like bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter
and the clinical uncertainty of the long term health complications of
electronic cigarette consumption, the protection of public health warrants
scrutiny and potential regulation of social media marketing.",false advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",false advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",false advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",false advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",false advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",false advertising
http://arxiv.org/abs/1111.0387v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms,absence of centralized
monitoring points, and lack of clear lines of defense. Most of the routing
protocols for MANETs are thus vulnerable to various types of attacks. Ad hoc
on-demand distance vector routing (AODV) is a very popular routing algorithm.
However, it is vulnerable to the well-known black hole attack, where a
malicious node falsely advertises good paths to a destination node during the
route discovery process. This attack becomes more sever when a group of
malicious nodes cooperate each other. In this paper, a defense mechanism is
presented against a coordinated attack by multiple black hole nodes in a MANET.
The simulation carried out on the proposed scheme has produced results that
demonstrate the effectiveness of the mechanism in detection of the attack while
maintaining a reasonable level of throughput in the network.",false advertising
http://arxiv.org/abs/1302.4882v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms, absence of centralized
monitoring points, and lack of clear lines of defense. Protecting the network
layer of a MANET from malicious attacks is an important and challenging
security issue, since most of the routing protocols for MANETs are vulnerable
to various types of attacks. Ad hoc on-demand distance vector routing (AODV) is
a very popular routing algorithm. However, it is vulnerable to the well-known
black hole attack, where a malicious node falsely advertises good paths to a
destination node during the route discovery process but drops all packets in
the data forwarding phase. This attack becomes more severe when a group of
malicious nodes cooperate each other. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of the nodes.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead and computation overhead in the nodes.",false advertising
http://arxiv.org/abs/1709.10388v1,"The online ads trading platform plays a crucial role in connecting publishers
and advertisers and generates tremendous value in facilitating the convenience
of our lives. It has been evolving into a more and more complicated structure.
In this paper, we consider the problem of maximizing the revenue for the seller
side via utilizing proper reserve price for the auctions in a dynamical way.
  Predicting the optimal reserve price for each auction in the repeated auction
marketplaces is a non-trivial problem. However, we were able to come up with an
efficient method of improving the seller revenue by mainly focusing on
adjusting the reserve price for those high-value inventories. Previously, no
dedicated work has been performed from this perspective. Inspired by Paul and
Michael, our model first identifies the value of the inventory by predicting
the top bid price bucket using a cascade of classifiers. The cascade is
essential in significantly reducing the false positive rate of a single
classifier. Based on the output of the first step, we build another cluster of
classifiers to predict the price separations between the top two bids. We
showed that although the high-value auctions are only a small portion of all
the traffic, successfully identifying them and setting correct reserve price
would result in a significant revenue lift. Moreover, our optimization is
compatible with all other reserve price models in the system and does not
impact their performance. In other words, when combined with other models, the
enhancement on exchange revenue will be aggregated. Simulations on randomly
sampled Yahoo ads exchange (YAXR) data showed stable and expected lift after
applying our model.",false advertising
http://arxiv.org/abs/1807.07741v1,"Employers actively look for talents having not only specific hard skills but
also various soft skills. To analyze the soft skill demands on the job market,
it is important to be able to detect soft skill phrases from job advertisements
automatically. However, a naive matching of soft skill phrases can lead to
false positive matches when a soft skill phrase, such as friendly, is used to
describe a company, a team, or another entity, rather than a desired candidate.
  In this paper, we propose a phrase-matching-based approach which
differentiates between soft skill phrases referring to a candidate vs.
something else. The disambiguation is formulated as a binary text
classification problem where the prediction is made for the potential soft
skill based on the context where it occurs. To inform the model about the soft
skill for which the prediction is made, we develop several approaches,
including soft skill masking and soft skill tagging.
  We compare several neural network based approaches, including CNN, LSTM and
Hierarchical Attention Model. The proposed tagging-based input representation
using LSTM achieved the highest recall of 83.92% on the job dataset when fixing
a precision to 95%.",false advertising
http://arxiv.org/abs/1908.07087v2,"Given the reach of web platforms, bad actors have considerable incentives to
manipulate and defraud users at the expense of platform integrity. This has
spurred research in numerous suspicious behavior detection tasks, including
detection of sybil accounts, false information, and payment scams/fraud. In
this paper, we draw the insight that many such initiatives can be tackled in a
common framework by posing a detection task which seeks to find groups of
entities which share too many properties with one another across multiple
attributes (sybil accounts created at the same time and location, propaganda
spreaders broadcasting articles with the same rhetoric and with similar
reshares, etc.) Our work makes four core contributions: Firstly, we posit a
novel formulation of this task as a multi-view graph mining problem, in which
distinct views reflect distinct attribute similarities across entities, and
contextual similarity and attribute importance are respected. Secondly, we
propose a novel suspiciousness metric for scoring entity groups given the
abnormality of their synchronicity across multiple views, which obeys intuitive
desiderata that existing metrics do not. Finally, we propose the SliceNDice
algorithm which enables efficient extraction of highly suspicious entity
groups, and demonstrate its practicality in production, in terms of strong
detection performance and discoveries on Snapchat's large advertiser ecosystem
(89% precision and numerous discoveries of real fraud rings), marked
outperformance of baselines (over 97% precision/recall in simulated settings)
and linear scalability.",false advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",false advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",false advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",false advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",false advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",false advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",false advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",false advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",false advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",false advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",false advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",false advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",false advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",false advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",false advertising
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",illegal advertising
http://arxiv.org/abs/1910.05424v1,"Illegal fishing is prevalent throughout the world and heavily impacts the
health of our oceans, the sustainability and profitability of fisheries, and
even acts to destabilize geopolitical relations. To achieve the United Nations'
Sustainable Development Goal of ""Life Below Water"", our ability to detect and
predict illegal fishing must improve. Recent advances have been made through
the use of vessel location data, however, most analyses to date focus on
anomalous spatial behaviors of vessels one at a time. To improve predictions,
we develop a method inspired by complex systems theory to monitor the anomalous
multi-scale behavior of whole fleets as they respond to nearby illegal
activities. Specifically, we analyze changes in the multiscale geospatial
organization of fishing fleets operating on the Patagonia Shelf, an important
fishing region with chronic exposure to illegal fishing. We show that legally
operating (and visible) vessels respond anomalously to nearby illegal
activities (by vessels that are difficult to detect). Indeed, precursor
behaviors are identified, suggesting a path towards pre-empting illegal
activities. This approach offers a promising step towards a global system for
detecting, predicting and deterring illegal activities at sea in near
real-time. Doing so will be a big step forward to achieving sustainable life
underwater.",illegal advertising
http://arxiv.org/abs/1710.02546v1,"The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.",illegal advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",illegal advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",illegal advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",illegal advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",illegal advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",illegal advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",illegal advertising
http://arxiv.org/abs/1905.05543v2,"The non-indexed parts of the Internet (the Darknet) have become a haven for
both legal and illegal anonymous activity. Given the magnitude of these
networks, scalably monitoring their activity necessarily relies on automated
tools, and notably on NLP tools. However, little is known about what
characteristics texts communicated through the Darknet have, and how well
off-the-shelf NLP tools do on this domain. This paper tackles this gap and
performs an in-depth investigation of the characteristics of legal and illegal
text in the Darknet, comparing it to a clear net website with similar content
as a control condition. Taking drug-related websites as a test case, we find
that texts for selling legal and illegal drugs have several linguistic
characteristics that distinguish them from one another, as well as from the
control condition, among them the distribution of POS tags, and the coverage of
their named entities in Wikipedia.",illegal advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",illegal advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",illegal advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",illegal advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",illegal advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",illegal advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",illegal advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",illegal advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",illegal advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",illegal advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",illegal advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",illegal advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",illegal advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",illegal advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",illegal advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",illegal advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",illegal advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",illegal advertising
http://arxiv.org/abs/1909.03602v2,"With the recent prevalence of Reinforcement Learning (RL), there have been
tremendous interests in utilizing RL for online advertising in recommendation
platforms (e.g. e-commerce and news feed sites). However, most RL-based
advertising algorithms focus on solely optimizing the revenue of ads while
ignoring possible negative influence of ads on user experience of recommended
items (products, articles and videos). Developing an optimal advertising
algorithm in recommendations faces immense challenges because interpolating ads
improperly or too frequently may decrease user experience, while interpolating
fewer ads will reduce the advertising revenue. Thus, in this paper, we propose
a novel advertising strategy for the rec/ads trade-off. To be specific, we
develop a reinforcement learning based framework that can continuously update
its advertising strategies and maximize reward in the long run. Given a
recommendation list, we design a novel Deep Q-network architecture that can
determine three internally related tasks jointly, i.e., (i) whether to
interpolate an ad or not in the recommendation list, and if yes, (ii) the
optimal ad and (iii) the optimal location to interpolate. The experimental
results based on real-world data demonstrate the effectiveness of the proposed
framework.",illegal advertising
http://arxiv.org/abs/1910.06859v1,"Readers take decisions about going through the complete news based on many
factors. The emotional impact of the news title on reader is one of the most
important factors. Cognitive ergonomics tries to strike the balance between
work, product and environment with human needs and capabilities. The utmost
need to integrate emotions in the news as well as advertisements cannot be
denied. The idea is that news or advertisement should be able to engage the
reader on emotional and behavioral platform. While achieving this objective
there is need to learn about reader behavior and use computational psychology
while presenting as well as writing news or advertisements. This paper based on
Machine Learning, tries to map behavior of the reader with the
news/advertisements and also provide inputs for affective value for building
personalized news or advertisements presentations. The affective value of the
news is determined and news artifacts are mapped to reader. The algorithm
suggests the most suitable news for readers while understanding emotional
traits required for personalization. This work can be used to improve reader
satisfaction through embedding emotions in the reading material and
prioritizing news presentations. It can be used to map personal reading
material range, personalized programs and ranking programs, advertisements with
reference to individuals.",illegal advertising
http://arxiv.org/abs/1508.03080v1,"We study how privacy technologies affect user and advertiser behavior in a
simple economic model of targeted advertising. In our model, a consumer first
decides whether or not to buy a good, and then an advertiser chooses an
advertisement to show the consumer. The consumer's value for the good is
correlated with her type, which determines which ad the advertiser would prefer
to show to her---and hence, the advertiser would like to use information about
the consumer's purchase decision to target the ad that he shows.
  In our model, the advertiser is given only a differentially private signal
about the consumer's behavior---which can range from no signal at all to a
perfect signal, as we vary the differential privacy parameter. This allows us
to study equilibrium behavior as a function of the level of privacy provided to
the consumer. We show that this behavior can be highly counter-intuitive, and
that the effect of adding privacy in equilibrium can be completely different
from what we would expect if we ignored equilibrium incentives. Specifically,
we show that increasing the level of privacy can actually increase the amount
of information about the consumer's type contained in the signal the advertiser
receives, lead to decreased utility for the consumer, and increased profit for
the advertiser, and that generally these quantities can be non-monotonic and
even discontinuous in the privacy level of the signal.",illegal advertising
http://arxiv.org/abs/1703.02091v4,"Taobao, as the largest online retail platform in the world, provides billions
of online display advertising impressions for millions of advertisers every
day. For commercial purposes, the advertisers bid for specific spots and target
crowds to compete for business traffic. The platform chooses the most suitable
ads to display in tens of milliseconds. Common pricing methods include cost per
mille (CPM) and cost per click (CPC). Traditional advertising systems target
certain traits of users and ad placements with fixed bids, essentially regarded
as coarse-grained matching of bid and traffic quality. However, the fixed bids
set by the advertisers competing for different quality requests cannot fully
optimize the advertisers' key requirements. Moreover, the platform has to be
responsible for the business revenue and user experience. Thus, we proposed a
bid optimizing strategy called optimized cost per click (OCPC) which
automatically adjusts the bid to achieve finer matching of bid and traffic
quality of page view (PV) request granularity. Our approach optimizes
advertisers' demands, platform business revenue and user experience and as a
whole improves traffic allocation efficiency. We have validated our approach in
Taobao display advertising system in production. The online A/B test shows our
algorithm yields substantially better results than previous fixed bid manner.",illegal advertising
http://arxiv.org/abs/1811.10921v3,"Online advertising is the major source of income for a large portion of
Internet Services. There exists a body of literature aiming at optimizing ads
engagement, understanding the privacy and ethical implications of online
advertising, etc. However, to the best of our knowledge, no previous work
analyses at large scale the exposure of real users to online advertising. This
paper performs a comprehensive analysis of the exposure of users to ads and
advertisers using a dataset including more than 7M ads from 140K unique
advertisers delivered to more than 5K users that was collected between October
2016 and May 2018. The study focuses on Facebook, which is the second largest
advertising platform only to Google in terms of revenue, and accounts for more
than 2.2B monthly active users. Our analysis reveals that Facebook users are
exposed (in median) to 70 ads per week, which come from 12 advertisers. Ads
represent between 10% and 15% of all the information received in users'
newsfeed. A small increment of 1% in the portion of ads in the newsfeed could
roughly represent a revenue increase of 8.17M USD per week for Facebook.
Finally, we also reveal that Facebook users are overprofiled since in the best
case only 22.76% of the interests Facebook assigns to users for advertising
purpose are actually related to the ads those users receive.",illegal advertising
http://arxiv.org/abs/1605.03892v1,"In the framework of distributed network computing, it is known that, for
every network predicate, each network configuration that satisfies this
predicate can be proved using distributed certificates which can be verified
locally. However, this requires to leak information about the identities of the
nodes in the certificates, which might not be applicable in a context in which
privacy is desirable. Unfortunately, it is known that if one insists on
certificates independent of the node identities, then not all network
predicates can be proved using distributed certificates that can be verified
locally. In this paper, we prove that, for every network predicate, there is a
distributed protocol satisfying the following two properties: (1) for every
network configuration that is legal w.r.t. the predicate, and for any attempt
by an adversary to prove the illegality of that configuration using distributed
certificates, there is a locally verifiable proof that the adversary is wrong,
also using distributed certificates; (2) for every network configuration that
is illegal w.r.t. the predicate, there is a proof of that illegality, using
distributed certificates, such that no matter the way an adversary assigns its
own set of distributed certificates in an attempt to prove the legality of the
configuration, the actual illegality of the configuration will be locally
detected. In both cases, the certificates are independent of the identities of
the nodes. These results are achieved by investigating the so-called local
hierarchy of complexity classes in which the certificates do not exploit the
node identities. Indeed, we give a characterization of such a hierarchy, which
is of its own interest",illegal advertising
http://arxiv.org/abs/1507.00627v1,"The paper aims to mapping the potential vulnerable areas to illegal dumping
of household waste from rural areas in the extra- Carpathian region of Neamt
County. These areas are ordinary in the proximity of built-up areas and buffers
areas of 1 km were delimited for every locality. Based on various map layers in
vector formats (land use, rivers, built-up areas, roads etc) an assessment
method is performed to highlight the potential areas vulnerable to illegal
dumping inside these buffer areas at local scale. The results are correlated to
field observations and current situation of waste management systems. The maps
outline local disparities due to various geographical conditions of county.
This approach is a necessary tool in EIA studies particularly for rural waste
management systems at local and regional scale which are less studied in
current literature than urban areas.",illegal advertising
http://arxiv.org/abs/1802.08365v6,"Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.",misleading advertising
http://arxiv.org/abs/1908.10679v1,"Customers make a lot of reviews on online shopping websites every day, e.g.,
Amazon and Taobao. Reviews affect the buying decisions of customers, meanwhile,
attract lots of spammers aiming at misleading buyers. Xianyu, the largest
second-hand goods app in China, suffering from spam reviews. The anti-spam
system of Xianyu faces two major challenges: scalability of the data and
adversarial actions taken by spammers. In this paper, we present our technical
solutions to address these challenges. We propose a large-scale anti-spam
method based on graph convolutional networks (GCN) for detecting spam
advertisements at Xianyu, named GCN-based Anti-Spam (GAS) model. In this model,
a heterogeneous graph and a homogeneous graph are integrated to capture the
local context and global context of a comment. Offline experiments show that
the proposed method is superior to our baseline model in which the information
of reviews, features of users and items being reviewed are utilized.
Furthermore, we deploy our system to process million-scale data daily at
Xianyu. The online performance also demonstrates the effectiveness of the
proposed method.",misleading advertising
http://arxiv.org/abs/1901.00546v1,"Adversarial examples are delicately perturbed inputs, which aim to mislead
machine learning models towards incorrect outputs. While most of the existing
work focuses on generating adversarial perturbations in multi-class
classification problems, many real-world applications fall into the multi-label
setting in which one instance could be associated with more than one label. For
example, a spammer may generate adversarial spams with malicious advertising
while maintaining the other labels such as topic labels unchanged. To analyze
the vulnerability and robustness of multi-label learning models, we investigate
the generation of multi-label adversarial perturbations. This is a challenging
task due to the uncertain number of positive labels associated with one
instance, as well as the fact that multiple labels are usually not mutually
exclusive with each other. To bridge this gap, in this paper, we propose a
general attacking framework targeting on multi-label classification problem and
conduct a premier analysis on the perturbations for deep neural networks.
Leveraging the ranking relationships among labels, we further design a
ranking-based framework to attack multi-label ranking algorithms. We specify
the connection between the two proposed frameworks and separately design two
specific methods grounded on each of them to generate targeted multi-label
perturbations. Experiments on real-world multi-label image classification and
ranking problems demonstrate the effectiveness of our proposed frameworks and
provide insights of the vulnerability of multi-label deep learning models under
diverse targeted attacking strategies. Several interesting findings including
an unpolished defensive strategy, which could potentially enhance the
interpretability and robustness of multi-label deep learning models, are
further presented and discussed at the end.",misleading advertising
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",misleading advertising
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",misleading advertising
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",misleading advertising
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",misleading advertising
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",misleading advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",misleading advertising
http://arxiv.org/abs/1811.03194v3,"Perceptual ad-blocking is a novel approach that detects online advertisements
based on their visual content. Compared to traditional filter lists, the use of
perceptual signals is believed to be less prone to an arms race with web
publishers and ad networks. We demonstrate that this may not be the case. We
describe attacks on multiple perceptual ad-blocking techniques, and unveil a
new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual
ad-blocking can also introduce new vulnerabilities that let an attacker bypass
web security boundaries and mount DDoS attacks.
  We first analyze the design space of perceptual ad-blockers and present a
unified architecture that incorporates prior academic and commercial work. We
then explore a variety of attacks on the ad-blocker's detection pipeline, that
enable publishers or ad networks to evade or detect ad-blocking, and at times
even abuse its high privilege level to bypass web security boundaries.
  On one hand, we show that perceptual ad-blocking must visually classify
rendered web content to escape an arms race centered on obfuscation of page
markup. On the other, we present a concrete set of attacks on visual
ad-blockers by constructing adversarial examples in a real web page context.
For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and
native web content that misleads perceptual ad-blocking with 100% success
rates. In one of our attacks, we demonstrate how a malicious user can upload
adversarial content, such as a perturbed image in a Facebook post, that fools
the ad-blocker into removing another users' non-ad content.
  Moving beyond the Web and visual domain, we also build adversarial examples
for AdblockRadio, an open source radio client that uses machine learning to
detects ads in raw audio streams.",misleading advertising
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",misleading advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",misleading advertising
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",misleading advertising
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",misleading advertising
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",misleading advertising
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",misleading advertising
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",misleading advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",misleading advertising
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",misleading advertising
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",misleading advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",misleading advertising
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",misleading advertising
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",misleading advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",misleading advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",misleading advertising
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",misleading advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",misleading advertising
http://arxiv.org/abs/1909.03602v2,"With the recent prevalence of Reinforcement Learning (RL), there have been
tremendous interests in utilizing RL for online advertising in recommendation
platforms (e.g. e-commerce and news feed sites). However, most RL-based
advertising algorithms focus on solely optimizing the revenue of ads while
ignoring possible negative influence of ads on user experience of recommended
items (products, articles and videos). Developing an optimal advertising
algorithm in recommendations faces immense challenges because interpolating ads
improperly or too frequently may decrease user experience, while interpolating
fewer ads will reduce the advertising revenue. Thus, in this paper, we propose
a novel advertising strategy for the rec/ads trade-off. To be specific, we
develop a reinforcement learning based framework that can continuously update
its advertising strategies and maximize reward in the long run. Given a
recommendation list, we design a novel Deep Q-network architecture that can
determine three internally related tasks jointly, i.e., (i) whether to
interpolate an ad or not in the recommendation list, and if yes, (ii) the
optimal ad and (iii) the optimal location to interpolate. The experimental
results based on real-world data demonstrate the effectiveness of the proposed
framework.",misleading advertising
http://arxiv.org/abs/1910.06859v1,"Readers take decisions about going through the complete news based on many
factors. The emotional impact of the news title on reader is one of the most
important factors. Cognitive ergonomics tries to strike the balance between
work, product and environment with human needs and capabilities. The utmost
need to integrate emotions in the news as well as advertisements cannot be
denied. The idea is that news or advertisement should be able to engage the
reader on emotional and behavioral platform. While achieving this objective
there is need to learn about reader behavior and use computational psychology
while presenting as well as writing news or advertisements. This paper based on
Machine Learning, tries to map behavior of the reader with the
news/advertisements and also provide inputs for affective value for building
personalized news or advertisements presentations. The affective value of the
news is determined and news artifacts are mapped to reader. The algorithm
suggests the most suitable news for readers while understanding emotional
traits required for personalization. This work can be used to improve reader
satisfaction through embedding emotions in the reading material and
prioritizing news presentations. It can be used to map personal reading
material range, personalized programs and ranking programs, advertisements with
reference to individuals.",misleading advertising
http://arxiv.org/abs/1508.03080v1,"We study how privacy technologies affect user and advertiser behavior in a
simple economic model of targeted advertising. In our model, a consumer first
decides whether or not to buy a good, and then an advertiser chooses an
advertisement to show the consumer. The consumer's value for the good is
correlated with her type, which determines which ad the advertiser would prefer
to show to her---and hence, the advertiser would like to use information about
the consumer's purchase decision to target the ad that he shows.
  In our model, the advertiser is given only a differentially private signal
about the consumer's behavior---which can range from no signal at all to a
perfect signal, as we vary the differential privacy parameter. This allows us
to study equilibrium behavior as a function of the level of privacy provided to
the consumer. We show that this behavior can be highly counter-intuitive, and
that the effect of adding privacy in equilibrium can be completely different
from what we would expect if we ignored equilibrium incentives. Specifically,
we show that increasing the level of privacy can actually increase the amount
of information about the consumer's type contained in the signal the advertiser
receives, lead to decreased utility for the consumer, and increased profit for
the advertiser, and that generally these quantities can be non-monotonic and
even discontinuous in the privacy level of the signal.",misleading advertising
http://arxiv.org/abs/1703.02091v4,"Taobao, as the largest online retail platform in the world, provides billions
of online display advertising impressions for millions of advertisers every
day. For commercial purposes, the advertisers bid for specific spots and target
crowds to compete for business traffic. The platform chooses the most suitable
ads to display in tens of milliseconds. Common pricing methods include cost per
mille (CPM) and cost per click (CPC). Traditional advertising systems target
certain traits of users and ad placements with fixed bids, essentially regarded
as coarse-grained matching of bid and traffic quality. However, the fixed bids
set by the advertisers competing for different quality requests cannot fully
optimize the advertisers' key requirements. Moreover, the platform has to be
responsible for the business revenue and user experience. Thus, we proposed a
bid optimizing strategy called optimized cost per click (OCPC) which
automatically adjusts the bid to achieve finer matching of bid and traffic
quality of page view (PV) request granularity. Our approach optimizes
advertisers' demands, platform business revenue and user experience and as a
whole improves traffic allocation efficiency. We have validated our approach in
Taobao display advertising system in production. The online A/B test shows our
algorithm yields substantially better results than previous fixed bid manner.",misleading advertising
http://arxiv.org/abs/1811.10921v3,"Online advertising is the major source of income for a large portion of
Internet Services. There exists a body of literature aiming at optimizing ads
engagement, understanding the privacy and ethical implications of online
advertising, etc. However, to the best of our knowledge, no previous work
analyses at large scale the exposure of real users to online advertising. This
paper performs a comprehensive analysis of the exposure of users to ads and
advertisers using a dataset including more than 7M ads from 140K unique
advertisers delivered to more than 5K users that was collected between October
2016 and May 2018. The study focuses on Facebook, which is the second largest
advertising platform only to Google in terms of revenue, and accounts for more
than 2.2B monthly active users. Our analysis reveals that Facebook users are
exposed (in median) to 70 ads per week, which come from 12 advertisers. Ads
represent between 10% and 15% of all the information received in users'
newsfeed. A small increment of 1% in the portion of ads in the newsfeed could
roughly represent a revenue increase of 8.17M USD per week for Facebook.
Finally, we also reveal that Facebook users are overprofiled since in the best
case only 22.76% of the interests Facebook assigns to users for advertising
purpose are actually related to the ads those users receive.",misleading advertising
http://arxiv.org/abs/1909.02156v1,"Interactions between bids to show ads online can lead to an advertiser's ad
being shown to more men than women even when the advertiser does not target
towards men. We design bidding strategies that advertisers can use to avoid
such emergent discrimination without having to modify the auction mechanism. We
mathematically analyze the strategies to determine the additional cost to the
advertiser for avoiding discrimination, proving our strategies to be optimal in
some settings. We use simulations to understand other settings.",misleading advertising
http://arxiv.org/abs/1307.4980v7,"In sponsored search, advertisement (abbreviated ad) slots are usually sold by
a search engine to an advertiser through an auction mechanism in which
advertisers bid on keywords. In theory, auction mechanisms have many desirable
economic properties. However, keyword auctions have a number of limitations
including: the uncertainty in payment prices for advertisers; the volatility in
the search engine's revenue; and the weak loyalty between advertiser and search
engine. In this paper we propose a special ad option that alleviates these
problems. In our proposal, an advertiser can purchase an option from a search
engine in advance by paying an upfront fee, known as the option price. He then
has the right, but no obligation, to purchase among the pre-specified set of
keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks
in a specified period of time. The proposed option is closely related to a
special exotic option in finance that contains multiple underlying assets
(multi-keyword) and is also multi-exercisable (multi-click). This novel
structure has many benefits: advertisers can have reduced uncertainty in
advertising; the search engine can improve the advertisers' loyalty as well as
obtain a stable and increased expected revenue over time. Since the proposed ad
option can be implemented in conjunction with the existing keyword auctions,
the option price and corresponding fixed CPCs must be set such that there is no
arbitrage between the two markets. Option pricing methods are discussed and our
experimental results validate the development. Compared to keyword auctions, a
search engine can have an increased expected revenue by selling an ad option.",misleading advertising
http://arxiv.org/abs/1507.08874v2,"While substantial effort has been devoted to understand fraudulent activity
in traditional online advertising (search and banner), more recent forms such
as video ads have received little attention. The understanding and
identification of fraudulent activity (i.e., fake views) in video ads for
advertisers, is complicated as they rely exclusively on the detection
mechanisms deployed by video hosting portals. In this context, the development
of independent tools able to monitor and audit the fidelity of these systems
are missing today and needed by both industry and regulators.
  In this paper we present a first set of tools to serve this purpose. Using
our tools, we evaluate the performance of the audit systems of five major
online video portals. Our results reveal that YouTube's detection system
significantly outperforms all the others. Despite this, a systematic evaluation
indicates that it may still be susceptible to simple attacks. Furthermore, we
find that YouTube penalizes its videos' public and monetized view counters
differently, the former being more aggressive. This means that views identified
as fake and discounted from the public view counter are still monetized. We
speculate that even though YouTube's policy puts in lots of effort to
compensate users after an attack is discovered, this practice places the burden
of the risk on the advertisers, who pay to get their ads displayed.",aggressive advertisement
http://arxiv.org/abs/1712.03086v1,"In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.",aggressive advertisement
http://arxiv.org/abs/1901.06244v1,"On October 14, 2015, Tesla Inc. an American electric car company, released
the initial version of the Autopilot system. This system promised to provide
semi-autonomous driving using the existing hardware already installed on Tesla
vehicles. On March 23rd, 2018, a Tesla vehicle ran into a divider at highway
speed, killing the driver. This occurred under the control of the Autopilot
system with no driver intervention. Critics argue that though Tesla gives
drivers warnings in its owner's manual, it is ultimately unethical to release a
system that is marketed as an Autopilot yet still makes grave mistakes that any
human driver would not make. Others defend Tesla by stating that their
advisories are suitable and that drivers should ultimately be at fault for any
mistakes of the Autopilot. This paper will scrutinize the ethical implications
of Tesla's choice to develop, market, and ship a beta product that requires
extensive testing. It will further analyze the implications of Tesla's
aggressive advertisement of the product under the name Autopilot along with
associated marketing materials. By applying the joint ACM/IEEE-CS Software
Engineering Code of Ethics, this paper will show that Tesla's choices and
actions during this event are inconsistent with the code and are unethical
since they are responsible for adequately testing and honestly marketing their
product.",aggressive advertisement
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",aggressive advertisement
http://arxiv.org/abs/1812.02978v1,"Users of Online Social Networks (OSNs) interact with each other more than
ever. In the context of a public discussion group, people receive, read, and
write comments in response to articles and postings. In the absence of access
control mechanisms, OSNs are a great environment for attackers to influence
others, from spreading phishing URLs, to posting fake news. Moreover, OSN user
behavior can be predicted by social science concepts which include conformity
and the bandwagon effect. In this paper, we show how social recommendation
systems affect the occurrence of malicious URLs on Facebook. We exploit
temporal features to build a prediction framework, having greater than 75%
accuracy, to predict whether the following group users' behavior will increase
or not. Included in this work, we demarcate classes of URLs, including those
malicious URLs classified as creating critical damage, as well as those of a
lesser nature which only inflict light damage such as aggressive commercial
advertisements and spam content. It is our hope that the data and analyses in
this paper provide a better understanding of OSN user reactions to different
categories of malicious URLs, thereby providing a way to mitigate the influence
of these malicious URL attacks.",aggressive advertisement
http://arxiv.org/abs/1903.07581v2,"In the recent political climate, the topic of news quality has drawn
attention both from the public and the academic communities. The growing
distrust of traditional news media makes it harder to find a common base of
accepted truth. In this work, we design and build MediaRank
(www.media-rank.com), a fully automated system to rank over 50,000 online news
sources around the world. MediaRank collects and analyzes one million news
webpages and two million related tweets everyday. We base our algorithmic
analysis on four properties journalists have established to be associated with
reporting quality: peer reputation, reporting bias / breadth, bottomline
financial pressure, and popularity.
  Our major contributions of this paper include: (i) Open, interpretable
quality rankings for over 50,000 of the world's major news sources. Our
rankings are validated against 35 published news rankings, including French,
German, Russian, and Spanish language sources. MediaRank scores correlate
positively with 34 of 35 of these expert rankings. (ii) New computational
methods for measuring influence and bottomline pressure. To the best of our
knowledge, we are the first to study the large-scale news reporting citation
graph in-depth. We also propose new ways to measure the aggressiveness of
advertisements and identify social bots, establishing a connection between both
types of bad behavior. (iii) Analyzing the effect of media source bias and
significance. We prove that news sources cite others despite different
political views in accord with quality measures. However, in four
English-speaking countries (US, UK, Canada, and Australia), the highest ranking
sources all disproportionately favor left-wing parties, even when the majority
of news sources exhibited conservative slants.",aggressive advertisement
http://arxiv.org/abs/1309.5018v1,"We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.",aggressive advertisement
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",aggressive advertisement
http://arxiv.org/abs/1109.6263v1,"Most search engines sell slots to place advertisements on the search results
page through keyword auctions. Advertisers offer bids for how much they are
willing to pay when someone enters a search query, sees the search results, and
then clicks on one of their ads. Search engines typically order the
advertisements for a query by a combination of the bids and expected
clickthrough rates for each advertisement. In this paper, we extend a model of
Yahoo's and Google's advertising auctions to include an effect where repeatedly
showing less relevant ads has a persistent impact on all advertising on the
search engine, an impact we designate as the pollution effect. In Monte-Carlo
simulations using distributions fitted to Yahoo data, we show that a modest
pollution effect is sufficient to dramatically change the advertising rank
order that yields the optimal advertising revenue for a search engine. In
addition, if a pollution effect exists, it is possible to maximize revenue
while also increasing advertiser, and publisher utility. Our results suggest
that search engines could benefit from making relevant advertisements less
expensive and irrelevant advertisements more costly for advertisers than is the
current practice.",aggressive advertisement
http://arxiv.org/abs/1604.06648v1,"The problem of aggression for Internet communities is rampant. Anonymous
forums usually called imageboards are notorious for their aggressive and
deviant behaviour even in comparison with other Internet communities. This
study is aimed at studying ways of automatic detection of verbal expression of
aggression for the most popular American (4chan.org) and Russian (2ch.hk)
imageboards. A set of 1,802,789 messages was used for this study. The machine
learning algorithm word2vec was applied to detect the state of aggression. A
decent result is obtained for English (88%), the results for Russian are yet to
be improved.",aggressive advertisement
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",aggressive advertisement
http://arxiv.org/abs/1202.4030v1,"A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.",aggressive advertisement
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",aggressive advertisement
http://arxiv.org/abs/1208.3561v3,"We study pool-based active learning of half-spaces. We revisit the aggressive
approach for active learning in the realizable case, and show that it can be
made efficient and practical, while also having theoretical guarantees under
reasonable assumptions. We further show, both theoretically and experimentally,
that it can be preferable to mellow approaches. Our efficient aggressive active
learner of half-spaces has formal approximation guarantees that hold when the
pool is separable with a margin. While our analysis is focused on the
realizable setting, we show that a simple heuristic allows using the same
algorithm successfully for pools with low error as well. We further compare the
aggressive approach to the mellow approach, and prove that there are cases in
which the aggressive approach results in significantly better label complexity
compared to the mellow approach. We demonstrate experimentally that substantial
improvements in label complexity can be achieved using the aggressive approach,
for both realizable and low-error settings.",aggressive advertisement
http://arxiv.org/abs/1607.01076v1,"Prison facilities, mental correctional institutions, sports bars and places
of public protest are prone to sudden violence and conflicts. Surveillance
systems play an important role in mitigation of hostile behavior and
improvement of security by detecting such provocative and aggressive
activities. This research proposed using automatic aggressive behavior and
anger detection to improve the effectiveness of the surveillance systems. An
emotion and aggression aware component will make the surveillance system highly
responsive and capable of alerting the security guards in real time. This
research proposed facial expression, head, hand and body movement and speech
tracking for detecting anger and aggressive actions. Recognition was achieved
using support vector machines and rule based features. The multimodal affect
recognition precision rate for anger improved by 15.2% and recall rate improved
by 11.7% when behavioral rule based features were used in aggressive action
detection.",aggressive advertisement
http://arxiv.org/abs/1904.08770v1,"This paper attempt to study the effectiveness of text representation schemes
on two tasks namely: User Aggression and Fact Detection from the social media
contents. In User Aggression detection, The aim is to identify the level of
aggression from the contents generated in the Social media and written in the
English, Devanagari Hindi and Romanized Hindi. Aggression levels are
categorized into three predefined classes namely: `Non-aggressive`, `Overtly
Aggressive`, and `Covertly Aggressive`. During the disaster-related incident,
Social media like, Twitter is flooded with millions of posts. In such emergency
situations, identification of factual posts is important for organizations
involved in the relief operation. We anticipated this problem as a combination
of classification and Ranking problem. This paper presents a comparison of
various text representation scheme based on BoW techniques, distributed
word/sentence representation, transfer learning on classifiers. Weighted $F_1$
score is used as a primary evaluation metric. Results show that text
representation using BoW performs better than word embedding on machine
learning classifiers. While pre-trained Word embedding techniques perform
better on classifiers based on deep neural net. Recent transfer learning model
like ELMO, ULMFiT are fine-tuned for the Aggression classification task.
However, results are not at par with pre-trained word embedding model. Overall,
word embedding using fastText produce best weighted $F_1$-score than Word2Vec
and Glove. Results are further improved using pre-trained vector model.
Statistical significance tests are employed to ensure the significance of the
classification results. In the case of lexically different test Dataset, other
than training Dataset, deep neural models are more robust and perform
substantially better than machine learning classifiers.",aggressive advertisement
http://arxiv.org/abs/1709.03946v1,"The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.",aggressive advertisement
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",aggressive advertisement
http://arxiv.org/abs/1505.03235v1,"Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.",aggressive advertisement
http://arxiv.org/abs/1603.02484v1,"Interactive advertising which characterized by interactivity has become the
mainstream of advertising by gradually replacing traditional one-way
advertising during the new media era. This paper has obeyed the research
outline below, literature review, background description, assumption proposed,
and empirical analysis. Therefore, this paper proposed the communication model
of interactive advertising and drawn two related important conclusions,
interactivity has brought positive effect to advertising communication,
different type of consumers tend to use different interactive options in
different ways. Furthermore, this paper also presented three related
optimization strategies to improving the communication of interactive
advertising, namely, 1.changing communication model from one-way to two-way,
2.renovating new communication process and effect-generated path, 3.renovating
new strategy portfolio to improving the communication effect of interactive
advertising.",aggressive advertisement
http://arxiv.org/abs/1905.10928v1,"Real-Time Bidding (RTB) is an important paradigm in display advertising,
where advertisers utilize extended information and algorithms served by Demand
Side Platforms (DSPs) to improve advertising performance. A common problem for
DSPs is to help advertisers gain as much value as possible with budget
constraints. However, advertisers would routinely add certain key performance
indicator (KPI) constraints that the advertising campaign must meet due to
practical reasons. In this paper, we study the common case where advertisers
aim to maximize the quantity of conversions, and set cost-per-click (CPC) as a
KPI constraint. We convert such a problem into a linear programming problem and
leverage the primal-dual method to derive the optimal bidding strategy. To
address the applicability issue, we propose a feedback control-based solution
and devise the multivariable control system. The empirical study based on
real-word data from Taobao.com verifies the effectiveness and superiority of
our approach compared with the state of the art in the industry practices.",aggressive advertisement
http://arxiv.org/abs/1907.07275v2,"Online advertising relies on trackers and data brokers to show targeted ads
to users. To improve targeting, different entities in the intricately
interwoven online advertising and tracking ecosystems are incentivized to share
information with each other through client-side or server-side mechanisms.
Inferring data sharing between entities, especially when it happens at the
server-side, is an important and challenging research problem. In this paper,
we introduce KASHF: a novel method to infer data sharing relationships between
advertisers and trackers by studying how an advertiser's bidding behavior
changes as we manipulate the presence of trackers. We operationalize this
insight by training an interpretable machine learning model that uses the
presence of trackers as features to predict the bidding behavior of an
advertiser. By analyzing the machine learning model, we are able to infer
relationships between advertisers and trackers irrespective of whether data
sharing occurs at the client-side or the server-side. We are also able to
identify several server-side data sharing relationships that are validated
externally but are not detected by client-side cookie syncing.",aggressive advertisement
http://arxiv.org/abs/0906.4982v1,"The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.",aggressive advertisement
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",aggressive advertisement
http://arxiv.org/abs/1605.07167v1,"On today's Web, users trade access to their private data for content and
services. Advertising sustains the business model of many websites and
applications. Efficient and successful advertising relies on predicting users'
actions and tastes to suggest a range of products to buy. It follows that,
while surfing the Web users leave traces regarding their identity in the form
of activity patterns and unstructured data. We analyse how advertising networks
build user footprints and how the suggested advertising reacts to changes in
the user behaviour.",aggressive advertisement
http://arxiv.org/abs/cs/0607117v1,"Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study ""prefix
position auctions"" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an ""envy-free"" or ""symmetric"" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.",aggressive advertisement
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",aggressive advertisement
http://arxiv.org/abs/1901.07366v2,"Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.",aggressive advertisement
http://arxiv.org/abs/1903.11461v1,"Historians have regularly debated whether advertisements can be used as a
viable source to study the past. Their main concern centered on the question of
agency. Were advertisements a reflection of historical events and societal
debates, or were ad makers instrumental in shaping society and the ways people
interacted with consumer goods? Using techniques from econometrics (Granger
causality test) and complexity science (Adaptive Fractal Analysis), this paper
analyzes to what extent advertisements shaped or reflected society. We found
evidence that indicate a fundamental difference between the dynamic behavior of
word use in articles and advertisements published in a century of Dutch
newspapers. Articles exhibit persistent trends that are likely to be reflective
of communicative memory. Contrary to this, advertisements have a more irregular
behavior characterized by short bursts and fast decay, which, in part, mirrors
the dynamic through which advertisers introduced terms into public discourse.
On the issue of whether advertisements shaped or reflected society, we found
particular product types that seemed to be collectively driven by a causality
going from advertisements to articles. Generally, we found support for a
complex interaction pattern dubbed the consumption junction. Finally, we
discovered noteworthy patterns in terms of causality and long-range
dependencies for specific product groups. All in, this study shows how methods
from econometrics and complexity science can be applied to humanities data to
improve our understanding of complex cultural-historical phenomena such as the
role of advertising in society.",aggressive advertisement
http://arxiv.org/abs/1612.00408v1,"We propose an automated method for detecting aggressive prostate cancer(CaP)
(Gleason score >=7) based on a comprehensive analysis of the lesion and the
surrounding normal prostate tissue which has been simultaneously captured in
T2-weighted MR images, diffusion-weighted images (DWI) and apparent diffusion
coefficient maps (ADC). The proposed methodology was tested on a dataset of 79
patients (40 aggressive, 39 non-aggressive). We evaluated the performance of a
wide range of popular quantitative imaging features on the characterization of
aggressive versus non-aggressive CaP. We found that a group of 44
discriminative predictors among 1464 quantitative imaging features can be used
to produce an area under the ROC curve of 0.73.",aggressive advertisement
http://arxiv.org/abs/1805.00886v1,"The quadrotor task of negotiating aggressive attitude maneuvers while
adhering to motor constraints is addressed here. The majority of high level
quadrotor Nonlinear Control System (NCS) solutions ignore motor control
authority limitations, especially important during aggressive attitude
maneuvers, generating unrealizable thrusts and negating the validity of the
accompanying stability proofs. Here, an attitude control framework is
developed, comprised by a thrust allocation strategy and a specially designed
geometric attitude tracking controller, allowing the quadrotor to achieve
aggressive attitude maneuvers, while complying to actuator constraints and
simultaneously staying ""close"" to a desired position command in a
computationally inexpensive way. This is a novel contribution resulting in
thrusts realizable by available quadrotors during aggressive attitude
maneuvers, and enhanced performance guaranteed by valid stability proofs. Also,
it is shown that the developed controller can be combined with a collective
thrust expression in producing a position/yaw tracking controller. Through
rigorous stability proofs, both the position and attitude frameworks are shown
to have desirable closed loop properties that are almost global. This
establishes a quadrotor control solution allowing the vehicle to negotiate
aggressive maneuvers position/attitude on SE(3). Simulations illustrate and
validate the effectiveness and capabilities of the developed solution.",aggressive advertisement
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",aggressive advertisement
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",aggressive advertisement
http://arxiv.org/abs/1701.08744v1,"This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.",aggressive advertisement
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",aggressive advertisement
http://arxiv.org/abs/0805.1759v1,"Modern commercial Internet search engines display advertisements along side
the search results in response to user queries. Such sponsored search relies on
market mechanisms to elicit prices for these advertisements, making use of an
auction among advertisers who bid in order to have their ads shown for specific
keywords. We present an overview of the current systems for such auctions and
also describe the underlying game-theoretic aspects. The game involves three
parties--advertisers, the search engine, and search users--and we present
example research directions that emphasize the role of each. The algorithms for
bidding and pricing in these games use techniques from three mathematical
areas: mechanism design, optimization, and statistical estimation. Finally, we
present some challenges in sponsored search advertising.",sponsored advertising
http://arxiv.org/abs/1803.07347v3,"Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers' side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users' side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.",sponsored advertising
http://arxiv.org/abs/1907.12118v1,"Sponsored search has more than 20 years of history, and it has been proven to
be a successful business model for online advertising. Based on the
pay-per-click pricing model and the keyword targeting technology, the sponsored
system runs online auctions to determine the allocations and prices of search
advertisements. In the traditional setting, advertisers should manually create
lots of ad creatives and bid on some relevant keywords to target their
audience. Due to the huge amount of search traffic and a wide variety of ad
creations, the limits of manual optimizations from advertisers become the main
bottleneck for improving the efficiency of this market. Moreover, as many
emerging advertising forms and supplies are growing, it's crucial for sponsored
search platform to pay more attention to the ROI metrics of ads for getting the
marketing budgets of advertisers. In this paper, we present the AiAds system
developed at Baidu, which use machine learning techniques to build an automated
and intelligent advertising system. By designing and implementing the automated
bidding strategy, the intelligent targeting and the intelligent creation
models, the AiAds system can transform the manual optimizations into multiple
automated tasks and optimize these tasks in advanced methods. AiAds is a
brand-new architecture of sponsored search system which changes the bidding
language and allocation mechanism, breaks the limit of keyword targeting with
end-to-end ad retrieval framework and provides global optimization of ad
creation. This system can increase the advertiser's campaign performance, the
user experience and the revenue of the advertising platform simultaneously and
significantly. We present the overall architecture and modeling techniques for
each module of the system and share our lessons learned in solving several key
challenges.",sponsored advertising
http://arxiv.org/abs/0809.0116v1,"Internet search results are a growing and highly profitable advertising
platform. Search providers auction advertising slots to advertisers on their
search result pages. Due to the high volume of searches and the users' low
tolerance for search result latency, it is imperative to resolve these auctions
fast. Current approaches restrict the expressiveness of bids in order to
achieve fast winner determination, which is the problem of allocating slots to
advertisers so as to maximize the expected revenue given the advertisers' bids.
The goal of our work is to permit more expressive bidding, thus allowing
advertisers to achieve complex advertising goals, while still providing fast
and scalable techniques for winner determination.",sponsored advertising
http://arxiv.org/abs/1207.4701v1,"Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.",sponsored advertising
http://arxiv.org/abs/0906.4764v2,"In this paper, we propose a bid optimizer for sponsored keyword search
auctions which leads to better retention of advertisers by yielding attractive
utilities to the advertisers without decreasing the revenue to the search
engine. The bid optimizer is positioned as a key value added tool the search
engine provides to the advertisers. The proposed bid optimizer algorithm
transforms the reported values of the advertisers for a keyword into a
correlated bid profile using many ideas from cooperative game theory. The
algorithm is based on a characteristic form game involving the search engine
and the advertisers. Ideas from Nash bargaining theory are used in formulating
the characteristic form game to provide for a fair share of surplus among the
players involved. The algorithm then computes the nucleolus of the
characteristic form game since we find that the nucleolus is an apt way of
allocating the gains of cooperation among the search engine and the
advertisers. The algorithm next transforms the nucleolus into a correlated bid
profile using a linear programming formulation. This bid profile is input to a
standard generalized second price mechanism (GSP) for determining the
allocation of sponsored slots and the prices to be be paid by the winners. The
correlated bid profile that we determine is a locally envy-free equilibrium and
also a correlated equilibrium of the underlying game. Through detailed
simulation experiments, we show that the proposed bid optimizer retains more
customers than a plain GSP mechanism and also yields better long-run utilities
to the search engine and the advertisers.",sponsored advertising
http://arxiv.org/abs/1903.04149v1,"Online advertising has been the major monetization approach for Internet
companies. Advertisers invest budgets to bid for real-time impressions to gain
direct and indirect returns. Existing works have been concentrating on
optimizing direct returns brought by advertising traffic. However, indirect
returns induced by advertising traffic such as influencing the online organic
traffic and offline mouth-to-mouth marketing provide extra significant
motivation to advertisers. Modeling and quantization of causal effects between
the overall advertising return and budget enable the advertisers to spend their
money more judiciously. In this paper, we model the overall return as
individual advertising effect in causal inference with multiple treatments and
bound the expected estimation error with learnable factual loss and distance of
treatment-specific context distributions. Accordingly, a representation and
hypothesis network is used to minimize the loss bound. We apply the learned
causal effect in the online bidding engine of an industry-level sponsored
search system. Online experiments show that the causal inference based bidding
outperforms the existing online bidding algorithm.",sponsored advertising
http://arxiv.org/abs/1501.02785v5,"The growing demand for data has driven the Service Providers (SPs) to provide
differential treatment of traffic to generate additional revenue streams from
Content Providers (CPs). While SPs currently only provide best-effort services
to their CPs, it is plausible to envision a model in near future, where CPs are
willing to sponsor quality of service for their content in exchange of sharing
a portion of their profit with SPs. This quality sponsoring becomes invaluable
especially when the available resources are scarce such as in wireless
networks, and can be accommodated in a non-neutral network. In this paper, we
consider the problem of Quality-Sponsored Data (QSD) in a non-neutral network.
In our model, SPs allow CPs to sponsor a portion of their resources, and price
it appropriately to maximize their payoff. The payoff of the SP depends on the
monetary revenue and the satisfaction of end-users both for the non-sponsored
and sponsored content, while CPs generate revenue through advertisement. We
analyze the market dynamics and equilibria in two different frameworks, i.e.
sequential and bargaining game frameworks, and provide strategies for (i) SPs:
to determine if and how to price resources, and (ii) CPs: to determine if and
what quality to sponsor. The frameworks characterize different sets of
equilibrium strategies and market outcomes depending on the parameters of the
market.",sponsored advertising
http://arxiv.org/abs/1307.4980v7,"In sponsored search, advertisement (abbreviated ad) slots are usually sold by
a search engine to an advertiser through an auction mechanism in which
advertisers bid on keywords. In theory, auction mechanisms have many desirable
economic properties. However, keyword auctions have a number of limitations
including: the uncertainty in payment prices for advertisers; the volatility in
the search engine's revenue; and the weak loyalty between advertiser and search
engine. In this paper we propose a special ad option that alleviates these
problems. In our proposal, an advertiser can purchase an option from a search
engine in advance by paying an upfront fee, known as the option price. He then
has the right, but no obligation, to purchase among the pre-specified set of
keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks
in a specified period of time. The proposed option is closely related to a
special exotic option in finance that contains multiple underlying assets
(multi-keyword) and is also multi-exercisable (multi-click). This novel
structure has many benefits: advertisers can have reduced uncertainty in
advertising; the search engine can improve the advertisers' loyalty as well as
obtain a stable and increased expected revenue over time. Since the proposed ad
option can be implemented in conjunction with the existing keyword auctions,
the option price and corresponding fixed CPCs must be set such that there is no
arbitrage between the two markets. Option pricing methods are discussed and our
experimental results validate the development. Compared to keyword auctions, a
search engine can have an increased expected revenue by selling an ad option.",sponsored advertising
http://arxiv.org/abs/0706.1318v1,"We present an algorithm for constructing an optimal slate of sponsored search
advertisements which respects the ordering that is the outcome of a generalized
second price auction, but which must also accommodate complicating factors such
as overall budget constraints. The algorithm is easily fast enough to use on
the fly for typical problem sizes, or as a subroutine in an overall
optimization.",sponsored advertising
http://arxiv.org/abs/1806.05799v2,"As the largest e-commerce platform, Taobao helps advertisers reach billions
of search queries each day via sponsored search, which has also contributed
considerable revenue to the platform. An efficient bidding strategy to cater to
diverse advertiser demands while balancing platform revenue and consumer
experience is significant to a healthy and sustainable marketing ecosystem. In
this paper we propose \emph{Customer Intelligent Agent (CIA)}, a bidding
optimization framework which implements an impression-level bidding to reflect
advertisers' conversion willingness and budget control. In this way, CIA is
capable of fulfilling various e-commerce advertiser demands on different
levels, such as Gross Merchandise Volume optimization, style comparison etc.
Additionally, a replay based simulation system is designed to predict the
performance of different take-rate. CIA unifies the benefits of three parties
in the marketing ecosystem without changing the Generalized Second Price
mechanism. Our extensive offline simulations and large-scale online experiments
on \emph{Taobao Search Advertising (TSA)} platform verify the high
effectiveness of the CIA framework. Moreover, CIA has been deployed online as a
major bidding tool in TSA.",sponsored advertising
http://arxiv.org/abs/1609.01951v3,"The proliferation of public Wi-Fi hotspots has brought new business
potentials for Wi-Fi networks, which carry a significant amount of global
mobile data traffic today. In this paper, we propose a novel Wi-Fi monetization
model for venue owners (VOs) deploying public Wi-Fi hotspots, where the VOs can
generate revenue by providing two different Wi-Fi access schemes for mobile
users (MUs): (i) the premium access, in which MUs directly pay VOs for their
Wi-Fi usage, and (ii) the advertising sponsored access, in which MUs watch
advertisements in exchange of the free usage of Wi-Fi. VOs sell their ad spaces
to advertisers (ADs) via an ad platform, and share the ADs' payments with the
ad platform. We formulate the economic interactions among the ad platform, VOs,
MUs, and ADs as a three-stage Stackelberg game. In Stage I, the ad platform
announces its advertising revenue sharing policy. In Stage II, VOs determine
the Wi-Fi prices (for MUs) and advertising prices (for ADs). In Stage III, MUs
make access choices and ADs purchase advertising spaces. We analyze the
sub-game perfect equilibrium (SPE) of the proposed game systematically, and our
analysis shows the following useful observations. First, the ad platform's
advertising revenue sharing policy in Stage I will affect only the VOs' Wi-Fi
prices but not the VOs' advertising prices in Stage II. Second, both the VOs'
Wi-Fi prices and advertising prices are non-decreasing in the advertising
concentration level and non-increasing in the MU visiting frequency. Numerical
results further show that the VOs are capable of generating large revenues
through mainly providing one type of Wi-Fi access (the premium access or
advertising sponsored access), depending on their advertising concentration
levels and MU visiting frequencies.",sponsored advertising
http://arxiv.org/abs/1006.1019v2,"Sponsored search mechanisms have drawn much attention from both academic
community and industry in recent years since the seminal papers of [13] and
[14]. However, most of the existing literature concentrates on the mechanism
design and analysis within the scope of only one search engine in the market.
In this paper we propose a mathematical framework for modeling the interaction
of publishers, advertisers and end users in a competitive market. We first
consider the monopoly market model and provide optimal solutions for both ex
ante and ex post cases, which represents the long-term and short-term revenues
of search engines respectively. We then analyze the strategic behaviors of end
users and advertisers under duopoly and prove the existence of equilibrium for
both search engines to co-exist from ex-post perspective. To show the more
general ex ante results, we carry out extensive simulations under different
parameter settings. Our analysis and observation in this work can provide
useful insight in regulating the sponsored search market and protecting the
interests of advertisers and end users.",sponsored advertising
http://arxiv.org/abs/1902.04385v1,"During the summer of 2018, Facebook, Google, and Twitter created policies and
implemented transparent archives that include U.S. political advertisements
which ran on their platforms. Through our analysis of over 1.3 million ads with
political content, we show how different types of political advertisers are
disseminating U.S. political messages using Facebook, Google, and Twitter's
advertising platforms. We find that in total, ads with political content
included in these archives have generated between 8.67 billion - 33.8 billion
impressions and that sponsors have spent over $300 million USD on advertising
with U.S. political content.
  We are able to improve our understanding of political advertisers on these
platforms. We have also discovered a significant amount of advertising by quasi
for-profit media companies that appeared to exist for the sole purpose of
creating deceptive online communities focused on spreading political messaging
and not for directly generating profits. Advertising by such groups is a
relatively recent phenomenon, and appears to be thriving on online platforms
due to the lower regulatory requirements compared to traditional advertising
platforms.
  We have found through our attempts to collect and analyze this data that
there are many limitations and weaknesses that enable intentional or accidental
deception and bypassing of the current implementations of these transparency
archives. We provide several suggestions for how these archives could be made
more robust and useful. Overall, these efforts by Facebook, Google, and Twitter
have improved political advertising transparency of honest and, in some cases,
possibly dishonest advertisers on their platforms. We thank the people at these
companies who have built these archives and continue to improve them.",sponsored advertising
http://arxiv.org/abs/1606.07189v1,"As one of the leading platforms for creative content, Tumblr offers
advertisers a unique way of creating brand identity. Advertisers can tell their
story through images, animation, text, music, video, and more, and promote that
content by sponsoring it to appear as an advertisement in the streams of Tumblr
users. In this paper we present a framework that enabled one of the key
targeted advertising components for Tumblr, specifically gender and interest
targeting. We describe the main challenges involved in development of the
framework, which include creating the ground truth for training gender
prediction models, as well as mapping Tumblr content to an interest taxonomy.
For purposes of inferring user interests we propose a novel semi-supervised
neural language model for categorization of Tumblr content (i.e., post tags and
post keywords). The model was trained on a large-scale data set consisting of
6.8 billion user posts, with very limited amount of categorized keywords, and
was shown to have superior performance over the bag-of-words model. We
successfully deployed gender and interest targeting capability in Yahoo
production systems, delivering inference for users that cover more than 90% of
daily activities at Tumblr. Online performance results indicate advantages of
the proposed approach, where we observed 20% lift in user engagement with
sponsored posts as compared to untargeted campaigns.",sponsored advertising
http://arxiv.org/abs/1902.10374v1,"Advertising (ad for short) keyword suggestion is important for sponsored
search to improve online advertising and increase search revenue. There are two
common challenges in this task. First, the keyword bidding problem: hot ad
keywords are very expensive for most of the advertisers because more
advertisers are bidding on more popular keywords, while unpopular keywords are
difficult to discover. As a result, most ads have few chances to be presented
to the users. Second, the inefficient ad impression issue: a large proportion
of search queries, which are unpopular yet relevant to many ad keywords, have
no ads presented on their search result pages. Existing retrieval-based or
matching-based methods either deteriorate the bidding competition or are unable
to suggest novel keywords to cover more queries, which leads to inefficient ad
impressions. To address the above issues, this work investigates to use
generative neural networks for keyword generation in sponsored search. Given a
purchased keyword (a word sequence) as input, our model can generate a set of
keywords that are not only relevant to the input but also satisfy the domain
constraint which enforces that the domain category of a generated keyword is as
expected. Furthermore, a reinforcement learning algorithm is proposed to
adaptively utilize domain-specific information in keyword generation. Offline
evaluation shows that the proposed model can generate keywords that are
diverse, novel, relevant to the source keyword, and accordant with the domain
constraint. Online evaluation shows that generative models can improve coverage
(COV), click-through rate (CTR), and revenue per mille (RPM) substantially in
sponsored search.",sponsored advertising
http://arxiv.org/abs/1312.7056v1,"The technological transformation and automation of digital content delivery
has revolutionized the media industry. Advertising landscape is gradually
shifting its traditional media forms to the emergent of Internet advertising.
In this paper, the types of internet advertising to be discussed on are
contextual and sponsored search ads. These types of advertising have the
central challenge of finding the best match between a given context and a
suitable advertisement, through a principled method. Furthermore, there are
four main players that exist in the Internet advertising ecosystem: users,
advertisers, ad exchange and publishers. Hence, to find ways to counter the
central challenge, the paper addresses two objectives: how to successfully make
the best contextual ads selections to match to a web page content to ensure
that there is a valuable connection between the web page and the contextual
ads. All methods, discussions, conclusion and future recommendations are
presented as per sections. Hence, in order to prove the working mechanism of
matching contextual ads and web pages, web pages together with the ads matching
system are developed as a prototype.",sponsored advertising
http://arxiv.org/abs/0707.1057v2,"A mediator is a well-known construct in game theory, and is an entity that
plays on behalf of some of the agents who choose to use its services, while the
rest of the agents participate in the game directly. We initiate a game
theoretic study of sponsored search auctions, such as those used by Google and
Yahoo!, involving {\em incentive driven} mediators. We refer to such mediators
as {\em for-profit} mediators, so as to distinguish them from mediators
introduced in prior work, who have no monetary incentives, and are driven by
the altruistic goal of implementing certain desired outcomes. We show that in
our model, (i) players/advertisers can improve their payoffs by choosing to use
the services of the mediator, compared to directly participating in the
auction; (ii) the mediator can obtain monetary benefit by managing the
advertising burden of its group of advertisers; and (iii) the payoffs of the
mediator and the advertisers it plays for are compatible with the incentive
constraints from the advertisers who do dot use its services. A simple
intuition behind the above result comes from the observation that the mediator
has more information about and more control over the bid profile than any
individual advertiser, allowing her to reduce the payments made to the
auctioneer, while still maintaining incentive constraints. Further, our results
indicate that there are significant opportunities for diversification in the
internet economy and we should expect it to continue to develop richer
structure, with room for different types of agents to coexist.",sponsored advertising
http://arxiv.org/abs/1803.00259v1,"Bidding optimization is one of the most critical problems in online
advertising. Sponsored search (SS) auction, due to the randomness of user query
behavior and platform nature, usually adopts keyword-level bidding strategies.
In contrast, the display advertising (DA), as a relatively simpler scenario for
auction, has taken advantage of real-time bidding (RTB) to boost the
performance for advertisers. In this paper, we consider the RTB problem in
sponsored search auction, named SS-RTB. SS-RTB has a much more complex dynamic
environment, due to stochastic user query behavior and more complex bidding
policies based on multiple keywords of an ad. Most previous methods for DA
cannot be applied. We propose a reinforcement learning (RL) solution for
handling the complex dynamic environment. Although some RL methods have been
proposed for online advertising, they all fail to address the ""environment
changing"" problem: the state transition probabilities vary between two days.
Motivated by the observation that auction sequences of two days share similar
transition patterns at a proper aggregation level, we formulate a robust MDP
model at hour-aggregation level of the auction data and propose a
control-by-model framework for SS-RTB. Rather than generating bid prices
directly, we decide a bidding model for impressions of each hour and perform
real-time bidding accordingly. We also extend the method to handle the
multi-agent problem. We deployed the SS-RTB system in the e-commerce search
auction platform of Alibaba. Empirical experiments of offline evaluation and
online A/B test demonstrate the effectiveness of our method.",sponsored advertising
http://arxiv.org/abs/1808.04067v1,"With a sponsored content scheme in a wireless network, a sponsored content
service provider can pay to a network operator on behalf of the mobile
users/subscribers to lower down the network subscription fees at the reasonable
cost in terms of receiving some amount of advertisements. As such, content
providers, network operators and mobile users are all actively motivated to
participate in the sponsored content ecosystem. Meanwhile, in 5G cellular
networks, caching technique is employed to improve content service quality,
which stores potentially popular contents on edge networks nodes to serve
mobile users. In this work, we propose the joint sponsored and edge caching
content service market model. We investigate an interplay between the sponsored
content service provider and the edge caching content service provider under
the non-cooperative game framework. Furthermore, a three-stage Stackelberg game
is formulated to model the interactions among the network operator, content
service provider, and mobile users. Sub-game perfect equilibrium in each stage
is analyzed by backward induction. The existence of Stackelberg equilibrium is
validated by employing the bilevel optimization programming. Based on the game
properties, we propose a sub-gradient based iterative algorithm, which ensures
to converge to the Stackelberg equilibrium.",sponsored advertising
http://arxiv.org/abs/1210.4847v1,"We consider the budget optimization problem faced by an advertiser
participating in repeated sponsored search auctions, seeking to maximize the
number of clicks attained under that budget. We cast the budget optimization
problem as a Markov Decision Process (MDP) with censored observations, and
propose a learning algorithm based on the wellknown Kaplan-Meier or
product-limit estimator. We validate the performance of this algorithm by
comparing it to several others on a large set of search auction data from
Microsoft adCenter, demonstrating fast convergence to optimal performance.",sponsored advertising
http://arxiv.org/abs/1806.08211v1,"In online internet advertising, machine learning models are widely used to
compute the likelihood of a user engaging with product related advertisements.
However, the performance of traditional machine learning models is often
impacted due to variations in user and advertiser behavior. For example, search
engine traffic for florists usually tends to peak around Valentine's day,
Mother's day, etc. To overcome, this challenge, in this manuscript we propose
three models which are able to incorporate the effects arising due to
variations in product demand. The proposed models are a combination of product
demand features, specialized data sampling methodologies and ensemble
techniques. We demonstrate the performance of our proposed models on datasets
obtained from a real-world setting. Our results show that the proposed models
more accurately predict the outcome of users interactions with product related
advertisements while simultaneously being robust to fluctuations in user and
advertiser behaviors.",sponsored advertising
http://arxiv.org/abs/1112.6361v1,"In a sponsored search auction the advertisement slots on a search result page
are generally ordered by click-through rate. Bidders have a valuation, which is
usually assumed to be linear in the click-through rate, a budget constraint,
and receive at most one slot per search result page (round). We study
multi-round sponsored search auctions, where the different rounds are linked
through the budget constraints of the bidders and the valuation of a bidder for
all rounds is the sum of the valuations for the individual rounds. All
mechanisms published so far either study one-round sponsored search auctions or
the setting where every round has only one slot and all slots have the same
click-through rate, which is identical to a multi-item auction.
  This paper contains the following three results: (1) We give the first
mechanism for the multi-round sponsored search problem where different slots
have different click-through rates. Our mechanism is incentive compatible in
expectation, individually rational in expectation, Pareto optimal in
expectation, and also ex-post Pareto optimal for each realized outcome. (2)
Additionally we study the combinatorial setting, where each bidder is only
interested in a subset of the rounds. We give a deterministic, incentive
compatible, individually rational, and Pareto optimal mechanism for the setting
where all slots have the same click-through rate. (3) We present an
impossibility result for auctions where bidders have diminishing marginal
valuations. Specifically, we show that even for the multi-unit (one slot per
round) setting there is no incentive compatible, individually rational, and
Pareto optimal mechanism for private diminishing marginal valuations and public
budgets.",sponsored advertising
http://arxiv.org/abs/1405.2484v1,"Sponsored search auctions constitute one of the most successful applications
of microeconomic mechanisms. In mechanism design, auctions are usually designed
to incentivize advertisers to bid their truthful valuations and to assure both
the advertisers and the auctioneer a non-negative utility. Nonetheless, in
sponsored search auctions, the click-through-rates (CTRs) of the advertisers
are often unknown to the auctioneer and thus standard truthful mechanisms
cannot be directly applied and must be paired with an effective learning
algorithm for the estimation of the CTRs. This introduces the critical problem
of designing a learning mechanism able to estimate the CTRs at the same time as
implementing a truthful mechanism with a revenue loss as small as possible
compared to an optimal mechanism designed with the true CTRs. Previous work
showed that, when dominant-strategy truthfulness is adopted, in single-slot
auctions the problem can be solved using suitable exploration-exploitation
mechanisms able to achieve a per-step regret (over the auctioneer's revenue) of
order $O(T^{-1/3})$ (where T is the number of times the auction is repeated).
It is also known that, when truthfulness in expectation is adopted, a per-step
regret (over the social welfare) of order $O(T^{-1/2})$ can be obtained. In
this paper we extend the results known in the literature to the case of
multi-slot auctions. In this case, a model of the user is needed to
characterize how the advertisers' valuations change over the slots. We adopt
the cascade model that is the most famous model in the literature for sponsored
search auctions. We prove a number of novel upper bounds and lower bounds both
on the auctioneer's revenue loss and social welfare w.r.t. to the VCG auction
and we report numerical simulations investigating the accuracy of the bounds in
predicting the dependency of the regret on the auction parameters.",sponsored advertising
http://arxiv.org/abs/0805.0766v1,"Sponsored search involves running an auction among advertisers who bid in
order to have their ad shown next to search results for specific keywords.
Currently, the most popular auction for sponsored search is the ""Generalized
Second Price"" (GSP) auction in which advertisers are assigned to slots in the
decreasing order of their ""score,"" which is defined as the product of their bid
and click-through rate. In the past few years, there has been significant
research on the game-theoretic issues that arise in an advertiser's interaction
with the mechanism as well as possible redesigns of the mechanism, but this
ranking order has remained standard.
  From a search engine's perspective, the fundamental question is: what is the
best assignment of advertisers to slots? Here ""best"" could mean ""maximizing
user satisfaction,"" ""most efficient,"" ""revenue-maximizing,"" ""simplest to
interact with,"" or a combination of these. To answer this question we need to
understand the behavior of a search engine user when she sees the displayed
ads, since that defines the commodity the advertisers are bidding on, and its
value. Most prior work has assumed that the probability of a user clicking on
an ad is independent of the other ads shown on the page.
  We propose a simple Markovian user model that does not make this assumption.
We then present an algorithm to determine the most efficient assignment under
this model, which turns out to be different than that of GSP. A truthful
auction then follows from an application of the Vickrey-Clarke-Groves (VCG)
mechanism. Further, we show that our assignment has many of the desirable
properties of GSP that makes bidding intuitive. At the technical core of our
result are a number of insights about the structure of the optimal assignment.",sponsored advertising
http://arxiv.org/abs/1001.1414v2,"In pay-per click sponsored search auctions which are currently extensively
used by search engines, the auction for a keyword involves a certain number of
advertisers (say k) competing for available slots (say m) to display their ads.
This auction is typically conducted for a number of rounds (say T). There are
click probabilities mu_ij associated with each agent-slot pairs. The goal of
the search engine is to maximize social welfare of the advertisers, that is,
the sum of values of the advertisers. The search engine does not know the true
values advertisers have for a click to their respective ads and also does not
know the click probabilities mu_ij s. A key problem for the search engine
therefore is to learn these click probabilities during the T rounds of the
auction and also to ensure that the auction mechanism is truthful. Mechanisms
for addressing such learning and incentives issues have recently been
introduced and are aptly referred to as multi-armed-bandit (MAB) mechanisms.
When m = 1, characterizations for truthful MAB mechanisms are available in the
literature and it has been shown that the regret for such mechanisms will be
O(T^{2/3}). In this paper, we seek to derive a characterization in the
realistic but non-trivial general case when m > 1 and obtain several
interesting results.",sponsored advertising
http://arxiv.org/abs/1403.5768v1,"We consider the problem of designing optimal online-ad investment strategies
for a single advertiser, who invests at multiple sponsored search sites
simultaneously, with the objective of maximizing his average revenue subject to
the advertising budget constraint. A greedy online investment scheme is
developed to achieve an average revenue that can be pushed to within
$O(\epsilon)$ of the optimal, for any $\epsilon>0$, with a tradeoff that the
temporal budget violation is $O(1/\epsilon)$. Different from many existing
algorithms, our scheme allows the advertiser to \emph{asynchronously} update
his investments on each search engine site, hence applies to systems where the
timescales of action update intervals are heterogeneous for different sites. We
also quantify the impact of inaccurate estimation of the system dynamics and
show that the algorithm is robust against imperfect system knowledge.",sponsored advertising
http://arxiv.org/abs/1712.10110v5,"On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.",sponsored advertising
http://arxiv.org/abs/1807.11790v1,"Sponsored search in E-commerce platforms such as Amazon, Taobao and Tmall
provides sellers an effective way to reach potential buyers with most relevant
purpose. In this paper, we study the auction mechanism optimization problem in
sponsored search on Alibaba's mobile E-commerce platform. Besides generating
revenue, we are supposed to maintain an efficient marketplace with plenty of
quality users, guarantee a reasonable return on investment (ROI) for
advertisers, and meanwhile, facilitate a pleasant shopping experience for the
users. These requirements essentially pose a constrained optimization problem.
Directly optimizing over auction parameters yields a discontinuous, non-convex
problem that denies effective solutions. One of our major contribution is a
practical convex optimization formulation of the original problem. We devise a
novel re-parametrization of auction mechanism with discrete sets of
representative instances. To construct the optimization problem, we build an
auction simulation system which estimates the resulted business indicators of
the selected parameters by replaying the auctions recorded from real online
requests. We summarized the experiments on real search traffics to analyze the
effects of fidelity of auction simulation, the efficacy under various
constraint targets and the influence of regularization. The experiment results
show that with proper entropy regularization, we are able to maximize revenue
while constraining other business indicators within given ranges.",sponsored advertising
http://arxiv.org/abs/1607.01869v1,"Sponsored search represents a major source of revenue for web search engines.
This popular advertising model brings a unique possibility for advertisers to
target users' immediate intent communicated through a search query, usually by
displaying their ads alongside organic search results for queries deemed
relevant to their products or services. However, due to a large number of
unique queries it is challenging for advertisers to identify all such relevant
queries. For this reason search engines often provide a service of advanced
matching, which automatically finds additional relevant queries for advertisers
to bid on. We present a novel advanced matching approach based on the idea of
semantic embeddings of queries and ads. The embeddings were learned using a
large data set of user search sessions, consisting of search queries, clicked
ads and search links, while utilizing contextual information such as dwell time
and skipped ads. To address the large-scale nature of our problem, both in
terms of data and vocabulary size, we propose a novel distributed algorithm for
training of the embeddings. Finally, we present an approach for overcoming a
cold-start problem associated with new ads and queries. We report results of
editorial evaluation and online tests on actual search traffic. The results
show that our approach significantly outperforms baselines in terms of
relevance, coverage, and incremental revenue. Lastly, we open-source learned
query embeddings to be used by researchers in computational advertising and
related fields.",sponsored advertising
http://arxiv.org/abs/1409.0697v4,"Advertisement (abbreviated ad) options are a recent development in online
advertising. Simply, an ad option is a first look contract in which a publisher
or search engine grants an advertiser a right but not obligation to enter into
transactions to purchase impressions or clicks from a specific ad slot at a
pre-specified price on a specific delivery date. Such a structure provides
advertisers with more flexibility of their guaranteed deliveries. The valuation
of ad options is an important topic and previous studies on ad options pricing
have been mostly restricted to the situations where the underlying prices
follow a geometric Brownian motion (GBM). This assumption is reasonable for
sponsored search; however, some studies have also indicated that it is not
valid for display advertising. In this paper, we address this issue by
employing a stochastic volatility (SV) model and discuss a lattice framework to
approximate the proposed SV model in option pricing. Our developments are
validated by experiments with real advertising data: (i) we find that the SV
model has a better fitness over the GBM model; (ii) we validate the proposed
lattice model via two sequential Monte Carlo simulation methods; (iii) we
demonstrate that advertisers are able to flexibly manage their guaranteed
deliveries by using the proposed options, and publishers can have an increased
revenue when some of their inventories are sold via ad options.",sponsored advertising
http://arxiv.org/abs/0711.1569v1,"The auction theory literature has so far focused mostly on the design of
mechanisms that takes the revenue or the efficiency as a yardstick. However,
scenarios where the {\it capacity}, which we define as \textit{``the number of
bidders the auctioneer wants to have a positive probability of getting the
item''}, is a fundamental concern are ubiquitous in the information economy.
For instance, in sponsored search auctions (SSA's) or in online ad-exchanges,
the true value of an ad-slot for an advertiser is inherently derived from the
conversion-rate, which in turn depends on whether the advertiser actually
obtained the ad-slot or not; thus, unless the capacity of the underlying
auction is large, key parameters, such as true valuations and
advertiser-specific conversion rates, will remain unknown or uncertain leading
to inherent inefficiencies in the system. In general, the same holds true for
all information goods/digital goods. We initiate a study of mechanisms, which
take capacity as a yardstick, in addition to revenue/efficiency. We show that
in the case of a single indivisible item one simple way to incorporate capacity
constraints is via designing mechanisms to sell probability distributions, and
that under certain conditions, such optimal probability distributions could be
identified using a Linear programming approach. We define a quantity called
{\it price of capacity} to capture the tradeoff between capacity and
revenue/efficiency. We also study the case of sponsored search auctions.
Finally, we discuss how general such an approach via probability spikes can be
made, and potential directions for future investigations.",sponsored advertising
http://arxiv.org/abs/0807.1297v1,"In sponsored search, a number of advertising slots is available on a search
results page, and have to be allocated among a set of advertisers competing to
display an ad on the page. This gives rise to a bipartite matching market that
is typically cleared by the way of an automated auction. Several auction
mechanisms have been proposed, with variants of the Generalized Second Price
(GSP) being widely used in practice.
  A rich body of work on bipartite matching markets builds upon the stable
marriage model of Gale and Shapley and the assignment model of Shapley and
Shubik. We apply insights from this line of research into the structure of
stable outcomes and their incentive properties to advertising auctions.
  We model advertising auctions in terms of an assignment model with linear
utilities, extended with bidder and item specific maximum and minimum prices.
Auction mechanisms like the commonly used GSP or the well-known
Vickrey-Clarke-Groves (VCG) are interpreted as simply computing a
\emph{bidder-optimal stable matching} in this model, for a suitably defined set
of bidder preferences. In our model, the existence of a stable matching is
guaranteed, and under a non-degeneracy assumption a bidder-optimal stable
matching exists as well. We give an algorithm to find such matching in
polynomial time, and use it to design truthful mechanism that generalizes GSP,
is truthful for profit-maximizing bidders, implements features like
bidder-specific minimum prices and position-specific bids, and works for rich
mixtures of bidders and preferences.",sponsored advertising
http://arxiv.org/abs/0901.3754v1,"Ad auctions in sponsored search support ``broad match'' that allows an
advertiser to target a large number of queries while bidding only on a limited
number. While giving more expressiveness to advertisers, this feature makes it
challenging to optimize bids to maximize their returns: choosing to bid on a
query as a broad match because it provides high profit results in one bidding
for related queries which may yield low or even negative profits.
  We abstract and study the complexity of the {\em bid optimization problem}
which is to determine an advertiser's bids on a subset of keywords (possibly
using broad match) so that her profit is maximized. In the query language model
when the advertiser is allowed to bid on all queries as broad match, we present
an linear programming (LP)-based polynomial-time algorithm that gets the
optimal profit. In the model in which an advertiser can only bid on keywords,
ie., a subset of keywords as an exact or broad match, we show that this problem
is not approximable within any reasonable approximation factor unless P=NP. To
deal with this hardness result, we present a constant-factor approximation when
the optimal profit significantly exceeds the cost. This algorithm is based on
rounding a natural LP formulation of the problem. Finally, we study a budgeted
variant of the problem, and show that in the query language model, one can find
two budget constrained ad campaigns in polynomial time that implement the
optimal bidding strategy. Our results are the first to address bid optimization
under the broad match feature which is common in ad auctions.",sponsored advertising
http://arxiv.org/abs/1505.02002v1,"From its origins in the mid 90s, the application of the concept of virality
to commercial communication has represented an opportunity for brands to cross
the traditional barriers of the audience concerning advertising and turn it
into active communicator of brand messages. Viral marketing is based, since
then, on two basic principles: offer free and engaging content that mask its
commercial purpose to the individual and using a peer-to-peer dissemination
system. The transformation of the passive spectator into an active user who
broadcasts advertising messages promoted by sponsors, responds to needs and
motivations of individuals and content features which have been described by
previous research in this field, mainly through quantitative methods based on
user perceptions. This paper focusses on those elements detected in its
previous research as promoters of the sharing action in the 25 most shared
viral video ads between 2006 and 2013 using content analysis. The results
obtained show the most common features in these videos and the prominent
presence of surprise and joy as dominant emotions in the most successful viral
videos.",sponsored advertising
http://arxiv.org/abs/1601.02377v1,"User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.",sponsored advertising
http://arxiv.org/abs/1903.02703v1,"We consider a market where a seller sells multiple units of a commodity in a
social network. Each node/buyer in the social network can only directly
communicate with her neighbours, i.e. the seller can only sell the commodity to
her neighbours if she could not find a way to inform other buyers. In this
paper, we design a novel promotion mechanism that incentivizes all buyers, who
are aware of the sale, to invite all their neighbours to join the sale, even
though there is no guarantee that their efforts will be paid. While traditional
sale promotions such as sponsored search auctions cannot guarantee a positive
return for the advertiser (the seller), our mechanism guarantees that the
seller's revenue is better than not using the advertising. More importantly,
the seller does not need to pay if the advertising is not beneficial to her.",sponsored advertising
http://arxiv.org/abs/1906.08732v2,"Fairness in advertising is a topic of particular concern motivated by
theoretical and empirical observations in both the computer science and
economics literature. We examine the problem of fairness in advertising for
general purpose platforms that service advertisers from many different
categories. First, we propose inter-category and intra-category fairness
desiderata that take inspiration from individual fairness and envy-freeness.
Second, we investigate the ""platform utility"" (a proxy for the quality of the
allocation) achievable by mechanisms satisfying these desiderata. More
specifically, we compare the utility of fair mechanisms against the unfair
optimal, and we show by construction that our fairness desiderata are
compatible with utility. That is, we construct a family of fair mechanisms with
high utility that perform close to optimally within a class of fair mechanisms.
Our mechanisms also enjoy nice implementation properties including
metric-obliviousness, which allows the platform to produce fair allocations
without needing to know the specifics of the fairness requirements.",sponsored advertising
http://arxiv.org/abs/1908.09185v1,"We study a natural model of coordinated social ad campaigns over a social
network, based on models of Datta et al. and Aslay et al. Multiple advertisers
are willing to pay the host - up to a known budget - per user exposure, whether
the exposure is sponsored or orgain (i.e. shared by a friend). Campaigns are
seeded with sponsored ads to some users, but no user must be exposed to too
many sponsored ads. Thus, while ad campaigns proceed independently over the
network, they need to be carefully coordinated with respect to their seed sets.
  We study the objective of maximizing host's total ad revenue. Our main result
is to show that under a broad class of influence models, the problem can be
reduced to maximizing a submodular function subject to two matroid constraints;
it can therefore be approximated within a factor essentially 1/2 in polynomial
time. When there is no bound on the individual seed set sizes of advertisers,
the constraints correspond only to a single matroid, and the guarantee can be
improved to 1-1/e; in that case, a factor 1/2 is achieved by a practical greedy
algorithm. The 1-1/e approximation algorithm for matroid-constrained problem is
far from practical; however, we show that specifically under the Independent
Cascade model, LP rounding and Reverse Reachability techniques can be combined
to obtain a 1-1/e approximation algorithm.
  Our theoretical results are complemented by experiments evaluating the extent
to which the coordination of multiple ad campaigns inhibits the revenue
obtained from each individual campaign, as a function of the similarity of the
influence networks and strength of ties in the networks. Our experiments
suggest that as networks for different advertisers become less similar, the
harmful effect of competition decreases. With respect to tie strengths, we show
that the most harm is done in an intermediate range.",sponsored advertising
http://arxiv.org/abs/1810.08885v2,"Fraud has severely detrimental impacts on the business of social networks and
other online applications. A user can become a fake celebrity by purchasing
""zombie followers"" on Twitter. A merchant can boost his reputation through fake
reviews on Amazon. This phenomenon also conspicuously exists on Facebook, Yelp
and TripAdvisor, etc. In all the cases, fraudsters try to manipulate the
platform's ranking mechanism by faking interactions between the fake accounts
they control and the target customers.",fake followers
http://arxiv.org/abs/1806.07516v2,"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.",fake followers
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",fake followers
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",fake followers
http://arxiv.org/abs/1606.02409v2,"In the standard formulation of mechanism design, a key assumption is that the
designer has reliable information and technology to determine a prior
distribution on types of the agents. In the meanwhile, as pointed out by the
Wilson's Principle, a mechanism should reply as little as possible on the
accuracy of prior type distribution. In this paper, we put forward a model to
formalize and quantify this statement.
  In our model, each agent has a type distribution. In addition, the agent can
commit to a fake distribution and bids consistently and credibly with respect
to the fake distribution (i.e., plays Bayes equilibrium under the fake
distributions). We study the equilibria of the induced distribution-committing
games in several well-known mechanisms. Our results can be summarized as
follows: (1) the game induced by Myerson's auction under our model is
strategically equivalent to the first price auction under the standard model.
As a consequence, they are revenue-equivalent as well. (2) the second-price
auction yields weakly better revenue than several reserve-based and
virtual-value-based auctions, under our fake distribution model. These results
echo the recent literature on prior-independent mechanism design.",fake followers
http://arxiv.org/abs/1712.05999v1,"This article presents a preliminary approach towards characterizing political
fake news on Twitter through the analysis of their meta-data. In particular, we
focus on more than 1.5M tweets collected on the day of the election of Donald
Trump as 45th president of the United States of America. We use the meta-data
embedded within those tweets in order to look for differences between tweets
containing fake news and tweets not containing them. Specifically, we perform
our analysis only on tweets that went viral, by studying proxies for users'
exposure to the tweets, by characterizing accounts spreading fake news, and by
looking at their polarization. We found significant differences on the
distribution of followers, the number of URLs on tweets, and the verification
of the users.",fake followers
http://arxiv.org/abs/1605.07984v1,"This paper presents a method to validate the true patrons of a brand, group,
artist or any other entity on the social networking site Twitter. We analyze
the trend of total number of tweets, average retweets and total number of
followers for various nodes for different social and political backgrounds. We
argue that average retweets to follower ratio reveals the overall value of the
individual accounts and helps estimate the true to fake account ratio.",fake followers
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",fake followers
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake followers
http://arxiv.org/abs/1708.06233v1,"We model the spread of news as a social learning game on a network. Agents
can either endorse or oppose a claim made in a piece of news, which itself may
be either true or false. Agents base their decision on a private signal and
their neighbors' past actions. Given these inputs, agents follow strategies
derived via multi-agent deep reinforcement learning and receive utility from
acting in accordance with the veracity of claims. Our framework yields
strategies with agent utility close to a theoretical, Bayes optimal benchmark,
while remaining flexible to model re-specification. Optimized strategies allow
agents to correctly identify most false claims, when all agents receive
unbiased private signals. However, an adversary's attempt to spread fake news
by targeting a subset of agents with a biased private signal can be successful.
Even more so when the adversary has information about agents' network position
or private signal. When agents are aware of the presence of an adversary they
re-optimize their strategies in the training stage and the adversary's attack
is less effective. Hence, exposing agents to the possibility of fake news can
be an effective way to curtail the spread of fake news in social networks. Our
results also highlight that information about the users' private beliefs and
their social network structure can be extremely valuable to adversaries and
should be well protected.",fake followers
http://arxiv.org/abs/1802.00156v1,"When information flow fails, when Democrats and Republicans do not talk to
each other, when Israelis and Palestinians do not talk to each other, and when
North Koreans and South Koreans do not talk to each other, mis-perceptions,
biases and fake news arise. In this paper we present an in-depth study of
political polarization and social division using Twitter data and Monte Carlo
simulations. First, we study at the aggregate level people's inclination to
retweet within their own ideological circle. Introducing the concept of cocoon
ratio, we show that Donald Trump's followers are 2.56 more likely to retweet a
fellow Trump follower than to retweet a Hillary Clinton follower. Second, going
down to the individual level, we show that the tendency of retweeting
exclusively within one's ideological circle is stronger for women than for men
and that such tendency weakens as one's social capital grows. Third, we use a
one-dimensional Ising model to simulate how a society with high cocoon ratios
could end up becoming completely divided. We conclude with a discussion of our
findings with respect to fake news.",fake followers
http://arxiv.org/abs/1210.4517v1,"The proliferation of location-based social networks (LBSNs) has provided the
community with an abundant source of information that can be exploited and used
in many different ways. LBSNs offer a number of conveniences to its
participants, such as - but not limited to - a list of places in the vicinity
of a user, recommendations for an area never explored before provided by other
peers, tracking of friends, monetary rewards in the form of special deals from
the venues visited as well as a cheap way of advertisement for the latter.
However, service convenience and security have followed disjoint paths in LBSNs
and users can misuse the offered features. The major threat for the service
providers is that of fake check-ins. Users can easily manipulate the
localization module of the underlying application and declare their presence in
a counterfeit location. The incentives for these behaviors can be both earning
monetary as well as virtual rewards. Therefore, while fake check-ins driven
from the former motive can cause monetary losses, those aiming in virtual
rewards are also harmful. In particular, they can significantly degrade the
services offered from the LBSN providers (such as recommendations) or third
parties using these data (e.g., urban planners). In this paper, we propose and
analyze a honeypot venue-based solution, enhanced with a challenge-response
scheme, that flags users who are generating fake spatial information. We
believe that our work will stimulate further research on this important topic
and will provide new directions with regards to possible solutions.",fake followers
http://arxiv.org/abs/1711.09918v1,"Online social networking sites are experimenting with the following
crowd-powered procedure to reduce the spread of fake news and misinformation:
whenever a user is exposed to a story through her feed, she can flag the story
as misinformation and, if the story receives enough flags, it is sent to a
trusted third party for fact checking. If this party identifies the story as
misinformation, it is marked as disputed. However, given the uncertain number
of exposures, the high cost of fact checking, and the trade-off between flags
and exposures, the above mentioned procedure requires careful reasoning and
smart algorithms which, to the best of our knowledge, do not exist to date.
  In this paper, we first introduce a flexible representation of the above
procedure using the framework of marked temporal point processes. Then, we
develop a scalable online algorithm, Curb, to select which stories to send for
fact checking and when to do so to efficiently reduce the spread of
misinformation with provable guarantees. In doing so, we need to solve a novel
stochastic optimal control problem for stochastic differential equations with
jumps, which is of independent interest. Experiments on two real-world datasets
gathered from Twitter and Weibo show that our algorithm may be able to
effectively reduce the spread of fake news and misinformation.",fake followers
http://arxiv.org/abs/1809.04364v1,"This paper proposes a method for utilizing thermal features of the hand for
the purpose of presentation attack detection (PAD) that can be employed in a
hand biometrics system's pipeline. By envisaging two different operational
modes of our system, and by employing a DCNN-based classifiers fine-tuned with
a dataset of real and fake hand representations captured in both visible and
ther- mal spectrum, we were able to bring two important deliverables. First, a
PAD method operating in an open-set mode, capable of correctly discerning 100%
of fake thermal samples, achieving Attack Presentation Classification Error
Rate (APCER) and Bona-Fide Presentation Classification Error Rate (BPCER) equal
to 0%, which can be easily implemented into any existing system as a separate
component. Second, a hand biometrics system operating in a closed-set mode,
that has PAD built right into the recognition pipeline, and operating
simultaneously with the user-wise classification, achieving rank-1 recognition
accuracy of up to 99.75%. We also show that thermal images of the human hand,
in addition to liveness features they carry, can also improve classification
accuracy of a biometric system, when coupled with visible light images. To
follow the reproducibility guidelines and to stimulate further research in this
area, we share the trained model weights, source codes, and a newly created
dataset of fake hand representations with interested researchers.",fake followers
http://arxiv.org/abs/1811.06002v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip and GEM detectors due to appearance in
them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. We introduce a renewed method
that is a significant improvement of our previous two-stage approach based on
hit preprocessing using directed K-d tree search followed a deep neural
classifier. We combine these two stages in one by applying recurrent neural
network that simultaneously determines whether a set of points belongs to a
true track or not and predicts where to look for the next point of track on the
next coordinate plane of the detector. We show that proposed deep network is
more accurate, faster and does not require any special preprocessing stage.
Preliminary results of our approach for simulated events of the BM@N GEM
detector are presented.",fake followers
http://arxiv.org/abs/1812.03859v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip in GEM detectors due to the appearance
in them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. On the basis of our previous
two-stage approach based on hits preprocessing using directed K-d tree search
followed by a deep neural classifier we introduce here two new tracking
algorithms. Both algorithms combine those two stages in one while using
different types of deep neural nets. We show that both proposed deep networks
do not require any special preprocessing stage, are more accurate, faster and
can be easier parallelized. Preliminary results of our new approaches for
simulated events are presented.",fake followers
http://arxiv.org/abs/1907.03048v1,"Download fraud is a prevalent threat in mobile App markets, where fraudsters
manipulate the number of downloads of Apps via various cheating approaches.
Purchased fake downloads can mislead recommendation and search algorithms and
further lead to bad user experience in App markets. In this paper, we
investigate download fraud problem based on a company's App Market, which is
one of the most popular Android App markets. We release a honeypot App on the
App Market and purchase fake downloads from fraudster agents to track fraud
activities in the wild. Based on our interaction with the fraudsters, we
categorize download fraud activities into three types according to their
intentions: boosting front end downloads, optimizing App search ranking, and
enhancing user acquisition&retention rate. For the download fraud aimed at
optimizing App search ranking, we select, evaluate, and validate several
features in identifying fake downloads based on billions of download data. To
get a comprehensive understanding of download fraud, we further gather stances
of App marketers, fraudster agencies, and market operators on download fraud.
The followed analysis and suggestions shed light on the ways to mitigate
download fraud in App markets and other social platforms. To the best of our
knowledge, this is the first work that investigates the download fraud problem
in mobile App markets.",fake followers
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake followers
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake followers
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake followers
http://arxiv.org/abs/1610.07525v4,"In the past decade, network structures have penetrated nearly every aspect of
our lives. The detection of anomalous vertices in these networks has become
increasingly important, such as in exposing computer network intruders or
identifying fake online reviews. In this study, we present a novel unsupervised
two-layered meta-classifier that can detect irregular vertices in complex
networks solely by using features extracted from the network topology.
Following the reasoning that a vertex with many improbable links has a higher
likelihood of being anomalous,we employed our method on 10 networks of various
scales, from a network of several dozen students to online social networks with
millions of users. In every scenario, we were able to identify anomalous
vertices with lower false positive rates and higher AUCs compared to other
prevalent methods. Moreover, we demonstrated that the presented algorithm is
efficient both in revealing fake users and in disclosing the most influential
people in social networks.",fake followers
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake followers
http://arxiv.org/abs/0908.0809v1,"We use generating functional analysis to study minority-game type market
models with generalized strategy valuation updates that control the psychology
of agents' actions. The agents' choice between trend following and contrarian
trading, and their vigor in each, depends on the overall state of the market.
Even in `fake history' models, the theory now involves an effective overall bid
process (coupled to the effective agent process) which can exhibit profound
remanence effects and new phase transitions. For some models the bid process
can be solved directly, others require Maxwell-construction type
approximations.",fake followers
http://arxiv.org/abs/1012.3802v1,"This chapter presents a framework for detecting fake regions by using various
methods including watermarking technique and blind approaches. In particular,
we describe current categories on blind approaches which can be divided into
five: pixel-based techniques, format-based techniques, camera-based techniques,
physically-based techniques and geometric-based techniques. Then we take a
second look on the geometric-based techniques and further categorize them in
detail. In the following section, the state-of-the-art methods involved in the
geometric technique are elaborated.",fake followers
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake followers
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake followers
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake followers
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake followers
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake followers
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake followers
http://arxiv.org/abs/1809.00620v2,"Online advertisements that masquerade as non-advertising content pose
numerous risks to users. Such hidden advertisements appear on social media
platforms when content creators or ""influencers"" endorse products and brands in
their content. While the Federal Trade Commission (FTC) requires content
creators to disclose their endorsements in order to prevent deception and harm
to users, we do not know whether and how content creators comply with the FTC's
guidelines. In this paper, we studied disclosures within affiliate marketing,
an endorsement-based advertising strategy used by social media content
creators. We examined whether content creators follow the FTC's disclosure
guidelines, how they word the disclosures, and whether these disclosures help
users identify affiliate marketing content as advertisements. To do so, we
first measured the prevalence of and identified the types of disclosures in
over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a
user study with 1,791 participants to test the efficacy of these disclosures.
Our findings reveal that only about 10% of affiliate marketing content on both
platforms contains any disclosures at all. Further, users fail to understand
shorter, non-explanatory disclosures. Based on our findings, we make various
design and policy suggestions to help improve advertising disclosure practices
on social media platforms.",influencer disclosure
http://arxiv.org/abs/1709.04049v5,"Crowdfunding has emerged as a prominent way for entrepreneurs to secure
funding without sophisticated intermediation. In crowdfunding, an entrepreneur
often has to decide how to disclose the campaign status in order to collect as
many contributions as possible. Such decisions are difficult to make primarily
due to incomplete information. We propose information design as a tool to help
the entrepreneur to improve revenue by influencing backers' beliefs. We
introduce a heuristic algorithm to dynamically compute information-disclosure
policies for the entrepreneur, followed by an empirical evaluation to
demonstrate its competitiveness over the widely-adopted immediate-disclosure
policy. Our results demonstrate that the immediate-disclosure policy is not
optimal when backers follow thresholding policies despite its ease of
implementation. With appropriate heuristics, an entrepreneur can benefit from
dynamic information disclosure. Our work sheds light on information design in a
dynamic setting where agents make decisions using thresholding policies.",influencer disclosure
http://arxiv.org/abs/1811.08460v2,"Nowadays, all major web browsers have a private browsing mode. However, the
mode's benefits and limitations are not particularly understood. Through the
use of survey studies, prior work has found that most users are either unaware
of private browsing or do not use it. Further, those who do use private
browsing generally have misconceptions about what protection it provides.
However, prior work has not investigated \emph{why} users misunderstand the
benefits and limitations of private browsing. In this work, we do so by
designing and conducting a three-part study: (1) an analytical approach
combining cognitive walkthrough and heuristic evaluation to inspect the user
interface of private mode in different browsers; (2) a qualitative,
interview-based study to explore users' mental models of private browsing and
its security goals; (3) a participatory design study to investigate why
existing browser disclosures, the in-browser explanations of private browsing
mode, do not communicate the security goals of private browsing to users.
Participants critiqued the browser disclosures of three web browsers: Brave,
Firefox, and Google Chrome, and then designed new ones. We find that the user
interface of private mode in different web browsers violates several
well-established design guidelines and heuristics. Further, most participants
had incorrect mental models of private browsing, influencing their
understanding and usage of private mode. Additionally, we find that existing
browser disclosures are not only vague, but also misleading. None of the three
studied browser disclosures communicates or explains the primary security goal
of private browsing. Drawing from the results of our user study, we extract a
set of design recommendations that we encourage browser designers to validate,
in order to design more effective and informative browser disclosures related
to private mode.",influencer disclosure
http://arxiv.org/abs/1812.01790v1,"k-Anonymity by microaggregation is one of the most commonly used
anonymization techniques. This success is owe to the achievement of a worth of
interest tradeoff between information loss and identity disclosure risk.
However, this method may have some drawbacks. On the disclosure limitation
side, there is a lack of protection against attribute disclosure. On the data
utility side, dealing with a real datasets is a challenging task to achieve.
Indeed, the latter are characterized by their large number of attributes and
the presence of noisy data, such that outliers or, even, data with missing
values. Generating an anonymous individual data useful for data mining tasks,
while decreasing the influence of noisy data is a compelling task to achieve.
In this paper, we introduce a new microaggregation method, called HM-PFSOM,
based on fuzzy possibilistic clustering. Our proposed method operates through
an hybrid manner. This means that the anonymization process is applied per
block of similar data. Thus, we can help to decrease the information loss
during the anonymization process. The HMPFSOM approach proposes to study the
distribution of confidential attributes within each sub-dataset. Then,
according to the latter distribution, the privacy parameter k is determined, in
such a way to preserve the diversity of confidential attributes within the
anonymized microdata. This allows to decrease the disclosure risk of
confidential information.",influencer disclosure
http://arxiv.org/abs/1602.02710v1,"We study the strategic aspects of social influence in a society of agents
linked by a trust network, introducing a new class of games called games of
influence. A game of influence is an infinite repeated game with incomplete
information in which, at each stage of interaction, an agent can make her
opinions visible (public) or invisible (private) in order to influence other
agents' opinions. The influence process is mediated by a trust network, as we
assume that the opinion of a given agent is only affected by the opinions of
those agents that she considers trustworthy (i.e., the agents in the trust
network that are directly linked to her). Each agent is endowed with a goal,
expressed in a suitable temporal language inspired from linear temporal logic
(LTL). We show that games of influence provide a simple abstraction to explore
the effects of the trust network structure on the agents' behaviour, by
considering solution concepts from game-theory such as Nash equilibrium, weak
dominance and winning strategies.",influencer disclosure
http://arxiv.org/abs/1111.3013v3,"In this thesis we consider the problem of information hiding in the scenarios
of interactive systems, statistical disclosure control, and refinement of
specifications. We apply quantitative approaches to information flow in the
first two cases, and we propose improvements for the usual solutions based on
process equivalences for the third case. In the first scenario we consider the
problem of defining the information leakage in interactive systems where
secrets and observables can alternate during the computation and influence each
other. We show that the information-theoretic approach which interprets such
systems as (simple) noisy channels is not valid. The principle can be recovered
if we consider channels with memory and feedback. We also propose the use of
directed information from input to output as the real measure of leakage in
interactive systems. In the second scenario we consider the problem of
statistical disclosure control, which concerns how to reveal accurate
statistics about a set of respondents while preserving the privacy of
individuals. We focus on the concept of differential privacy, a notion that has
become very popular in the database community. We show how to model the query
system in terms of an information-theoretic channel, and we compare the notion
of differential privacy with that of min-entropy leakage.In the third scenario
we address the problem of using process equivalences to characterize
information-hiding properties. We show that, in the presence of nondeterminism,
this approach may rely on the assumption that the scheduler ""works for the
benefit of the protocol"", and this is often not a safe assumption. We present a
formalism in which we can specify admissible schedulers and, correspondingly,
safe versions of complete-trace equivalence and bisimulation, and we show that
safe equivalences can be used to establish information-hiding properties.",influencer disclosure
http://arxiv.org/abs/1901.07311v1,"A tremendous amount of individual-level data is generated each day, of use to
marketing, decision makers, and machine learning applications. This data often
contain private and sensitive information about individuals, which can be
disclosed by adversaries. An adversary can recognize the underlying
individual's identity for a data record by looking at the values of
quasi-identifier attributes, known as identity disclosure, or can uncover
sensitive information about an individual through attribute disclosure. In
Statistical Disclosure Control, multiple disclosure risk measures have been
proposed. These share two drawbacks: they do not consider identity and
attribute disclosure concurrently in the risk measure, and they make
restrictive assumptions on an adversary's knowledge by assuming certain
attributes are quasi-identifiers and there is a clear boundary between
quasi-identifiers and sensitive information. In this paper, we present a novel
disclosure risk measure that addresses these limitations, by presenting a
single combined metric of identity and attribute disclosure risk, and providing
flexibility in modeling adversary's knowledge. We have developed an efficient
algorithm for computing the proposed risk measure and evaluated the feasibility
and performance of our approach on a real-world data set from the domain of
social work.",influencer disclosure
http://arxiv.org/abs/1809.00751v3,"We study the use of Bayesian persuasion (i.e., strategic use of information
disclosure/signaling) in endogenous team formation. This is an important
consideration in settings such as crowdsourcing competitions, open science
challenges and group-based assignments, where a large number of agents organize
themselves into small teams which then compete against each other. A central
tension here is between the strategic interests of agents who want to have the
highest-performing team, and that of the principal who wants teams to be
balanced. Moreover, although the principal cannot choose the teams or modify
rewards, she often has additional knowledge of agents' abilities, and can
leverage this information asymmetry to provide signals that influence team
formation. Our work uncovers the critical role of self-awareness (i.e.,
knowledge of one's own abilities) for the design of such mechanisms. For
settings with two-member teams and binary-valued agents partitioned into a
constant number of prior classes, we provide signaling mechanisms which are
asymptotically optimal when agents are agnostic of their own abilities. On the
other hand, when agents are self-aware, then we show that there is no signaling
mechanism that can do better than not releasing information, while satisfying
agent participation constraints.",influencer disclosure
http://arxiv.org/abs/1803.08488v2,"While disclosures relating to various forms of Internet advertising are well
established and follow specific formats, endorsement marketing disclosures are
often open-ended in nature and written by individual publishers. Because such
marketing often appears as part of publishers' actual content, ensuring that it
is adequately disclosed is critical so that end-users can identify it as such.
In this paper, we characterize disclosures relating to affiliate marketing---a
type of endorsement based marketing---on two popular social media platforms:
YouTube & Pinterest. We find that only roughly one-tenth of affiliate content
on both platforms contains disclosures. Based on our findings, we make policy
recommendations geared towards various stakeholders in the affiliate marketing
industry, highlighting how both social media platforms and affiliate companies
can enable better disclosure practices.",influencer disclosure
http://arxiv.org/abs/1702.01745v1,"This dissertation is based on five empirical research articles investigating
the different latent factors that motivate and hinder the process of
digital-photo interaction in computer-mediated platforms. Study I examine the
current practices surrounding digital photos in the context of personal photo
repositories (N=15). Study II investigates the gratifications and impeding
factors associated with photo-tagging activity on Facebook (N=67). Study III
develops and tests an instrument for understanding the gratifications of
Facebook photo-sharing (N=368). Study IV examines the impact of various aspects
of privacy in relation to photo-sharing intentions on Facebook (N=378).
Finally, study V investigates the age and gender differences regarding various
aspects of privacy and trust in the context of photo-sharing activity on
Facebook (N=378). The dissertation reveals the following findings: First, lack
of ""social features"" is one of the essential reasons for non-acceptance of
tagging feature in standalone photo management applications. Second,
photo-sharing and photo-tagging adoption and popularity can be attributed to
various factors such as affection, attention, communication, disclosure, habit,
information sharing, self-expression, socialization, and social influence.
Third, age, gender, and activity influence photo-sharing and photo-tagging
gratifications. Fourth, in the context of Facebook photo-sharing, various
aspects of privacy significantly impact users trust and activity and
consequently photo-sharing intentions. Fifth, women and young Facebook users
are significantly more concerned about the privacy of their shared photos.
Sixth, privacy-protective measures are significantly exercised more by young
Facebook users, yet they exhibit more trust and a higher level of activity on
Facebook.",influencer disclosure
http://arxiv.org/abs/1410.4617v2,"We view a distributed system as a graph of active locations with
unidirectional channels between them, through which they pass messages. In this
context, the graph structure of a system constrains the propagation of
information through it.
  Suppose a set of channels is a cut set between an information source and a
potential sink. We prove that, if there is no disclosure from the source to the
cut set, then there can be no disclosure to the sink. We introduce a new
formalization of partial disclosure, called *blur operators*, and show that the
same cut property is preserved for disclosure to within a blur operator. This
cut-blur property also implies a compositional principle, which ensures limited
disclosure for a class of systems that differ only beyond the cut.",influencer disclosure
http://arxiv.org/abs/1807.05738v1,"Opinion polls suggest that the public value their privacy, with majorities
calling for greater control of their data. However, individuals continue to use
online services which place their personal information at risk, comprising a
Privacy Paradox. Previous work has analysed this phenomenon through
after-the-fact comparisons, but not studied disclosure behaviour during
questioning. We physically surveyed UK cities to study how the British public
regard privacy and how perceptions differ between demographic groups. Through
analysis of optional data disclosure, we empirically examined whether those who
claim to value their privacy act privately with their own data. We found that
both opinions and self-reported actions have little effect on disclosure, with
over 99\% of individuals revealing private data needlessly. We show that not
only do individuals act contrary to their opinions, they disclose information
needlessly even whilst describing themselves as private. We believe our
findings encourage further analysis of data disclosure, as a means of studying
genuine privacy behaviour.",influencer disclosure
http://arxiv.org/abs/1809.09682v2,"Motivated by applications where privacy is important, we consider planning
problems for robots acting in the presence of an observer. We first formulate
and then solve planning problems subject to stipulations on the information
divulged during plan execution --- the appropriate solution concept being both
a plan and an information disclosure policy. We pose this class of problem
under a worst-case model within the framework of procrustean graphs,
formulating the disclosure policy as a particular type of map on edge labels.
We devise algorithms that, given a planning problem supplemented with an
information stipulation, can find a plan, associated disclosure policy, or both
if some exists. Both the plan and associated disclosure policy may depend
subtlety on additional information available to the observer, such as whether
the observer knows the robot's plan (e.g., leaked via a side-channel). Our
implementation finds a plan and a suitable disclosure policy, jointly, when any
such pair exists, albeit for small problem instances.",influencer disclosure
http://arxiv.org/abs/1203.3870v1,"Every time the customer (individual or company) has to release personal
information to its service provider (e.g., an online store or a cloud computing
provider), it faces a trade-off between the benefits gained (enhanced or
cheaper services) and the risks it incurs (identity theft and fraudulent uses).
The amount of personal information released is the major decision variable in
that trade-off problem, and has a proxy in the maximum loss the customer may
incur. We find the conditions for a unique optimal solution to exist for that
problem as that maximizing the customer's surplus. We also show that the
optimal amount of personal information is influenced most by the immediate
benefits the customer gets, i.e., the price and the quantity of service offered
by the service provider, rather than by maximum loss it may incur. Easy
spenders take larger risks with respect to low-spenders, but an increase in
price drives customers towards a more careful risk-taking attitude anyway. A
major role is also played by the privacy level, which the service provider
employs to regulate the benefits released to the customers. We also provide a
closed form solution for the limit case of a perfectly secure provider, showing
that the results do not differ significantly from those obtained in the general
case. The trade-off analysis may be employed by the customer to determine its
level of exposure in the relationship with its service provider.",influencer disclosure
http://arxiv.org/abs/1808.06587v1,"Plasma electrolytic oxidation (PEO) was carried out to form black ceramic
coatings on AZ31 magnesium alloy in aluminate-tungstate electrolyte. Influences
of current frequency (such as 100, 1000 and 2000 Hz) on properties of the PEO
coatings have been explored. It was found that increment of frequency led to
coatings with higher thickness, higher roughness and darker appearance. In
order to explore the coating formation mechanism at different frequencies, the
micro-discharges at different frequencies were investigated by real-time
imaging method. Microstructure of coatings was characterized by scanning
electron microscopy (SEM) assisted with energy dispersive X-ray spectroscopy
(EDS). Coatings formed at 100 Hz show the most compact microstructure while
porosity of the coatings, both in the pore number and pore size, is greatly
increased at higher frequencies. The coatings formed at higher frequencies show
an increased atomic ratio of W/Al. The cancel of the negative pulse from the
pulsed waveform can also lead to a higher W/Al ratio, and hence a darker
coating. The higher porosity of the coatings was attributed to the stronger
plasma discharges and gas evolution at higher frequencies. The stronger plasma
discharge is induced by the fact that there is not sufficient time for the
cooling down of discharge channels at higher frequencies.",influencer disclosure
http://arxiv.org/abs/1710.00101v1,"Traffic analysis is a type of attack on secure communications systems, in
which the adversary extracts useful patterns and information from the observed
traffic. This paper improves and extends an efficient traffic analysis attack,
called ""statistical disclosure attack."" Moreover, we propose a solution to
defend against the improved (and, a fortiori, the original) statistical
disclosure attack. Our solution delays the attacker considerably, meaning that
he should gather significantly more observations to be able to deduce
meaningful information from the traffic.",influencer disclosure
http://arxiv.org/abs/1904.01711v1,"Perfect data privacy seems to be in fundamental opposition to the economical
and scientific opportunities associated with extensive data exchange. Defying
this intuition, this paper develops a framework that allows the disclosure of
collective properties of datasets without compromising the privacy of
individual data samples. We present an algorithm to build an optimal disclosure
strategy/mapping, and discuss it fundamental limits on finite and
asymptotically large datasets. Furthermore, we present explicit expressions to
the asymptotic performance of this scheme in some scenarios, and study cases
where our approach attains maximal efficiency. We finally discuss suboptimal
schemes to provide sample privacy guarantees to large datasets with a reduced
computational cost.",influencer disclosure
http://arxiv.org/abs/0909.2290v1,"Several anonymization techniques, such as generalization and bucketization,
have been designed for privacy preserving microdata publishing. Recent work has
shown that generalization loses considerable amount of information, especially
for high-dimensional data. Bucketization, on the other hand, does not prevent
membership disclosure and does not apply for data that do not have a clear
separation between quasi-identifying attributes and sensitive attributes.
  In this paper, we present a novel technique called slicing, which partitions
the data both horizontally and vertically. We show that slicing preserves
better data utility than generalization and can be used for membership
disclosure protection. Another important advantage of slicing is that it can
handle high-dimensional data. We show how slicing can be used for attribute
disclosure protection and develop an efficient algorithm for computing the
sliced data that obey the l-diversity requirement. Our workload experiments
confirm that slicing preserves better utility than generalization and is more
effective than bucketization in workloads involving the sensitive attribute.
Our experiments also demonstrate that slicing can be used to prevent membership
disclosure.",influencer disclosure
http://arxiv.org/abs/1701.08419v1,"Recently, the permutation paradigm has been proposed in data anonymization to
describe any micro data masking method as permutation, paving the way for
performing meaningful analytical comparisons of methods, something that is
difficult currently in statistical disclosure control research. This paper
explores some consequences of this paradigm by establishing some class of
universal measures of disclosure risk and information loss that can be used for
the evaluation and comparison of any method, under any parametrization and
independently of the characteristics of the data to be anonymized. These
measures lead to the introduction in data anonymization of the concepts of
dominance in disclosure risk and information loss, which formalise the fact
that different parties involved in micro data transaction can all have
different sensitivities to privacy and information.",influencer disclosure
http://arxiv.org/abs/1808.00356v1,"The proliferation of network-connected devices and applications has resulted
in people receiving dozens, or hundreds, of notifications per day. When people
are in the presence of others, each notification poses some risk of accidental
information disclosure; onlookers may see notifications appear above the lock
screen of a mobile phone, on the periphery of a desktop or laptop display, or
projected onscreen during a presentation. In this paper, we quantify the
prevalence of these accidental disclosures in the context of email
notifications, and we study people's relevant preferences and concerns. Our
results are compiled from an exploratory retrospective survey of 131
respondents, and a separate contextual-labeling study in which 169 participants
labeled 1,040 meeting-email pairs. We find that, for 53% of people, at least 1
in 10 email notifications poses an information disclosure risk. We also find
that the real or perceived severity of these risks depend both on user
characteristics and attributes of the meeting or email (e.g. the number of
recipients or attendees). We conclude by exploring machine learning algorithms
to predict people's comfort levels given an email notification and a context,
then we present implications for the design of future contextually-relevant
notification systems.",influencer disclosure
http://arxiv.org/abs/1901.00716v2,"In Privacy Preserving Data Publishing, various privacy models have been
developed for employing anonymization operations on sensitive individual level
datasets, in order to publish the data for public access while preserving the
privacy of individuals in the dataset. However, there is always a trade-off
between preserving privacy and data utility; the more changes we make on the
confidential dataset to reduce disclosure risk, the more information the data
loses and the less data utility it preserves. The optimum privacy technique is
the one that results in a dataset with minimum disclosure risk and maximum data
utility. In this paper, we propose an improved suppression method, which
reduces the disclosure risk and enhances the data utility by targeting the
highest risk records and keeping other records intact. We have shown the
effectiveness of our approach through an experiment on a real-world
confidential dataset.",influencer disclosure
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",influencer disclosure
http://arxiv.org/abs/1811.06026v2,"In a social learning setting, there is a set of actions, each of which has a
fixed but unknown expected payoff. Agents arrive one by one, each chooses an
action with the goal of maximizing the payoff. A disclosure policy coordinates
the choices of the agents by sending messages about the history of past
actions. These messages can alter agents' incentives towards ""exploration"",
taking potentially sub-optimal actions for the sake of learning more about
their rewards. The goal of the disclosure policy is to incentivize exploration
so as to minimize the regret of the chosen action sequence. Prior work achieves
much progress with disclosure policies that merely recommend an action to each
user. However, all this work relies heavily on trust and rationality
assumptions, standard in economic theory, yet quite problematic in the context
of the motivating applications.
  In this paper, we design disclosure policies which incentivize good
performance under more plausible behavioral assumptions. We would like to
retain the trustworthiness of revealing the full history, while avoiding the
herding behavior that it may induce. We focus on messages, called unbiased
subhistories, consisting of the actions and rewards from a subsequence of past
agents, where the subsequence is chosen ahead of time. We posit a flexible
model of agent response, which we argue is plausible for our disclosure
policies. Our main result is a disclosure policy using unbiased, transitive
subhistories that obtains regret $\tilde{O}(\sqrt{\#rounds})$. We also exhibit
simpler policies with higher, but still sublinear, regret. These policies can
be interpreted as dividing a sublinear number of agents into constant-sized
focus groups, whose histories are then fed to future agents.",influencer disclosure
http://arxiv.org/abs/1504.07313v1,"We present a novel framework, called Private Disclosure of Information (PDI),
which is aimed to prevent an adversary from inferring certain sensitive
information about subjects using the data that they disclosed during
communication with an intended recipient. We show cases where it is possible to
achieve perfect privacy regardless of the adversary's auxiliary knowledge while
preserving full utility of the information to the intended recipient and
provide sufficient conditions for such cases. We also demonstrate the
applicability of PDI on a real-world data set that simulates a health
tele-monitoring scenario.",influencer disclosure
http://arxiv.org/abs/1407.6350v1,"We analyze how the sparsity of a typical aggregate social relation impacts
the network overhead of online communication systems designed to provide
k-anonymity. Once users are grouped in anonymity sets there will likely be few
related pairs of users between any two particular sets, and so the sets need to
be large in order to provide cover traffic between them. We can reduce the
associated overhead by having both parties in a communication specify both the
origin and the target sets of the communication. We propose to call this
communication primitive ""symmetric disclosure."" If in order to retrieve
messages a user specifies a group from which he expects to receive them, the
negative impact of the sparsity is offset.",influencer disclosure
http://arxiv.org/abs/1310.6299v2,"Provenance is an increasing concern due to the ongoing revolution in sharing
and processing scientific data on the Web and in other computer systems. It is
proposed that many computer systems will need to become provenance-aware in
order to provide satisfactory accountability, reproducibility, and trust for
scientific or other high-value data. To date, there is not a consensus
concerning appropriate formal models or security properties for provenance. In
previous work, we introduced a formal framework for provenance security and
proposed formal definitions of properties called disclosure and obfuscation.
  In this article, we study refined notions of positive and negative disclosure
and obfuscation in a concrete setting, that of a general-purpose programing
language. Previous models of provenance have focused on special-purpose
languages such as workflows and database queries. We consider a higher-order,
functional language with sums, products, and recursive types and functions, and
equip it with a tracing semantics in which traces themselves can be replayed as
computations. We present an annotation-propagation framework that supports many
provenance views over traces, including standard forms of provenance studied
previously. We investigate some relationships among provenance views and
develop some partial solutions to the disclosure and obfuscation problems,
including correct algorithms for disclosure and positive obfuscation based on
trace slicing.",influencer disclosure
http://arxiv.org/abs/1307.0966v1,"We focus on two mainstream privacy models: k-anonymity and differential
privacy. Once a privacy model has been selected, the goal is to enforce it
while preserving as much data utility as possible. The main objective of this
thesis is to improve the data utility in k-anonymous and differentially private
data releases. k-Anonymity has several drawbacks. On the disclosure limitation
side, there is a lack of protection against attribute disclosure and against
informed intruders. On the data utility side, dealing with a large number of
quasi-identifier attributes is problematic. We propose a relaxation of
k-anonymity that deals with these issues.
  Differential privacy limits disclosure risk through noise addition. The
Laplace distribution is commonly used for the random noise. We show that the
Laplace distribution is not optimal: the same disclosure limitation guarantee
can be attained by adding less noise. Optimal univariate and multivariate
noises are characterized and constructed.
  Common mechanisms to attain differential privacy do not take into account the
users prior knowledge; they implicitly assume zero initial knowledge about the
query response. We propose a mechanism that focuses on limiting the knowledge
gain over the prior knowledge.
  Microaggregation-based k-anonymity and differential privacy can be combined
to produce microdata releases with the strong privacy guarantees of
differential privacy and improved data accuracy.
  The last contribution delves into the relation between t-closeness and
differential privacy. We see that for a specific distance and under some
reasonable assumptions on the intruders knowledge, t-closeness leads to
differential privacy.",influencer disclosure
http://arxiv.org/abs/physics/0509090v2,"We study the effect of globalization on the Korean market, one of the
emerging markets. Some characteristics of the Korean market are different from
those of the mature market according to the latest market data, and this is due
to the influence of foreign markets or investors. We concentrate on the market
network structures over the past two decades with knowledge of the history of
the market, and determine the globalization effect and market integration as a
function of time.",influencer marketing
http://arxiv.org/abs/1505.02766v1,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.",influencer marketing
http://arxiv.org/abs/1511.00750v1,"The purchasing behaviour of consumers is often influenced by numerous
factors, including the visibility of the products and the influence of other
customers through their own purchases or their recommendations.
  Motivated by trial-offer and freemium markets and a number of online markets
for cultural products, leisure services, and retail, this paper studies the
dynamics of a marketplace ran by a single firm and which is visited by
heterogeneous consumers whose choice preferences can be modeled using a Mixed
Multinomial Logit. In this marketplace, consumers are influenced by past
purchases, the inherent appeal of the products, and the visibility of each
product. The resulting market generalizes recent models already verified in
cultural markets.
  We examine various marketing policies for this market and analyze their
long-term dynamics and the potential benefits of social influence. In
particular, we show that the heterogeneity of the customers complicates the
market significantly: Many of the optimality and computational properties of
the corresponding homogeneous market no longer hold. To remedy these
limitations, we explore a market segmentation strategy and quantify its
benefits. The theoretical results are complemented by Monte Carlo simulations
conducted on examples of interest.",influencer marketing
http://arxiv.org/abs/1505.02469v3,"Social influence is ubiquitous in cultural markets, from book recommendations
in Amazon, to song popularities in iTunes and the ranking of newspaper articles
in the online edition of the New York Times to mention only a few. Yet social
influence is often presented in a bad light, often because it supposedly
increases market unpredictability.
  Here we study a model of trial-offer markets, in which participants try
products and later decide whether to purchase. We consider a simple policy
which ranks the products by quality when presenting them to market
participants. We show that, in this setting, market efficiency always benefits
from social influence. Moreover, we prove that the market converges almost
surely to a monopoly for the product of highest quality, making the market both
predictable and asymptotically optimal. Computational experiments confirm that
the quality ranking policy identifies ""blockbusters"" in reasonable time,
outperforms other policies, and is highly predictable. These results indicate
that social influence does not necessarily increase market unpredicatibility.
The outcome really depends on how social influence is used.",influencer marketing
http://arxiv.org/abs/1009.0309v1,"In this paper, inspired by the work of Megiddo on the formation of
preferences and strategic analysis, we consider an early market model studied
in the field of economic theory, in which each trader's utility may be
influenced by the bundles of goods obtained by her social neighbors. The goal
of this paper is to understand and characterize the impact of social influence
on the complexity of computing and approximating market equilibria.
  We present complexity-theoretic and algorithmic results for approximating
market equilibria in this model with focus on two concrete influence models
based on the traditional linear utility functions. Recall that an Arrow-Debreu
market equilibrium in a conventional exchange market with linear utility
functions can be computed in polynomial time by convex programming. Our
complexity results show that even a bounded-degree, planar influence network
can significantly increase the difficulty of equilibrium computation even in
markets with only a constant number of goods. Our algorithmic results suggest
that finding an approximate equilibrium in markets with hierarchical influence
networks might be easier than that in markets with arbitrary neighborhood
structures. By demonstrating a simple market with a constant number of goods
and a bounded-degree, planar influence graph whose equilibrium is PPAD-hard to
approximate, we also provide a counterexample to a common belief, which we
refer to as the myth of a constant number of goods, that equilibria in markets
with a constant number of goods are easy to compute or easy to approximate.",influencer marketing
http://arxiv.org/abs/1508.07272v1,"Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.",influencer marketing
http://arxiv.org/abs/1907.05028v1,"The Viral Marketing is a relatively new form of marketing that exploits
social networks to promote a brand, a product, etc. The idea behind it is to
find a set of influencers on the network that can trigger a large cascade of
propagation and adoptions. In this paper, we will introduce an evidential
opinion-based influence maximization model for viral marketing. Besides, our
approach tackles three opinions based scenarios for viral marketing in the real
world. The first scenario concerns influencers who have a positive opinion
about the product. The second scenario deals with influencers who have a
positive opinion about the product and produce effects on users who also have a
positive opinion. The third scenario involves influence users who have a
positive opinion about the product and produce effects on the negative opinion
of other users concerning the product in question. Next, we proposed six
influence measures, two for each scenario. We also use an influence
maximization model that the set of detected influencers for each scenario.
Finally, we show the performance of the proposed model with each influence
measure through some experiments conducted on a generated dataset and a real
world dataset collected from Twitter.",influencer marketing
http://arxiv.org/abs/physics/0607071v1,"Financial markets are subject to long periods of polarized behavior, such as
bull-market or bear-market phases, in which the vast majority of market
participants seem to almost exclusively choose one action (between buying or
selling) over the other. From the point of view of conventional economic
theory, such events are thought to reflect the arrival of ``external news''
that justifies the observed behavior. However, empirical observations of the
events leading up to such market phases, as well events occurring during the
lifetime of such a phase, have often failed to find significant correlation
between news from outside the market and the behavior of the agents comprising
the market. In this paper, we explore the alternative hypothesis that the
occurrence of such market polarizations are due to interactions amongst the
agents in the market, and not due to any influence external to it. In
particular, we present a model where the market (i.e., the aggregate behavior
of all the agents) is observed to become polarized even though individual
agents regularly change their actions (buy or sell) on a time-scale much
shorter than that of the market polarization phase.",influencer marketing
http://arxiv.org/abs/1503.00823v1,"In a stock market, the price fluctuations are interactive, that is, one
listed company can influence others. In this paper, we seek to study the
influence relationships among listed companies by constructing a directed
network on the basis of Chinese stock market. This influence network shows
distinct topological properties, particularly, a few large companies that can
lead the tendency of stock market are recognized. Furthermore, by analyzing the
subnetworks of listed companies distributed in several significant economic
sectors, it is found that the influence relationships are totally different
from one economic sector to another, of which three types of connectivity as
well as hub-like listed companies are identified. In addition, the rankings of
listed companies obtained from the centrality metrics of influence network are
compared with that according to the assets, which gives inspiration to uncover
and understand the importance of listed companies in the stock market. These
empirical results are meaningful in providing these topological properties of
Chinese stock market and economic sectors as well as revealing the
interactively influence relationships among listed companies.",influencer marketing
http://arxiv.org/abs/1512.07251v4,"This paper considers trial-offer markets where consumer preferences are
modeled by a multinomial logit with social influence and position bias. The
social signal for a product is given by its current market share raised to
power r (or equivalently the number of purchases raised to the power of r). The
paper shows that, when r is strictly between 0 and 1, and a static position
assignment (e.g., a quality ranking) is used, the market converges to a unique
equilibrium where the market shares depend only on product quality, not their
initial appeals or the early dynamics. When r is greater than 1, the market
becomes unpredictable. In many cases, the market goes to a monopoly for some
product: Which product becomes a monopoly depends on the initial conditions of
the market. These theoretical results are complemented by an agent-based
simulation which indicates that convergence is fast when r is between 0 and 1,
and that the quality ranking dominates the well-known popularity ranking in
terms of market efficiency. These results shed a new light on the role of
social influence which is often blamed for unpredictability, inequalities, and
inefficiencies in markets. In contrast, this paper shows that, with a proper
social signal and position assignment for the products, the market becomes
predictable, and inequalities and inefficiencies can be controlled
appropriately.",influencer marketing
http://arxiv.org/abs/1604.01672v1,"In this paper, we investigate the effect of brand in market competition.
Specifically, we propose a variant Hotelling model where companies and
customers are represented by points in an Euclidean space, with axes being
product features. $N$ companies compete to maximize their own profits by
optimally choosing their prices, while each customer in the market, when
choosing sellers, considers the sum of product price, discrepancy between
product feature and his preference, and a company's brand name, which is
modeled by a function of its market area of the form $-\beta\cdot\text{(Market
Area)}^q$, where $\beta$ captures the brand influence and $q$ captures how
market share affects the brand. By varying the parameters $\beta$ and $q$, we
derive existence results of Nash equilibrium and equilibrium market prices and
shares. In particular, we prove that pure Nash equilibrium always exists when
$q=0$ for markets with either one and two dominating features, and it always
exists in a single dominating feature market when market affects brand name
linearly, i.e., $q=1$. Moreover, we show that at equilibrium, a company's price
is proportional to its market area over the competition intensity with its
neighbors, a result that quantitatively reconciles the common belief of a
company's pricing power. We also study an interesting ""wipe out"" phenomenon
that only appears when $q>0$, which is similar to the ""undercut"" phenomenon in
the Hotelling model, where companies may suddenly lose the entire market area
with a small price increment. Our results offer novel insight into market
pricing and positioning under competition with brand effect.",influencer marketing
http://arxiv.org/abs/1407.7015v1,"Prediction markets are often used as mechanisms to aggregate information
about a future event, for example, whether a candidate will win an election.
The event is typically assumed to be exogenous. In reality, participants may
influence the outcome, and therefore (1) running the prediction market could
change the incentives of participants in the process that creates the outcome
(for example, agents may want to change their vote in an election), and (2)
simple results such as the myopic incentive compatibility of proper scoring
rules no longer hold in the prediction market itself. We introduce a model of
games of this kind, where agents first trade in a prediction market and then
take an action that influences the market outcome. Our two-stage two-player
model, despite its simplicity, captures two aspects of real-world prediction
markets: (1) agents may directly influence the outcome, (2) some of the agents
instrumental in deciding the outcome may not take part in the prediction
market. We show that this game has two different types of perfect Bayesian
equilibria, which we term LPP and HPP, depending on the values of the belief
parameters: in the LPP domain, equilibrium prices reveal expected market
outcomes conditional on the participants' private information, whereas HPP
equilibria are collusive -- participants effectively coordinate in an
uninformative and untruthful way.",influencer marketing
http://arxiv.org/abs/cs/0109108v1,"This paper examines the effects of licensing conditions, in particular of
spectrum fees, on the pricing and diffusion of mobile communications services.
Seemingly exorbitant sums paid for 3G licenses in the UK, Germany in 2000 and
similarly high fees paid by U.S. carriers in the re-auctioning of PCS licenses
early in 2001 raised concerns as to the impacts of the market entry regime on
the mobile communications market.
  The evidence from the GSM and PCS markets reviewed in this paper suggests
that market entry fees do indeed influence the subsequent development of the
market. We discuss three potential transmission channels by which license fees
can influence the price and quantity of service sold in a wireless market: an
increase in average cost, an increase in incremental costs, and impacts of sunk
costs on the emerging market structure.
  From this conceptual debate, an empirical model is developed and tested using
cross-sectional data for the residential mobile voice market. We utilize a
structural equation approach, modeling the supply and demand relationships
subject to the constraint that supply equals demand. The results confirm the
existence of a positive effect of license fees on the cost of supply. However,
we also find that higher market concentration has a positive effect on the
overall supply in the market, perhaps supporting a Schumpeterian view that a
certain degree of market concentration facilitates efficiency.",influencer marketing
http://arxiv.org/abs/1601.05283v2,"The paper proposes a way to add marketing into the standard threshold model
of social networks. Within this framework, the paper studies logical properties
of the influence relation between sets of agents in social networks. Two
different forms of this relation are considered: one for promotional marketing
and the other for preventive marketing. In each case a sound and complete
logical system describing properties of the influence relation is proposed.
Both systems could be viewed as extensions of Armstrong's axioms of functional
dependency from the database theory.",influencer marketing
http://arxiv.org/abs/1408.1542v4,"Social influence has been shown to create significant unpredictability in
cultural markets, providing one potential explanation why experts routinely
fail at predicting commercial success of cultural products. To counteract the
difficulty of making accurate predictions, ""measure and react"" strategies have
been advocated but finding a concrete strategy that scales for very large
markets has remained elusive so far. Here we propose a ""measure and optimize""
strategy based on an optimization policy that uses product quality, appeal, and
social influence to maximize expected profits in the market at each decision
point. Our computational experiments show that our policy leverages social
influence to produce significant performance benefits for the market, while our
theoretical analysis proves that our policy outperforms in expectation any
policy not displaying social information. Our results contrast with earlier
work which focused on showing the unpredictability and inequalities created by
social influence. Not only do we show for the first time that dynamically
showing consumers positive social information under our policy increases the
expected performance of the seller in cultural markets. We also show that, in
reasonable settings, our policy does not introduce significant unpredictability
and identifies ""blockbusters"". Overall, these results shed new light on the
nature of social influence and how it can be leveraged for the benefits of the
market.",influencer marketing
http://arxiv.org/abs/1107.0028v1,"With the recent technological feasibility of electronic commerce over the
Internet, much attention has been given to the design of electronic markets for
various types of electronically-tradable goods. Such markets, however, will
normally need to function in some relationship with markets for other related
goods, usually those downstream or upstream in the supply chain. Thus, for
example, an electronic market for rubber tires for trucks will likely need to
be strongly influenced by the rubber market as well as by the truck market. In
this paper we design protocols for exchange of information between a sequence
of markets along a single supply chain. These protocols allow each of these
markets to function separately, while the information exchanged ensures
efficient global behavior across the supply chain. Each market that forms a
link in the supply chain operates as a double auction, where the bids on one
side of the double auction come from bidders in the corresponding segment of
the industry, and the bids on the other side are synthetically generated by the
protocol to express the combined information from all other links in the chain.
The double auctions in each of the markets can be of several types, and we
study several variants of incentive compatible double auctions, comparing them
in terms of their efficiency and of the market revenue.",influencer marketing
http://arxiv.org/abs/1709.09302v3,"We consider a market in which capacity-constrained generators compete in
scalar-parameterized supply functions to serve an inelastic demand spread
throughout a transmission constrained power network. The market clears
according to a locational marginal pricing mechanism, in which the independent
system operator (ISO) determines the generators' production quantities to
minimize the revealed cost of meeting demand, while ensuring that network
transmission and generator capacity constraints are met. Under the stylizing
assumption that both the ISO and generators choose their strategies
simultaneously, we establish the existence of Nash equilibria for the
underlying market, and derive an upper bound on the allocative efficiency loss
at Nash equilibrium relative to the socially optimal level. We also
characterize an upper bound on the markup of locational marginal prices at Nash
equilibrium above their perfectly competitive levels. Of particular relevance
to ex ante market power monitoring, these bounds reveal the role of certain
market structures---specifically, the \emph{market share} and \emph{residual
supply index} of a producer---in predicting the degree to which that producer
is able to exercise market power to influence the market outcome to its
advantage. Finally, restricting our attention to the simpler setting of a
two-node power network, we provide a characterization of market structures
under which a Braess-like paradox occurs due to the exercise of market
power---that is to say, we provide a necessary and sufficient condition on
market structure under which the strengthening of the network's transmission
line capacity results in the (counterintuitive) increase in the total cost of
generation at Nash equilibrium.",influencer marketing
http://arxiv.org/abs/1906.05911v1,"Influencing a target audience through social media content has become a new
focus of interest for marketing leaders. While a large amount of heterogeneous
data is produced by influencers on a daily basis, professionals in the
influ-encer marketing (IM) field still rely on simple quantitative metrics such
as community size or engagement rate to estimate the value of an influencer. As
few research papers have proposed a framework to quantify the performance of an
influencer by using quantitative influencer data (number of followers,
engagement,...), qualitative information (age, country, city, etc...), natural
text (profile and post descriptions) and visual information (images and
videos), we decided to explore these variables and quantitatively evaluate
their dependencies. By analyzing 713,824 influencers on 5 social media
platforms over a period of one year, we identified relationships between value
proposition data (engagement, reach, audience), demographics, natural text
patterns and visual information. The main goal of this paper is to provide IM
professionals with a modern methodology to better understand the value of their
influencers and to feed machine learning algorithms for clustering, scoring or
recommendation.",influencer marketing
http://arxiv.org/abs/1709.06519v1,"Predicting investors reactions to financial and political news is important
for the early detection of stock market jitters. Evidence from several recent
studies suggests that online social media could improve prediction of stock
market movements. However, utilizing such information to predict strong stock
market fluctuations has not been explored so far. In this work, we propose a
novel event detection method on Twitter, tailored to detect financial and
political events that influence a specific stock market. The proposed approach
applies a bursty topic detection method on a stream of tweets related to
finance or politics followed by a classification process which filters-out
events that do not influence the examined stock market. We train our classifier
to recognise real events by using solely information about stock market
volatility, without the need of manual labeling. We model Twitter events as
feature vectors that encompass a rich variety of information, such as the
geographical distribution of tweets, their polarity, information about their
authors as well as information about bursty words associated with the event. We
show that utilizing only information about tweets polarity, like most previous
studies, results in wasting important information. We apply the proposed method
on high-frequency intra-day data from the Greek and Spanish stock market and we
show that our financial event detector successfully predicts most of the stock
market jitters.",influencer marketing
http://arxiv.org/abs/1702.07810v2,"We analyze sources of error in prediction market forecasts in order to bound
the difference between a security's price and the ground truth it estimates. We
consider cost-function-based prediction markets in which an automated market
maker adjusts security prices according to the history of trade. We decompose
the forecasting error into three components: sampling error, arising because
traders only possess noisy estimates of ground truth; market-maker bias,
resulting from the use of a particular market maker (i.e., cost function) to
facilitate trade; and convergence error, arising because, at any point in time,
market prices may still be in flux. Our goal is to make explicit the tradeoffs
between these error components, influenced by design decisions such as the
functional form of the cost function and the amount of liquidity in the market.
We consider a specific model in which traders have exponential utility and
exponential-family beliefs representing noisy estimates of ground truth. In
this setting, sampling error vanishes as the number of traders grows, but there
is a tradeoff between the other two components. We provide both upper and lower
bounds on market-maker bias and convergence error, and demonstrate via
numerical simulations that these bounds are tight. Our results yield new
insights into the question of how to set the market's liquidity parameter and
into the forecasting benefits of enforcing coherent prices across securities.",influencer marketing
http://arxiv.org/abs/1809.04227v1,"Most recent works model the market structure of the stock market as a
correlation network of the stocks. They apply pre-defined patterns to extract
correlation information from the time series of stocks. Without considering the
influences of the evolving market structure to the market index trends, these
methods hardly obtain the market structure models which are compatible with the
market principles. Advancements in deep learning have shown their incredible
modeling capacity on various finance-related tasks. However, the learned inner
parameters, which capture the essence of the finance time series, are not
further exploited about their representation in the financial fields. In this
work, we model the financial market structure as a deep co-investment network
and propose a Deep Co-investment Network Learning (DeepCNL) method. DeepCNL
automatically learns deep co-investment patterns between any pairwise stocks,
where the rise-fall trends of the market index are used for distance
supervision. The learned inner parameters of the trained DeepCNL, which encodes
the temporal dynamics of deep co-investment patterns, are used to build the
co-investment network between the stocks as the investment structure of the
corresponding market. We verify the effectiveness of DeepCNL on the real-world
stock data and compare it with the existing methods on several financial tasks.
The experimental results show that DeepCNL not only has the ability to better
reflect the stock market structure that is consistent with widely-acknowledged
financial principles but also is more capable to approximate the investment
activities which lead to the stock performance reported in the real news or
research reports than other alternatives.",influencer marketing
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detecting fake reviews
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detecting fake reviews
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detecting fake reviews
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detecting fake reviews
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",detecting fake reviews
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detecting fake reviews
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detecting fake reviews
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detecting fake reviews
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",detecting fake reviews
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",detecting fake reviews
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",detecting fake reviews
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",detecting fake reviews
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",detecting fake reviews
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",detecting fake reviews
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",detecting fake reviews
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detecting fake reviews
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",detecting fake reviews
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",detecting fake reviews
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detecting fake reviews
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detecting fake reviews
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",detecting fake reviews
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detecting fake reviews
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detecting fake reviews
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detecting fake reviews
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",detecting fake reviews
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detecting fake reviews
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",detecting fake reviews
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detecting fake reviews
http://arxiv.org/abs/1706.00884v1,"Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.",detecting fake reviews
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",detecting fake reviews
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",detecting fake reviews
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",detecting fake reviews
http://arxiv.org/abs/1709.06916v2,"Popular User-Review Social Networks (URSNs)---such as Dianping, Yelp, and
Amazon---are often the targets of reputation attacks in which fake reviews are
posted in order to boost or diminish the ratings of listed products and
services. These attacks often emanate from a collection of accounts, called
Sybils, which are collectively managed by a group of real users. A new advanced
scheme, which we term elite Sybil attacks, recruits organically highly-rated
accounts to generate seemingly-trustworthy and realistic-looking reviews. These
elite Sybil accounts taken together form a large-scale sparsely-knit Sybil
network for which existing Sybil fake-review defense systems are unlikely to
succeed. In this paper, we conduct the first study to define, characterize, and
detect elite Sybil attacks. We show that contemporary elite Sybil attacks have
a hybrid architecture, with the first tier recruiting elite Sybil workers and
distributing tasks by Sybil organizers, and with the second tier posting fake
reviews for profit by elite Sybil workers. We design ElsieDet, a three-stage
Sybil detection scheme, which first separates out suspicious groups of users,
then identifies the campaign windows, and finally identifies elite Sybil users
participating in the campaigns. We perform a large-scale empirical study on ten
million reviews from Dianping, by far the most popular URSN service in China.
Our results show that reviews from elite Sybil users are more spread out
temporally, craft more convincing reviews, and have higher filter bypass rates.
We also measure the impact of Sybil campaigns on various industries (such as
cinemas, hotels, restaurants) as well as chain stores, and demonstrate that
monitoring elite Sybil users over time can provide valuable early alerts
against Sybil campaigns.",detecting fake reviews
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",detecting fake reviews
http://arxiv.org/abs/1509.05935v1,"How to tell if a review is real or fake? What does the underworld of
fraudulent reviewing look like? Detecting suspicious reviews has become a major
issue for many online services. We propose the use of a clique-finding approach
to discover well-organized suspicious reviewers. From a Yelp dataset with over
one million reviews, we construct multiple Reviewer Similarity graphs to link
users that have unusually similar behavior: two reviewers are connected in the
graph if they have reviewed the same set of venues within a few days. From
these graphs, our algorithms extracted many large cliques and quasi-cliques,
the largest one containing a striking 11 users who coordinated their review
activities in identical ways. Among the detected cliques, a large portion
contain Yelp Scouts who are paid by Yelp to review venues in new areas. Our
work sheds light on their little-known operation.",detecting fake reviews
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",detecting fake reviews
http://arxiv.org/abs/1511.06030v2,"Review fraud is a pervasive problem in online commerce, in which fraudulent
sellers write or purchase fake reviews to manipulate perception of their
products and services. Fake reviews are often detected based on several signs,
including 1) they occur in short bursts of time; 2) fraudulent user accounts
have skewed rating distributions. However, these may both be true in any given
dataset. Hence, in this paper, we propose an approach for detecting fraudulent
reviews which combines these 2 approaches in a principled manner, allowing
successful detection even when one of these signs is not present. To combine
these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD)
model, a flexible Bayesian model of user rating behavior. Based on our model we
formulate a likelihood-based suspiciousness metric, Normalized Expected
Surprise Total (NEST). We propose a linear-time algorithm for performing
Bayesian inference using our model and computing the metric. Experiments on
real data show that BIRDNEST successfully spots review fraud in large,
real-world graphs: the 50 most suspicious users of the Flipkart platform
flagged by our algorithm were investigated and all identified as fraudulent by
domain experts at Flipkart.",detecting fake reviews
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",detecting fake reviews
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",detecting fake reviews
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",detecting fake reviews
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake review detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake review detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake review detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake review detection
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake review detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake review detection
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",fake review detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake review detection
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",fake review detection
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",fake review detection
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",fake review detection
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",fake review detection
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",fake review detection
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake review detection
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",fake review detection
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake review detection
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",fake review detection
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",fake review detection
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake review detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake review detection
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",fake review detection
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake review detection
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake review detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake review detection
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake review detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake review detection
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",fake review detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake review detection
http://arxiv.org/abs/1706.00884v1,"Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.",fake review detection
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake review detection
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake review detection
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake review detection
http://arxiv.org/abs/1709.06916v2,"Popular User-Review Social Networks (URSNs)---such as Dianping, Yelp, and
Amazon---are often the targets of reputation attacks in which fake reviews are
posted in order to boost or diminish the ratings of listed products and
services. These attacks often emanate from a collection of accounts, called
Sybils, which are collectively managed by a group of real users. A new advanced
scheme, which we term elite Sybil attacks, recruits organically highly-rated
accounts to generate seemingly-trustworthy and realistic-looking reviews. These
elite Sybil accounts taken together form a large-scale sparsely-knit Sybil
network for which existing Sybil fake-review defense systems are unlikely to
succeed. In this paper, we conduct the first study to define, characterize, and
detect elite Sybil attacks. We show that contemporary elite Sybil attacks have
a hybrid architecture, with the first tier recruiting elite Sybil workers and
distributing tasks by Sybil organizers, and with the second tier posting fake
reviews for profit by elite Sybil workers. We design ElsieDet, a three-stage
Sybil detection scheme, which first separates out suspicious groups of users,
then identifies the campaign windows, and finally identifies elite Sybil users
participating in the campaigns. We perform a large-scale empirical study on ten
million reviews from Dianping, by far the most popular URSN service in China.
Our results show that reviews from elite Sybil users are more spread out
temporally, craft more convincing reviews, and have higher filter bypass rates.
We also measure the impact of Sybil campaigns on various industries (such as
cinemas, hotels, restaurants) as well as chain stores, and demonstrate that
monitoring elite Sybil users over time can provide valuable early alerts
against Sybil campaigns.",fake review detection
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake review detection
http://arxiv.org/abs/1509.05935v1,"How to tell if a review is real or fake? What does the underworld of
fraudulent reviewing look like? Detecting suspicious reviews has become a major
issue for many online services. We propose the use of a clique-finding approach
to discover well-organized suspicious reviewers. From a Yelp dataset with over
one million reviews, we construct multiple Reviewer Similarity graphs to link
users that have unusually similar behavior: two reviewers are connected in the
graph if they have reviewed the same set of venues within a few days. From
these graphs, our algorithms extracted many large cliques and quasi-cliques,
the largest one containing a striking 11 users who coordinated their review
activities in identical ways. Among the detected cliques, a large portion
contain Yelp Scouts who are paid by Yelp to review venues in new areas. Our
work sheds light on their little-known operation.",fake review detection
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",fake review detection
http://arxiv.org/abs/1511.06030v2,"Review fraud is a pervasive problem in online commerce, in which fraudulent
sellers write or purchase fake reviews to manipulate perception of their
products and services. Fake reviews are often detected based on several signs,
including 1) they occur in short bursts of time; 2) fraudulent user accounts
have skewed rating distributions. However, these may both be true in any given
dataset. Hence, in this paper, we propose an approach for detecting fraudulent
reviews which combines these 2 approaches in a principled manner, allowing
successful detection even when one of these signs is not present. To combine
these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD)
model, a flexible Bayesian model of user rating behavior. Based on our model we
formulate a likelihood-based suspiciousness metric, Normalized Expected
Surprise Total (NEST). We propose a linear-time algorithm for performing
Bayesian inference using our model and computing the metric. Experiments on
real data show that BIRDNEST successfully spots review fraud in large,
real-world graphs: the 50 most suspicious users of the Flipkart platform
flagged by our algorithm were investigated and all identified as fraudulent by
domain experts at Flipkart.",fake review detection
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake review detection
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake review detection
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake review detection
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",tracking fake reviews
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",tracking fake reviews
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",tracking fake reviews
http://arxiv.org/abs/1512.05457v2,"How can web services that depend on user generated content discern fake
social engagement activities by spammers from legitimate ones? In this paper,
we focus on the social site of YouTube and the problem of identifying bad
actors posting inorganic contents and inflating the count of social engagement
metrics. We propose an effective method, Leas (Local Expansion at Scale), and
show how the fake engagement activities on YouTube can be tracked over time by
analyzing the temporal graph based on the engagement behavior pattern between
users and YouTube videos. With the domain knowledge of spammer seeds, we
formulate and tackle the problem in a semi-supervised manner --- with the
objective of searching for individuals that have similar pattern of behavior as
the known seeds --- based on a graph diffusion process via local spectral
subspace. We offer a fast, scalable MapReduce deployment adapted from the
localized spectral clustering algorithm. We demonstrate the effectiveness of
our deployment at Google by achieving an manual review accuracy of 98% on
YouTube Comments graph in practice. Comparing with the state-of-the-art
algorithm CopyCatch, Leas achieves 10 times faster running time. Leas is
actively in use at Google, searching for daily deceptive practices on YouTube's
engagement graph spanning over a billion users.",tracking fake reviews
http://arxiv.org/abs/0812.5036v1,"The expected performance of track reconstruction with LHC events using the
CMS silicon tracker is presented. Track finding and fitting is accomplished
with Kalman Filter techniques that achieve efficiencies above 99% on single
muons with pT>1 GeV/c. Difficulties arise in the context of standard LHC events
with a high density of charged particles, where the rate of fake combinatorial
tracks is very large for low pT tracks, and nuclear interactions in the tracker
material reduce the tracking efficiency for charged hadrons. Recent
improvements with the CMS track reconstruction now allow to efficiently
reconstruct charged tracks with pT down to few hundred MeV/c and as few as
three crossed layers, with a very small fake fraction, by making use of an
optimal rejection of fake tracks in conjunction with an iterative tracking
procedure.",tracking fake reviews
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",tracking fake reviews
http://arxiv.org/abs/1706.05924v2,"An important challenge in the process of tracking and detecting the
dissemination of misinformation is to understand the political gap between
people that engage with the so called ""fake news"". A possible factor
responsible for this gap is opinion polarization, which may prompt the general
public to classify content that they disagree or want to discredit as fake. In
this work, we study the relationship between political polarization and content
reported by Twitter users as related to ""fake news"". We investigate how
polarization may create distinct narratives on what misinformation actually is.
We perform our study based on two datasets collected from Twitter. The first
dataset contains tweets about US politics in general, from which we compute the
degree of polarization of each user towards the Republican and Democratic
Party. In the second dataset, we collect tweets and URLs that co-occurred with
""fake news"" related keywords and hashtags, such as #FakeNews and
#AlternativeFact, as well as reactions towards such tweets and URLs. We then
analyze the relationship between polarization and what is perceived as
misinformation, and whether users are designating information that they
disagree as fake. Our results show an increase in the polarization of users and
URLs associated with fake-news keywords and hashtags, when compared to
information not labeled as ""fake news"". We discuss the impact of our findings
on the challenges of tracking ""fake news"" in the ongoing battle against
misinformation.",tracking fake reviews
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",tracking fake reviews
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",tracking fake reviews
http://arxiv.org/abs/1810.08885v2,"Fraud has severely detrimental impacts on the business of social networks and
other online applications. A user can become a fake celebrity by purchasing
""zombie followers"" on Twitter. A merchant can boost his reputation through fake
reviews on Amazon. This phenomenon also conspicuously exists on Facebook, Yelp
and TripAdvisor, etc. In all the cases, fraudsters try to manipulate the
platform's ranking mechanism by faking interactions between the fake accounts
they control and the target customers.",tracking fake reviews
http://arxiv.org/abs/1907.09177v1,"Advanced neural language models (NLMs) are widely used in sequence generation
tasks because they are able to produce fluent and meaningful sentences. They
can also be used to generate fake reviews, which can then be used to attack
online review systems and influence the buying decisions of online shoppers. A
problem in fake review generation is how to generate the desired
sentiment/topic. Existing solutions first generate an initial review based on
some keywords and then modify some of the words in the initial review so that
the review has the desired sentiment/topic. We overcome this problem by using
the GPT-2 NLM to generate a large number of high-quality reviews based on a
review with the desired sentiment and then using a BERT based text classifier
(with accuracy of 96\%) to filter out reviews with undesired sentiments.
Because none of the words in the review are modified, fluent samples like the
training data can be generated from the learned distribution. A subjective
evaluation with 80 participants demonstrated that this simple method can
produce reviews that are as fluent as those written by people. It also showed
that the participants tended to distinguish fake reviews randomly. Two
countermeasures, GROVER and GLTR, were found to be able to accurately detect
fake review.",tracking fake reviews
http://arxiv.org/abs/1803.03443v3,"Social media can be a double-edged sword for society, either as a convenient
channel exchanging ideas or as an unexpected conduit circulating fake news
through a large population. While existing studies of fake news focus on
theoretical modeling of propagation or identification methods based on machine
learning, it is important to understand the realistic mechanisms between
theoretical models and black-box methods. Here we track large databases of fake
news and real news in both, Weibo in China and Twitter in Japan from different
culture, which include their complete traces of re-postings. We find in both
online social networks that fake news spreads distinctively from real news even
at early stages of propagation, e.g. five hours after the first re-postings.
Our finding demonstrates collective structural signals that help to understand
the different propagation evolution of fake news and real news. Different from
earlier studies, identifying the topological properties of the information
propagation at early stages may offer novel features for early detection of
fake news in social media.",tracking fake reviews
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",tracking fake reviews
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",tracking fake reviews
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",tracking fake reviews
http://arxiv.org/abs/1611.09900v1,"This paper studied generating natural languages at particular contexts or
situations. We proposed two novel approaches which encode the contexts into a
continuous semantic representation and then decode the semantic representation
into text sequences with recurrent neural networks. During decoding, the
context information are attended through a gating mechanism, addressing the
problem of long-range dependency caused by lengthy sequences. We evaluate the
effectiveness of the proposed approaches on user review data, in which rich
contexts are available and two informative contexts, sentiments and products,
are selected for evaluation. Experiments show that the fake reviews generated
by our approaches are very natural. Results of fake review detection with human
judges show that more than 50\% of the fake reviews are misclassified as the
real reviews, and more than 90\% are misclassified by existing state-of-the-art
fake review detection algorithm.",tracking fake reviews
http://arxiv.org/abs/1811.06002v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip and GEM detectors due to appearance in
them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. We introduce a renewed method
that is a significant improvement of our previous two-stage approach based on
hit preprocessing using directed K-d tree search followed a deep neural
classifier. We combine these two stages in one by applying recurrent neural
network that simultaneously determines whether a set of points belongs to a
true track or not and predicts where to look for the next point of track on the
next coordinate plane of the detector. We show that proposed deep network is
more accurate, faster and does not require any special preprocessing stage.
Preliminary results of our approach for simulated events of the BM@N GEM
detector are presented.",tracking fake reviews
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",tracking fake reviews
http://arxiv.org/abs/1904.05386v1,"The rise of ubiquitous misinformation, disinformation, propaganda and
post-truth, often referred to as fake news, raises some concerns over the role
of Internet and social media in modern democratic societies. Due to its rapid
and widespread diffusion, online fake news have not only an individual or
societal cost (e.g., hamper the integrity of elections), but they can lead to
significant economic losses (e.g., affect stock market performance) or risks to
national security. Blockchain and other Distributed Ledger Technologies (DLTs)
guarantee the provenance and traceability of the data by providing a
transparent, immutable and verifiable record of transactions while creating a
peer-to-peer platform for exchanging, storing and securing information. This
article aims to explore the potential of DLTs and blockchain to combat fake
news, reviewing initiatives that are currently under development and
identifying their main current challenges. Moreover, some recommendations are
enumerated to guide future researchers on issues that will have to be tackled
to face fake news, as an integral part of strengthening the resilience against
cyber-threats of today's online media.",tracking fake reviews
http://arxiv.org/abs/1812.03859v1,"One of the most important problems of data processing in high energy and
nuclear physics is the event reconstruction. Its main part is the track
reconstruction procedure which consists in looking for all tracks that
elementary particles leave when they pass through a detector among a huge
number of points, so-called hits, produced when flying particles fire detector
coordinate planes. Unfortunately, the tracking is seriously impeded by the
famous shortcoming of multiwired, strip in GEM detectors due to the appearance
in them a lot of fake hits caused by extra spurious crossings of fired strips.
Since the number of those fakes is several orders of magnitude greater than for
true hits, one faces with the quite serious difficulty to unravel possible
track-candidates via true hits ignoring fakes. On the basis of our previous
two-stage approach based on hits preprocessing using directed K-d tree search
followed by a deep neural classifier we introduce here two new tracking
algorithms. Both algorithms combine those two stages in one while using
different types of deep neural nets. We show that both proposed deep networks
do not require any special preprocessing stage, are more accurate, faster and
can be easier parallelized. Preliminary results of our new approaches for
simulated events are presented.",tracking fake reviews
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",tracking fake reviews
http://arxiv.org/abs/1805.02400v4,"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible.",tracking fake reviews
http://arxiv.org/abs/1408.5536v1,"The hit combinatorial problem is a main challenge for track reconstruction
and triggering at high rate experiments. At hadron colliders the dominant
fraction of hits is due to low momentum tracks for which multiple scattering
(MS) effects dominate the hit resolution. MS is also the dominating source for
hit confusion and track uncertainties in low energy precision experiments. In
all such environments, where MS dominates, track reconstruction and fitting can
be largely simplified by using three-dimensional (3D) hit-triplets as provided
by pixel detectors. This simplification is possible since track uncertainties
are solely determined by MS if high precision spatial information is provided.
Fitting of hit-triplets is especially simple for tracking detectors in
solenoidal magnetic fields. The over-constrained 3D-triplet method provides a
complete set of track parameters and is robust against fake hit combinations.
The triplet method is ideally suited for pixel detectors where hits can be
treated as 3D-space points. With the advent of relatively cheap and
industrially available CMOS-sensors the construction of highly granular full
scale pixel tracking detectors seems to be possible also for experiments at LHC
or future high energy (hadron) colliders. In this paper tracking performance
studies for full-scale pixel detectors, including their optimisation for
3D-triplet tracking, are presented. The results obtained for different types of
tracker geometries and different reconstruction methods are compared. The
potential of reducing the number of tracking layers and -- along with that --
the material budget using this new tracking concept is discussed. The
possibility of using 3D-triplet tracking for triggering and fast online
reconstruction is highlighted.",tracking fake reviews
http://arxiv.org/abs/1808.09922v1,"Today's social media platforms enable to spread both authentic and fake news
very quickly. Some approaches have been proposed to automatically detect such
""fake"" news based on their content, but it is difficult to agree on universal
criteria of authenticity (which can be bypassed by adversaries once known).
Besides, it is obviously impossible to have each news item checked by a human.
  In this paper, we a mechanism to limit the spread of fake news which is not
based on content. It can be implemented as a plugin on a social media platform.
The principle is as follows: a team of fact-checkers reviews a small number of
news items (the most popular ones), which enables to have an estimation of each
user's inclination to share fake news items. Then, using a Bayesian approach,
we estimate the trustworthiness of future news items, and treat accordingly
those of them that pass a certain ""untrustworthiness"" threshold.
  We then evaluate the effectiveness and overhead of this technique on a large
Twitter graph. We show that having a few thousands users exposed to one given
news item enables to reach a very precise estimation of its reliability. We
thus identify more than 99% of fake news items with no false positives. The
performance impact is very small: the induced overhead on the 90th percentile
latency is less than 3%, and less than 8% on the throughput of user operations.",tracking fake reviews
http://arxiv.org/abs/1811.12349v2,"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.",tracking fake reviews
http://arxiv.org/abs/1512.09008v1,"We propose a novel fast track finding system capable of reconstructing four
dimensional particle trajectories in real time using precise space and time
information of the hits. Recent developments in silicon pixel detectors
achieved 150 ps time resolution and intense R&D is in progress to improve the
timing performance, aiming at 10 ps. The use of the precise space and time
information allows the suppression of background hits not compatible with the
time of passage of the particle and the determination of its time evolution.
The fast track finding device that we are proposing is based on a massively
parallel algorithm implemented in commercial field-programmable gate array
using a pipelined architecture. We describe the algorithm and its
implementation for a tracking system prototype based on 8 planes of silicon
sensors used as a case study. According to simulations the suppression of noise
hits is effective in reducing fake track combinations and improving real-time
track reconstruction in presence of background hits. The system provides
offline-like tracks with sub-microsecond latency and it is capable to determine
the time of the track with picosecond resolution assuming 10 ps resolution for
the hits.",tracking fake reviews
http://arxiv.org/abs/1807.11024v1,"Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.",tracking fake reviews
http://arxiv.org/abs/1303.3630v1,"The ATLAS Inner Detector is responsible for particle tracking in ATLAS
experiment at CERN Large Hadron Collider (LHC) and comprises silicon and gas
based detectors. The combination of both silicon and gas based detectors
provides high precision impact parameter and momentum measurement of charged
particles, with high efficiency and small fake rate. The ID has been used to
exploit fully the physics potential of the LHC since the first proton-proton
collisions at 7 TeV were delivered in 2009. The performance of track and vertex
reconstruction is presented, as well as the operation aspects of the Inner
Detector and the data quality during the many months of data taking.",tracking fake reviews
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",tracking fake reviews
http://arxiv.org/abs/1603.01511v1,"Massive amounts of misinformation have been observed to spread in
uncontrolled fashion across social media. Examples include rumors, hoaxes, fake
news, and conspiracy theories. At the same time, several journalistic
organizations devote significant efforts to high-quality fact checking of
online claims. The resulting information cascades contain instances of both
accurate and inaccurate information, unfold over multiple time scales, and
often reach audiences of considerable size. All these factors pose challenges
for the study of the social dynamics of online news sharing. Here we introduce
Hoaxy, a platform for the collection, detection, and analysis of online
misinformation and its related fact-checking efforts. We discuss the design of
the platform and present a preliminary analysis of a sample of public tweets
containing both fake news and fact checking. We find that, in the aggregate,
the sharing of fact-checking content typically lags that of misinformation by
10--20 hours. Moreover, fake news are dominated by very active users, while
fact checking is a more grass-roots activity. With the increasing risks
connected to massive online misinformation, social news observatories have the
potential to help researchers, journalists, and the general public understand
the dynamics of real and fake news sharing.",tracking fake reviews
http://arxiv.org/abs/0810.3348v2,"We briefly present the eXtremely Fast Tracker stereo track upgrade for the
CDF Level 2 trigger system. This upgrade enabled full 3D track reconstruction
at Level 2 of the 3-Level CDF online triggering system. Using information
provided by the stereo layers of the Central Outer Tracker, we can decrease the
trigger rate due to fake tracks by requiring the tracks to be consistent with a
single vertex in all three dimensions but also by using the track information
to ""point"" to the various detector components. We will also discuss the
effectiveness of the Level 2 stereo track algorithm at achieving reduced
trigger rates with high efficiencies during high luminosity running.",tracking fake reviews
http://arxiv.org/abs/1908.00256v1,"Conformal tracking is an innovative and comprehensive pattern recognition
technique using a cellular automaton-based track finding performed in a
conformally-mapped space. It is particularly well-suited for light-weight
silicon systems with high position resolution, such as the next generation of
tracking detectors designed for future electron-positron colliders. The
algorithm has been developed and validated with simulated data of the CLICdet
tracker. It has demonstrated not only excellent performance in terms of
tracking efficiency, fake rate and track parameters resolution but also
robustness against the high beam-induced background levels. Thanks to its
geometry-agnostic nature and its modularity, the algorithm is very flexible and
can easily be adapted to other detector designs and experimental environments
at future $e^+e^-$ colliders.",tracking fake reviews
http://arxiv.org/abs/physics/0412069v1,"The problem of inferring the binomial parameter p from x successes obtained
in n trials is reviewed and extended to take into account the presence of
background, that can affect the data in two ways: a) fake successes are due to
a background modeled as a Poisson process of known intensity; b) fake trials
are due to a background modeled as a Poisson process of known intensity, each
trial being characterized by a known success probability p_b.",tracking fake reviews
http://arxiv.org/abs/0710.2818v1,"A track finding algorithm has been developed for reconstruction of e+e-
pairs. It combines the information of the electromagnetic calorimeter with the
information provided by the Tracker. Results on reconstruction efficiency of
converted photons, as well as on fake rate are shown for single isolated
photons and for photons from H->gamma gamma events with pile-up events at 10^33
cm^-2 s^-1 LHC luminosity.",tracking fake reviews
http://arxiv.org/abs/1706.02693v2,"Data ecosystems are becoming larger and more complex due to online tracking,
wearable computing, and the Internet of Things. But privacy concerns are
threatening to erode the potential benefits of these systems. Recently, users
have developed obfuscation techniques that issue fake search engine queries,
undermine location tracking algorithms, or evade government surveillance.
Interestingly, these techniques raise two conflicts: one between each user and
the machine learning algorithms which track the users, and one between the
users themselves. In this paper, we use game theory to capture the first
conflict with a Stackelberg game and the second conflict with a mean field
game. We combine both into a dynamic and strategic bi-level framework which
quantifies accuracy using empirical risk minimization and privacy using
differential privacy. In equilibrium, we identify necessary and sufficient
conditions under which 1) each user is incentivized to obfuscate if other users
are obfuscating, 2) the tracking algorithm can avoid this by promising a level
of privacy protection, and 3) this promise is incentive-compatible for the
tracking algorithm.",tracking fake reviews
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",tracking fake reviews
http://arxiv.org/abs/1509.04098v2,"$\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.",tracking fake reviews
http://arxiv.org/abs/1210.4517v1,"The proliferation of location-based social networks (LBSNs) has provided the
community with an abundant source of information that can be exploited and used
in many different ways. LBSNs offer a number of conveniences to its
participants, such as - but not limited to - a list of places in the vicinity
of a user, recommendations for an area never explored before provided by other
peers, tracking of friends, monetary rewards in the form of special deals from
the venues visited as well as a cheap way of advertisement for the latter.
However, service convenience and security have followed disjoint paths in LBSNs
and users can misuse the offered features. The major threat for the service
providers is that of fake check-ins. Users can easily manipulate the
localization module of the underlying application and declare their presence in
a counterfeit location. The incentives for these behaviors can be both earning
monetary as well as virtual rewards. Therefore, while fake check-ins driven
from the former motive can cause monetary losses, those aiming in virtual
rewards are also harmful. In particular, they can significantly degrade the
services offered from the LBSN providers (such as recommendations) or third
parties using these data (e.g., urban planners). In this paper, we propose and
analyze a honeypot venue-based solution, enhanced with a challenge-response
scheme, that flags users who are generating fake spatial information. We
believe that our work will stimulate further research on this important topic
and will provide new directions with regards to possible solutions.",tracking fake reviews
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",web beacon behavioral tracking
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",web beacon behavioral tracking
http://arxiv.org/abs/1709.10237v1,"Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.",web beacon behavioral tracking
http://arxiv.org/abs/1503.03388v1,"This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.",web beacon behavioral tracking
http://arxiv.org/abs/1706.05569v3,"In this paper, we develop a system for the low-cost indoor localization and
tracking problem using radio signal strength indicator, Inertial Measurement
Unit (IMU), and magnetometer sensors. We develop a novel and simplified
probabilistic IMU motion model as the proposal distribution of the sequential
Monte-Carlo technique to track the robot trajectory. Our algorithm can globally
localize and track a robot with a priori unknown location, given an informative
prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the
problem as an optimization problem that serves as the Back-end of the algorithm
mentioned above (Front-end). Thus, by simultaneously solving for the robot
trajectory and the map of BLE beacons, we recover a continuous and smooth
trajectory of the robot, corrected locations of the BLE beacons, and the
time-varying IMU bias. The evaluations achieved using hardware show that
through the proposed closed-loop system the localization performance can be
improved; furthermore, the system becomes robust to the error in the map of
beacons by feeding back the optimized map to the Front-end.",web beacon behavioral tracking
http://arxiv.org/abs/1812.01514v3,"Web tracking has been extensively studied over the last decade. To detect
tracking, previous studies and user tools rely on filter lists. However, it has
been shown that filter lists miss trackers. In this paper, we propose an
alternative method to detect trackers inspired by analyzing behavior of
invisible pixels. By crawling 84,658 webpages from 8,744 domains, we detect
that third-party invisible pixels are widely deployed: they are present on more
than 94.51% of domains and constitute 35.66% of all third-party images. We
propose a fine-grained behavioral classification of tracking based on the
analysis of invisible pixels. We use this classification to detect new
categories of tracking and uncover new collaborations between domains on the
full dataset of 4,216,454 third-party requests. We demonstrate that two popular
methods to detect tracking, based on EasyList&EasyPrivacy and on Disconnect
lists respectively miss 25.22% and 30.34% of the trackers that we detect.
Moreover, we find that if we combine all three lists 379,245 requests
originated from 8,744 domains still track users on 68.70% of websites.",web beacon behavioral tracking
http://arxiv.org/abs/1904.01725v1,"Web traffic is a valuable data source, typically used in the marketing space
to track brand awareness and advertising effectiveness. However, web traffic is
also a rich source of information for cybersecurity monitoring efforts. To
better understand the threat of malicious cyber actors, this study develops a
methodology to monitor and evaluate web activity using data archived from
Google Analytics. Google Analytics collects and aggregates web traffic,
including information about web visitors' location, date and time of visit,
visited webpages, and searched keywords. This study seeks to streamline
analysis of this data and uses rule-based anomaly detection and predictive
modeling to identify web traffic that deviates from normal patterns. Rather
than evaluating pieces of web traffic individually, the methodology seeks to
emulate real user behavior by creating a new unit of analysis: the user
session. User sessions group individual pieces of traffic from the same
location and date, which transforms the available information from single
point-in-time snapshots to dynamic sessions showing users' trajectory and
intent. The result is faster and better insight into large volumes of noisy web
traffic.",web beacon behavioral tracking
http://arxiv.org/abs/1901.06601v1,"Recent years have seen interest in device tracking and localization using
acoustic signals. State-of-the-art acoustic motion tracking systems however do
not achieve millimeter accuracy and require large separation between
microphones and speakers, and as a result, do not meet the requirements for
many VR/AR applications. Further, tracking multiple concurrent acoustic
transmissions from VR devices today requires sacrificing accuracy or frame
rate. We present MilliSonic, a novel system that pushes the limits of acoustic
based motion tracking. Our core contribution is a novel localization algorithm
that can provably achieve sub-millimeter 1D tracking accuracy in the presence
of multipath, while using only a single beacon with a small 4-microphone
array.Further, MilliSonic enables concurrent tracking of up to four smartphones
without reducing frame rate or accuracy. Our evaluation shows that MilliSonic
achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for
smartphones, which is 5x more accurate than state-of-the-art systems.
MilliSonic enables two previously infeasible interaction applications: a) 3D
tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D
tracking for the Google Cardboard VR system using a small microphone array.",web beacon behavioral tracking
http://arxiv.org/abs/1702.05116v2,"Cyclic pursuit frameworks, which are built upon pursuit interactions between
neighboring agents in a cycle graph, provide an efficient way to create useful
global behaviors in a collective of autonomous robots. Previous work had
considered cyclic pursuit with a constant bearing (CB) pursuit law, and
demonstrated the existence of circling equilibria for the corresponding
dynamics. In this work, we propose a beacon-referenced version of the CB
pursuit law, wherein a stationary beacon provides an additional reference for
the individual agents in a collective. When implemented in a cyclic framework,
we show that the resulting dynamics admit relative equilibria corresponding to
a circling orbit around the beacon, with the circling radius and the
distribution of agents along the orbit determined by parameters of the proposed
pursuit law. We also derive necessary conditions for stability of the circling
equilibria, which provides a guide for parameter selection. Finally, by
introducing a change of variables, we demonstrate the existence of a family of
invariant manifolds related to spiraling motions around the beacon which
preserve the ""pure shape"" of the collective, and study the reduced dynamics on
a representative manifold.",web beacon behavioral tracking
http://arxiv.org/abs/1506.05367v1,"We propose and investigate a compressive architecture for estimation and
tracking of sparse spatial channels in millimeter (mm) wave picocellular
networks. The base stations are equipped with antenna arrays with a large
number of elements (which can fit within compact form factors because of the
small carrier wavelength) and employ radio frequency (RF) beamforming, so that
standard least squares adaptation techniques (which require access to
individual antenna elements) are not applicable. We focus on the downlink, and
show that ""compressive beacons,"" transmitted using pseudorandom phase settings
at the base station array, and compressively processed using pseudorandom phase
settings at the mobile array, provide information sufficient for accurate
estimation of the two-dimensional (2D) spatial frequencies associated with the
directions of departure of the dominant rays from the base station, and the
associated complex gains. This compressive approach is compatible with coarse
phase-only control, and is based on a near-optimal sequential algorithm for
frequency estimation which can exploit the geometric continuity of the channel
across successive beaconing intervals to reduce the overhead to less than 1%
even for very large (32 x 32) arrays. Compressive beaconing is essentially
omnidirectional, and hence does not enjoy the SNR and spatial reuse benefits of
beamforming obtained during data transmission. We therefore discuss system
level design considerations for ensuring that the beacon SNR is sufficient for
accurate channel estimation, and that inter-cell beacon interference is
controlled by an appropriate reuse scheme.",web beacon behavioral tracking
http://arxiv.org/abs/1603.09533v1,"Despite current controversy over e-cigarettes as a smoking cessation aid, we
present early work based on a web survey (N=249) that shows that some
e-cigarette users (46.2%) want to quit altogether, and that behavioral feedback
that can be tracked can fulfill that purpose. Based on our survey findings, we
designed VapeTracker, an early prototype that can attach to any e-cigarette
device to track vaping activity. We discuss our future research on vaping
cessation, addressing how to improve our VapeTracker prototype, ambient
feedback mechanisms, and the future inclusion of behavior change models to
support quitting e-cigarettes.",web beacon behavioral tracking
http://arxiv.org/abs/1906.00166v1,"Websites employ third-party ads and tracking services leveraging cookies and
JavaScript code, to deliver ads and track users' behavior, causing privacy
concerns. To limit online tracking and block advertisements, several
ad-blocking (black) lists have been curated consisting of URLs and domains of
well-known ads and tracking services. Using Internet Archive's Wayback Machine
in this paper, we collect a retrospective view of the Web to analyze the
evolution of ads and tracking services and evaluate the effectiveness of
ad-blocking blacklists. We propose metrics to capture the efficacy of
ad-blocking blacklists to investigate whether these blacklists have been
reactive or proactive in tackling the online ad and tracking services. We
introduce a stability metric to measure the temporal changes in ads and
tracking domains blocked by ad-blocking blacklists, and a diversity metric to
measure the ratio of new ads and tracking domains detected. We observe that ads
and tracking domains in websites change over time, and among the ad-blocking
blacklists that we investigated, our analysis reveals that some blacklists were
more informed with the existence of ads and tracking domains, but their rate of
change was slower than other blacklists. Our analysis also shows that Alexa top
5K websites in the US, Canada, and the UK have the most number of ads and
tracking domains per website, and have the highest proactive scores. This
suggests that ad-blocking blacklists are updated by prioritizing ads and
tracking domains reported in the popular websites from these countries.",web beacon behavioral tracking
http://arxiv.org/abs/1902.04262v1,"In Interactive Information Retrieval (IIR) experiments the user's gaze motion
on web pages is often recorded with eye tracking. The data is used to analyze
gaze behavior or to identify Areas of Interest (AOI) the user has looked at. So
far, tools for analyzing eye tracking data have certain limitations in
supporting the analysis of gaze behavior in IIR experiments. Experiments often
consist of a huge number of different visited web pages. In existing analysis
tools the data can only be analyzed in videos or images and AOIs for every
single web page have to be specified by hand, in a very time consuming process.
In this work, we propose the reading protocol software which breaks eye
tracking data down to the textual level by considering the HTML structure of
the web pages. This has a lot of advantages for the analyst. First and
foremost, it can easily be identified on a large scale what has actually been
viewed and read on the stimuli pages by the subjects. Second, the web page
structure can be used to filter to AOIs. Third, gaze data of multiple users can
be presented on the same page, and fourth, fixation times on text can be
exported and further processed in other tools. We present the software, its
validation, and example use cases with data from three existing IIR
experiments.",web beacon behavioral tracking
http://arxiv.org/abs/1204.3141v1,"Location information of sensor nodes has become an essential part of many
applications in Wireless Sensor Networks (WSN). The importance of location
estimation and object tracking has made them the target of many security
attacks. Various methods have tried to provide location information with high
accuracy, while lots of them have neglected the fact that WSNs may be deployed
in hostile environments. In this paper, we address the problem of securely
tracking a Mobile Node (MN) which has been noticed very little previously. A
novel secure tracking algorithm is proposed based on Extended Kalman Filter
(EKF) that is capable of tracking a Mobile Node (MN) with high resolution in
the presence of compromised or colluding malicious beacon nodes. It filters out
and identifies the malicious beacon data in the process of tracking. The
proposed method considerably outperforms the previously proposed secure
algorithms in terms of either detection rate or MSE. The experimental data
based on different settings for the network has shown promising results.",web beacon behavioral tracking
http://arxiv.org/abs/1811.06193v1,"Tracking users' activities on the World Wide Web (WWW) allows researchers to
analyze each user's internet behavior as time passes and for the amount of time
spent on a particular domain. This analysis can be used in research design, as
researchers may access to their participant's behaviors while browsing the web.
Web search behavior has been a subject of interest because of its real-world
applications in marketing, digital advertisement, and identifying potential
threats online. In this paper, we present an image-processing based method to
extract domains which are visited by a participant over multiple browsers
during a lab session. This method could provide another way to collect users'
activities during an online session given that the session recorder collected
the data. The method can also be used to collect the textual content of
web-pages that an individual visits for later analysis",web beacon behavioral tracking
http://arxiv.org/abs/1801.07759v1,"Web cookies are ubiquitously used to track and profile the behavior of users.
Although there is a solid empirical foundation for understanding the use of
cookies in the global world wide web, thus far, limited attention has been
devoted for country-specific and company-level analysis of cookies. To patch
this limitation in the literature, this paper investigates persistent
third-party cookies used in the Finnish web. The exploratory results reveal
some similarities and interesting differences between the Finnish and the
global web---in particular, popular Finnish web sites are mostly owned by media
companies, which have established their distinct partnerships with online
advertisement companies. The results reported can be also reflected against
current and future privacy regulation in the European Union.",web beacon behavioral tracking
http://arxiv.org/abs/1907.02142v1,"Open access WiFi hotspots are widely deployed in many public places,
including restaurants, parks, coffee shops, shopping malls, trains, airports,
hotels, and libraries. While these hotspots provide an attractive option to
stay connected, they may also track user activities and share user/device
information with third-parties, through the use of trackers in their captive
portal and landing websites. In this paper, we present a comprehensive privacy
analysis of 67 unique public WiFi hotspots located in Montreal, Canada, and
shed some light on the web tracking and data collection behaviors of these
hotspots. Our study reveals the collection of a significant amount of
privacy-sensitive personal data through the use of social login (e.g., Facebook
and Google) and registration forms, and many instances of tracking activities,
sometimes even before the user accepts the hotspot's privacy and terms of
service policies. Most hotspots use persistent third-party tracking cookies
within their captive portal site; these cookies can be used to follow the
user's browsing behavior long after the user leaves the hotspots, e.g., up to
20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)
the collected personal and unique device information with many third-party
tracking domains.",web beacon behavioral tracking
http://arxiv.org/abs/1612.00766v3,"Several studies have been conducted on understanding third-party user
tracking on the web. However, web trackers can only track users on sites where
they are embedded by the publisher, thus obtaining a fragmented view of a
user's online footprint. In this work, we investigate a different form of user
tracking, where browser extensions are repurposed to capture the complete
online activities of a user and communicate the collected sensitive information
to a third-party domain. We conduct an empirical study of spying browser
extensions on the Chrome Web Store. First, we present an in-depth analysis of
the spying behavior of these extensions. We observe that these extensions steal
a variety of sensitive user information, such as the complete browsing history
(e.g., the sequence of web traversals), online social network (OSN) access
tokens, IP address, and user geolocation. Second, we investigate the potential
for automatically detecting spying extensions by applying machine learning
schemes. We show that using a Recurrent Neural Network (RNN), the sequences of
browser API calls can be a robust feature, outperforming hand-crafted features
(used in prior work on malicious extensions) to detect spying extensions. Our
RNN based detection scheme achieves a high precision (90.02%) and recall
(93.31%) in detecting spying extensions.",web beacon behavioral tracking
http://arxiv.org/abs/1307.1542v1,"The investigation of the browsing behavior of users provides useful
information to optimize web site design, web browser design, search engines
offerings, and online advertisement. This has been a topic of active research
since the Web started and a large body of work exists. However, new online
services as well as advances in Web and mobile technologies clearly changed the
meaning behind ""browsing the Web"" and require a fresh look at the problem and
research, specifically in respect to whether the used models are still
appropriate. Platforms such as YouTube, Netflix or last.fm have started to
replace the traditional media channels (cinema, television, radio) and media
distribution formats (CD, DVD, Blu-ray). Social networks (e.g., Facebook) and
platforms for browser games attracted whole new, particularly less tech-savvy
audiences. Furthermore, advances in mobile technologies and devices made
browsing ""on-the-move"" the norm and changed the user behavior as in the mobile
case browsing is often being influenced by the user's location and context in
the physical world. Commonly used datasets, such as web server access logs or
search engines transaction logs, are inherently not capable of capturing the
browsing behavior of users in all these facets. DOBBS (DERI Online Behavior
Study) is an effort to create such a dataset in a non-intrusive, completely
anonymous and privacy-preserving way. To this end, DOBBS provides a browser
add-on that users can install, which keeps track of their browsing behavior
(e.g., how much time they spent on the Web, how long they stay on a website,
how often they visit a website, how they use their browser, etc.). In this
paper, we outline the motivation behind DOBBS, describe the add-on and captured
data in detail, and present some first results to highlight the strengths of
DOBBS.",web beacon behavioral tracking
http://arxiv.org/abs/1003.5325v1,"We examine the properties of all HTTP requests generated by a thousand
undergraduates over a span of two months. Preserving user identity in the data
set allows us to discover novel properties of Web traffic that directly affect
models of hypertext navigation. We find that the popularity of Web sites -- the
number of users who contribute to their traffic -- lacks any intrinsic mean and
may be unbounded. Further, many aspects of the browsing behavior of individual
users can be approximated by log-normal distributions even though their
aggregate behavior is scale-free. Finally, we show that users' click streams
cannot be cleanly segmented into sessions using timeouts, affecting any attempt
to model hypertext navigation using statistics of individual sessions. We
propose a strictly logical definition of sessions based on browsing activity as
revealed by referrer URLs; a user may have several active sessions in their
click stream at any one time. We demonstrate that applying a timeout to these
logical sessions affects their statistics to a lesser extent than a purely
timeout-based mechanism.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04103v1,"Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",web beacon behavioral tracking
http://arxiv.org/abs/1708.05625v2,"Simultaneous Localization and Mapping (SLAM) systems use commodity
visible/near visible digital sensors coupled with processing units that detect,
recognize and track image points in a camera stream. These systems are cheap,
fast and make use of readily available camera technologies. However, SLAM
systems suffer from issues of drift as well as sensitivity to lighting
variation such as shadows and changing brightness. Beaconless SLAM systems will
continue to suffer from this inherent drift problem irrespective of the
improvements in on-board camera resolution, speed and inertial sensor
precision. To cancel out destructive forms of drift, relocalization algorithms
are used which use known detected landmarks together with loop closure
processes to continually readjust the current location and orientation
estimates to match ""known"" positions. However this is inherently problematic
because these landmarks themselves may have been recorded with errors and they
may also change under different illumination conditions. In this note we
describe a unique beacon light coding system which is robust to desynchronized
clock bit drift. The described beacons and codes are designed to be used in
industrial or consumer environments for full standalone 6dof tracking or as
known error free landmarks in a SLAM pipeline.",web beacon behavioral tracking
http://arxiv.org/abs/1904.12568v1,"Quality of Experience (QoE) typically involves conducting experiments in
which stimuli are presented to participants and their judgments as well as
behavioral data are collected. Nowadays, many experiments require software for
the presentation of stimuli and the data collection from participants. While
different software solutions exist, these are not tailored to conduct
experiments on QoE. Moreover, replicating experiments or repeating the same
experiment in different settings (e. g., laboratory vs. crowdsourcing) can
further increase the software complexity. TheFragebogen is an open-source,
versatile, extendable software framework for the implementation of
questionnaires - especially for research on QoE. Implemented questionnaires can
be presented with a state-of-the-art web browser to support a broad range of
devices while the use of a web server being optional. Out-of-the-box,
TheFragebogen provides graphical exact scales as well as free-hand input, the
ability to collect behavioral data, and playback multimedia content.",web beacon behavioral tracking
http://arxiv.org/abs/1703.05174v1,"The intelligent transportation systems (ITS) framework from European
Telecommunication Standards Institute (ETSI) imposes requirements on the
exchange of periodic safety messages between components of ITS such as
vehicles. In particular, it requires ETSI standardized Decentralized Congestion
Control (DCC) algorithm to regulate the beaconing activity of vehicles based on
wireless channel utilization. However, the DCC state that defines the beaconing
behavior under heavy channel congestion, i.e., the Restrictive state, has a
serious connectivity problem that safety beacons do not reach other vehicles in
safety-critical distances. In this paper, we demonstrate the problem through
analysis, simulation, and on-road measurements. We suggest that DCC change the
transmit power setting for the Restrictive state before a full-scale deployment
of the ETSI ITS framework starts, and we discuss its consequences in terms of
changes in communicability and channel utilization.",web beacon behavioral tracking
http://arxiv.org/abs/1507.03509v1,"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger.""",web beacon behavioral tracking
http://arxiv.org/abs/1802.01050v1,"WebAssembly (wasm) has recently emerged as a promisingly portable,
size-efficient, fast, and safe binary format for the web. As WebAssembly can
interact freely with JavaScript libraries, this gives rise to a potential for
undesirable behavior to occur. It is therefore important to be able to detect
when this might happen. A way to do this is through taint tracking, where we
follow the flow of information by applying taint labels to data. In this paper,
we describe TaintAssembly, a taint tracking engine for interpreted WebAssembly,
that we have created by modifying the V8 JavaScript engine. We implement basic
taint tracking functionality, taint in linear memory, and a probabilistic
variant of taint. We then benchmark our TaintAssembly engine by incorporating
it into a Chromium build and running it on custom test scripts and various real
world WebAssembly applications. We find that our modifications to the V8 engine
do not incur significant overhead with respect to vanilla V8's interpreted
WebAssembly, making TaintAssembly suitable for development and debugging.",web beacon behavioral tracking
http://arxiv.org/abs/1611.06417v1,"Nowadays, many web databases ""hidden"" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.",web beacon behavioral tracking
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",web beacon behavioral tracking
http://arxiv.org/abs/1706.02577v1,"1. Behavioral analysis based on video recording is becoming increasingly
popular within research fields such as; ecology, medicine, ecotoxicology, and
toxicology. However, the programs available to analyze the data, which are;
free of cost, user-friendly, versatile, robust, fast and provide reliable
statistics for different organisms (invertebrates, vertebrates and mammals) are
significantly limited.
  2. We present an automated open-source executable software (ToxTrac) for
image-based tracking that can simultaneously handle several organisms monitored
in a laboratory environment. We compare the performance of ToxTrac with current
accessible programs on the web.
  3. The main advantages of ToxTrac are: i) no specific knowledge of the
geometry of the tracked bodies is needed; ii) processing speed, ToxTrac can
operate at a rate >25 frames per second in HD videos using modern desktop
computers; iii) simultaneous tracking of multiple organisms in multiple arenas;
iv) integrated distortion correction and camera calibration; v) robust against
false positives; vi) preservation of individual identification if crossing
occurs; vii) useful statistics and heat maps in real scale are exported in:
image, text and excel formats.
  4. ToxTrac can be used for high speed tracking of insects, fish, rodents or
other species, and provides useful locomotor information. We suggest using
ToxTrac for future studies of animal behavior independent of research area.
Download ToxTrac here: https://toxtrac.sourceforge.io",web beacon behavioral tracking
http://arxiv.org/abs/1011.3768v1,"Online social media are complementing and in some cases replacing
person-to-person social interaction and redefining the diffusion of
information. In particular, microblogs have become crucial grounds on which
public relations, marketing, and political battles are fought. We introduce an
extensible framework that will enable the real-time analysis of meme diffusion
in social media by mining, visualizing, mapping, classifying, and modeling
massive streams of public microblogging events. We describe a Web service that
leverages this framework to track political memes in Twitter and help detect
astroturfing, smear campaigns, and other misinformation in the context of U.S.
political elections. We present some cases of abusive behaviors uncovered by
our service. Finally, we discuss promising preliminary results on the detection
of suspicious memes via supervised learning based on features extracted from
the topology of the diffusion networks, sentiment analysis, and crowdsourced
annotations.",web beacon behavioral tracking
http://arxiv.org/abs/1805.01392v1,"Web tracking technologies are pervasive and operated by a few large
technology companies. This technology, and the use of the collected data has
been implicated in influencing elections, fake news, discrimination, and even
health decisions. Little is known about how this technology is deployed on
hospital or other health related websites. The websites of the 210 public
hospitals in the state of Illinois, USA were evaluated with a web tracker
identification tool. Web trackers were identified on 94% of hospital webs
sites, with an average of 3.5 trackers on the websites of general hospitals.
The websites of smaller critical access hospitals used an average of 2 web
trackers. The most common web tracker identified was Google Analytics, found on
74% of Illinois hospital websites. Of the web trackers discovered, 88% were
operated by Google and 26% by Facebook. In light of revelations about how web
browsing profiles have been used and misused, search bubbles, and the potential
for algorithmic discrimination hospital leadership and policy makers must
carefully consider if it is appropriate to use third party tracking technology
on hospital web sites.",web beacon behavioral tracking
http://arxiv.org/abs/1805.01130v1,"A growing body of evidence has shown that incorporating behavioral economics
principles into the design of financial incentive programs helps improve their
cost-effectiveness, promote individuals' short-term engagement, and increase
compliance in health behavior interventions. Yet, their effects on long-term
engagement have not been fully examined. In study designs where repeated
administration of incentives is required to ensure the regularity of behaviors,
the effectiveness of subsequent incentives may decrease as a result of the law
of diminishing marginal utility. In this paper, we introduce random-loss
incentive -- a new financial incentive based on loss aversion and
unpredictability principles -- to address the problem of individuals' growing
insensitivity to repeated interventions over time. We evaluate the new
incentive design by conducting a randomized controlled trial to measure the
influences of random losses on participants' dietary self-tracking and
self-reporting compliance using a mobile web application called Eat & Tell. The
results show that random losses are significantly more effective than fixed
losses in encouraging long-term engagement.",web beacon behavioral tracking
http://arxiv.org/abs/1702.02735v1,"Autonomous fixed-wing UAV landing based on differential GPS is now a
mainstream providing reliable and precise landing. But the task still remains
challenging when GPS availability is limited like for military UAVs. We discuss
a solution of this problem based on computer vision and dot markings along
stationary or makeshift runway. We focus our attempts on using infrared beacons
along with narrow-band filter as promising way to mark any makeshift runway and
utilize particle filtering to fuse both IMU and visual data. We believe that
unlike many other vision-based methods this solution is capable of tracking UAV
position up to engines stop. System overview, algorithm description and it's
evaluation on synthesized sequence along real recorded trajectory is presented.",web beacon behavioral tracking
http://arxiv.org/abs/1904.02813v1,"Activity tracking apps often make use of goals as one of their core
motivational tools. There are two critical components to this tool: setting a
goal, and subsequently achieving that goal. Despite its crucial role in how a
number of prominent self-tracking apps function, there has been relatively
little investigation of the goal-setting and achievement aspects of
self-tracking apps.
  Here we explore this issue, investigating a particular goal setting and
achievement process that is extensive, recorded, and crucial for both the app
and its users' success: weight loss goals in MyFitnessPal. We present a
large-scale study of 1.4 million users and weight loss goals, allowing for an
unprecedented detailed view of how people set and achieve their goals. We find
that, even for difficult long-term goals, behavior within the first 7 days
predicts those who ultimately achieve their goals, that is, those who lose at
least as much weight as they set out to, and those who do not. For instance,
high amounts of early weight loss, which some researchers have classified as
unsustainable, leads to higher goal achievement rates. We also show that early
food intake, self-monitoring motivation, and attitude towards the goal are
important factors. We then show that we can use our findings to predict goal
achievement with an accuracy of 79% ROC AUC just 7 days after a goal is set.
Finally, we discuss how our findings could inform steps to improve goal
achievement in self-tracking apps.",web beacon behavioral tracking
http://arxiv.org/abs/1004.1257v1,"World Wide Web is a huge repository of web pages and links. It provides
abundance of information for the Internet users. The growth of web is
tremendous as approximately one million pages are added daily. Users' accesses
are recorded in web logs. Because of the tremendous usage of web, the web log
files are growing at a faster rate and the size is becoming huge. Web data
mining is the application of data mining techniques in web data. Web Usage
Mining applies mining techniques in log data to extract the behavior of users
which is used in various applications like personalized services, adaptive web
sites, customer profiling, prefetching, creating attractive web sites etc., Web
usage mining consists of three phases preprocessing, pattern discovery and
pattern analysis. Web log data is usually noisy and ambiguous and preprocessing
is an important process before mining. For discovering patterns sessions are to
be constructed efficiently. This paper reviews existing work done in the
preprocessing stage. A brief overview of various data mining techniques for
discovering patterns, and pattern analysis are discussed. Finally a glimpse of
various applications of web usage mining is also presented.",web beacon behavioral tracking
http://arxiv.org/abs/1602.02046v1,"The intrusiveness and the increasing invasiveness of online advertising have,
in the last few years, raised serious concerns regarding user privacy and Web
usability. As a reaction to these concerns, we have witnessed the emergence of
a myriad of ad-blocking and anti-tracking tools, whose aim is to return control
to users over advertising. The problem with these technologies, however, is
that they are extremely limited and radical in their approach: users can only
choose either to block or allow all ads. With around 200 million people
regularly using these tools, the economic model of the Web ---in which users
get content free in return for allowing advertisers to show them ads--- is at
serious peril. In this paper, we propose a smart Web technology that aims at
bringing transparency to online advertising, so that users can make an informed
and equitable decision regarding ad blocking. The proposed technology is
implemented as a Web-browser extension and enables users to exert fine-grained
control over advertising, thus providing them with certain guarantees in terms
of privacy and browsing experience, while preserving the Internet economic
model. Experimental results in a real environment demonstrate the suitability
and feasibility of our approach, and provide preliminary findings on behavioral
targeting from real user browsing profiles.",web beacon behavioral tracking
http://arxiv.org/abs/1804.08424v1,"Computer Vision-based natural feature tracking is at the core of modern
Augmented Reality applications. Still, Web-based Augmented Reality typically
relies on location-based sensing (using GPS and orientation sensors) or
marker-based approaches to solve the pose estimation problem.
  We present an implementation and evaluation of an efficient natural feature
tracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our
system can track image targets at real-time frame rates tablet PCs (up to 60
Hz) and smartphones (up to 25 Hz).",web beacon behavioral tracking
http://arxiv.org/abs/1504.07858v1,"The time people spend in front of computers has been increasing steadily due
to the role computers play in modern society. Individuals who sit in front of
computers for an extended period of time, specifically with improper postures
may incur various health issues. In this work, individuals' behaviors in front
of computers are studied using web cameras. By means of non-rigid face tracking
system, data are analyzed to determine the 3D head pose, blink rate and yawn
frequency of computer users. When combining these visual cues, a system of
intelligent personal assistants for computer users is proposed.",web beacon behavioral tracking
http://arxiv.org/abs/1808.07293v1,"Privacy has deteriorated in the world wide web ever since the 1990s. The
tracking of browsing habits by different third-parties has been at the center
of this deterioration. Web cookies and so-called web beacons have been the
classical ways to implement third-party tracking. Due to the introduction of
more sophisticated technical tracking solutions and other fundamental
transformations, the use of classical image-based web beacons might be expected
to have lost their appeal. According to a sample of over thirty thousand images
collected from popular websites, this paper shows that such an assumption is a
fallacy: classical 1 x 1 images are still commonly used for third-party
tracking in the contemporary world wide web. While it seems that ad-blockers
are unable to fully block these classical image-based tracking beacons, the
paper further demonstrates that even limited information can be used to
accurately classify the third-party 1 x 1 images from other images. An average
classification accuracy of 0.956 is reached in the empirical experiment. With
these results the paper contributes to the ongoing attempts to better
understand the lack of privacy in the world wide web, and the means by which
the situation might be eventually improved.",web beacon
http://arxiv.org/abs/1507.03509v1,"Beacon attraction is a movement system whereby a robot (modeled as a point in
2D) moves in a free space so as to always locally minimize its Euclidean
distance to an activated beacon (which is also a point). This results in the
robot moving directly towards the beacon when it can, and otherwise sliding
along the edge of an obstacle. When a robot can reach the activated beacon by
this method, we say that the beacon attracts the robot. A beacon routing from
$p$ to $q$ is a sequence $b_1, b_2,$ ..., $b_{k}$ of beacons such that
activating the beacons in order will attract a robot from $p$ to $b_1$ to $b_2$
... to $b_{k}$ to $q$, where $q$ is considered to be a beacon. A routing set of
beacons is a set $B$ of beacons such that any two points $p, q$ in the free
space have a beacon routing with the intermediate beacons $b_1, b_2,$ ...,
$b_{k}$ all chosen from $B$. Here we address the question of ""how large must
such a $B$ be?"" in orthogonal polygons, and show that the answer is ""sometimes
as large as $[(n-4)/3]$, but never larger.""",web beacon
http://arxiv.org/abs/cs/0201003v1,"Random beacons-information sources that broadcast a stream of random digits
unknown by anyone beforehand-are useful for various cryptographic purposes. But
such beacons can be easily and undetectably sabotaged, so that their output is
known beforehand by a dishonest party, who can use this information to defeat
the cryptographic protocols supposedly protected by the beacon. We explore a
strategy to reduce this hazard by combining the outputs from several
noninteracting (eg spacelike-separated) beacons by XORing them together to
produce a single digit stream which is more trustworthy than any individual
beacon, being random and unpredictable if at least one of the contributing
beacons is honest. If the contributing beacons are not spacelike separated, so
that a dishonest beacon can overhear and adapt to earlier outputs of other
beacons, the beacons' trustworthiness can still be enhanced to a lesser extent
by a time sharing strategy. We point out some disadvantages of alternative
trust amplification methods based on one-way hash functions.",web beacon
http://arxiv.org/abs/1605.07329v1,"Vehicular communication requires vehicles to self-organize through the
exchange of periodic beacons. Recent analysis on beaconing indicates that the
standards for beaconing restrict the desired performance of vehicular
applications. This situation can be attributed to the quality of the available
transmission medium, persistent change in the traffic situation and the
inability of standards to cope with application requirements. To this end, this
paper is motivated by the classifications and capability evaluations of
existing adaptive beaconing approaches. To begin with, we explore the anatomy
and the performance requirements of beaconing. Then, the beaconing design is
analyzed to introduce a design-based beaconing taxonomy. A survey of the
state-of-the-art is conducted with an emphasis on the salient features of the
beaconing approaches. We also evaluate the capabilities of beaconing approaches
using several key parameters. A comparison among beaconing approaches is
presented, which is based on the architectural and implementation
characteristics. The paper concludes by discussing open challenges in the
field.",web beacon
http://arxiv.org/abs/1505.05106v1,"We establish tight bounds for beacon-based coverage problems, and improve the
bounds for beacon-based routing problems in simple rectilinear polygons.
Specifically, we show that $\lfloor \frac{n}{6} \rfloor$ beacons are always
sufficient and sometimes necessary to cover a simple rectilinear polygon $P$
with $n$ vertices. We also prove tight bounds for the case where $P$ is
monotone, and we present an optimal linear-time algorithm that computes the
beacon-based kernel of $P$. For the routing problem, we show that $\lfloor
\frac{3n-4}{8} \rfloor - 1$ beacons are always sufficient, and $\lceil
\frac{n}{4}\rceil-1$ beacons are sometimes necessary to route between all pairs
of points in $P$.",web beacon
http://arxiv.org/abs/1712.07416v1,"A beacon is a point-like object which can be enabled to exert a magnetic pull
on other point-like objects in space. Those objects then move towards the
beacon in a greedy fashion until they are either stuck at an obstacle or reach
the beacon's location. Beacons placed inside polyhedra can be used to route
point-like objects from one location to another. A second use case is to cover
a polyhedron such that every point-like object at an arbitrary location in the
polyhedron can reach at least one of the beacons once the latter is activated.
  The notion of beacon-based routing and guarding was introduced by Biro et al.
[FWCG'11] in 2011 and covered in detail by Biro in his PhD thesis [SUNY-SB'13],
which focuses on the two-dimensional case.
  We extend Biro's result to three dimensions by considering beacon routing in
polyhedra. We show that $\lfloor\frac{m+1}{3}\rfloor$ beacons are always
sufficient and sometimes necessary to route between any pair of points in a
given polyhedron $P$, where $m$ is the number of tetrahedra in a tetrahedral
decomposition of $P$. This is one of the first results that show that beacon
routing is also possible in three dimensions.",web beacon
http://arxiv.org/abs/1507.04988v1,"We consider the problem of localizing a target taking the help of a set of
anchor beacon nodes.A small number of beacon nodes are deployed at known
locations in the area.The target can detect a beacon provided it happens to lie
within the beacons's transmission range.Thus, the target contains a measurement
vector containing the readings of the beacons: '1' corresponding to a beacon if
it is able to detect the target and '0' if the beacon is not able to detect the
target.The goal is two fold: to determine the location of the target based on
the binary measurement vector at the target and to study the behavior of the
localization uncertainty as a function of the beacon transmission range and the
number of beacons deployed.Beacon transmission range means signal strength of
the beacon to transmit and receive the signals which is called as Received
Signal Strength.To localize the target, we propose a grid mapping based
approach, where the readings corresponding to locations on a grid overlaid on a
region of interest are used to localize a target.To study the behavior of the
localization uncertainty as a function of the sensing radius and number of
beacons,extensive simulations and numerical experiments are carried out.The
results provide insights into an importance of optimally setting the sensing
radius and the improvement obtainable with increasing number of beacons.",web beacon
http://arxiv.org/abs/1802.05735v1,"Traditionally, there have been few options for navigational aids for the
blind and visually impaired (BVI) in large indoor spaces. Some recent indoor
navigation systems allow users equipped with smartphones to interact with low
cost Bluetoothbased beacons deployed strategically within the indoor space of
interest to navigate their surroundings. A major challenge in deploying such
beacon-based navigation systems is the need to employ a time and
labor-expensive beacon planning process to identify potential beacon placement
locations and arrive at a topological structure representing the indoor space.
This work presents a technique called IBeaconMap for creating such topological
structures to use with beacon-based navigation that only needs the floor plans
of the indoor spaces of interest. IBeaconMap employs a combination of computer
vision and machine learning techniques to arrive at the required set of beacon
locations and a weighted connectivity graph (with directional orientations) for
subsequent navigational needs. Evaluations show IBeaconMap to be both fast and
reasonably accurate, potentially proving to be an essential tool to be utilized
before mass deployments of beacon-based indoor wayfinding systems of the
future.",web beacon
http://arxiv.org/abs/1503.08404v1,"Beacon node placement, node-to-node measurement, and target node positioning
are the three key steps for a localization process. However, compared with the
other two steps, beacon node placement still lacks a comprehensive, systematic
study in research literatures. To fill this gap, we address the Beacon Node
Placment (BNP) problem that deploys beacon nodes for minimal localization error
in this paper. BNP is difficult in that the localization error is determined by
a complicated combination of factors, i.e., the localization error differing
greatly under a different environment, with a different algorithm applied, or
with a different type of beacon node used. In view of the hardness of BNP, we
propose an approximate function to reduce time cost in localization error
calculation, and also prove its time complexity and error bound. By
approximation, a sub-optimal distribution of beacon nodes could be found within
acceptable time cost for placement. In the experiment, we test our method and
compare it with other node placement methods under various settings and
environments. The experimental results show feasibility and effectiveness of
our method in practice.",web beacon
http://arxiv.org/abs/1709.10237v1,"Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.",web beacon
http://arxiv.org/abs/0810.3966v3,"What would SETI Beacon transmitters be like if built by civilizations with a
variety of motivations, but who cared about cost? We studied in a companion
paper how, for fixed power density in the far field, we could build a
cost-optimum interstellar Beacon system. Here we consider, if someone like us
were to produce a Beacon, how should we look for it? High-power transmitters
might be built for wide variety of motives other than twoway communication;
Beacons built to be seen over thousands of light years are such. Altruistic
Beacon builders will have to contend with other altruistic causes, just as
humans do, so may select for economy of effort. Cost, spectral lines near 1 GHz
and interstellar scintillation favor radiating frequencies substantially above
the classic water hole. Therefore the transmission strategy for a distant,
cost-conscious Beacon will be a rapid scan of the galactic plane, to cover the
angular space. Such pulses will be infrequent events for the receiver. Such
Beacons built by distant advanced, wealthy societies will have very different
characteristics from what SETI researchers seek. Future searches should pay
special attention to areas along the galactic disk where SETI searches have
seen coherent signals that have not recurred on the limited listening time
intervals we have used. We will need to wait for recurring events that may
arrive in intermittent bursts. Several new SETI search strategies emerge from
these ideas. We propose a new test for SETI Beacons, based on the Life Plane
hypotheses.",web beacon
http://arxiv.org/abs/1803.05946v1,"The beacon model is a recent paradigm for guiding the trajectory of messages
or small robotic agents in complex environments. A beacon is a fixed point with
an attraction pull that can move points within a given polygon. Points move
greedily towards a beacon: if unobstructed, they move along a straight line to
the beacon, and otherwise they slide on the edges of the polygon. The Euclidean
distance from a moving point to a beacon is monotonically decreasing. A given
beacon attracts a point if the point eventually reaches the beacon.
  The problem of attracting all points within a polygon with a set of beacons
can be viewed as a variation of the art gallery problem. Unlike most
variations, the beacon attraction has the intriguing property of being
asymmetric, leading to separate definitions of attraction region and inverse
attraction region. The attraction region of a beacon is the set of points that
it attracts. It is connected and can be computed in linear time for simple
polygons. By contrast, it is known that the inverse attraction region of a
point---the set of beacon positions that attract it---could have $\Omega(n)$
disjoint connected components.
  In this paper, we prove that, in spite of this, the total complexity of the
inverse attraction region of a point in a simple polygon is linear, and present
a $O(n \log n)$ time algorithm to construct it. This improves upon the best
previous algorithm which required $O(n^3)$ time and $O(n^2)$ space. Furthermore
we prove a matching $\Omega(n\log n)$ lower bound for this task in the
algebraic computation tree model of computation, even if the polygon is
monotone.",web beacon
http://arxiv.org/abs/1504.07192v1,"This work presents a mobile sign-on scheme, which utilizes Bluetooth Low
Energy beacons for location awareness and Attribute-Based Encryption for
expressive, broadcast-style key exchange. Bluetooth Low Energy beacons
broadcast encrypted messages with encoded access policies. Within range of the
beacons, a user with appropriate attributes is able to decrypt the broadcast
message and obtain parameters that allow the user to perform a short or
simplified login. The effect is a ""traveling"" sign-on that accompanies the user
throughout different locations.",web beacon
http://arxiv.org/abs/0810.3964v2,"This paper considers galactic scale Beacons from the point of view of expense
to a builder on Earth. For fixed power density in the far field, what is the
cost-optimum interstellar Beacon system? Experience shows an optimum tradeoff,
depending on transmission frequency and on antenna size and power. This emerges
by minimizing the cost of producing a desired effective isotropic radiated
power, which in turn determines the maximum range of detectability of a
transmitted signal. We derive general relations for cost-optimal aperture and
power. For linear dependence of capital cost on transmitter power and antenna
area, minimum capital cost occurs when the cost is equally divided between
antenna gain and radiated power. For non-linear power law dependence a similar
simple division occurs. This is validated in cost data for many systems;
industry uses this cost optimum as a rule-of-thumb. Costs of pulsed
cost-efficient transmitters are estimated from these relations using current
cost parameters ($/W, $/m2) as a basis. Galactic-scale Beacons demand effective
isotropic radiated power >1017 W, emitted powers are >1 GW, with antenna areas
> km2. We show the scaling and give examples of such Beacons. Thrifty beacon
systems would be large and costly, have narrow searchlight beams and short
dwell times when the Beacon would be seen by an alien oberver at target areas
in the sky. They may revisit an area infrequently and will likely transmit at
higher microwave frequencies, ~10 GHz. The natural corridor to broadcast is
along the galactic spiral radius or along the spiral galactic arm we are in.
Our second paper argues that nearly all SETI searches to date had little chance
of seeing such Beacons.",web beacon
http://arxiv.org/abs/1407.6965v3,"Cooperative inter-vehicular applications rely on the exchange of broadcast
single-hop status messages among vehicles, called beacons. The aggregated load
on the wireless channel due to periodic beacons can prevent the transmission of
other types of messages, what is called channel congestion due to beaconing
activity. In this paper we approach the problem of controlling the beaconing
rate on each vehicle by modeling it as a Network Utility Maximization (NUM)
problem. This allows us to formally apply the notion of fairness of a beaconing
rate allocation in vehicular networks and to control the trade-off between
efficiency and fairness. The NUM methodology provides a rigorous framework to
design a broad family of simple and decentralized algorithms, with proved
convergence guarantees to a fair allocation solution. In this context, we focus
exclusively in beaconing rate control and propose the Fair Adaptive Beaconing
Rate for Intervehicular Communications (FABRIC) algorithm, which uses a
particular scaled gradient projection algorithm to solve the dual of the NUM
problem. The desired fairness notion in the allocation can be established with
an algorithm parameter. Simulation results validate our approach and show that
FABRIC converges to fair rate allocations in multi-hop and dynamic scenarios.",web beacon
http://arxiv.org/abs/1805.04548v1,"The DFINITY blockchain computer provides a secure, performant and flexible
consensus mechanism. At its core, DFINITY contains a decentralized randomness
beacon which acts as a verifiable random function (VRF) that produces a stream
of outputs over time. The novel technique behind the beacon relies on the
existence of a unique-deterministic, non-interactive, DKG-friendly threshold
signatures scheme. The only known examples of such a scheme are pairing-based
and derived from BLS.
  The DFINITY blockchain is layered on top of the DFINITY beacon and uses the
beacon as its source of randomness for leader selection and leader ranking. A
""weight"" is attributed to a chain based on the ranks of the leaders who propose
the blocks in the chain, and that weight is used to select between competing
chains. The DFINITY blockchain is layered on top of the DFINITY beacon and uses
the beacon as its source of randomness for leader selection and leader ranking
blockchain is further hardened by a notarization process which dramatically
improves the time to finality and eliminates the nothing-at-stake and selfish
mining attacks.
  DFINITY consensus algorithm is made to scale through continuous quorum
selections driven by the random beacon. In practice, DFINITY achieves block
times of a few seconds and transaction finality after only two confirmations.
The system gracefully handles temporary losses of network synchrony including
network splits, while it is provably secure under synchrony.",web beacon
http://arxiv.org/abs/1208.2403v1,"IEEE 802.15.4 standard is designed for low power and low data rate
applications with high reliability. It operates in beacon enable and non-beacon
enable modes. In this work, we analyze delay, throughput, load, and end-to-end
delay of nonbeacon enable mode. Analysis of these parameters are performed at
varying data rates. Evaluation of non beacon enabled mode is done in a 10 node
network. We limit our analysis to non beacon or unslotted version because, it
performs better than other. Protocol performance is examined by changing
different Medium Access Control (MAC) parameters. We consider a full size MAC
packet with payload size of 114 bytes. In this paper we show that maximum
throughput and lowest delay is achieved at highest data rate.",web beacon
http://arxiv.org/abs/1702.05116v2,"Cyclic pursuit frameworks, which are built upon pursuit interactions between
neighboring agents in a cycle graph, provide an efficient way to create useful
global behaviors in a collective of autonomous robots. Previous work had
considered cyclic pursuit with a constant bearing (CB) pursuit law, and
demonstrated the existence of circling equilibria for the corresponding
dynamics. In this work, we propose a beacon-referenced version of the CB
pursuit law, wherein a stationary beacon provides an additional reference for
the individual agents in a collective. When implemented in a cyclic framework,
we show that the resulting dynamics admit relative equilibria corresponding to
a circling orbit around the beacon, with the circling radius and the
distribution of agents along the orbit determined by parameters of the proposed
pursuit law. We also derive necessary conditions for stability of the circling
equilibria, which provides a guide for parameter selection. Finally, by
introducing a change of variables, we demonstrate the existence of a family of
invariant manifolds related to spiraling motions around the beacon which
preserve the ""pure shape"" of the collective, and study the reduced dynamics on
a representative manifold.",web beacon
http://arxiv.org/abs/1703.08612v2,"The ability of robots to estimate their location is crucial for a wide
variety of autonomous operations. In settings where GPS is unavailable,
measurements of transmissions from fixed beacons provide an effective means of
estimating a robot's location as it navigates. The accuracy of such a
beacon-based localization system depends both on how beacons are distributed in
the environment, and how the robot's location is inferred based on noisy and
potentially ambiguous measurements. We propose an approach for making these
design decisions automatically and without expert supervision, by explicitly
searching for the placement and inference strategies that, together, are
optimal for a given environment. Since this search is computationally
expensive, our approach encodes beacon placement as a differential neural layer
that interfaces with a neural network for inference. This formulation allows us
to employ standard techniques for training neural networks to carry out the
joint optimization. We evaluate this approach on a variety of environments and
settings, and find that it is able to discover designs that enable high
localization accuracy.",web beacon
http://arxiv.org/abs/1910.00507v1,"To address 5G challenges, IEEE 802.11 is currently developing new amendments
to the Wi-Fi standard, the most promising of which is 802.11ax. A key scenario
considered by the developers of this amendment is dense and overlapped networks
typically present in residential buildings, offices, airports, stadiums, and
other places of a modern city. Being crucial for Wi-Fi hotspots, the hidden
station problem becomes even more challenging for dense and overlapped
networks, where even access points (APs) can be hidden. In this case, user
stations can experience continuous collisions of beacons sent by different APs,
which can cause disassociation and break Internet access. In this paper, we
show that beacon collisions are rather typical for residential networks and may
lead to unexpected and irreproducible malfunction. We investigate how often
beacon collisions occur, and describe a number of mechanisms which can be used
to avoid beacon collisions in dense deployment. Specifically, we pay much
attention to those mechanisms which are currently under consideration of the
IEEE 802.11ax group.",web beacon
http://arxiv.org/abs/1503.03388v1,"This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.",web beacon
http://arxiv.org/abs/1212.2404v1,"Vehicular Ad-Hoc Networks (VANETs) are special forms of Mobile Ad-Hoc
Networks (MANETs) that allows vehicles to communicate together in the absence
of fixed infrastructure.In this type of network beaconing is the means used to
discover the nodes in its eighborhood.For routing protocol successful delivery
of beacons containing speed, direction and position of a car is extremely
important.Otherwise, routing information should not be modified/manipulated
during transmission without detection, in order to ensure the routing
information, messages must be signed and provided with a certificate to attest
valid network participants. In this work we present a beaconing protocol with
key exchange to prepare the generation of a signature to protect the routing
information protocol 'Greedy Perimeter Stateless Routing'.",web beacon
http://arxiv.org/abs/1703.04150v1,"Location sensing is a key enabling technology for Ubicomp to support
contextual interaction. However, the laboratories where calibrated testing of
location technologies is done are very different to the domestic situations
where `context' is a problematic social construct. This study reports
measurements of Bluetooth beacons, informed by laboratory studies, but done in
diverse domestic settings. The design of these surveys has been motivated by
the natural environment implied in the Bluetooth beacon standards - relating
the technical environment of the beacon to the function of spaces within the
home. This research method can be considered as a situated, `ethnographic'
technical response to the study of physical infrastructure that arises through
social processes. The results offer insights for the future design of `seamful'
approaches to indoor location sensing, and to the ways that context might be
constructed and interpreted in a seamful manner.",web beacon
http://arxiv.org/abs/1806.02325v1,"We consider a sensing application where the sensor nodes are wirelessly
powered by an energy beacon. We focus on the problem of jointly optimizing the
energy allocation of the energy beacon to different sensors and the data
transmission powers of the sensors in order to minimize the field
reconstruction error at the sink. In contrast to the standard ideal linear
energy harvesting (EH) model, we consider practical non-linear EH models. We
investigate this problem under two different frameworks: i) an optimization
approach where the energy beacon knows the utility function of the nodes,
channel state information and the energy harvesting characteristics of the
devices; hence optimal power allocation strategies can be designed using an
optimization problem and ii) a learning approach where the energy beacon
decides on its strategies adaptively with battery level information and
feedback on the utility function. Our results illustrate that deep
reinforcement learning approach can obtain the same error levels with the
optimization approach and provides a promising alternative to the optimization
framework.",web beacon
http://arxiv.org/abs/1812.02349v2,"An ultrasonic Positioning System (UPS) has outperformed RF-based systems in
terms of its accuracy for years. However, few of the developed solutions have
been deployed in practice to satisfy the localization demand of today's smart
devices, which lack ultrasonic sensors and were considered as being `deaf' to
ultrasound. A recent finding demonstrates that ultrasound may be audible to the
smart devices under certain conditions due to their microphone's nonlinearity.
Inspired by this insight, this work revisits the ultrasonic positioning
technique and builds a practical UPS, called UPS+ for ultrasound-incapable
smart devices. The core concept is to deploy two types of indoor beacon
devices, which will advertise ultrasonic beacons at two different ultrasonic
frequencies respectively. Their superimposed beacons are shifted to a
low-frequency by virtue of the nonlinearity effect at the receiver's
microphone. This underlying property functions as an implicit ultrasonic
downconverter without throwing harm to the hearing system of humans. We
demonstrate UPS+, a fully functional UPS prototype, with centimeter-level
localization accuracy using custom-made beacon hardware and well-designed
algorithms.",web beacon
http://arxiv.org/abs/1909.11737v1,"As an emerging technology with exceptional low energy consumption and
low-latency data transmissions, Bluetooth Low Energy (BLE) has gained
significant momentum in various application domains, such as Indoor
Positioning, Home Automation, and Wireless Personal Area Network (WPAN)
communications. With various novel protocol stack features, BLE is finding use
on resource-constrained sensor nodes as well as more powerful gateway devices.
Particularly proximity detection using BLE beacons has been a popular usage
scenario ever since the release of Bluetooth 4.0, primarily due to the beacons'
energy efficiency and ease of deployment. However, with the rapid rise of the
Internet of Things (IoT), BLE is likely to be a significant component in many
other applications with widely varying performance and Quality-of-Service (QoS)
requirements and there is a need for a consolidated view of the role that BLE
will play in applications beyond beaconing. This paper comprehensively surveys
state-of-the-art applications built with BLE, obstacles to adoption of BLE in
new application areas, and current solutions from academia and industry that
further expand the capabilities of BLE.",web beacon
http://arxiv.org/abs/1506.05367v1,"We propose and investigate a compressive architecture for estimation and
tracking of sparse spatial channels in millimeter (mm) wave picocellular
networks. The base stations are equipped with antenna arrays with a large
number of elements (which can fit within compact form factors because of the
small carrier wavelength) and employ radio frequency (RF) beamforming, so that
standard least squares adaptation techniques (which require access to
individual antenna elements) are not applicable. We focus on the downlink, and
show that ""compressive beacons,"" transmitted using pseudorandom phase settings
at the base station array, and compressively processed using pseudorandom phase
settings at the mobile array, provide information sufficient for accurate
estimation of the two-dimensional (2D) spatial frequencies associated with the
directions of departure of the dominant rays from the base station, and the
associated complex gains. This compressive approach is compatible with coarse
phase-only control, and is based on a near-optimal sequential algorithm for
frequency estimation which can exploit the geometric continuity of the channel
across successive beaconing intervals to reduce the overhead to less than 1%
even for very large (32 x 32) arrays. Compressive beaconing is essentially
omnidirectional, and hence does not enjoy the SNR and spatial reuse benefits of
beamforming obtained during data transmission. We therefore discuss system
level design considerations for ensuring that the beacon SNR is sufficient for
accurate channel estimation, and that inter-cell beacon interference is
controlled by an appropriate reuse scheme.",web beacon
http://arxiv.org/abs/1706.05569v3,"In this paper, we develop a system for the low-cost indoor localization and
tracking problem using radio signal strength indicator, Inertial Measurement
Unit (IMU), and magnetometer sensors. We develop a novel and simplified
probabilistic IMU motion model as the proposal distribution of the sequential
Monte-Carlo technique to track the robot trajectory. Our algorithm can globally
localize and track a robot with a priori unknown location, given an informative
prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the
problem as an optimization problem that serves as the Back-end of the algorithm
mentioned above (Front-end). Thus, by simultaneously solving for the robot
trajectory and the map of BLE beacons, we recover a continuous and smooth
trajectory of the robot, corrected locations of the BLE beacons, and the
time-varying IMU bias. The evaluations achieved using hardware show that
through the proposed closed-loop system the localization performance can be
improved; furthermore, the system becomes robust to the error in the map of
beacons by feeding back the optimized map to the Front-end.",web beacon
http://arxiv.org/abs/1706.07549v1,"This letter studies a radio-frequency (RF) multi-user wireless power transfer
(WPT) system, where an energy transmitter (ET) with a large number of antennas
delivers energy wirelessly to multiple distributed energy receivers (ERs). We
investigate a low-complexity WPT scheme based on the retrodirective beamforming
technique, where all ERs send a common beacon signal simultaneously to the ET
in the uplink and the ET simply conjugates and amplifies its received
sum-signal and transmits to all ERs in the downlink for WPT. We show that such
a low-complexity scheme achieves the massive multiple-input multiple-output
(MIMO) energy beamforming gain. However, a ""doubly near-far"" issue exists due
to the round-trip (uplink beacon and downlink WPT) signal propagation loss
where the harvested power of a far ER from the ET can be significantly lower
than that of a near ER if the same uplink beacon power is used. To tackle this
problem, we propose a distributed uplink beacon power update algorithm, where
each ER independently adjusts the beacon power based on its current harvested
power in an iterative manner. It is shown that the proposed algorithm converges
quickly to a unique fixed-point solution, which helps achieve the desired user
fairness with best efforts.",web beacon
http://arxiv.org/abs/1807.10573v1,"Collision avoidance is a critical task in many applications, such as ADAS
(advanced driver-assistance systems), industrial automation and robotics. In an
industrial automation setting, certain areas should be off limits to an
automated vehicle for protection of people and high-valued assets. These areas
can be quarantined by mapping (e.g., GPS) or via beacons that delineate a
no-entry area. We propose a delineation method where the industrial vehicle
utilizes a LiDAR {(Light Detection and Ranging)} and a single color camera to
detect passive beacons and model-predictive control to stop the vehicle from
entering a restricted space. The beacons are standard orange traffic cones with
a highly reflective vertical pole attached. The LiDAR can readily detect these
beacons, but suffers from false positives due to other reflective surfaces such
as worker safety vests. Herein, we put forth a method for reducing false
positive detection from the LiDAR by projecting the beacons in the camera
imagery via a deep learning method and validating the detection using a neural
network-learned projection from the camera to the LiDAR space. Experimental
data collected at Mississippi State University's Center for Advanced Vehicular
Systems (CAVS) shows the effectiveness of the proposed system in keeping the
true detection while mitigating false positives.",web beacon
http://arxiv.org/abs/1301.7170v1,"Vehicular Ad hoc Networks (VANET) is one of the most challenging research
areas in the field of Mobile Ad Hoc Networks. In this research, we propose a
new mechanism for increasing network visibility, by taking the information
gained from periodic safety messages (beacons), and inserting it into a
'neighbor' table. The table will be propagated to all neighbors giving a wider
vision for each vehicle belonging to the network. It will also decrease the
risk of collision at road junctions as each vehicle will have prior knowledge
oncoming vehicles before reaching the junction.",web beacon
http://arxiv.org/abs/1306.0682v2,"We consider the problem of localizing two sensors using signals of
opportunity from beacons with known positions. Beacons and sensors have
asynchronous local clocks or oscillators with unknown clock skews and offsets.
We model clock skews as random, and analyze the biases introduced by clock
asynchronism in the received signals. By deriving the equivalent Fisher
information matrix for the modified Bayesian Cram\'er-Rao lower bound (CRLB) of
sensor position and velocity estimation, we quantify the errors caused by clock
asynchronism.",web beacon
http://arxiv.org/abs/1509.02054v1,"Precise autonomous navigation remains a substantial challenge to all
underwater platforms. Inertial Measurement Units (IMU) and Doppler Velocity
Logs (DVL) have complementary characteristics and are promising sensors that
could enable fully autonomous underwater navigation in unexplored areas without
relying on additional external Global Positioning System (GPS) or acoustic
beacons. This paper addresses the combined IMU/DVL navigation system from the
viewpoint of observability. We show by analysis that under moderate conditions
the combined system is observable. Specifically, the DVL parameters, including
the scale factor and misalignment angles, can be calibrated in-situ without
using external GPS or acoustic beacon sensors. Simulation results using a
practical estimator validate the analytic conclusions.",web beacon
http://arxiv.org/abs/1611.05134v1,"While deep neural networks have succeeded in several visual applications,
such as object recognition, detection, and localization, by reaching very high
classification accuracies, it is important to note that many real-world
applications demand vary- ing costs for different types of misclassification
errors, thus requiring cost-sensitive classification algorithms. Current models
of deep neural networks for cost-sensitive classification are restricted to
some specific network structures and limited depth. In this paper, we propose a
novel framework that can be applied to deep neural networks with any structure
to facilitate their learning of meaningful representations for cost-sensitive
classification problems. Furthermore, the framework allows end- to-end training
of deeper networks directly. The framework is designed by augmenting auxiliary
neurons to the output of each hidden layer for layer-wise cost estimation, and
including the total estimation loss within the optimization objective.
Experimental results on public benchmark visual data sets with two cost
information settings demonstrate that the proposed frame- work outperforms
state-of-the-art cost-sensitive deep learning models.",hidden cost detection
http://arxiv.org/abs/1602.03979v1,"In this paper, we focus on hidden period identification and the periodic
decomposition of signals. Based on recent results on the Ramanujan subspace, we
reveal the conjugate symmetry of the Ramanujan subspace with a set of complex
exponential basis functions and represent the subspace as the union of a series
of conjugate subspaces. With these conjugate subspaces, the signal periodic
model is introduced to characterize the periodic structure of a signal. To
achieve the decomposition of the proposed model, the conjugate subspace
matching pursuit (CSMP) algorithm is proposed based on two different greedy
strategies. The CSMP is performed iteratively in two stages. In the first
stage, the dominant hidden period is chosen with the periodicity strategy.
Then, the dominant conjugate subspace is chosen with the energy strategy in the
second stage. Compared with the current state-of-the-art methods for hidden
period identification, the main advantages provided by the CSMP are the
following: (i) the capability of identifying all the hidden periods in the
range from $1$ to the maximum hidden period $Q$ of a signal of any length,
without truncating the signal; (ii) the ability to identify the time-varying
hidden period with its shifted version; and (iii) the low computational cost,
without generating and using a large over-complete dictionary. Moreover, we
provide examples and applications to demonstrate the abilities of the proposed
two-stage CSMP algorithm, which include hidden period identification, signal
approximation, time-varying period detection, and pitch detection of speech.",hidden cost detection
http://arxiv.org/abs/1506.04541v1,"Meter measurements in the power grid are susceptible to manipulation by
adversaries, that can lead to errors in state estimation. This paper presents a
general framework to study attacks on state estimation by adversaries capable
of injecting bad-data into measurements and further, of jamming their
reception. Through these two techniques, a novel `detectable jamming' attack is
designed that changes the state estimation despite failing bad-data detection
checks. Compared to commonly studied `hidden' data attacks, these attacks have
lower costs and a wider feasible operating region. It is shown that the entire
domain of jamming costs can be divided into two regions, with distinct
graph-cut based formulations for the design of the optimal attack. The most
significant insight arising from this result is that the adversarial capability
to jam measurements changes the optimal 'detectable jamming' attack design only
if the jamming cost is less than half the cost of bad-data injection. A
polynomial time approximate algorithm for attack vector construction is
developed and its efficacy in attack design is demonstrated through simulations
on IEEE test systems.",hidden cost detection
http://arxiv.org/abs/1901.02818v2,"Small, low-cost, wireless cameras are becoming increasingly commonplace
making surreptitious observation of people more difficult to detect. Previous
work in detecting hidden cameras has only addressed limited environments in
small spaces where the user has significant control of the environment. To
address this problem in a less constrained scope of environments, we introduce
the concept of similarity of simultaneous observation where the user utilizes a
camera (Wi-Fi camera, camera on a mobile phone or laptop) to compare timing
patterns of data transmitted by potentially hidden cameras and the timing
patterns that are expected from the scene that the known camera is recording.
To analyze the patterns, we applied several similarity measures and
demonstrated an accuracy of over 87% and and F1 score of 0.88 using an
efficient threshold-based classification. Furthermore, we used our data set to
train a neural network and saw improved results with accuracy as high as 97%
and an F1 score over 0.95 for both indoors and outdoors settings. From these
results, we conclude that similarity of simultaneous observation is a feasible
method for detecting hidden wireless cameras that are streaming video of a
user. Our work removes significant limitations that have been put on previous
detection methods.",hidden cost detection
http://arxiv.org/abs/1509.04639v1,"Jamming refers to the deletion, corruption or damage of meter measurements
that prevents their further usage. This is distinct from adversarial data
injection that changes meter readings while preserving their utility in state
estimation. This paper presents a generalized attack regime that uses jamming
of secure and insecure measurements to greatly expand the scope of common
'hidden' and 'detectable' data injection attacks in literature. For 'hidden'
attacks, it is shown that with jamming, the optimal attack is given by the
minimum feasible cut in a specific weighted graph. More importantly, for
'detectable' data attacks, this paper shows that the entire range of relative
costs for adversarial jamming and data injection can be divided into three
separate regions, with distinct graph-cut based constructions for the optimal
attack. Approximate algorithms for attack design are developed and their
performances are demonstrated by simulations on IEEE test cases. Further, it is
proved that prevention of such attacks require security of all grid
measurements. This work comprehensively quantifies the dual adversarial
benefits of jamming: (a) reduced attack cost and (b) increased resilience to
secure measurements, that strengthen the potency of data attacks.",hidden cost detection
http://arxiv.org/abs/1908.03839v2,"Facial landmark detection is a crucial prerequisite for many face analysis
applications. Deep learning-based methods currently dominate the approach of
addressing the facial landmark detection. However, such works generally
introduce a large number of parameters, resulting in high memory cost. In this
paper, we aim for lightweight as well as effective solutions to facial landmark
detection. To this end, we propose an effective lightweight model, namely
Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2
as the encoder and three deconvolutional layers as the decoder. The proposed
MobileFAN, with only 8% of the model size and lower computational cost,
achieves superior or equivalent performance compared with state-of-the-art
models. Moreover, by transferring the geometric structural information of a
face graph from a large complex model to our proposed MobileFAN through
feature-aligned distillation and feature-similarity distillation, the
performance of MobileFAN is further improved in effectiveness and efficiency
for face alignment. Extensive experiment results on three challenging facial
landmark estimation benchmarks including COFW, 300W and WFLW show the
superiority of our proposed MobileFAN against state-of-the-art methods.",hidden cost detection
http://arxiv.org/abs/1308.6745v1,"Cloud Computing is a recent computing model provides consistent access to
wide area distributed resources. It revolutionized the IT world with its
services provision infrastructure, less maintenance cost, data and service
availability assurance, rapid accessibility and scalability. Grid and Cloud
Computing Intrusion Detection System detects encrypted node communication and
find the hidden attack trial which inspects and detects those attacks that
network based and host based cant identify. It incorporates Knowledge and
behavior analysis to identify specific intrusions. Signature based IDS monitor
the packets in the network and identifies those threats by matching with
database but It fails to detect those attacks that are not included in
database. Signature based IDS will perform poor capturing in large volume of
anomalies. Another problem is that Cloud Service Provider hides the attack that
is caused by intruder, due to distributed nature cloud environment has high
possibility for vulnerable resources. By impersonating legitimate users, the
intruders can use a services abundant resources maliciously. In Proposed System
we combine few concepts which are available with new intrusion detection
techniques. Here to merge Entropy based System with Anomaly detection System
for providing multilevel Distributed Denial of Service. This is done in two
steps: First, Users are allowed to pass through router in network site in that
it incorporates Detection Algorithm and detects for legitimate user. Second,
again it pass through router placed in cloud site in that it incorporates
confirmation Algorithm and checks for threshold value, if its beyond the
threshold value it considered as legitimate user, else its an intruder found in
environment.",hidden cost detection
http://arxiv.org/abs/1904.12354v1,"Respiratory infections and chronic respiratory diseases impose a heavy health
burden worldwide. Coughing is one of the most common symptoms of many such
infections, and can be indicative of flare-ups of chronic respiratory diseases.
Whether at a clinical or public health level, the capacity to identify bouts of
coughing can aid understanding of population and individual health status.
Developing health monitoring models in the context of respiratory diseases and
also seasonal diseases with symptoms such as cough has the potential to improve
quality of life, help clinicians and public health authorities with their
decisions and decrease the cost of health services. In this paper, we
investigated the ability to which a simple machine learning approach in the
form of Hidden Markov Models (HMMs) could be used to classify different states
of coughing using univariate (with a single energy band as the input feature)
and multivariate (with a multiple energy band as the input features) binned
time series using both of cough data. We further used the model to distinguish
cough events from other events and environmental noise. Our Hidden Markov
algorithm achieved 92% AUR (Area Under Receiver Operating Characteristic Curve)
in classifying coughing events in noisy environments. Moreover, comparison of
univariate with multivariate HMMs suggest a high accuracy of multivariate HMMs
for cough event classifications.",hidden cost detection
http://arxiv.org/abs/1709.07567v1,"With the rapid development of Internet and the sharp increase of network
crime, network security has become very important and received a lot of
attention. We model security issues as stochastic systems. This allows us to
find weaknesses in existing security systems and propose new solutions.
Exploring the vulnerabilities of existing security tools can prevent
cyber-attacks from taking advantages of the system weaknesses. We propose a
hybrid network security scheme including intrusion detection systems (IDSs) and
honeypots scattered throughout the network. This combines the advantages of two
security technologies. A honeypot is an activity-based network security system,
which could be the logical supplement of the passive detection policies used by
IDSs. This integration forces us to balance security performance versus cost by
scheduling device activities for the proposed system. By formulating the
scheduling problem as a decentralized partially observable Markov decision
process (DEC-POMDP), decisions are made in a distributed manner at each device
without requiring centralized control. The partially observable Markov decision
process (POMDP) is a useful choice for controlling stochastic systems. As a
combination of two Markov models, POMDPs combine the strength of hidden Markov
Model (HMM) (capturing dynamics that depend on unobserved states) and that of
Markov decision process (MDP) (taking the decision aspect into account).
Decision making under uncertainty is used in many parts of business and
science.We use here for security tools.We adopt a high-quality approximation
solution for finite-space POMDPs with the average cost criterion, and their
extension to DEC-POMDPs. We show how this tool could be used to design a
network security framework.",hidden cost detection
http://arxiv.org/abs/1906.07745v1,"Obtaining the state of the art performance of deep learning models imposes a
high cost to model generators, due to the tedious data preparation and the
substantial processing requirements. To protect the model from unauthorized
re-distribution, watermarking approaches have been introduced in the past
couple of years. The watermark allows the legitimate owner to detect copyright
violations of their model. We investigate the robustness and reliability of
state-of-the-art deep neural network watermarking schemes. We focus on
backdoor-based watermarking and show that an adversary can remove the watermark
fully by just relying on public data and without any access to the model's
sensitive information such as the training data set, the trigger set or the
model parameters. We as well prove the security inadequacy of the
backdoor-based watermarking in keeping the watermark hidden by proposing an
attack that detects whether a model contains a watermark.",hidden cost detection
http://arxiv.org/abs/1907.04601v1,"Detection of patterns hidden in data is a core challenge across many
disciplines. In this manuscript, we highlight two low-cost tools for the
extraction of latent patterns - namely, the latent entropy and the latent
dimension measures. Here the aim is to disentangle the effects of
predictability and the memory of a system in the presence of unobservable
(latent) effects. Our results reveal a drastically-increased sensitivity and
performance for a broad range of problems. Disparate and striking examples
included are 1) revealing unexplored material inhomogeneities from
magneto-optical Kerr effect measurements, 2) new insights into the
predictability of the climate phenomenon El Ni\~no, and 3) the analysis of an
amateur telescope movie of the Andromeda galaxy exposing features otherwise
unobservable without recourse to larger instruments. Last but not least, we
show how these measures reveal otherwise invisible bias patterns imposed by
popular data compression tools.",hidden cost detection
http://arxiv.org/abs/1901.02057v1,"The manufacturing sector is heavily influenced by artificial
intelligence-based technologies with the extraordinary increases in
computational power and data volumes. It has been reported that 35% of US
manufacturers are currently collecting data from sensors for manufacturing
processes enhancement. Nevertheless, many are still struggling to achieve the
'Industry 4.0', which aims to achieve nearly 50% reduction in maintenance cost
and total machine downtime by proper health management. For increasing
productivity and reducing operating costs, a central challenge lies in the
detection of faults or wearing parts in machining operations. Here we propose a
data-driven, end-to-end framework for monitoring of manufacturing systems. This
framework, derived from deep learning techniques, evaluates fused sensory
measurements to detect and even predict faults and wearing conditions. This
work exploits the predictive power of deep learning to extract hidden
degradation features from noisy data. We demonstrate the proposed framework on
several representative experimental manufacturing datasets drawn from a wide
variety of applications, ranging from mechanical to electrical systems. Results
reveal that the framework performs well in all benchmark applications examined
and can be applied in diverse contexts, indicating its potential for use as a
critical corner stone in smart manufacturing.",hidden cost detection
http://arxiv.org/abs/0807.2043v1,"Intrusion Detection is an invaluable part of computer networks defense. An
important consideration is the fact that raising false alarms carries a
significantly lower cost than not detecting at- tacks. For this reason, we
examine how cost-sensitive classification methods can be used in Intrusion
Detection systems. The performance of the approach is evaluated under different
experimental conditions, cost matrices and different classification models, in
terms of expected cost, as well as detection and false alarm rates. We find
that even under unfavourable conditions, cost-sensitive classification can
improve performance significantly, if only slightly.",hidden cost detection
http://arxiv.org/abs/1810.10999v1,"Recurrent neural networks (RNNs) provide state-of-the-art performance in
processing sequential data but are memory intensive to train, limiting the
flexibility of RNN models which can be trained. Reversible RNNs---RNNs for
which the hidden-to-hidden transition can be reversed---offer a path to reduce
the memory requirements of training, as hidden states need not be stored and
instead can be recomputed during backpropagation. We first show that perfectly
reversible RNNs, which require no storage of the hidden activations, are
fundamentally limited because they cannot forget information from their hidden
state. We then provide a scheme for storing a small number of bits in order to
allow perfect reversal with forgetting. Our method achieves comparable
performance to traditional models while reducing the activation memory cost by
a factor of 10--15. We extend our technique to attention-based
sequence-to-sequence models, where it maintains performance while reducing
activation memory cost by a factor of 5--10 in the encoder, and a factor of
10--15 in the decoder.",hidden cost detection
http://arxiv.org/abs/1008.5057v1,"We consider the evaluation of approximate top-k queries from relations with
a-priori unknown values. Such relations can arise for example in the context of
expensive predicates, or cloud-based data sources. The task is to find an
approximate top-k set that is close to the exact one while keeping the total
processing cost low. The cost of a query is the sum of the costs of the entries
that are read from the hidden relation. A novel aspect of this work is that we
consider prior information about the values in the hidden matrix. We propose an
algorithm that uses regression models at query time to assess whether a row of
the matrix can enter the top-k set given that only a subset of its values are
known. The regression models are trained with existing data that follows the
same distribution as the relation subjected to the query. To evaluate the
algorithm and to compare it with a method proposed previously in literature, we
conduct experiments using data from a context sensitive Wikipedia search
engine. The results indicate that the proposed method outperforms the baseline
algorithms in terms of the cost while maintaining a high accuracy of the
returned results.",hidden cost detection
http://arxiv.org/abs/1406.2519v1,"Network steganography encompasses the information hiding techniques that can
be applied in communication network environments and that utilize hidden data
carriers for this purpose. In this paper we introduce a characteristic called
steganographic cost which is an indicator for the degradation or distortion of
the carrier caused by the application of the steganographic method. Based on
exemplary cases for single- and multi-method steganographic cost analyses we
observe that it can be an important characteristic that allows to express
hidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR
(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.
Steganographic cost can moreover be helpful to analyse the relationships
between two or more steganographic methods applied to the same hidden data
carrier.",hidden cost detection
http://arxiv.org/abs/1604.00676v1,"As a widely used non-linear activation, Rectified Linear Unit (ReLU)
separates noise and signal in a feature map by learning a threshold or bias.
However, we argue that the classification of noise and signal not only depends
on the magnitude of responses, but also the context of how the feature
responses would be used to detect more abstract patterns in higher layers. In
order to output multiple response maps with magnitude in different ranges for a
particular visual pattern, existing networks employing ReLU and its variants
have to learn a large number of redundant filters. In this paper, we propose a
multi-bias non-linear activation (MBA) layer to explore the information hidden
in the magnitudes of responses. It is placed after the convolution layer to
decouple the responses to a convolution kernel into multiple maps by
multi-thresholding magnitudes, thus generating more patterns in the feature
space at a low computational cost. It provides great flexibility of selecting
responses to different visual patterns in different magnitude ranges to form
rich representations in higher layers. Such a simple and yet effective scheme
achieves the state-of-the-art performance on several benchmarks.",hidden cost detection
http://arxiv.org/abs/1601.07241v1,"Intelligent geographic information system (IGIS) is one of the promising
topics in GIS field. It aims at making GIS tools more sensitive for large
volumes of data stored inside GIS systems by integrating GIS with other
computer sciences such as Expert system (ES) Data Warehouse (DW), Decision
Support System (DSS), or Knowledge Discovery Database (KDD). One of the main
branches of IGIS is the Geographic Knowledge Discovery (GKD) which tries to
discover the implicit knowledge in the spatial databases. The main difference
between traditional KDD techniques and GKD techniques is hidden in the nature
of spatial data sets. In other words in the traditional data set the values of
each object are supposed to be independent from other objects in the same data
set, whereas the spatial dataset tends to be highly correlated according to the
first law of geography. The spatial outlier detection is one of the most
popular spatial data mining techniques which is used to detect spatial objects
whose non-spatial attributes values are extremely different from those of their
neighboring objects. Analyzing the behavior of these objects may produce an
interesting knowledge, which has an effective role in the decision-making
process. In this thesis, a new definition for the spatial neighborhood
relationship by is proposed considering the weights of the most effective
parameters of neighboring objects in a given spatial dataset. The spatial
parameters taken into our consideration are; distance, cost, and number of
direct connections between neighboring objects. A new model to detect spatial
outliers is also presented based on the new definition of the spatial
neighborhood relationship. This model is adapted to be applied to polygonal
objects. The proposed model is applied to an existing project for supporting
literacy in Fayoum governorate in Arab Republic of Egypt (ARE).",hidden cost detection
http://arxiv.org/abs/1702.07462v1,"We introduce a new paradigm that is important for community detection in the
realm of network analysis. Networks contain a set of strong, dominant
communities, which interfere with the detection of weak, natural community
structure. When most of the members of the weak communities also belong to
stronger communities, they are extremely hard to be uncovered. We call the weak
communities the hidden community structure.
  We present a novel approach called HICODE (HIdden COmmunity DEtection) that
identifies the hidden community structure as well as the dominant community
structure. By weakening the strength of the dominant structure, one can uncover
the hidden structure beneath. Likewise, by reducing the strength of the hidden
structure, one can more accurately identify the dominant structure. In this
way, HICODE tackles both tasks simultaneously.
  Extensive experiments on real-world networks demonstrate that HICODE
outperforms several state-of-the-art community detection methods in uncovering
both the dominant and the hidden structure. In the Facebook university social
networks, we find multiple non-redundant sets of communities that are strongly
associated with residential hall, year of registration or career position of
the faculties or students, while the state-of-the-art algorithms mainly locate
the dominant ground truth category. In the Due to the difficulty of labeling
all ground truth communities in real-world datasets, HICODE provides a
promising approach to pinpoint the existing latent communities and uncover
communities for which there is no ground truth. Finding this unknown structure
is an extremely important community detection problem.",hidden cost detection
http://arxiv.org/abs/1610.09520v1,"This paper was originally submitted to Xinova as a response to a Request for
Invention (RFI) on new event monitoring methods. In this paper, a new object
tracking algorithm using multiple cameras for surveillance applications is
proposed. The proposed system can detect sudden-appearance-changes and
occlusions using a hidden Markovian statistical model. The experimental results
confirm that our system detect the sudden-appearance changes and occlusions
reliably.",hidden cost detection
http://arxiv.org/abs/1704.00774v3,"Increasing the capacity of recurrent neural networks (RNN) usually involves
augmenting the size of the hidden layer, with significant increase of
computational cost. Recurrent neural tensor networks (RNTN) increase capacity
using distinct hidden layer weights for each word, but with greater costs in
memory usage. In this paper, we introduce restricted recurrent neural tensor
networks (r-RNTN) which reserve distinct hidden layer weights for frequent
vocabulary words while sharing a single set of weights for infrequent words.
Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve
language model performance over RNNs using only a small fraction of the
parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated
Recurrent Units and Long Short-Term Memory.",hidden cost detection
http://arxiv.org/abs/1611.08743v4,"Advertisements and subscriptions are tightly coupled to generate publication
routing paths in content--based publish/subscribe systems. Tight coupling
requires instantaneous updates in routing tables to generate alternative paths
which prevents offering scalable and robust dynamic routing in cyclic overlays
when link congestion is detected. We propose, OctopiA, first distributed
publish/subscribe system for content--based inter--cluster dynamic routing
using purpose--built structured cyclic overlays. OctopiA uses a novel concept
of subscription subgrouping, which divides subscriptions into disjoint sets
called subscription subgroups. The purpose--built structured cyclic overlay is
divided into identical clusters where subscriptions in each subgroup are
broadcast to an exclusive cluster. Our advertisement and subscription
forwarding algorithms use subscription subgrouping to eliminate tight coupling
to offer inter--cluster dynamic routing without requiring updates in routing
tables. Experiments on a cluster testbed with real world data show that OctopiA
reduces the number of saved advertisements in routing tables by 93%,
subscription broadcast delay by 33%, static and dynamic publication delivery
delays by 25% and 54%, respectively.",hidden subscription block
http://arxiv.org/abs/0909.1781v1,"The growing amount of XML encoded data exchanged over the Internet increases
the importance of XML based publish-subscribe (pub-sub) and content based
routing systems. The input in such systems typically consists of a stream of
XML documents and a set of user subscriptions expressed as XML queries. The
pub-sub system then filters the published documents and passes them to the
subscribers. Pub-sub systems are characterized by very high input ratios,
therefore the processing time is critical. In this paper we propose a ""pure
hardware"" based solution, which utilizes XPath query blocks on FPGA to solve
the filtering problem. By utilizing the high throughput that an FPGA provides
for parallel processing, our approach achieves drastically better throughput
than the existing software or mixed (hardware/software) architectures. The
XPath queries (subscriptions) are translated to regular expressions which are
then mapped to FPGA devices. By introducing stacks within the FPGA we are able
to express and process a wide range of path queries very efficiently, on a
scalable environment. Moreover, the fact that the parser and the filter
processing are performed on the same FPGA chip, eliminates expensive
communication costs (that a multi-core system would need) thus enabling very
fast and efficient pipelining. Our experimental evaluation reveals more than
one order of magnitude improvement compared to traditional pub/sub systems.",hidden subscription block
http://arxiv.org/abs/1812.02386v1,"Blockchains have recently been under the spotlight due to the boom of
cryptocurrencies and decentralized applications. There is an increasing demand
for querying the data stored in a blockchain database. To ensure query
integrity, the user can maintain the entire blockchain database and query the
data locally. However, this approach is not economic, if not infeasible,
because of the blockchain's huge data size and considerable maintenance costs.
In this paper, we take the first step toward investigating the problem of
verifiable query processing over blockchain databases. We propose a novel
framework, called vChain, that alleviates the storage and computing costs of
the user and employs verifiable queries to guarantee the results' integrity. To
support verifiable Boolean range queries, we propose an accumulator-based
authenticated data structure that enables dynamic aggregation over arbitrary
query attributes. Two new indexes are further developed to aggregate
intra-block and inter-block data records for efficient query verification. We
also propose an inverted prefix tree structure to accelerate the processing of
a large number of subscription queries simultaneously. Security analysis and
empirical study validate the robustness and practicality of the proposed
techniques.",hidden subscription block
http://arxiv.org/abs/1211.0834v2,"We investigate stationary hidden Markov processes for which mutual
information between the past and the future is infinite. It is assumed that the
number of observable states is finite and the number of hidden states is
countably infinite. Under this assumption, we show that the block mutual
information of a hidden Markov process is upper bounded by a power law
determined by the tail index of the hidden state distribution. Moreover, we
exhibit three examples of processes. The first example, considered previously,
is nonergodic and the mutual information between the blocks is bounded by the
logarithm of the block length. The second example is also nonergodic but the
mutual information between the blocks obeys a power law. The third example
obeys the power law and is ergodic.",hidden subscription block
http://arxiv.org/abs/1503.04344v1,"Recently, economic depression, which scoured all over the world, affects
business organizations and banking sectors. Such economic pose causes a severe
attrition for banks and customer retention becomes impossible. Accordingly,
marketing managers are in need to increase marketing campaigns, whereas
organizations evade both expenses and business expansion. In order to solve
such riddle, data mining techniques is used as an uttermost factor in data
analysis, data summarizations, hidden pattern discovery, and data
interpretation. In this paper, rough set theory and decision tree mining
techniques have been implemented, using a real marketing data obtained from
Portuguese marketing campaign related to bank deposit subscription [Moro et
al., 2011]. The paper aims to improve the efficiency of the marketing campaigns
and helping the decision makers by reducing the number of features, that
describes the dataset and spotting on the most significant ones, and predict
the deposit customer retention criteria based on potential predictive rules.",hidden subscription block
http://arxiv.org/abs/1811.07088v2,"Although many scalable event matching algorithms have been proposed to
achieve scalability for large-scale content-based networks, content-based
publish/subscribe networks (especially for large-scale real time systems) still
suffer performance deterioration when subscription scale increases. While
subscription aggregation techniques can be useful to reduce the amount of
subscription dissemination traffic and the subscription table size by
exploiting the similarity among subscriptions, efficient subscription
aggregation is not a trivial task to accomplish. Previous research works have
proved that it is either a NP-Complete or a co-NP complete problem. In this
paper, we propose DLS (Discrete Label Set), a novel subscription representation
model, and design algorithms to achieve the mapping from traditional Boolean
predicate model to the DLS model. Based on the DLS model, we propose a
subscription aggregation algorithm with O(1) time complexity in most cases, and
an event matching algorithm with O(1) time complexity. The significant
performance improvement is at the cost of memory consumption and controllable
false positive rate. Our theoretical analysis shows that these algorithms are
inherently scalable and can achieve real time event matching in a large-scale
content-based publish/subscribe network. We discuss the tradeoff between
memory, false positive rate and partition granules of content space.
Experimental results show that proposed algorithms achieve expected
performance. With the increasing of computer memory capacity and the dropping
of memory price, more and more large-scale real time applications can benefit
from our proposed DLS model, such as stock quote distribution, earthquake
monitoring, and severe weather alert.",hidden subscription block
http://arxiv.org/abs/cs/0607015v1,"Latent Semantic Analysis (LSA) is a well known method for information
retrieval. It has also been applied as a model of cognitive processing and
word-meaning acquisition. This dual importance of LSA derives from its capacity
to modulate the meaning of words by contexts, dealing successfully with
polysemy and synonymy. The underlying reasons that make the method work are not
clear enough. We propose that the method works because it detects an underlying
block structure (the blocks corresponding to topics) in the term by document
matrix. In real cases this block structure is hidden because of perturbations.
We propose that the correct explanation for LSA must be searched in the
structure of singular vectors rather than in the profile of singular values.
Using Perron-Frobenius theory we show that the presence of disjoint blocks of
documents is marked by sign-homogeneous entries in the vectors corresponding to
the documents of one block and zeros elsewhere. In the case of nearly disjoint
blocks, perturbation theory shows that if the perturbations are small the zeros
in the leading vectors are replaced by small numbers (pseudo-zeros). Since the
singular values of each block might be very different in magnitude, their order
does not mirror the order of blocks. When the norms of the blocks are similar,
LSA works fine, but we propose that when the topics have different sizes, the
usual procedure of selecting the first k singular triplets (k being the number
of blocks) should be replaced by a method that selects the perturbed Perron
vectors for each block.",hidden subscription block
http://arxiv.org/abs/1707.09123v1,"How to establish the matching (or corresponding) between two different 3D
shapes is a classical problem. This paper focused on the research on shape
mapping of 3D mesh models, and proposed a shape mapping algorithm based on
Hidden Markov Random Field and EM algorithm, as introducing a hidden state
random variable associated with the adjacent blocks of shape matching when
establishing HMRF. This algorithm provides a new theory and method to ensure
the consistency of the edge data of adjacent blocks, and the experimental
results show that the algorithm in this paper has a great improvement on the
shape mapping of 3D mesh models.",hidden subscription block
http://arxiv.org/abs/cs/0606085v1,"We propose a simple universal (that is, distribution--free) steganographic
system in which covertexts with and without hidden texts are statistically
indistinguishable. The stegosystem can be applied to any source generating
i.i.d. covertexts with unknown distribution, and the hidden text is transmitted
exactly, with zero probability of error. Moreover, the proposed steganographic
system has two important properties. First, the rate of transmission of hidden
information approaches the Shannon entropy of the covertext source as the size
of blocks used for hidden text encoding tends to infinity. Second, if the size
of the alphabet of the covertext source and its minentropy tend to infinity
then the number of bits of hidden text per letter of covertext tends to
$\log(n!)/n$ where $n$ is the (fixed) size of blocks used for hidden text
encoding. The proposed stegosystem uses randomization.",hidden subscription block
http://arxiv.org/abs/1307.2015v1,"We envision a publish/subscribe ontology system that is able to index
millions of user subscriptions and filter them against ontology data that
arrive in a streaming fashion. In this work, we propose a SPARQL extension
appropriate for a publish/subscribe setting; our extension builds on the
natural semantic graph matching of the language and supports the creation of
full-text subscriptions. Subsequently, we propose a main-memory subscription
indexing algorithm which performs both semantic and full-text matching at low
complexity and minimal filtering time. Thus, when ontology data are published
matching subscriptions are identified and notifications are forwarded to users.",hidden subscription block
http://arxiv.org/abs/1108.1510v1,"We investigate a stationary process's crypticity---a measure of the
difference between its hidden state information and its observed
information---using the causal states of computational mechanics. Here, we
motivate crypticity and cryptic order as physically meaningful quantities that
monitor how hidden a hidden process is. This is done by recasting previous
results on the convergence of block entropy and block-state entropy in a
geometric setting, one that is more intuitive and that leads to a number of new
results. For example, we connect crypticity to how an observer synchronizes to
a process. We show that the block-causal-state entropy is a convex function of
block length. We give a complete analysis of spin chains. We present a
classification scheme that surveys stationary processes in terms of their
possible cryptic and Markov orders. We illustrate related entropy convergence
behaviors using a new form of foliated information diagram. Finally, along the
way, we provide a variety of interpretations of crypticity and cryptic order to
establish their naturalness and pervasiveness. Hopefully, these will inspire
new applications in spatially extended and network dynamical systems.",hidden subscription block
http://arxiv.org/abs/1010.4786v2,"Similar to what happens between humans in the real world, in open multi-agent
systems distributed over the Internet, such as online social networks or wiki
technologies, agents often form coalitions by agreeing to act as a whole in
order to achieve certain common goals. However, agent coalitions are not always
a desirable feature of a system, as malicious or corrupt agents may collaborate
in order to subvert or attack the system. In this paper, we consider the
problem of hidden coalitions, whose existence and the purposes they aim to
achieve are not known to the system, and which carry out so-called underhand
attacks. We give a first approach to hidden coalitions by introducing a
deterministic method that blocks the actions of potentially dangerous agents,
i.e. possibly belonging to such coalitions. We also give a non-deterministic
version of this method that blocks the smallest set of potentially dangerous
agents. We calculate the computational cost of our two blocking methods, and
prove their soundness and completeness.",hidden subscription block
http://arxiv.org/abs/1808.02316v1,"This work is devoted to elaboration on the idea to use block term
decomposition for group data analysis and to raise the possibility of modelling
group activity with (Lr, 1) and Tucker blocks. A new generalization of block
tensor decomposition was considered in application to group data analysis.
Suggested approach was evaluated on multilabel classification task for a set of
images. This contribution also reports results of investigation on clustering
with proposed tensor models in comparison with known matrix models, namely
common orthogonal basis extraction and group independent component analysis.",hidden subscription block
http://arxiv.org/abs/1407.6968v1,"Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to
execute unmodified critical sections concurrently, even if they are protected
by the same lock. To ensure correctness, the transactions used to execute these
critical sections ""subscribe"" to the lock by reading it and checking that it is
available. A recent paper proposed using the tempting ""lazy subscription""
optimization for a similar technique in a different context, namely
transactional systems that use a single global lock (SGL) to protect all
transactional data. We identify several pitfalls that show that lazy
subscription \emph{is not safe} for TLE because unmodified critical sections
executing before subscribing to the lock may behave incorrectly in a number of
subtle ways. We also show that recently proposed compiler support for modifying
transaction code to ensure subscription occurs before any incorrect behavior
could manifest is not sufficient to avoid all of the pitfalls we identify. We
further argue that extending such compiler support to avoid all pitfalls would
add substantial complexity and would usually limit the extent to which
subscription can be deferred, undermining the effectiveness of the
optimization. Hardware extensions suggested in the recent proposal also do not
address all of the pitfalls we identify. In this extended version of our WTTM
2014 paper, we describe hardware extensions that make lazy subscription safe,
both for SGL-based transactional systems and for TLE, without the need for
special compiler support. We also explain how nontransactional loads can be
exploited, if available, to further enhance the effectiveness of lazy
subscription.",hidden subscription block
http://arxiv.org/abs/1705.07069v3,"We consider Oblivious Shuffling and K-Oblivious Shuffling, a refinement
thereof. We provide efficient algorithms for both and discuss their application
to the design of Oblivious RAM. The task of K-Oblivious Shuffling is to
obliviously shuffle N encrypted blocks that have been randomly allocated on the
server in such a way that an adversary learns nothing about the new allocation
of blocks. The security guarantee should hold also with respect to an adversary
that has learned the initial position of K touched blocks out of the N blocks.
The classical notion of Oblivious Shuffling is obtained for K = N.
  We present a family of algorithms for Oblivious Shuffling. Our first
construction, CacheShuffleRoot, is tailored for clients with $O(\sqrt{N})$
blocks of memory and uses $(4+\epsilon)N$ blocks of bandwidth, for every
$\epsilon > 0$. CacheShuffleRoot is a 4.5x improvement over previous best known
results on practical sizes of N. We also present CacheShuffle that obliviously
shuffles using O(S) blocks of client memory with $O(N\log_S N)$ blocks of
bandwidth.
  We then turn to K-Oblivious Shuffling and give algorithms that require 2N +
f(K) blocks of bandwidth, for some function f. That is, any extra bandwidth
above the 2N lower bound depends solely on K. We present KCacheShuffleBasic
that uses O(K) client storage and exactly 2N blocks of bandwidth. For smaller
client storage requirements, we show KCacheShuffle, which uses O(S) client
storage and requires $2N+(1+\epsilon)O(K\log_S K)$ blocks of bandwidth.
  Finally, we consider the case in which, in addition to the N blocks, the
server stores D dummy blocks whose content is is irrelevant but still their
positions must be hidden by the shuffling. For this case, we design algorithm
KCacheShuffleDummy that, for N + D blocks and K touched blocks, uses O(K)
client storage and $D+(2+\epsilon)N$ blocks of bandwidth.",hidden subscription block
http://arxiv.org/abs/1607.05171v1,"The Long Term Evolution (LTE) is the latest mobile standard being implemented
globally to provide connectivity and access to advanced services for personal
mobile devices. Moreover, LTE networks are considered to be one of the main
pillars for the deployment of Machine to Machine (M2M) communication systems
and the spread of the Internet of Things (IoT). As an enabler for advanced
communications services with a subscription count in the billions, security is
of capital importance in LTE. Although legacy GSM (Global System for Mobile
Communications) networks are known for being insecure and vulnerable to rogue
base stations, LTE is assumed to guarantee confidentiality and strong
authentication. However, LTE networks are vulnerable to security threats that
tamper availability, privacy and authentication. This manuscript, which
summarizes and expands the results presented by the author at ShmooCon 2016
\cite{jover2016lte}, investigates the insecurity rationale behind LTE protocol
exploits and LTE rogue base stations based on the analysis of real LTE radio
link captures from the production network. Implementation results are discussed
from the actual deployment of LTE rogue base stations, IMSI catchers and
exploits that can potentially block a mobile device. A previously unknown
technique to potentially track the location of mobile devices as they move from
cell to cell is also discussed, with mitigations being proposed.",hidden subscription block
http://arxiv.org/abs/1308.4922v2,"Unsupervised deep learning is one of the most powerful representation
learning techniques. Restricted Boltzman machine, sparse coding, regularized
auto-encoders, and convolutional neural networks are pioneering building blocks
of deep learning. In this paper, we propose a new building block -- distributed
random models. The proposed method is a special full implementation of the
product of experts: (i) each expert owns multiple hidden units and different
experts have different numbers of hidden units; (ii) the model of each expert
is a k-center clustering, whose k-centers are only uniformly sampled examples,
and whose output (i.e. the hidden units) is a sparse code that only the
similarity values from a few nearest neighbors are reserved. The relationship
between the pioneering building blocks, several notable research branches and
the proposed method is analyzed. Experimental results show that the proposed
deep model can learn better representations than deep belief networks and
meanwhile can train a much larger network with much less time than deep belief
networks.",hidden subscription block
http://arxiv.org/abs/1512.06425v2,"Acyclic overlays used for broker-based publish/subscribe systems provide
unique paths for content-based routing from a publisher to interested
subscribers. Cyclic overlays may provide multiple paths, however, the
subscription broadcast process generates one content-based routing path per
subscription. This poses serious challenges in offering dynamic routing of
notifications when congestion is detected because instantaneous updates in
routing tables are required to generate alternative routing paths. This paper
introduces the first subscription-based publish/subscribe system, OctopiS,
which offers inter-cluster dynamic routing when congestion in the output queues
is detected. OctopiS is based on a formally defined Structured Cyclic Overlay
Topology (SCOT). SCOT is divided into homogeneous clusters where each cluster
has equal number of brokers and connects to other clusters through multiple
inter-cluster overlay links. These links are used to provide parallel routing
paths between publishers and subscribers connected to brokers in different
clusters. While aiming at deployment at data center networks, OctopiS generates
subscription-trees of shortest lengths used by Static Notification Routing
(SNR) algorithm. Dynamic Notification Routing (DNR) algorithm uses a bit-vector
mechanism to exploit the structuredness of a clustered SCOT to offer
inter-cluster dynamic routing without making updates in routing tables and
minimizing load on overwhelmed brokers and congested links. Experiments on a
cluster testbed with real world data show that OctopiS is scalable and reduces
the number of inter-broker messages in subscription delivery by 89%,
subscription delay by 77%, end-to-end notification delay in static and dynamic
routing by 47% and 58% respectively, and the lengths of output queues of
brokers in dynamic routing paths by 59%.",hidden subscription block
http://arxiv.org/abs/1706.10172v1,"In this paper we investigate the behavioural differences between mobile phone
customers with prepaid and postpaid subscriptions. Our study reveals that (a)
postpaid customers are more active in terms of service usage and (b) there are
strong structural correlations in the mobile phone call network as connections
between customers of the same subscription type are much more frequent than
those between customers of different subscription types. Based on these
observations we provide methods to detect the subscription type of customers by
using information about their personal call statistics, and also their
egocentric networks simultaneously. The key of our first approach is to cast
this classification problem as a problem of graph labelling, which can be
solved by max-flow min-cut algorithms. Our experiments show that, by using both
user attributes and relationships, the proposed graph labelling approach is
able to achieve a classification accuracy of $\sim 87\%$, which outperforms by
$\sim 7\%$ supervised learning methods using only user attributes. In our
second problem we aim to infer the subscription type of customers of external
operators. We propose via approximate methods to solve this problem by using
node attributes, and a two-ways indirect inference method based on observed
homophilic structural correlations. Our results have straightforward
applications in behavioural prediction and personal marketing.",hidden subscription block
http://arxiv.org/abs/1801.05071v2,"Low probability of detection (or covert) communication refers to the scenario
where information must be sent reliably to a receiver, but with low probability
of detection by an adversary. Recent works on the fundamental limits of this
communication problem have established achievability and converse bounds that
are asymptotic in the block length of the code. This paper uses Gallager's
random coding bound to derive a new achievability bound that is applicable to
low probability of detection communication in the finite block length regime.
Further insights are unveiled that are otherwise hidden in previous asymptotic
analyses.",hidden subscription block
http://arxiv.org/abs/1301.2316v1,"DAG models with hidden variables present many difficulties that are not
present when all nodes are observed. In particular, fully observed DAG models
are identified and correspond to well-defined sets ofdistributions, whereas
this is not true if nodes are unobserved. Inthis paper we characterize exactly
the set of distributions given by a class of one-dimensional Gaussian latent
variable models. These models relate two blocks of observed variables, modeling
only the cross-covariance matrix. We describe the relation of this model to the
singular value decomposition of the cross-covariance matrix. We show that,
although the model is underidentified, useful information may be extracted. We
further consider an alternative parametrization in which one latent variable is
associated with each block. Our analysis leads to some novel covariance
equivalence results for Gaussian hidden variable models.",hidden subscription block
http://arxiv.org/abs/1805.06951v1,"Coordinate ascent variational inference is an important algorithm for
inference in probabilistic models, but it is slow because it updates only a
single variable at a time. Block coordinate methods perform inference faster by
updating blocks of variables in parallel. However, the speed and stability of
these algorithms depends on how the variables are partitioned into blocks. In
this paper, we give a stable parallel algorithm for inference in deep
exponential families that doesn't require the variables to be partitioned into
blocks. We achieve this by lower bounding the ELBO by a new objective we call
the forest mixture bound (FM bound) that separates the inference problem for
variables within a hidden layer. We apply this to the simple case when all
random variables are Gaussian and show empirically that the algorithm converges
faster for models that are inherently more forest-like.",hidden subscription block
http://arxiv.org/abs/1604.06853v1,"Cyclic or general overlays may provide multiple paths between publishers and
subscribers. However, an advertisement tree and a matching subscription
activates only one path for notifications routing in publish/subscribe systems.
This poses serious challenges in handling network conditions like congestion,
and link or broker failures. Further, content-based dynamic routing of
notifications requires instantaneous updates in routing paths, which is not a
scalable option. This paper introduces a clustering approach with a bit-vector
technique for inter-cluster dynamic routing of notifications in a structured
cyclic topology that provides multiple paths between publishers and interested
subscribers. The advertisement forwarding process exploits the structured
nature of the overlay topology to generate advertisement trees of length 1
without generating duplicate messages in the advertisement forwarding process.
Issued subscriptions are divided into multiple disjoint subgropus, where each
subscription is broadcast to a cluster, which is a limited part of the
structured cyclic overlay network. We implemented novel static and
intra-cluster dynamic routing algorithms in the proposed overlay topology for
our advertisement-based publish/subscribe system, called OctopiA. We also
performed a pragmatic comparison of our two algorithms with the
state-of-the-art. Experiments on a cluster testbed show that our approach
generates fewer inter-broker messages, and is scalable.",hidden subscription block
http://arxiv.org/abs/1401.3842v1,"Call control features (e.g., call-divert, voice-mail) are primitive options
to which users can subscribe off-line to personalise their service. The
configuration of a feature subscription involves choosing and sequencing
features from a catalogue and is subject to constraints that prevent
undesirable feature interactions at run-time. When the subscription requested
by a user is inconsistent, one problem is to find an optimal relaxation, which
is a generalisation of the feedback vertex set problem on directed graphs, and
thus it is an NP-hard task. We present several constraint programming
formulations of the problem. We also present formulations using partial
weighted maximum Boolean satisfiability and mixed integer linear programming.
We study all these formulations by experimentally comparing them on a variety
of randomly generated instances of the feature subscription problem.",hidden subscription block
http://arxiv.org/abs/1905.12032v1,"Despite the great achievements of deep neural networks (DNNs), the
vulnerability of state-of-the-art DNNs raises security concerns of DNNs in many
application domains requiring high reliability.We propose the fault sneaking
attack on DNNs, where the adversary aims to misclassify certain input images
into any target labels by modifying the DNN parameters. We apply ADMM
(alternating direction method of multipliers) for solving the optimization
problem of the fault sneaking attack with two constraints: 1) the
classification of the other images should be unchanged and 2) the parameter
modifications should be minimized. Specifically, the first constraint requires
us not only to inject designated faults (misclassifications), but also to hide
the faults for stealthy or sneaking considerations by maintaining model
accuracy. The second constraint requires us to minimize the parameter
modifications (using L0 norm to measure the number of modifications and L2 norm
to measure the magnitude of modifications). Comprehensive experimental
evaluation demonstrates that the proposed framework can inject multiple
sneaking faults without losing the overall test accuracy performance.",sneak into basket block
http://arxiv.org/abs/1207.5466v1,"In order to generate synthetic basket data sets for better benchmark testing,
it is important to integrate characteristics from real-life databases into the
synthetic basket data sets. The characteristics that could be used for this
purpose include the frequent itemsets and association rules. The problem of
generating synthetic basket data sets from frequent itemsets is generally
referred to as inverse frequent itemset mining. In this paper, we show that the
problem of approximate inverse frequent itemset mining is {\bf NP}-complete.
Then we propose and analyze an approximate algorithm for approximate inverse
frequent itemset mining, and discuss privacy issues related to the synthetic
basket data set. In particular, we propose an approximate algorithm to
determine the privacy leakage in a synthetic basket data set.",sneak into basket block
http://arxiv.org/abs/1901.05577v2,"In order to better engage with customers, retailers rely on extensive
customer and product databases which allows them to better understand customer
behaviour and purchasing patterns. This has long been a challenging task as
customer modelling is a multi-faceted, noisy and time-dependent problem. The
most common way to tackle this problem is indirectly through task-specific
supervised learning prediction problems, with relatively little literature on
modelling a customer by directly simulating their future transactions. In this
paper we propose a method for generating realistic sequences of baskets that a
given customer is likely to purchase over a period of time. Customer embedding
representations are learned using a Recurrent Neural Network (RNN) which takes
into account the entire sequence of transaction data. Given the customer state
at a specific point in time, a Generative Adversarial Network (GAN) is trained
to generate a plausible basket of products for the following week. The newly
generated basket is then fed back into the RNN to update the customer's state.
The GAN is thus used in tandem with the RNN module in a pipeline alternating
between basket generation and customer state updating steps. This allows for
sampling over a distribution of a customer's future sequence of baskets, which
then can be used to gain insight into how to service the customer more
effectively. The methodology is empirically shown to produce baskets that
appear similar to real baskets and enjoy many common properties, including
frequencies of different product types, brands, and prices. Furthermore, the
generated data is able to replicate most of the strongest sequential patterns
that exist between product types in the real data.",sneak into basket block
http://arxiv.org/abs/1609.03617v1,"Diversity is a fundamental feature of ecosystems, even when the concept of
ecosystem is extended to sociology or economics. Diversity can be intended as
the count of different items, animals, or, more generally, interactions. There
are two classes of stylized facts that emerge when diversity is taken into
account. The first are Diversity explosions: evolutionary radiations in
biology, or the process of escaping 'Poverty Traps' in economics are two well
known examples. The second is nestedness: entities with a very diverse set of
interactions are the only ones that interact with more specialized ones. In a
single sentence: specialists interact with generalists. Nestedness is observed
in a variety of bipartite networks of interactions: Biogeographic,
macroeconomic and mutualistic to name a few. This indicates that entities
diversify following a pattern. Since they appear in such very different
systems, these two stylized facts point out that the build up of diversity is
driven by a fundamental probabilistic mechanism, and here we sketch its minimal
features. We show how the contraction of a random tripartite network, which is
maximally entropic in all its degree distributions but one, can reproduce
stylized facts of real data with great accuracy which is qualitatively lost
when that degree distribution is changed. We base our reasoning on the
combinatoric picture that the nodes on one layer of these bipartite networks
can be described as combinations of a number of fundamental building blocks.
The stylized facts of diversity that we observe in real systems can be
explained with an extreme heterogeneity (a scale-free distribution) in the
number of meaningful combinations in which each building block is involved. We
show that if the usefulness of the building blocks has a scale-free
distribution, then maximally entropic baskets of building blocks will give rise
to very rich behaviors.",sneak into basket block
http://arxiv.org/abs/1905.13131v2,"Personalization in marketing aims at improving the shopping experience of
customers by tailoring services to individuals. In order to achieve this,
businesses must be able to make personalized predictions regarding the next
purchase. That is, one must forecast the exact list of items that will comprise
the next purchase, i.e., the so-called market basket. Despite its relevance to
firm operations, this problem has received surprisingly little attention in
prior research, largely due to its inherent complexity. In fact,
state-of-the-art approaches are limited to intuitive decision rules for pattern
extraction. However, the simplicity of the pre-coded rules impedes performance,
since decision rules operate in an autoregressive fashion: the rules can only
make inferences from past purchases of a single customer without taking into
account the knowledge transfer that takes place between customers. In contrast,
our research overcomes the limitations of pre-set rules by contributing a novel
predictor of market baskets from sequential purchase histories: our predictions
are based on similarity matching in order to identify similar purchase habits
among the complete shopping histories of all customers. Our contributions are
as follows: (1) We propose similarity matching based on subsequential dynamic
time warping (SDTW) as a novel predictor of market baskets. Thereby, we can
effectively identify cross-customer patterns. (2) We leverage the Wasserstein
distance for measuring the similarity among embedded purchase histories. (3) We
develop a fast approximation algorithm for computing a lower bound of the
Wasserstein distance in our setting. An extensive series of computational
experiments demonstrates the effectiveness of our approach. The accuracy of
identifying the exact market baskets based on state-of-the-art decision rules
from the literature is outperformed by a factor of 4.0.",sneak into basket block
http://arxiv.org/abs/1304.1582v2,"On March 11, 2013 I talked with Warren Wiscombe about his contributions to
scientific computer programming, atmospheric science and radiative transfer.
Our conversation is divided into three parts related to light scattering,
radiative transfer and his general thoughts about scientific programming. There
are some reflections on how radiative transfer parameterizations gradually
sneaked in to modern climate and atmospheric Global Circulation Models. Why
some software programs such as light scattering code MIEV and DISORT are very
successful and why some of them fizzle. We talked about the role of tools in
modern science, open source movement, repeatability of scientific results,
computer languages, computer programs as objects of arts, and even if programs
can be revolutionary.",sneak into basket block
http://arxiv.org/abs/1404.0286v1,"We study wear-leveling techniques for cuckoo hashing, showing that it is
possible to achieve a memory wear bound of $\log\log n+O(1)$ after the
insertion of $n$ items into a table of size $Cn$ for a suitable constant $C$
using cuckoo hashing. Moreover, we study our cuckoo hashing method empirically,
showing that it significantly improves on the memory wear performance for
classic cuckoo hashing and linear probing in practice.",sneak into basket block
http://arxiv.org/abs/1809.09621v1,"Complementary products recommendation is an important problem in e-commerce.
Such recommendations increase the average order price and the number of
products in baskets. Complementary products are typically inferred from basket
data. In this study, we propose the BB2vec model. The BB2vec model learns
vector representations of products by analyzing jointly two types of data -
Baskets and Browsing sessions (visiting web pages of products). These vector
representations are used for making complementary products recommendation. The
proposed model alleviates the cold start problem by delivering better
recommendations for products having few or no purchases. We show that the
BB2vec model has better performance than other models which use only basket
data.",sneak into basket block
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",sneak into basket block
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",sneak into basket block
http://arxiv.org/abs/1809.03222v2,"In this paper we analyse the bipartite Colombian firms-products network,
throughout a period of five years, from 2010 to 2014. Our analysis depicts a
strongly modular system, with several groups of firms specializing in the
export of specific categories of products. These clusters have been detected by
running the bipartite variant of the traditional modularity maximization,
revealing a bi-modular structure. Interestingly, this finding is refined by
applying a recently-proposed algorithm for projecting bipartite networks on the
layer of interest and, then, running the Louvain algorithm on the resulting
monopartite representations. Important structural differences emerge upon
comparing the Colombian firms-products network with the World Trade Web, in
particular, the bipartite representation of the latter is not characterized by
a similar block-structure, as the modularity maximization fails in revealing
(bipartite) nodes clusters. This points out that economic systems behave
differently at different scales: while countries tend to diversify their
production --potentially exporting a large number of different products-- firms
specialize in exporting (substantially very limited) baskets of basically
homogeneous products.",sneak into basket block
http://arxiv.org/abs/1809.00072v2,"Resistive crossbars have emerged as promising building blocks for realizing
DNNs due to their ability to compactly and efficiently realize the dominant DNN
computational kernel, viz., vector-matrix multiplication. However, a key
challenge with resistive crossbars is that they suffer from a range of device
and circuit level non-idealities such as interconnect parasitics, peripheral
circuits, sneak paths, and process variations. These non-idealities can lead to
errors in vector-matrix multiplication that eventually degrade the DNN's
accuracy. There has been no study of the impact of non-idealities on the
accuracy of large-scale DNNs, in part because existing device and circuit
models are infeasible to use in application-level evaluation. In this work, we
present a fast and accurate simulation framework to enable evaluation and
re-training of large-scale DNNs on resistive crossbar based hardware fabrics.
  We first characterize the impact of crossbar non-idealities on errors
incurred in the realized vector-matrix multiplications and observe that the
errors have significant data and hardware-instance dependence that should be
considered. We propose a Fast Crossbar Model (FCM) to accurately capture the
errors arising due to crossbar non-idealities while being four-to-five orders
of magnitude faster than circuit simulation. Finally, we develop RxNN, a
software framework to evaluate and re-train DNNs on resistive crossbar systems.
RxNN is based on the popular Caffe machine learning framework, and we use it to
evaluate a suite of large-scale DNNs developed for the ImageNet Challenge
(ILSVRC). Our experiments reveal that resistive crossbar non-idealities can
lead to significant accuracy degradations (9.6%-32%) for these large-scale
DNNs. To the best of our knowledge, this work is the first quantitative
evaluation of the accuracy of large-scale DNNs on resistive crossbar based
hardware.",sneak into basket block
http://arxiv.org/abs/1210.6447v1,"The objective of this paper is to take some aspects of disk scheduling and
scheduling algorithms. The disk scheduling is discussed with a sneak peak in
general and selection of algorithm in particular.",sneak into basket block
http://arxiv.org/abs/1507.02077v1,"Passive crossbar arrays based upon memristive devices, at crosspoints, hold
great promise for the future high-density and non-volatile memories. The most
significant challenge facing memristive device based crossbars today is the
problem of sneak-path currents. In this paper, we investigate a memristive
device with intrinsic rectification behavior to suppress the sneak-path
currents in crossbar arrays. The device model is implemented in Verilog-A
language and is simulated to match device characteristics readily available in
the literature. Then, we systematically evaluate the read operation performance
of large-scale crossbar arrays utilizing our proposed model in terms of read
margin and power consumption while considering different crossbar sizes,
interconnect resistance values, HRS/LRS (High Resistance State/Low Resistance
State) values, rectification ratios and different read-schemes. The outcomes of
this study are understanding the trade-offs among read margin, power
consumption, read-schemes and most importantly providing a guideline for
circuit designers to improve the performance of a memory based crossbar
structure. In addition, read operation performance comparison of the intrinsic
rectifying memristive device model with other memristive device models are
studied.",sneak into basket block
http://arxiv.org/abs/0708.3446v2,"We present extended multi block approach in the LIPI Public Cluster. The
multi block approach enables a cluster to be divided into several independent
blocks which run jobs owned by different users simultaneously. Previously, we
have maintained the blocks using single master node for all blocks due to
efficiency and resource limitations. Following recent advancements and
expansion of node\'s number, we have modified the multi block approach with
multiple master nodes, each of them is responsible for a single block. We argue
that this approach improves the overall performance significantly, for
especially data intensive computational works.",sneak into basket block
http://arxiv.org/abs/1908.05203v2,"The Q-method has been utilized over time in various areas, including
information systems. In this study, we used a systematic mapping to illustrate
how the Q-method was applied within Information Systems (IS) community and
proposing towards the integration of Q-method into the Design Sciences Research
(DSR) process as a tool for future research DSR-based IS studies. In this
mapping study, we collected peer-reviewed journals from Basket-of-Eight
journals and the digital library of the Association for Information Systems
(AIS). Then we grouped the publications according to the process of DSR, and
different variables for preparing Q-method from IS publications. We found that
the potential of the Q-methodology can be used to support each main research
stage of DSR processes and can serve as the useful tool to evaluate a system in
the IS topic of system analysis and design",sneak into basket block
http://arxiv.org/abs/1607.04950v2,"Sophisticated malware authors can sneak hidden malicious code into portable
executable files, and this code can be hard to detect, especially if encrypted
or compressed. However, when an executable file switches between code regimes
(e.g. native, encrypted, compressed, text, and padding), there are
corresponding shifts in the file's representation as an entropy signal. In this
paper, we develop a method for automatically quantifying the extent to which
patterned variations in a file's entropy signal make it ""suspicious."" In
Experiment 1, we use wavelet transforms to define a Suspiciously Structured
Entropic Change Score (SSECS), a scalar feature that quantifies the
suspiciousness of a file based on its distribution of entropic energy across
multiple levels of spatial resolution. Based on this single feature, it was
possible to raise predictive accuracy on a malware detection task from 50.0% to
68.7%, even though the single feature was applied to a heterogeneous corpus of
malware discovered ""in the wild."" In Experiment 2, we describe how
wavelet-based decompositions of software entropy can be applied to a parasitic
malware detection task involving large numbers of samples and features. By
extracting only string and entropy features (with wavelet decompositions) from
software samples, we are able to obtain almost 99% detection of parasitic
malware with fewer than 1% false positives on good files. Moreover, the
addition of wavelet-based features uniformly improved detection performance
across plausible false positive rates, both in a strings-only model (e.g., from
80.90% to 82.97%) and a strings-plus-entropy model (e.g. from 92.10% to 94.74%,
and from 98.63% to 98.90%). Overall, wavelet decomposition of software entropy
can be useful for machine learning models for detecting malware based on
extracting millions of features from executable files.",sneak into basket block
http://arxiv.org/abs/1611.09512v1,"A new, fair relay selection scheme is proposed for a dual-hop
decode-and-forward network with randomly-distributed relays. Most of the
reported works in the literature achieve fairness at the expense of degrading
the outage probability performance. In addition, they often assume that the
number and locations of the relays are known. In contrast, the proposed scheme
achieves fairness in a random field of relays without deteriorating the outage
probability performance. In this scheme, each relay maintains a countdown timer
whose initial value is a function of the relay location and a tunable parameter
which controls the level of fairness. The optimum value of this parameter is
evaluated in an offline manner so as to achieve fairness by making the average
powers consumed by the relays as close as possible. An exact analytical
expression is derived for the average power consumed by each relay. This
expression is then used to show the superiority of the proposed scheme over
opportunistic relaying and random relay selection schemes.",countdown timer
http://arxiv.org/abs/1901.00748v1,"Protecting the passengers' safety and increasing ridership are two never
ending pursuits of public transit agencies. One of the proposed methods to
achieve both goals for subway service is to implement real time train arriving
countdown clocks in subway stations. Metropolitan Transportation Authority
(MTA) of New York City (NYC) chose to install such countdown clocks in their
stations starting from 2007 on a selection of subway lines. Due to the recent
development of Bluetooth Beacon technology, the MTA could now install countdown
clocks and train trackers in a non intrusive manner with much faster speed. As
a result, the MTA is aiming to install countdown clocks in every subway station
on every line. However, with such an aggressive plan, the impact of countdown
clocks on subway ridership has not been fully studied. This paper proposes
using Panel Regression methods, specifically, Random Effect (RE) model and
Fixed Effect (FE) model to quantify the impact of countdown clocks on subway
ridership. Machine Learning methods, namely Random Forest (RF) with AdaBoost
and Decision Tree (DT) Regression, are also used as alternative data driven
approaches for the FE and RE model. The results show that for the G line
service, which runs between Brooklyn and Queens, the introduction of countdown
clocks could increase weekly ridership by about 1783 per station. The study
also found that the machine learning methods provide better accuracy in
predicting the ridership than RE and FE models.",countdown timer
http://arxiv.org/abs/0909.1241v1,"Timer-based mechanisms are often used to help a given (sink) node select the
best helper node among many available nodes. Specifically, a node transmits a
packet when its timer expires, and the timer value is a monotone non-increasing
function of its local suitability metric. The best node is selected
successfully if no other node's timer expires within a 'vulnerability' window
after its timer expiry, and so long as the sink can hear the available nodes.
In this paper, we show that the optimal metric-to-timer mapping that (i)
maximizes the probability of success or (ii) minimizes the average selection
time subject to a minimum constraint on the probability of success, maps the
metric into a set of discrete timer values. We specify, in closed-form, the
optimal scheme as a function of the maximum selection duration, the
vulnerability window, and the number of nodes. An asymptotic characterization
of the optimal scheme turns out to be elegant and insightful. For any
probability distribution function of the metric, the optimal scheme is
scalable, distributed, and performs much better than the popular inverse metric
timer mapping. It even compares favorably with splitting-based selection, when
the latter's feedback overhead is accounted for.",countdown timer
http://arxiv.org/abs/1706.04252v1,"A great deal of effort has gone into trying to model social influence ---
including the spread of behavior, norms, and ideas --- on networks. Most models
of social influence tend to assume that individuals react to changes in the
states of their neighbors without any time delay, but this is often not true in
social contexts, where (for various reasons) different agents can have
different response times. To examine such situations, we introduce the idea of
a timer into threshold models of social influence. The presence of timers on
nodes delays the adoption --- i.e., change of state --- of each agent, which in
turn delays the adoptions of its neighbors. With a homogeneous-distributed
timer, in which all nodes exhibit the same amount of delay, adoption delays are
also homogeneous, so the adoption order of nodes remains the same. However,
heterogeneously-distributed timers can change the adoption order of nodes and
hence the ""adoption paths"" through which state changes spread in a network.
Using a threshold model of social contagions, we illustrate that heterogeneous
timers can either accelerate or decelerate the spread of adoptions compared to
an analogous situation with homogeneous timers, and we investigate the
relationship of such acceleration or deceleration with respect to timer
distribution and network structure. We derive an analytical approximation for
the temporal evolution of the fraction of adopters by modifying a pair
approximation of the Watts threshold model, and we find good agreement with
numerical computations. We also examine our new timer model on networks
constructed from empirical data.",countdown timer
http://arxiv.org/abs/1806.07258v2,"Power and energy consumption is becoming key challenges to deploy the first
exascale supercomputer successfully. Large-scale HPC applications waste a
significant amount of power in communication and synchronization-related idle
times. However, due to the time scale at which communication happens,
transitioning in low power states during communication's idle times may
introduce unacceptable overhead in applications' execution time. In this paper,
we present COUNTDOWN, a runtime library, supported by a methodology and
analysis tool for identifying and automatically reducing the power consumption
of the computing elements during communication and synchronization. COUNTDOWN
saves energy without imposing significant time-to-completion increase by
lowering CPUs power consumption only during idle times for which power state
transition overhead are negligible. This is done transparently to the user,
without requiring labor-intensive and error-prone application code
modifications, nor requiring recompilation of the application. We test our
methodology in a production Tier-0 system. For the NAS benchmarks, COUNTDOWN
saves between 6% and 50% energy, with a time-to-solution penalty lower than 5%.
In a complete production --- Quantum ESPRESSO --- for a 3.5K cores run,
COUNTDOWN saves 22.36% energy, with a performance penalty below 3%. Energy
saving increases to 37% with a performance penalty of 6.38%, if the application
is executed without communication tuning.",countdown timer
http://arxiv.org/abs/0910.1217v1,"A feature of current membrane systems is the fact that objects and membranes
are persistent. However, this is not true in the real world. In fact, cells and
intracellular proteins have a well-defined lifetime. Inspired from these
biological facts, we define a model of systems of mobile membranes in which
each membrane and each object has a timer representing their lifetime. We show
that systems of mutual mobile membranes with and without timers have the same
computational power. An encoding of timed safe mobile ambients into systems of
mutual mobile membranes with timers offers a relationship between two
formalisms used in describing biological systems.",countdown timer
http://arxiv.org/abs/1502.05450v1,"The Countdown game is one of the oldest TV show running in the world. It
started broadcasting in 1972 on the french television and in 1982 on British
channel 4, and it has been running since in both countries. The game, while
extremely popular, never received any serious scientific attention, probably
because it seems too simple at first sight. We present in this article an
in-depth analysis of the numbers round of the countdown game. This includes a
complexity analysis of the game, an analysis of existing algorithms, the
presentation of a new algorithm that increases resolution speed by a factor of
20. It also includes some leads on how to turn the game into a more difficult
one, both for a human player and for a computer, and even to transform it into
a probably undecidable problem.",countdown timer
http://arxiv.org/abs/1710.09494v2,"Watchdog timers are devices that are commonly used to monitor the health of
safety-critical hardware and software systems. Their primary function is to
raise an alarm if the monitored systems fail to emit periodic ""heartbeats"" that
signal their well-being. In this paper we design and verify a molecular
watchdog timer for monitoring the health of programmed molecular nanosystems.
This raises new challenges because our molecular watchdog timer and the system
that it monitors both operate in the probabilistic environment of chemical
kinetics, where many failures are certain to occur and it is especially hard to
detect the absence of a signal.
  Our molecular watchdog timer is the result of an incremental design process
that uses goal-oriented requirements engineering, simulation, stochastic
analysis, and software verification tools. We demonstrate the molecular
watchdog's functionality by having it monitor a molecular oscillator. Both the
molecular watchdog timer and the oscillator are implemented as chemical
reaction networks, which are the current programming language of choice for
many molecular programming applications.",countdown timer
http://arxiv.org/abs/1201.5190v1,"Ubiquitously during experiments one encounters a situation where time lapse
between two events has to measured. For example during the oscillations of a
pendulum or a vibrating reed, the powering of a lamp and achieving of its full
intensity. The powering of a relay and the closure of its contacts etc.
Situations like these call for a time measuring device between two events.
Hence this article describes a general Bi-Event timer that can be used in a
physics lab for ubiquitous time lapse measurements during experiments. These
measurements in turn can be used to interpret other parameters like velocity,
acceleration etc. The timer described here is simple to build and accurate in
performance. The Bi-event occurence can be applied as a signal to the inputs of
the timer either on separate lines or along a single path in series as voltage
pulses.",countdown timer
http://arxiv.org/abs/1708.02808v2,"Accommodating Machine-to-Machine applications and their requirements is one
of the challenges on the way from LTE towards 5G networks. The envisioned high
density of devices, alongside with their sporadic and synchronized transmission
patterns, might create signaling storms and overload in the current LTE
network. Here, we address the notorious random access (RA) challenge, namely,
scalability of the radio link connection establishment protocol in LTE
networks. We revisit the binary countdown technique for contention resolution
(BCCR), and apply it to the LTE RA procedure. We analytically investigate the
performance gains and trade-offs of applying BCCR in LTE. We further
simulatively compare BCCR RA with the state-of-the-art RA techniques, and
demonstrate its advantages in terms of delay and throughput.",countdown timer
http://arxiv.org/abs/1904.08705v1,"Massive connectivity for Internet of Things applications is expected to
challenge the way access reservation protocols are designed in 5G networks.
Since the number of devices and their density are envisioned to be orders of
magnitude larger, state-of-the-art access reservation, Random Access (RA)
procedure, might be a bottleneck for end-to-end delay. This would be especially
challenging for burst arrival scenarios: Semi-synchronous triggering of a large
number of devices due to a common event (blackout, emergency alarm, etc.). In
this article, to improve RA procedure scalability, we propose to combine Binary
Countdown Contention Resolution (BCCR) with the state-of-the-art Access Class
Barring (ACB). We present a joint analysis of ACB and BCCR and apply a
framework for treating RA as a bi-objective optimization, minimizing the
resource consumption and maximizing the throughput of the procedure in every
contention round. We use this framework to devise dynamic load-adaptive
algorithm and simulatively illustrate that the proposed algorithm reduces the
burst resolution delay while consuming less resources compared to the
state-of-the-art techniques.",countdown timer
http://arxiv.org/abs/0908.1437v1,"Experiments in mechanics can often be timed by the sounds they produce. In
such cases, digital audio recordings provide a simple way of measuring time
intervals with an accuracy comparable to that of photogate timers. We
illustrate this with an experiment in the physics of sports: to measure the
speed of a hard-kicked soccer ball.",countdown timer
http://arxiv.org/abs/1104.0064v1,"I detail applications of timer interrupts in a popular micro-controller
family to time critical applications in laser-cooling type experiments. I
demonstrate a low overhead 1-bit frequency locking scheme and a multichannel
experimental sequencer using the timer-counter intterrupts to achieve accurate
timing along with flexible interfaces. The general purpose nature of
micro-controllers can offer unique functionality compared with commercial
solutions due to the flexibility of a computer controlled interface without the
poor latencies associated with computer timing.",countdown timer
http://arxiv.org/abs/1108.1361v1,"In this article, we examine the Location Management costs in mobile
communication networks utilizing the timer-based method. From the study of the
probabilities that a mobile terminal changes a number of Location Areas between
two calls, we identify a threshold value of 0.7 for the Call-to-Mobility Ratio
(CMR) below which the application of the timer-based method is most
appropriate. We characterize the valley appearing in the evolution of the costs
with the timeout period, showing that the time interval required to reach 90%
of the stabilized costs grows with the mobility index, the paging cost per
Location Area and the movement dimension, in opposition to the behavior
presented by the time interval that achieves the minimum of the costs. The
results obtained for CMRs below the suggested 0.7 threshold show that the
valley appearing in the costs tends to disappear for CMRs within [0.001, 0.7]
in onedimensional movements and within [0.2, 0.7] in two-dimensional ones, and
when the normalized paging cost per Location Area is below 0.3.",countdown timer
http://arxiv.org/abs/1902.06040v1,"Due to a hard dependency between time steps, large-scale simulations of gas
using the Direct Simulation Monte Carlo (DSMC) method proceed at the pace of
the slowest processor. Scalability is therefore achievable only by ensuring
that the work done each time step is as evenly apportioned among the processors
as possible. Furthermore, as the simulated system evolves, the load shifts, and
thus this load-balancing typically needs to be performed multiple times over
the course of a simulation. Common methods generally use either crude
performance models or processor-level timers. We combine both to create a
timer-augmented cost function which both converges quickly and yields
well-balanced processor decompositions. When compared to a particle-based
performance model alone, our method achieves 2x speedup at steady-state on up
to 1024 processors for a test case consisting of a Mach 9 argon jet impacting a
solid wall.",countdown timer
http://arxiv.org/abs/1906.10860v2,"As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases.",countdown timer
http://arxiv.org/abs/1502.05983v1,"In this paper we extend the knowledge on the problem of empirically searching
for sorting networks of minimal depth. We present new search space pruning
techniques for the last four levels of a candidate sorting network by
considering only the output set representation of a network. We present an
algorithm for checking whether an $n$-input sorting network of depth $d$ exists
by considering the minimal up to permutation and reflection itemsets at each
level and using the pruning at the last four levels. We experimentally
evaluated this algorithm to find the optimal depth sorting networks for all $n
\leq 12$.",countdown timer
http://arxiv.org/abs/1701.01654v2,"Washing machine is of great domestic necessity as it frees us from the burden
of washing our clothes and saves ample of our time. This paper will cover the
aspect of designing and developing of Fuzzy Logic based, Smart Washing Machine.
The regular washing machine (timer based) makes use of multi-turned timer based
start-stop mechanism which is mechanical as is prone to breakage. In addition
to its starting and stopping issues, the mechanical timers are not efficient
with respect of maintenance and electricity usage. Recent developments have
shown that merger of digital electronics in optimal functionality of this
machine is possible and nowadays in practice. A number of international
renowned companies have developed the machine with the introduction of smart
artificial intelligence. Such a machine makes use of sensors and smartly
calculates the amount of run-time (washing time) for the main machine motor.
Realtime calculations and processes are also catered in optimizing the run-time
of the machine. The obvious result is smart time management, better economy of
electricity and efficiency of work. This paper deals with the indigenization of
FLC (Fuzzy Logic Controller) based Washing Machine, which is capable of
automating the inputs and getting the desired output (wash-time).",countdown timer
http://arxiv.org/abs/1711.03941v3,"Caching algorithms are usually described by the eviction method and analyzed
using a metric of hit probability. Since contents have different importance
(e.g. popularity), the utility of a high hit probability, and the cost of
transmission can vary across contents. In this paper, we consider timer-based
(TTL) policies across a cache network, where contents have differentiated
timers over which we optimize. Each content is associated with a utility
measured in terms of the corresponding hit probability. We start our analysis
from a linear cache network: we propose a utility maximization problem where
the objective is to maximize the sum of utilities and a cost minimization
problem where the objective is to minimize the content transmission cost across
the network. These frameworks enable us to design online algorithms for cache
management, for which we prove achieving optimal performance. Informed by the
results of our analysis, we formulate a non-convex optimization problem for a
general cache network. We show that the duality gap is zero, hence we can
develop a distributed iterative primal-dual algorithm for content management in
the network. Numerical evaluations show that our algorithm significant
outperforms path replication with traditional caching algorithms over some
network topologies. Finally, we consider a direct application of our cache
network model to content distribution.",countdown timer
http://arxiv.org/abs/1902.10369v3,"We consider the task of measuring time with probabilistic threshold gates
implemented by bio-inspired spiking neurons. In the model of spiking neural
networks, network evolves in discrete rounds, where in each round, neurons fire
in pulses in response to a sufficiently high membrane potential. This potential
is induced by spikes from neighboring neurons that fired in the previous round,
which can have either an excitatory or inhibitory effect. We first consider a
deterministic implementation of a neural timer and show that $\Theta(\log t)$
(deterministic) threshold gates are both sufficient and necessary. This raised
the question of whether randomness can be leveraged to reduce the number of
neurons. We answer this question in the affirmative by considering neural
timers with spiking neurons where the neuron $y$ is required to fire for $t$
consecutive rounds with probability at least $1-\delta$, and should stop firing
after at most $2t$ rounds with probability $1-\delta$ for some input parameter
$\delta \in (0,1)$. Our key result is a construction of a neural timer with
$O(\log\log 1/\delta)$ spiking neurons. Interestingly, this construction uses
only one spiking neuron, while the remaining neurons can be deterministic
threshold gates. We complement this construction with a matching lower bound of
$\Omega(\min\{\log\log 1/\delta, \log t\})$ neurons. This provides the first
separation between deterministic and randomized constructions in the setting of
spiking neural networks. Finally, we demonstrate the usefulness of compressed
counting networks for synchronizing neural networks.",countdown timer
http://arxiv.org/abs/1302.4656v1,"This paper proposes an approximate method, equivalent access intensity (EAI),
for the throughput analysis of CSMA wireless networks in which links have
finite offered-load and their MAC-layer transmit buffers may be empty from time
to time. Different from prior works that mainly considered the saturated
network, we take into account in our analysis the impacts of empty transmit
buffers on the interactions and dependencies among links in the network that is
more common in practice. It is known that the empty transmit buffer incurs
extra waiting time for a link to compete for the channel airtime usage, since
when it has no packet waiting for transmission, the link will not perform
channel competition. The basic idea behind EAI is that this extra waiting time
can be mapped to an equivalent ""longer"" backoff countdown time for the
unsaturated link, yielding a lower link access intensity that is defined as the
mean packet transmission time divided by the mean backoff countdown time. That
is, we can compute the ""equivalent access intensity"" of an unsaturated link to
incorporate the effects of the empty transmit buffer on its behavior of channel
competition. Then, prior saturated ideal CSMA network (ICN) model can be
adopted for link throughput computation. Specifically, we propose an iterative
algorithm, ""Compute-and-Compare"", to identify which links are unsaturated under
current offered-load and protocol settings, compute their ""equivalent access
intensities"" and calculate link throughputs. Simulation shows that our
algorithm has high accuracy under various offered-load and protocol settings.
We believe the ability to identify unsaturated links and compute links
throughputs as established in this paper will serve an important first step
toward the design and optimization of general CSMA wireless networks with
offered-load control.",countdown timer
http://arxiv.org/abs/1909.12684v1,"The power consumption of supercomputers is a major challenge for system
owners, users, and society. It limits the capacity of system installations, it
requires large cooling infrastructures, and it is the cause of a large carbon
footprint. Reducing power during application execution without changing the
application source code or increasing time-to-completion is highly desirable in
real-life high-performance computing scenarios. The power management run-time
frameworks proposed in the last decade are based on the assumption that the
duration of communication and application phases in an MPI application can be
predicted and used at run-time to trade-off communication slack with power
consumption. In this manuscript, we first show that this assumption is too
general and leads to mispredictions, slowing down applications, thereby
jeopardizing the claimed benefits. We then propose a new approach based on (i)
the separation of communication phases and slack during MPI calls and (ii) a
timeout algorithm to cope with the hardware power management latency, which
jointly makes it possible to achieve performance-neutral power saving in MPI
applications without requiring labor-intensive and risky application source
code modifications. We validate our approach in a tier-1 production environment
with widely adopted scientific applications. Our approach has a
time-to-completion overhead lower than 1%, while it successfully exploits slack
in communication phases to achieve an average energy saving of 10%. If we focus
on a large-scale application runs, the proposed approach achieves 22% energy
saving with an overhead of only 0.4%. With respect to state-of-the-art
approaches, COUNTDOWN Slack is the only that always leads to an energy saving
with negligible overhead (<3%).",countdown timer
http://arxiv.org/abs/cs/0701015v2,"We consider the problem of failure detection in dynamic networks such as
MANETs. Unreliable failure detectors are classical mechanisms which provide
information about process failures. However, most of current implementations
consider that the network is fully connected and that the initial number of
nodes of the system is known. This assumption is not applicable to dynamic
environments. Furthermore, such implementations are usually timer-based while
in dynamic networks there is no upper bound for communication delays since
nodes can move. This paper presents an asynchronous implementation of a failure
detector for unknown and mobile networks. Our approach does not rely on timers
and neither the composition nor the number of nodes in the system are known. We
prove that our algorithm can implement failure detectors of class <>S when
behavioral properties and connectivity conditions are satisfied by the
underlying system.",countdown timer
http://arxiv.org/abs/0705.3015v1,"Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.",countdown timer
http://arxiv.org/abs/0910.0316v1,"Rate based transport protocol determines the rate of data transmission
between the sender and receiver and then sends the data according to that rate.
To notify the rate to the sender, the receiver sends ACKplusRate packet based
on epoch timer expiry. In this paper, through detailed arguments and simulation
it is shown that the transmission of ACKplusRate packet based on epoch timer
expiry consumes more energy in network with low mobility. To overcome this
problem, a new technique called Dynamic Rate Feedback (DRF) is proposed. DRF
sends ACKplusRate whenever there is a change in rate of (plus or minus) 25
percent than the previous rate. Based on ns2 simulation DRF is compared with a
reliable transport protocol for ad hoc network (ATP)",countdown timer
http://arxiv.org/abs/1009.4992v1,"The main objective of this work is to design and construct a microcomputer
based system: to control electric appliances such as light, fan, heater,
washing machine, motor, TV, etc. The paper discusses two major approaches to
control home appliances. The first involves controlling home appliances using
timer option. The second approach is to control home appliances using voice
command. Moreover, it is also possible to control appliances using Graphical
User Interface. The parallel port is used to transfer data from computer to the
particular device to be controlled. An interface box is designed to connect the
high power loads to the parallel port. This system will play an important role
for the elderly and physically disable people to control their home appliances
in intuitive and flexible way. We have developed a system, which is able to
control eight electric appliances properly in these three modes.",countdown timer
http://arxiv.org/abs/0709.2618v2,"Wireless Sensor Networks research and demand are now in full expansion, since
people came to understand these are the key to a large number of issues in
industry, commerce, home automation, healthcare, agriculture and environment,
monitoring, public safety etc. One of the most challenging research problems in
sensor networks research is power awareness and power-saving techniques. In
this master's thesis, we have studied one particular power-saving technique,
i.e. frequency scaling. In particular, we analysed the close relationship
between clock frequencies in a microcontroller and several types of constraints
imposed on these frequencies, e.g. by other components of the microcontroller,
by protocol specifications, by external factors etc. Among these constraints,
we were especially interested in the ones imposed by the timer service and by
the serial ports' transmission rates. Our efforts resulted in a microcontroller
configuration management tool which aims at assisting application programmers
in choosing microcontroller configurations, in function of the particular needs
and constraints of their application.",countdown timer
http://arxiv.org/abs/1502.00050v1,"To circumvent the FLP impossibility result in a deterministic way several
protocols have been proposed on top of an asynchronous distributed system
enriched with additional assumptions. In the context of Byzantine failures for
systems where at most t processes may exhibit a Byzantine behavior, two
approaches have been investigated to solve the consensus problem.The first,
relies on the addition of synchrony, called Timer-Based, but the second is
based on the pattern of the messages that are exchanged, called Time-Free. This
paper shows that both types of assumptions are not antagonist and can be
combined to solve authenticated Byzantine consensus. This combined assumption
considers a correct process pi, called 2t-BW, and a set X of 2t processes such
that, eventually, for each query broadcasted by a correct process pj of X, pj
receives a response from pi 2 X among the (n- t) first responses to that query
or both links connecting pi and pj are timely. Based on this combination, a
simple hybrid authenticated Byzantine consensus protocol,benefiting from the
best of both worlds, is proposed. Whereas many hybrid protocols have been
designed for the consensus problem in the crash model, this is, to our
knowledge, the first hybrid deterministic solution to the Byzantine consensus
problem.",countdown timer
http://arxiv.org/abs/0808.3937v1,"The Distributed Coordination Function (DCF) aims at fair and efficient medium
access in IEEE 802.11. In face of its success, it is remarkable that there is
little consensus on the actual degree of fairness achieved, particularly
bearing its impact on quality of service in mind. In this paper we provide an
accurate model for the fairness of the DCF. Given M greedy stations we assume
fairness if a tagged station contributes a share of 1/M to the overall number
of packets transmitted. We derive the probability distribution of fairness
deviations and support our analytical results by an extensive set of
measurements. We find a closed-form expression for the improvement of long-term
over short-term fairness. Regarding the random countdown values we quantify the
significance of their distribution whereas we discover that fairness is largely
insensitive to the distribution parameters. Based on our findings we view the
DCF as emulating an ideal fair queuing system to quantify the deviations from a
fair rate allocation. We deduce a stochastic service curve model for the DCF to
predict packet delays in IEEE 802.11. We show how a station can estimate its
fair bandwidth share from passive measurements of its traffic arrivals and
departures.",countdown timer
http://arxiv.org/abs/1107.1633v1,"It is known that link throughputs of CSMA wireless networks can be computed
from a time-reversible Markov chain arising from an ideal CSMA network model
(ICN). In particular, this model yields general closed-form equations of link
throughputs. However, an idealized and important assumption made in ICN is that
the backoff countdown process is in ""contiuous-time"" and carrier sensing is
instantaneous. As a result, there is no collision in ICN. In practical CSMA
protocols such as IEEE 802.11, the stations count down in ""mini-timeslot"" and
the process is therefore a ""discrete-time"" process. In particular, two stations
may end their backoff process in the same mini-timeslot and then transmit
simultaneously, resulting in a packet collision. This paper is an attempt to
study how to compute link throughputs after taking such backoff collision
effects into account. We propose a generalized ideal CSMA network model (GICN)
to characterize the collision states as well as the interactions and dependency
among links in the network. We show that link throughputs and collision
probability can be computed from GICN. Simulation results validate GICN's
accuracy. Interestingly, we also find that the original ICN model yields fairly
accurate results despite the fact that collisions are not modeled.",countdown timer
http://arxiv.org/abs/1806.08324v2,"Probabilistic survival predictions from models trained with Maximum
Likelihood Estimation (MLE) can have high, and sometimes unacceptably high
variance. The field of meteorology, where the paradigm of maximizing sharpness
subject to calibration is popular, has addressed this problem by using scoring
rules beyond MLE, such as the Continuous Ranked Probability Score (CRPS). In
this paper we present the \emph{Survival-CRPS}, a generalization of the CRPS to
the survival prediction setting, with right-censored and interval-censored
variants. We evaluate our ideas on the mortality prediction task using two
different Electronic Health Record (EHR) data sets (STARR and MIMIC-III)
covering millions of patients, with suitable deep neural network architectures:
a Recurrent Neural Network (RNN) for STARR and a Fully Connected Network (FCN)
for MIMIC-III. We compare results between the two scoring rules while keeping
the network architecture and data fixed, and show that models trained with
Survival-CRPS result in sharper predictive distributions compared to those
trained by MLE, while still maintaining calibration.",countdown timer
http://arxiv.org/abs/1806.05822v1,"Message digest algorithms are one of the underlying building blocks of
blockchain platforms such as Ethereum. This paper analyses situations in which
the message digest collision resistance property can be exploited by attackers.
Two mitigations for possible attacks are described: longer message digest sizes
make attacks more difficult; and, including timeliness properties limits the
amount of time an attacker has to determine a hash collision.",limited time message
http://arxiv.org/abs/1902.02704v3,"Stickers are popularly used in messaging apps such as Hike to visually
express a nuanced range of thoughts and utterances to convey exaggerated
emotions. However, discovering the right sticker from a large and ever
expanding pool of stickers while chatting can be cumbersome. In this paper, we
describe a system for recommending stickers in real time as the user is typing
based on the context of conversation. We decompose the sticker recommendation
problem into two steps. First, we predict the message that the user is likely
to send in the chat. Second, we substitute the predicted message with an
appropriate sticker. Majority of Hike's messages are in the form of text which
is transliterated from users' native language to the Roman script. This leads
to numerous orthographic variations of the same message and complicates message
prediction. To address this issue, we learn dense representations of chat
messages and use them to cluster the messages that have same meaning. In the
subsequent steps we predict the message cluster instead of the message. Our
model employs a character level convolution network to capture the similar
intents in orthographic variants of chats. We validate our approach using
manually labelled data on two tasks. We also propose a novel hybrid message
prediction model, which can run with low latency on low end phones that have
severe computational limitations.",limited time message
http://arxiv.org/abs/0705.2065v1,"The churn rate of a peer-to-peer system places direct limitations on the rate
at which messages can be effectively communicated to a group of peers. These
limitations are independent of the topology and message transmission latency.
In this paper we consider a peer-to-peer network, based on the Engset model,
where peers arrive and depart independently at random. We show how the arrival
and departure rates directly limit the capacity for message streams to be
broadcast to all other peers, by deriving mean field models that accurately
describe the system behavior. Our models cover the unit and more general k
buffer cases, i.e. where a peer can buffer at most k messages at any one time,
and we give results for both single and multi-source message streams. We define
coverage rate as peer-messages per unit time, i.e. the rate at which a number
of peers receive messages, and show that the coverage rate is limited by the
churn rate and buffer size. Our theory introduces an Instantaneous Message
Exchange (IME) model and provides a template for further analysis of more
complicated systems. Using the IME model, and assuming random processes, we
have obtained very accurate equations of the system dynamics in a variety of
interesting cases, that allow us to tune a peer-to-peer system. It remains to
be seen if we can maintain this accuracy for general processes and when
applying a non-instantaneous model.",limited time message
http://arxiv.org/abs/1710.08803v2,"The Internet of Things (IoT) will encompass a massive number of machine type
devices that must wirelessly transmit, in near real-time, a diverse set of
messages sensed from their environment. Designing resource allocation schemes
to support such coexistent, heterogeneous communication is hence a key IoT
challenge. In particular, there is a need for self-organizing resource
allocation solutions that can account for unique IoT features, such as massive
scale and stringent resource constraints. In this paper, a novel finite memory
multi-state sequential learning framework is proposed to enable diverse IoT
devices to share limited communication resources, while transmitting both
delay-tolerant, periodic messages and urgent, critical messages. The proposed
learning framework enables the IoT devices to learn the number of critical
messages and to reallocate the communication resources for the periodic
messages to be used for the critical messages. Furthermore, the proposed
learning framework explicitly accounts for IoT device limitations in terms of
memory and computational capabilities. The convergence of the proposed learning
framework is proved, and the lowest expected delay that the IoT devices can
achieve using this learning framework is derived. Furthermore, the
effectiveness of the proposed learning algorithm in IoT networks with different
delay targets, network densities, probabilities of detection, and memory sizes
is analyzed in terms of the probability of a successful random access request
and percentage of devices that learned correctly. The results show that the
proposed learning algorithm is very effective in reducing the delay of urgent,
critical messages by intelligently reallocating the communication resources
allocated to the delay-tolerant, periodic messages.",limited time message
http://arxiv.org/abs/0804.4255v1,"Small-world networks are networks in which the graphical diameter of the
network is as small as the diameter of random graphs but whose nodes are highly
clustered when compared with the ones in a random graph. Examples of
small-world networks abound in sociology, biology, neuroscience and physics as
well as in human-made networks. This paper analyzes the average delivery time
of messages in dense small-world networks constructed on a plane. Iterative
equations for the average message delivery time in these networks are provided
for the situation in which nodes employ a simple greedy geographic routing
algorithm. It is shown that two network nodes communicate with each other only
through their short-range contacts, and that the average message delivery time
rises linearly if the separation between them is small. On the other hand, if
their separation increases, the average message delivery time rapidly saturates
to a constant value and stays almost the same for all large values of their
separation.",limited time message
http://arxiv.org/abs/1409.0998v1,"Ethernet is being considered as the backbone network protocol for
next-generation automotive control networks. In such networks, Controller Area
Network (CAN) messages related to automotive control can be sent from a CAN
network to other sub-networks via the backbone Ethernet bus and, if the CAN
messages have real-time constraints, these have to be guaranteed. This paper
presents a simulation environment for CAN--Ethernet Audio Video Bridging (AVB)
mixed networks based on OMNeT++. We use Ethernet AVB, which can guarantee
network bandwidth, to improve the real-time property of CAN messages through
the backbone Ethernet bus. To simulate the networks, we also developed a
CAN--Ethernet AVB gateway (GW) model. To verify the efficacy of our model, we
measured the latency of CAN messages sent from a CAN bus to an Ethernet AVB
node via the backbone Ethernet AVB bus in both bandwidth-guaranteed and
best-effort queue scenarios. The results indicate that the latency of Ethernet
AVB frames containing CAN messages is minimized and limited by the
bandwidth-guaranteed mechanism of Ethernet AVB.",limited time message
http://arxiv.org/abs/1602.04706v1,"We consider energy-efficient time synchronization in a wireless sensor
network where a head node (i.e., a gateway between wired and wireless networks
and a center of data fusion) is equipped with a powerful processor and supplied
power from outlet, and sensor nodes (i.e., nodes measuring data and connected
only through wireless channels) are limited in processing and battery-powered.
It is this asymmetry that our study focuses on; unlike most existing schemes to
save the power of all network nodes, we concentrate on battery-powered sensor
nodes in minimizing energy consumption for time synchronization. We present a
time synchronization scheme based on asynchronous source clock frequency
recovery and reverse two-way message exchanges combined with measurement data
report messages, where we minimize the number of message transmissions from
sensor nodes, and carry out the performance analysis of the estimation of both
measurement time and clock frequency with lower bounds for the latter.
Simulation results verify that the proposed scheme outperforms the schemes
based on conventional two-way message exchanges with and without clock
frequency recovery in terms of the accuracy of measurement time estimation and
the number of message transmissions and receptions at sensor nodes as an
indirect measure of energy efficiency.",limited time message
http://arxiv.org/abs/1108.6290v2,"BM compression is a straightforward and operable way to reduce buffer message
length as well as to improve system performance. In this paper, we thoroughly
discuss the principles and protocol progress of different compression schemes,
and for the first time present an original compression scheme which can nearly
remove all redundant information from buffer message. Theoretical limit of
compression rates are deduced in the theory of information. Through the
analysis of information content and simulation with our measured BM trace of
UUSee, the validity and superiority of our compression scheme are validated in
term of compression ratio.",limited time message
http://arxiv.org/abs/1012.4909v1,"Inter-vehicle communication (IVC) enables vehicles to exchange messages
within a limited broadcast range and thus self-organize into dynamical
vehicular ad hoc networks. For the foreseeable future, however, a direct
connectivity between equipped vehicles in one direction is rarely possible. We
therefore investigate an alternative mode in which messages are stored by relay
vehicles traveling in the opposite direction, and forwarded to vehicles in the
original direction at a later time. The wireless communication consists of two
`transversal' message hops across driving directions. Since direct connectivity
for transversal hops and a successful message transmission to vehicles in the
destination region is only a matter of time, the quality of this IVC strategy
can be described in terms of the distribution function for the total
transmission time. Assuming a Poissonian distance distribution between equipped
vehicles, we derive analytical probability distributions for message
transmission times and related propagation speeds for a deterministic and a
stochastic model of the maximum range of direct communication. By means of
integrated microscopic simulations of communication and bi-directional traffic
flows, we validated the theoretical expectation for multi-lane roadways. We
found little deviation of the analytical result for multi-lane scenarios, but
significant deviations for a single-lane. This can be explained by vehicle
platooning. We demonstrate the efficiency of the transverse hopping mechanism
for a congestion-warning application in a microscopic traffic simulation
scenario. Messages are created on an event-driven basis by equipped vehicles
entering and leaving a traffic jam. This application is operative for
penetration levels as low as 1%.",limited time message
http://arxiv.org/abs/1102.5425v1,"We explore the fundamental limits of distributed balls-into-bins algorithms.
We present an adaptive symmetric algorithm that achieves a bin load of two in
log* n+O(1) communication rounds using O(n) messages in total. Larger bin loads
can be traded in for smaller time complexities. We prove a matching lower bound
of (1-o(1))log* n on the time complexity of symmetric algorithms that guarantee
small bin loads at an asymptotically optimal message complexity of O(n). For
each assumption of the lower bound, we provide an algorithm violating it, in
turn achieving a constant maximum bin load in constant time.
  As an application, we consider the following problem. Given a fully connected
graph of n nodes, where each node needs to send and receive up to n messages,
and in each round each node may send one message over each link, deliver all
messages as quickly as possible to their destinations. We give a simple and
robust algorithm of time complexity O(log* n) for this task and provide a
generalization to the case where all nodes initially hold arbitrary sets of
messages. A less practical algorithm terminates within asymptotically optimal
O(1) rounds. All these bounds hold with high probability.",limited time message
http://arxiv.org/abs/1205.2639v1,"Efficiently finding the maximum a posteriori (MAP) configuration of a
graphical model is an important problem which is often implemented using
message passing algorithms. The optimality of such algorithms is only well
established for singly-connected graphs and other limited settings. This
article extends the set of graphs where MAP estimation is in P and where
message passing recovers the exact solution to so-called perfect graphs. This
result leverages recent progress in defining perfect graphs (the strong perfect
graph theorem), linear programming relaxations of MAP estimation and recent
convergent message passing schemes. The article converts graphical models into
nand Markov random fields which are straightforward to relax into linear
programs. Therein, integrality can be established in general by testing for
graph perfection. This perfection test is performed efficiently using a
polynomial time algorithm. Alternatively, known decomposition tools from
perfect graph theory may be used to prove perfection for certain families of
graphs. Thus, a general graph framework is provided for determining when MAP
estimation in any graphical model is in P, has an integral linear programming
relaxation and is exactly recoverable by message passing.",limited time message
http://arxiv.org/abs/1702.02736v1,"Instant messaging is one of the major channels of computer mediated
communication. However, humans are known to be very limited in understanding
others' emotions via text-based communication. Aiming on introducing emotion
sensing technologies to instant messaging, we developed EmotionPush, a system
that automatically detects the emotions of the messages end-users received on
Facebook Messenger and provides colored cues on their smartphones accordingly.
We conducted a deployment study with 20 participants during a time span of two
weeks. In this paper, we revealed five challenges, along with examples, that we
observed in our study based on both user's feedback and chat logs, including
(i)the continuum of emotions, (ii)multi-user conversations, (iii)different
dynamics between different users, (iv)misclassification of emotions and
(v)unconventional content. We believe this discussion will benefit the future
exploration of affective computing for instant messaging, and also shed light
on research of conversational emotion sensing.",limited time message
http://arxiv.org/abs/1401.2405v2,"In the recent years Vehicular Ad hoc Networks (VANET) became one of the most
challenging research area in the field of Mobile Ad hoc Networks (MANET).
Vehicles in VANET send emergency and safety periodic messages through one
control channel having a limited bandwidth, which causes a growing collision to
the channel especially in dense traffic situations. In this paper a protocol
Particle swarm optimization Beacon Power Control (PBPC) is proposed, which
makes dynamic transmission power control to adjust the transmission power of
the safety periodic messages that have been aggressively sent by all vehicles
on the road 10 times per a second, the proposed protocol aims to decrease the
packet collision resulted from periodic safety messages, which leads to control
the load on the channel while ensuring a high probability of message reception
within the safety distance of the sender vehicle.",limited time message
http://arxiv.org/abs/1207.3582v1,"We consider a real-time streaming system where messages are created
sequentially at the source, and are encoded for transmission to the receiver
over a packet erasure link. Each message must subsequently be decoded at the
receiver within a given delay from its creation time. The goal is to construct
an erasure correction code that achieves the maximum message size when all
messages must be decoded by their respective deadlines under a specified set of
erasure patterns (erasure model). We present an explicit intrasession code
construction that is asymptotically optimal under erasure models containing a
limited number of erasures per coding window, per sliding window, and
containing erasure bursts of a limited length.",limited time message
http://arxiv.org/abs/1711.08136v2,"In this paper, we propose a signal-aligned network coding (SNC) scheme for
K-user time-varying multiple-input multiple-output (MIMO) interference channels
with limited receiver cooperation. We assume that the receivers are connected
to a central processor via wired cooperation links with individual limited
capacities. Our SNC scheme determines the precoding matrices of the
transmitters so that the transmitted signals are aligned at each receiver. The
aligned signals are then decoded into noiseless integer combinations of
messages, also known as network-coded messages, by physical-layer network
coding. The key idea of our scheme is to ensure that independent integer
combinations of messages can be decoded at the receivers. Hence the central
processor can recover the original messages of the transmitters by solving the
linearly independent equations. We prove that our SNC scheme achieves full
degrees of freedom (DoF) by utilizing signal alignment and physical-layer
network coding. Simulation results show that our SNC scheme outperforms the
compute-and-forward scheme in the finite SNR regime of the two-user and the
three-user cases. The performance improvement of our SNC scheme mainly comes
from efficient utilization of the signal subspaces for conveying independent
linear equations of messages to the central processor.",limited time message
http://arxiv.org/abs/1909.08740v2,"WhatsApp is the most popular messaging app in the world. The closed nature of
the app, in addition to the ease of transferring multimedia and sharing
information to large-scale groups make WhatsApp unique among other platforms,
where an anonymous encrypted messages can become viral, reaching multiple users
in a short period of time. The personal feeling and immediacy of messages
directly delivered to the user's phone on WhatsApp was extensively abused to
spread unfounded rumors and create misinformation campaigns during recent
elections in Brazil and India. WhatsApp has been deploying measures to mitigate
this problem, such as reducing the limit for forwarding a message to at most
five users at once. Despite the welcomed effort to counter the problem, there
is no evidence so far on the real effectiveness of such restrictions. In this
work, we propose a methodology to evaluate the effectiveness of such measures
on the spreading of misinformation circulating on WhatsApp. We use an
epidemiological model and real data gathered from WhatsApp in Brazil, India and
Indonesia to assess the impact of limiting virality features in this kind of
network. Our results suggest that the current efforts deployed by WhatsApp can
offer significant delays on the information spread, but they are ineffective in
blocking the propagation of misinformation campaigns through public groups when
the content has a high viral nature.",limited time message
http://arxiv.org/abs/1502.03320v1,"In the CONGEST model, a communications network is an undirected graph whose
$n$ nodes are processors and whose $m$ edges are the communications links
between processors. At any given time step, a message of size $O(\log n)$ may
be sent by each node to each of its neighbors. We show for the synchronous
model: If all nodes start in the same round, and each node knows its ID and the
ID's of its neighbors, or in the case of MST, the distinct weights of its
incident edges and knows $n$, then there are Monte Carlo algorithms which
succeed w.h.p. to determine a minimum spanning forest (MST) and a spanning
forest (ST) using $O(n \log^2 n/\log\log n)$ messages for MST and $O(n \log n
)$ messages for ST, resp. These results contradict the ""folk theorem"" noted in
Awerbuch, et.al., JACM 1990 that the distributed construction of a broadcast
tree requires $\Omega(m)$ messages. This lower bound has been shown there and
in other papers for some CONGEST models; our protocol demonstrates the limits
of these models.
  A dynamic distributed network is one which undergoes online edge insertions
or deletions. We also show how to repair an MST or ST in a dynamic network with
asynchronous communication. An edge deletion can be processed in $O(n\log n
/\log \log n)$ expected messages in the MST, and $O(n)$ expected messages for
the ST problem, while an edge insertion uses $O(n)$ messages in the worst case.
We call this ""impromptu"" updating as we assume that between processing of edge
updates there is no preprocessing or storage of additional information.
Previous algorithms for this problem that use an amortized $o(m)$ messages per
update require substantial preprocessing and additional local storage between
updates.",limited time message
http://arxiv.org/abs/1602.02148v1,"Hash-based message authentication codes are an extremely simple yet hugely
effective construction for producing keyed message digests using shared
secrets. HMACs have seen widespread use as ad-hoc digital signatures in many
Internet applications. While messages signed with an HMAC are secure against
sender impersonation and tampering in transit, if used alone they are
susceptible to replay attacks. We propose a construction that extends HMACs to
produce a keyed message digest that has a finite validity period. We then
propose a message signature scheme that uses this time-dependent MAC along with
an unique message identifier to calculate a set of authentication factors using
which a recipient can readily detect and ignore replayed messages, thus
providing perfect resistance against replay attacks. We further analyse
time-based message authentication codes and show that they provide stronger
security guarantees than plain HMACs, even when used independently of the
aforementioned replay attack resistant message signature scheme.",limited time message
http://arxiv.org/abs/0901.2685v1,"Soft real-time applications require timely delivery of messages conforming to
the soft real-time constraints. Satisfying such requirements is a complex task
both due to the volatile nature of distributed environments, as well as due to
numerous domain-specific factors that affect message latency. Prompt detection
of the root-cause of excessive message delay allows a distributed system to
react accordingly. This may significantly improve compliance with the required
timeliness constraints.
  In this work, we present a novel approach for distributed performance
monitoring of soft-real time distributed systems. We propose to employ recent
distributed algorithms from the statistical signal processing and learning
domains, and to utilize them in a different context of online performance
monitoring and root-cause analysis, for pinpointing the reasons for violation
of performance requirements. Our approach is general and can be used for
monitoring of any distributed system, and is not limited to the soft real-time
domain.
  We have implemented the proposed framework in TransFab, an IBM prototype of
soft real-time messaging fabric. In addition to root-cause analysis, the
framework includes facilities to resolve resource allocation problems, such as
memory and bandwidth deficiency. The experiments demonstrate that the system
can identify and resolve latency problems in a timely fashion.",limited time message
http://arxiv.org/abs/cs/0601093v1,"We consider stability of scheduled multiaccess message communication with
random coding and joint maximum-likehood decoding of messages. The framework we
consider here models both the random message arrivals and the subsequent
reliable communication by suitably combining techniques from queueing theory
and information theory. The number of messages that may be scheduled for
simultaneous transmission is limited to a given maximum value, and the channels
from transmitters to receiver are quasi-static, flat, and have independent
fades. Requests for message transmissions are assumed to arrive according to an
i.i.d. arrival process. Then, (i) we derive an outer bound to the region of
message arrival rate vectors achievable by the class of stationary scheduling
policies, (ii) we show for any message arrival rate vector that satisfies the
outerbound, that there exists a stationary state-independent policy that
results in a stable system for the corresponding message arrival process, and
(iii) in the limit of large message lengths, we show that the stability region
of message nat arrival rate vectors has information-theoretic capacity region
interpretation.",limited time message
http://arxiv.org/abs/1108.6121v1,"We consider the decentralized binary hypothesis testing problem in networks
with feedback, where some or all of the sensors have access to compressed
summaries of other sensors' observations. We study certain two-message feedback
architectures, in which every sensor sends two messages to a fusion center,
with the second message based on full or partial knowledge of the first
messages of the other sensors. We also study one-message feedback
architectures, in which each sensor sends one message to a fusion center, with
a group of sensors having full or partial knowledge of the messages from the
sensors not in that group. Under either a Neyman-Pearson or a Bayesian
formulation, we show that the asymptotically optimal (in the limit of a large
number of sensors) detection performance (as quantified by error exponents)
does not benefit from the feedback messages, if the fusion center remembers all
sensor messages. However, feedback can improve the Bayesian detection
performance in the one-message feedback architecture if the fusion center has
limited memory; for that case, we determine the corresponding optimal error
exponents.",limited time message
http://arxiv.org/abs/1609.03109v1,"In the scope of VANETs, nature of exchanged safety/warning messages renders
itself highly location dependent as it is usually for incident reporting. Thus,
vehicles are required to periodically exchange beacon messages that include
speed, time and GPS location information. In this paper paper, we present a
physical layer assisted message authentication scheme that uses Angle of
Arrival (AoA) estimation to verify the message originator location based on the
claimed location information. Within the considered vehicular communication
settings, fundamental limits of AoA estimation are developed in terms of its
Cramer Rao Bound (CRB) and existence of efficient estimator. The problem of
deciding whether the received signal is originated from the claimed GPS
location is formulated as a two sided hypotheses testing problem whose solution
is given by Wald test statics. Moreover, we use correct decision, $P_D$, and
false alarm, $P_F$, probabilities as a quantitative performance measure. The
observation posterior likelihood function is shown to satisfy regularity
conditions necessary for asymptotic normality of the ML-AoA estimator. Thus, we
give $P_D$ and $P_F$ in a closed form.
  We extend the potential of physical layer contribution in security to provide
physical layer assisted secret key agreement (SKA) protocol. A public key (PK)
based SKA in which communicating vehicles are required to validate their
respective physical location. We show that the risk of the Man in the Middle
attack, which is common in PK-SKA protocols without a trusted third party, is
waived up to the literal meaning of the word ""middle"".",limited time message
http://arxiv.org/abs/1706.05267v1,"This paper introduces a new communication abstraction, called Set-Constrained
Delivery Broadcast (SCD-broadcast), whose aim is to provide its users with an
appropriate abstraction level when they have to implement objects or
distributed tasks in an asynchronous message-passing system prone to process
crash failures. This abstraction allows each process to broadcast messages and
deliver a sequence of sets of messages in such a way that, if a process
delivers a set of messages including a message m and later delivers a set of
messages including a message m ' , no process delivers first a set of messages
including m ' and later a set of message including m. After having presented an
algorithm implementing SCD-broadcast, the paper investigates its programming
power and its computability limits. On the ""power"" side it presents
SCD-broadcast-based algorithms, which are both simple and efficient, building
objects (such as snapshot and conflict-free replicated data), and distributed
tasks. On the ""computability limits"" side it shows that SCD-broadcast and
read/write registers are computationally equivalent.",limited time message
http://arxiv.org/abs/1211.5747v1,"Dynamic network reconfiguration is described as the process of replacing one
routing function with another while the network keeps running. The main
challenge is avoiding deadlock anomalies while keeping limitations on message
injection and forwarding minimal. Current approaches, whose complexity is so
high that their practical applicability is limited, either require the
existence of extra network resources like virtual channels, or they affect the
performance of the network during the reconfiguration process. In this paper we
present a simple, fast and efficient mechanism for dynamic network
reconfiguration which is based on regressive deadlock recoveries instead of
avoiding deadlocks. The mechanism which is referred to as DBR guarantees a
deadlock-free reconfiguration based on wormhole switching (WS) and it does not
require additional resources. In this approach, the need for a reliable message
transmission has led to a modified WS mechanism which includes additional flits
or control signals. DBR allows cycles to be formed and in such conditions when
a deadlock occurs, the messages suffer from time-out. Then, this method
releases the buffers and channels from the current node and thus the source
retransmits the message after a random time gap. Evaluating results reveal that
the mechanism shows substantial performance improvements over the other methods
and it works efficiently in different topologies with various routing
algorithms.",limited time message
http://arxiv.org/abs/1011.6129v1,"Push message delivery, where a client maintains an ``always-on'' connection
with a server in order to be notified of a (asynchronous) message arrival in
real-time, is increasingly being used in Internet services. The key message in
this paper is that push message delivery on the World Wide Web is not scalable
for servers, intermediate network elements, and battery-operated mobile device
clients. We present a measurement analysis of a commercially deployed WWW push
email service to highlight some of these issues. Next, we suggest content-based
optimization to reduce the always-on connection requirement of push messaging.
Our idea is based on exploiting the periodic nature of human-to-human
messaging. We show how machine learning can accurately model the times of a day
or week when messages are least likely to arrive; and turn off always-on
connections these times. We apply our approach to a real email data set and our
experiments demonstrate that the number of hours of active always-on
connections can be cut by half while still achieving real-time message delivery
for up to 90% of all messages.",limited time message
http://arxiv.org/abs/1906.09039v1,"Message bundling is an effective way to reduce the energy consumption for
message transmissions in wireless sensor networks. However, bundling more
messages could increase both end-to-end delay and message transmission
interval; the former needs to be maintained within a certain value for
time-sensitive applications like environmental monitoring, while the latter
affects time synchronization accuracy when the bundling includes
synchronization messages as well. Taking as an example a novel time
synchronization scheme recently proposed for energy efficiency, we propose an
optimal message bundling approach to reduce the message transmissions while
maintaining the user-defined requirements on end-to-end delay and time
synchronization accuracy. Through translating the objective of joint
maintenance to an integer linear programming problem, we compute a set of
optimal bundling numbers for the sensor nodes to constrain their link-level
delays, thereby achieve and maintain the required end-to-end delay and
synchronization accuracy while the message transmission is minimized.",limited time message
http://arxiv.org/abs/1208.5542v1,"For parallel breadth first search (BFS) algorithm on large-scale distributed
memory systems, communication often costs significantly more than arithmetic
and limits the scalability of the algorithm. In this paper we sufficiently
reduce the communication cost in distributed BFS by compressing and sieving the
messages. First, we leverage a bitmap compression algorithm to reduce the size
of messages before communication. Second, we propose a novel distributed
directory algorithm, cross directory, to sieve the redundant data in messages.
Experiments on a 6,144-core SMP cluster show our algorithm outperforms the
baseline implementation in Graph500 by 2.2 times, reduces its communication
time by 79.0%, and achieves a performance rate of 12.1 GTEPS (billion edge
visits per second)",limited time message
http://arxiv.org/abs/1904.11402v1,"WebRTC enables browsers to exchange data directly but the number of possible
concurrent connections to a single source is limited. We overcome the
limitation by organizing participants in a fat-tree overlay: when the maximum
number of connections of a tree node is reached, the new participants connect
to the node's children. Our design quickly scales when a large number of
participants join in a short amount of time, by relying on a novel scheme that
only requires local information to route connection messages: the destination
is derived from the hash value of the combined identifiers of the message's
source and of the node that is holding the message. The scheme provides
deterministic routing of a sequence of connection messages from a single source
and probabilistic balancing of newer connections among the leaves. We show that
this design puts at least 83% of nodes at the same depth as a deterministic
algorithm, can connect a thousand browser windows in 21-55 seconds in a local
network, and can be deployed for volunteer computing to tap into 320 cores in
less than 30 seconds on a local network to increase the total throughput on the
Collatz application by two orders of magnitude compared to a single core.",limited time message
http://arxiv.org/abs/1310.2351v2,"This paper suggests a message authentication scheme, which can be efficiently
used for secure digital signature creation. The algorithm used here is an
adjusted union of the concepts which underlie projective geometry and group
structure on circles. The authentication is done through a key, which iterates
over the complete message string to produce the signature. The iteration is not
only based on the frequency distribution of the message string alphabet, but
also on the probability of occurrence of another given reference string in the
message. The complete process can be easily computed in a small time, producing
signatures which are highly dependent on the message string. Consequently, the
odds in favor of existence of a forgery are highly reduced.",limited time message
http://arxiv.org/abs/1004.5181v1,"In this paper, the required amount of feedback overhead for multiple-input
multiple-output (MIMO) beamforming over time-varying channels is presented in
terms of the entropy of the feedback messages. In the case that each transmit
antenna has its own power amplifier which has individual power limit, it has
been known that only phase steering information is necessary to form the
optimal transmit beamforming vector. Since temporal correlation exists for
wireless fading channels, one can utilize the previous reported feedback
messages as prior information to efficiently encode the current feedback
message. Thus, phase tracking information, difference between two phase
steering information in adjacent feedback slots, is sufficient as a feedback
message. We show that while the entropy of the phase steering information is a
constant, the entropy of the phase tracking information is a function of the
temporal correlation parameter. For the phase tracking information, upperbounds
on the entropy are presented in the Gaussian entropy and the von-Mises entropy
by using the theory on the maximum entropy distributions. Derived results can
quantify the amount of reduction in feedback overhead of the phase tracking
information over the phase steering information. For application perspective,
the signal-to-noise ratio (SNR) gain of phase tracking beamforming over phase
steering beamforming is evaluated by using Monte-Carlo simulation. Also we show
that the derived entropies can determine the appropriate duration of the
feedback reports with respect to the degree of the channel variation rates.",limited time message
http://arxiv.org/abs/1610.00620v1,"Excessive tail end-to-end latency occurs with conventional message brokers as
a result of having massive numbers of geographically distributed devices
communicate through a message broker. On the other hand, broker-less messaging
systems, though ensure low latency, are highly dependent on the limitation of
direct device-to-device (D2D) communication technologies, and cannot scale well
as large numbers of resource-limited devices exchange messages. In this paper,
we propose FogMQ, a cloud-based message broker system that overcomes the
limitations of conventional systems by enabling autonomous discovery,
self-deployment, and online migration of message brokers across heterogeneous
cloud platforms. For each device, FogMQ provides a high capacity device cloning
service that subscribes to device messages. The clones facilitate near-the-edge
data analytics in resourceful cloud compute nodes. Clones in FogMQ apply Flock,
an algorithm mimicking flocking-like behavior to allow clones to dynamically
select and autonomously migrate to different heterogeneous cloud platforms in a
distributed manner.",limited time message
http://arxiv.org/abs/1710.00273v1,"Text messaging is the most widely used form of computer- mediated
communication (CMC). Previous findings have shown that linguistic factors can
reliably indicate messages as deceptive. For example, users take longer and use
more words to craft deceptive messages than they do truthful messages. Existing
research has also examined how factors, such as student status and gender,
affect rates of deception and word choice in deceptive messages. However, this
research has been limited by small sample sizes and has returned contradicting
findings. This paper aims to address these issues by using a dataset of text
messages collected from a large and varied set of participants using an Android
messaging application. The results of this paper show significant differences
in word choice and frequency of deceptive messages between male and female
participants, as well as between students and non-students.",limited time message
http://arxiv.org/abs/0908.3512v4,"A two-terminal interactive function computation problem with alternating
messages is studied within the framework of distributed block source coding
theory. For any finite number of messages, a single-letter characterization of
the sum-rate-distortion function was established in previous works using
standard information-theoretic techniques. This, however, does not provide a
satisfactory characterization of the infinite-message limit, which is a new,
unexplored dimension for asymptotic-analysis in distributed block source coding
involving potentially an infinite number of infinitesimal-rate messages. In
this paper, the infinite-message sum-rate-distortion function, viewed as a
functional of the joint source pmf and the distortion levels, is characterized
as the least element of a partially ordered family of functionals having
certain convex-geometric properties. The new characterization does not involve
evaluating the infinite-message limit of a finite-message sum-rate-distortion
expression. This characterization leads to a family of lower bounds for the
infinite-message sum-rate-distortion expression and a simple criterion to test
the optimality of any achievable infinite-message sum-rate-distortion
expression. For computing the amplewise Boolean AND function, the
infinite-message minimum sum-rates are characterized in closed analytic form.
These sum-rates are shown to be achievable using infinitely many
infinitesimal-rate messages. The new convex-geometric characterization is used
to develop an iterative algorithm for evaluating any finite-message
sumrate-distortion function. It is also used to construct the first examples
which demonstrate that for lossy source reproduction, two messages can strictly
improve the one-message Wyner-Ziv rate-distortion function settling an
unresolved question from a 1985 paper.",limited time message
http://arxiv.org/abs/1503.08040v4,"We study the approximate message-passing decoder for sparse superposition
coding on the additive white Gaussian noise channel and extend our preliminary
work [1]. We use heuristic statistical-physics-based tools such as the cavity
and the replica methods for the statistical analysis of the scheme. While
superposition codes asymptotically reach the Shannon capacity, we show that our
iterative decoder is limited by a phase transition similar to the one that
happens in Low Density Parity check codes. We consider two solutions to this
problem, that both allow to reach the Shannon capacity: i) a power allocation
strategy and ii) the use of spatial coupling, a novelty for these codes that
appears to be promising. We present in particular simulations suggesting that
spatial coupling is more robust and allows for better reconstruction at finite
code lengths. Finally, we show empirically that the use of a fast
Hadamard-based operator allows for an efficient reconstruction, both in terms
of computational time and memory, and the ability to deal with very large
messages.",limited time message
http://arxiv.org/abs/1812.09466v1,"We have examined fine-structure mixing between the rubidium $5^{2}P_{3/2}$
and $5^{2}P_{1/2}$ states along with quenching of these states due to
collisions with methane gas. Measurements are carried out using ultrafast laser
pulse excitation to populate one of the Rb $5^{2}P$ states, with the
fluorescence produced through collisional excitation transfer observed using
time-correlated single-photon counting. Fine-structure mixing rates and
quenching rates are determined by the time dependence of this fluorescence. As
Rb($5^{2}P$) collisional excitation transfer is relatively fast in methane gas,
measurements were performed at methane pressures of $2.5 - 25$ Torr, resulting
in a collisional transfer cross section ($5^{2}P_{3/2} \rightarrow
5^{2}P_{1/2}$) of $(4.23 \pm 0.13) \times 10^{-15}$ cm$^{2}$. Quenching rates
were found to be much slower and were performed over methane pressures of $50 -
4000$ Torr, resulting in a quenching cross section of $(7.52 \pm 0.10) \times
10^{-19}$ cm$^{2}$. These results represent a significant increase in precision
compared to previous work, and also resolve a discrepancy in previous quenching
measurements.",pressured selling
http://arxiv.org/abs/1611.09613v1,"We consider the problem of maximizing revenue when selling k items to a
single buyer with known valuation distributions. We show that for a single,
additive buyer whose valuations for for the items are distributed according to
i.i.d. distributions which are known to the seller, the ratio of revenue from
selling in a bundle to selling separately is at least 55.9% and this gap is
attainable.",pressured selling
http://arxiv.org/abs/1204.1846v3,"Maximizing the revenue from selling _more than one_ good (or item) to a
single buyer is a notoriously difficult problem, in stark contrast to the
one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms,
such as selling the goods separately, _guarantee_ at least 73% of the optimal
revenue when the valuations of the two goods are independent and identically
distributed, and at least $50\%$ when they are independent. For the case of
$k>2$ independent goods, we show that selling them separately guarantees at
least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and
identically distributed goods, we show that selling them as one bundle
guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional
results compare the revenues from the two simple mechanisms of selling the
goods separately and bundled, identify situations where bundling is optimal,
and extend the analysis to multiple buyers.",pressured selling
http://arxiv.org/abs/1208.2460v1,"Decision Taking is discussed in the context of the role it may play for a
selling agent in a search market, in particular for agents involved in the sale
of valuable and relatively unique items, such as a dwelling, a second hand car,
or a second hand recreational vessel.
  Detailed connections are made between the architecture of decision making
processes and a sample of software technology based concepts including
instruction sequences, multi-threading, and thread algebra.
  Ample attention is paid to the initialization or startup of a thread
dedicated to achieving a given objective, and to corresponding decision taking.
As an application, the selling of an item is taken as an objective to be
achieved by running a thread that was designed for that purpose.",pressured selling
http://arxiv.org/abs/1706.00219v1,"We consider a price competition between two sellers of perfect-complement
goods. Each seller posts a price for the good it sells, but the demand is
determined according to the sum of prices. This is a classic model by Cournot
(1838), who showed that in this setting a monopoly that sells both goods is
better for the society than two competing sellers. We show that non-trivial
pure Nash equilibria always exist in this game. We also quantify Cournot's
observation with respect to both the optimal welfare and the monopoly revenue.
We then prove a series of mostly negative results regarding the convergence of
best response dynamics to equilibria in such games.",pressured selling
http://arxiv.org/abs/1608.08779v2,"HyLL (Hybrid Linear Logic) and SELL (Subexponential Linear Logic) are logical
frameworks that have been extensively used for specifying systems that exhibit
modalities such as temporal or spatial ones. Both frameworks have linear logic
(LL) as a common ground and they admit (cut-free) complete focused proof
systems. The difference between the two logics relies on the way modalities are
handled. In HyLL, truth judgments are labelled by worlds and hybrid connectives
relate worlds with formulas. In SELL, the linear logic exponentials (!, ?) are
decorated with labels representing locations, and an ordering on such labels
defines the provability relation among resources in those locations. It is well
known that SELL, as a logical framework, is strictly more expressive than LL.
However, so far, it was not clear whether HyLL is more expressive than LL
and/or SELL. In this paper, we show an encoding of the HyLL's logical rules
into LL with the highest level of adequacy, hence showing that HyLL is as
expressive as LL. We also propose an encoding of HyLL into SELL $\doublecup$
(SELL plus quantification over locations) that gives better insights about the
meaning of worlds in HyLL. We conclude our expressiveness study by showing that
previous attempts of encoding Computational Tree Logic (CTL) operators into
HyLL cannot be extended to consider the whole set of temporal connectives. We
show that a system of LL with fixed points is indeed needed to faithfully
encode the behavior of such temporal operators.",pressured selling
http://arxiv.org/abs/1407.6131v1,"The discrete sell or hold problem (DSHP), which is introduced in \cite{H12},
is studied under the constraint that each asset can only take a constant number
of different values. We show that if each asset can take only two values, the
problem becomes polynomial-time solvable. However, even if each asset can take
three different values, DSHP is still NP-hard. An approximation algorithm is
also given under this setting.",pressured selling
http://arxiv.org/abs/1712.03518v1,"We consider the problem of maximizing revenue when selling 2 items to a
single buyer with known valuation distributions. Hart and Nisan showed that
selling each item separately using the optimal Myerson's price, gains at least
half of the revenue attainable by optimal auction for two items. We show that
in case the items have different revenues when sold separately the bound can be
tightened.",pressured selling
http://arxiv.org/abs/1712.08973v2,"Separate selling of two independent goods is shown to yield at least 62% of
the optimal revenue, and at least 73% when the goods satisfy the Myerson
regularity condition. This improves the 50% result of Hart and Nisan (2017,
originally circulated in 2012).",pressured selling
http://arxiv.org/abs/1307.8191v1,"The purpose of this research is to develop an Information System of Selling
and Services using Microsoft Visual Basic and Microsoft Access for it database.
The benefits of this research is to help CV Computer Plus in selling and
services data processing everyday. To develop this IS is used 5 (five) steps:
1) Planning, 2) Analysis, 3) Design, 4) Implementation, and 5) Evaluation. The
Information System can record the selling and services data, it also prepared
usefull reports. By using this IS, CV Computer Plus can operate their selling
and services efficiency and effectively. In the future it can be upgraded for
network application.",pressured selling
http://arxiv.org/abs/1504.07283v1,"The current Cloud infrastructure services (IaaS) market employs a
resource-based selling model: customers rent nodes from the provider and pay
per-node per-unit-time. This selling model places the burden upon customers to
predict their job resource requirements and durations. Inaccurate prediction by
customers can result in over-provisioning of resources, or under-provisioning
and poor job performance. Thanks to improved resource virtualization and
multi-tenant performance isolation, as well as common frameworks for batch
jobs, such as MapReduce, Cloud providers can predict job completion times more
accurately. We offer a new definition of QoS-levels in terms of job completion
times and we present a new QoS-based selling mechanism for batch jobs in a
multi-tenant OpenStack cluster. Our experiments show that the QoS-based
solution yields up to 40% improvement over the revenue of more standard selling
mechanisms based on a fixed per-node price across various demand and supply
conditions in a 240-VCPU OpenStack cluster.",pressured selling
http://arxiv.org/abs/1604.05603v1,"Ad exchanges are becoming an increasingly popular way to sell advertisement
slots on the internet. An ad exchange is basically a spot market for ad
impressions. A publisher who has already signed contracts reserving
advertisement impressions on his pages can choose between assigning a new ad
impression for a new page view to a contracted advertiser or to sell it at an
ad exchange. This leads to an online revenue maximization problem for the
publisher. Given a new impression to sell decide whether (a) to assign it to a
contracted advertiser and if so to which one or (b) to sell it at the ad
exchange and if so at which reserve price. We make no assumptions about the
distribution of the advertiser valuations that participate in the ad exchange
and show that there exists a simple primal-dual based online algorithm, whose
lower bound for the revenue converges to $R_{ADX} + R_A (1 - 1/e)$, where
$R_{ADX}$ is the revenue that the optimum algorithm achieves from the ad
exchange and $R_A$ is the revenue that the optimum algorithm achieves from the
contracted advertisers.",pressured selling
http://arxiv.org/abs/1801.02908v1,"A sequence of recent studies show that even in the simple setting of a single
seller and a single buyer with additive, independent valuations over $m$ items,
the revenue-maximizing mechanism is prohibitively complex. This problem has
been addressed using two main approaches: (i) Approximation: the best of two
simple mechanisms (sell each item separately, or sell all the items as one
bundle) gives $1/6$ of the optimal revenue [BILW14]. (ii) Enhanced competition:
running the simple VCG mechanism with additional $m$ buyers extracts at least
the optimal revenue in the original market [EFFTW17]. Both approaches, however,
suffer from severe drawbacks: On the one hand, losing $83\%$ of the revenue is
hardly acceptable in any application. On the other hand, attracting a linear
number of new buyers may be prohibitive. Our main result is that by combining
the two approaches one can achieve the best of both worlds. Specifically, for
any constant $\epsilon$ one can obtain a $(1-\epsilon)$ fraction of the optimal
revenue by running simple mechanisms --- either selling each item separately or
selling all items as a single bundle --- with substantially fewer additional
buyers: logarithmic, constant, or even none in some cases.",pressured selling
http://arxiv.org/abs/1806.09793v1,"With the considerable development of customer-to-customer (C2C) e-commerce in
the recent years, there is a big demand for an effective recommendation system
that suggests suitable websites for users to sell their items with some
specified needs. Nonetheless, e-commerce recommendation systems are mostly
designed for business-to-customer (B2C) websites, where the systems offer the
consumers the products that they might like to buy. Almost none of the related
research works focus on choosing selling sites for target items. In this paper,
we introduce an approach that recommends the selling websites based upon the
item's description, category, and desired selling price. This approach employs
NoSQL data-based machine learning techniques for building and training topic
models and classification models. The trained models can then be used to rank
the websites dynamically with respect to the user needs. The experimental
results with real-world datasets from Vietnam C2C websites will demonstrate the
effectiveness of our proposed method.",pressured selling
http://arxiv.org/abs/1204.5519v1,"The buying and selling of information is taking place at a scale
unprecedented in the history of commerce, thanks to the formation of online
marketplaces for user data. Data providing agencies sell user information to
advertisers to allow them to match ads to viewers more effectively. In this
paper we study the design of optimal mechanisms for a monopolistic data
provider to sell information to a buyer, in a model where both parties have
(possibly correlated) private signals about a state of the world, and the buyer
uses information learned from the seller, along with his own signal, to choose
an action (e.g., displaying an ad) whose payoff depends on the state of the
world.
  We provide sufficient conditions under which there is a simple one-round
protocol (i.e. a protocol where the buyer and seller each sends a single
message, and there is a single money transfer) achieving optimal revenue. In
these cases we present a polynomial-time algorithm that computes the optimal
mechanism. Intriguingly, we show that multiple rounds of partial information
disclosure (interleaved by payment to the seller) are sometimes necessary to
achieve optimal revenue if the buyer is allowed to abort his interaction with
the seller prematurely. We also prove some negative results about the inability
of simple mechanisms for selling information to approximate more complicated
ones in the worst case.",pressured selling
http://arxiv.org/abs/1404.2832v6,"Using duality theory techniques we derive simple, closed-form formulas for
bounding the optimal revenue of a monopolist selling many heterogeneous goods,
in the case where the buyer's valuations for the items come i.i.d. from a
uniform distribution and in the case where they follow independent (but not
necessarily identical) exponential distributions. We apply this in order to get
in both these settings specific performance guarantees, as functions of the
number of items $m$, for the simple deterministic selling mechanisms studied by
Hart and Nisan [EC 2012], namely the one that sells the items separately and
the one that offers them all in a single bundle.
  We also propose and study the performance of a natural randomized mechanism
for exponential valuations, called Proportional. As an interesting corollary,
for the special case where the exponential distributions are also identical, we
can derive that offering the goods in a single full bundle is the optimal
selling mechanism for any number of items. To our knowledge, this is the first
result of its kind: finding a revenue-maximizing auction in an additive setting
with arbitrarily many goods.",pressured selling
http://arxiv.org/abs/cs/0204051v1,"On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders.",pressured selling
http://arxiv.org/abs/1607.06123v2,"There is an abundance of temporal and non-temporal data in banking (and other
industries), but such temporal activity data can not be used directly with
classical machine learning models. In this work, we perform extensive feature
extraction from the temporal user activity data in an attempt to predict user
visits to different branches and credit card up-selling utilizing user
information and the corresponding activity data, as part of \emph{ECML/PKDD
Discovery Challenge 2016 on Bank Card Usage Analysis}. Our solution ranked
\nth{4} for \emph{Task 1} and achieved an AUC of \textbf{$0.7056$} for
\emph{Task 2} on public leaderboard.",pressured selling
http://arxiv.org/abs/1608.05117v1,"FERC Order 745 allows demand response owners to sell their load reduction in
the wholesale market. However, in order to be able to sell the load reduction,
some implementation challenges must be addressed, one of which is to establish
Customer Baseline Load (CBL) calculation methods with acceptable error
performance, which has proven to be very challenging so far. In this paper, the
error and financial performance of Randomized Controlled Trial (RCT) method,
applied to both granular and aggregated forms of the consumption load, are
investigated for a hypothetical demand response program offered to a real
dataset of residential customers .",pressured selling
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake testimonials
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake testimonials
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake testimonials
http://arxiv.org/abs/physics/0403122v1,"The origins of Sociophysics are discussed from a personal testimony. I trace
back its history to the late seventies. My twenty years of activities and
research to establish and promote the field are reviewed. In particular the
conflicting nature of Sociophysics with the physics community is revealed from
my own experience. Recent presentations of a supposed natural growth from
Social Sciences are criticized.",fake testimonials
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake testimonials
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake testimonials
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake testimonials
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake testimonials
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake testimonials
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake testimonials
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake testimonials
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake testimonials
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake testimonials
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake testimonials
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake testimonials
http://arxiv.org/abs/1811.00661v2,"In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.",fake testimonials
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake testimonials
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake testimonials
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake testimonials
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake testimonials
http://arxiv.org/abs/0803.4245v1,"This article describes epistemological distinctions between science and
history. Science investigates models of natural law using repeatable
experiments as the ultimate arbiter. In contrast, history investigates past
events by considering physical evidence, documentary evidence, and eyewitness
testimony. Because questions of natural law are repeatably testable by any
audience that exercises due experimental care, models of natural law are
inherently more objective and testable with greater certainty than theories of
past events.",fake testimonials
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",fake testimonials
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",fake testimonials
http://arxiv.org/abs/1803.08491v2,"The dynamics and influence of fake news on Twitter during the 2016 US
presidential election remains to be clarified. Here, we use a dataset of 171
million tweets in the five months preceding the election day to identify 30
million tweets, from 2.2 million users, which contain a link to news outlets.
Based on a classification of news outlets curated by www.opensources.co, we
find that 25% of these tweets spread either fake or extremely biased news. We
characterize the networks of information flow to find the most influential
spreaders of fake and traditional news and use causal modeling to uncover how
fake news influenced the presidential election. We find that, while top
influencers spreading traditional center and left leaning news largely
influence the activity of Clinton supporters, this causality is reversed for
the fake news: the activity of Trump supporters influences the dynamics of the
top fake news spreaders.",fake testimonials
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake testimonials
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake testimonials
http://arxiv.org/abs/1906.11126v1,"The generation and spread of fake news within new and online media sources is
emerging as a phenomenon of high societal significance. Combating them using
data-driven analytics has been attracting much recent scholarly interest. In
this study, we analyze the textual coherence of fake news articles vis-a-vis
legitimate ones. We develop three computational formulations of textual
coherence drawing upon the state-of-the-art methods in natural language
processing and data science. Two real-world datasets from widely different
domains which have fake/legitimate article labellings are then analyzed with
respect to textual coherence. We observe apparent differences in textual
coherence across fake and legitimate news articles, with fake news articles
consistently scoring lower on coherence as compared to legitimate news ones.
While the relative coherence shortfall of fake news articles as compared to
legitimate ones form the main observation from our study, we analyze several
aspects of the differences and outline potential avenues of further inquiry.",fake testimonials
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake testimonial detection
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake testimonial detection
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake testimonial detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake testimonial detection
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake testimonial detection
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake testimonial detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake testimonial detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake testimonial detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake testimonial detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake testimonial detection
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake testimonial detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake testimonial detection
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake testimonial detection
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake testimonial detection
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",fake testimonial detection
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake testimonial detection
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake testimonial detection
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake testimonial detection
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",fake testimonial detection
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",fake testimonial detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake testimonial detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake testimonial detection
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",fake testimonial detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake testimonial detection
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",fake testimonial detection
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",fake testimonial detection
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",fake testimonial detection
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",fake testimonial detection
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",fake testimonial detection
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",fake testimonial detection
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",fake testimonial detection
http://arxiv.org/abs/1908.09805v1,"Automatic detection of fake news --- texts that are deceitful and misleading
--- is a long outstanding and largely unsolved problem. Worse yet, recent
developments in language modeling allow for the automatic generation of such
texts. One approach that has recently gained attention detects these fake news
using stylometry-based provenance, i.e. tracing a text's writing style back to
its producing source and determining whether the source is malicious. This was
shown to be highly effective under the assumption that legitimate text is
produced by humans, and fake text is produced by a language model.
  In this work, we identify a fundamental problem with provenance-based
approaches against attackers that auto-generate fake news: fake and legitimate
texts can originate from nearly identical sources. First, a legitimate text
might be auto-generated in a similar process to that of fake text, and second,
attackers can automatically corrupt articles originating from legitimate human
sources. We demonstrate these issues by simulating attacks in such settings,
and find that the provenance approach fails to defend against them. Our
findings highlight the importance of assessing the veracity of the text rather
than solely relying on its style or source. We also open up a discussion on the
types of benchmarks that should be used to evaluate neural fake news detectors.",fake testimonial detection
http://arxiv.org/abs/1804.03508v1,"The fake news epidemic makes it imperative to develop a diagnostic framework
that is both parsimonious and valid to guide present and future efforts in fake
news detection. This paper represents one of the very first attempts to fill a
void in the research on this topic. The LeSiE (Lexical Structure, Simplicity,
Emotion) framework we created and validated allows lay people to identify
potential fake news without the use of calculators or complex statistics by
looking out for three simple cues.",fake testimonial detection
http://arxiv.org/abs/1903.01728v1,"Microblog has become a popular platform for people to post, share, and seek
information due to its convenience and low cost. However, it also facilitates
the generation and propagation of fake news, which could cause detrimental
societal consequences. Detecting fake news on microblogs is important for
societal good. Emotion is a significant indicator while verifying information
on social media. Existing fake news detection studies utilize emotion mainly
through users stances or simple statistical emotional features; and exploiting
the emotion information from both news content and user comments is also
limited. In the realistic scenarios, to impress the audience and spread
extensively, the publishers typically either post a tweet with intense emotion
which could easily resonate with the crowd, or post a controversial statement
unemotionally but aim to evoke intense emotion among the users. Therefore, in
this paper, we study the novel problem of exploiting emotion information for
fake news detection. We propose a new Emotion-based Fake News Detection
framework (EFN), which can i) learn content- and comment- emotion
representations for publishers and users respectively; and ii) exploit content
and social emotions simultaneously for fake news detection. Experimental
results on real-world dataset demonstrate the effectiveness of the proposed
framework.",fake testimonial detection
http://arxiv.org/abs/1708.07104v1,"The proliferation of misleading information in everyday access media outlets
such as social media feeds, news blogs, and online newspapers have made it
challenging to identify trustworthy news sources, thus increasing the need for
computational tools able to provide insights into the reliability of online
content. In this paper, we focus on the automatic identification of fake
content in online news. Our contribution is twofold. First, we introduce two
novel datasets for the task of fake news detection, covering seven different
news domains. We describe the collection, annotation, and validation process in
detail and present several exploratory analysis on the identification of
linguistic differences in fake and legitimate news content. Second, we conduct
a set of learning experiments to build accurate fake news detectors. In
addition, we provide comparative analyses of the automatic and manual
identification of fake news.",fake testimonial detection
http://arxiv.org/abs/1806.07516v2,"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.",fake testimonial detection
http://arxiv.org/abs/1901.09657v1,"News plays a significant role in shaping people's beliefs and opinions. Fake
news has always been a problem, which wasn't exposed to the mass public until
the past election cycle for the 45th President of the United States. While
quite a few detection methods have been proposed to combat fake news since
2015, they focus mainly on linguistic aspects of an article without any fact
checking. In this paper, we argue that these models have the potential to
misclassify fact-tampering fake news as well as under-written real news.
Through experiments on Fakebox, a state-of-the-art fake news detector, we show
that fact tampering attacks can be effective. To address these weaknesses, we
argue that fact checking should be adopted in conjunction with linguistic
characteristics analysis, so as to truly separate fake news from real news. A
crowdsourced knowledge graph is proposed as a straw man solution to collecting
timely facts about news events.",fake testimonial detection
http://arxiv.org/abs/1905.04260v1,"Over the past few years, we have been witnessing the rise of misinformation
on the Web. People fall victims of fake news during their daily lives and
assist their further propagation knowingly and inadvertently. There have been
many initiatives that are trying to mitigate the damage caused by fake news,
focusing on signals from either domain flag-lists, online social networks or
artificial intelligence. In this work, we present Check-It, a system that
combines, in an intelligent way, a variety of signals into a pipeline for fake
news identification. Check-It is developed as a web browser plugin with the
objective of efficient and timely fake news detection, respecting the user's
privacy. Experimental results show that Check-It is able to outperform the
state-of-the-art methods. On a dataset, consisting of 9 millions of articles
labeled as fake and real, Check-It obtains classification accuracies that
exceed 99%.",fake testimonial detection
http://arxiv.org/abs/1805.08751v2,"In recent years, due to the booming development of online social networks,
fake news for various commercial and political purposes has been appearing in
large numbers and widespread in the online world. With deceptive words, online
social network users can get infected by these online fake news easily, which
has brought about tremendous effects on the offline society already. An
important goal in improving the trustworthiness of information in online social
networks is to identify the fake news timely. This paper aims at investigating
the principles, methodologies and algorithms for detecting fake news articles,
creators and subjects from online social networks and evaluating the
corresponding performance. This paper addresses the challenges introduced by
the unknown characteristics of fake news and diverse connections among news
articles, creators and subjects. This paper introduces a novel automatic fake
news credibility inference model, namely FAKEDETECTOR. Based on a set of
explicit and latent features extracted from the textual information,
FAKEDETECTOR builds a deep diffusive network model to learn the representations
of news articles, creators and subjects simultaneously. Extensive experiments
have been done on a real-world fake news dataset to compare FAKEDETECTOR with
several state-of-the-art models, and the experimental results have demonstrated
the effectiveness of the proposed model.",fake testimonial detection
http://arxiv.org/abs/cs/0204051v1,"On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders.",low stock message
http://arxiv.org/abs/1305.7014v1,"In this paper, we present a software package for the data mining of Twitter
microblogs for the purpose of using them for the stock market analysis. The
package is written in R langauge using apropriate R packages. The model of
tweets has been considered. We have also compared stock market charts with
frequent sets of keywords in Twitter microblogs messages.",low stock message
http://arxiv.org/abs/1204.5369v1,"In recent years there has been a growing interest in crowdsourcing
methodologies to be used in experimental research for NLP tasks. In particular,
evaluation of systems and theories about persuasion is difficult to accommodate
within existing frameworks. In this paper we present a new cheap and fast
methodology that allows fast experiment building and evaluation with
fully-automated analysis at a low cost. The central idea is exploiting existing
commercial tools for advertising on the web, such as Google AdWords, to measure
message impact in an ecological setting. The paper includes a description of
the approach, tips for how to use AdWords for scientific research, and results
of pilot experiments on the impact of affective text variations which confirm
the effectiveness of the approach.",low stock message
http://arxiv.org/abs/1801.00588v1,"Traditional stock market prediction approaches commonly utilize the
historical price-related data of the stocks to forecast their future trends. As
the Web information grows, recently some works try to explore financial news to
improve the prediction. Effective indicators, e.g., the events related to the
stocks and the people's sentiments towards the market and stocks, have been
proved to play important roles in the stocks' volatility, and are extracted to
feed into the prediction models for improving the prediction accuracy. However,
a major limitation of previous methods is that the indicators are obtained from
only a single source whose reliability might be low, or from several data
sources but their interactions and correlations among the multi-sourced data
are largely ignored.
  In this work, we extract the events from Web news and the users' sentiments
from social media, and investigate their joint impacts on the stock price
movements via a coupled matrix and tensor factorization framework.
Specifically, a tensor is firstly constructed to fuse heterogeneous data and
capture the intrinsic relations among the events and the investors' sentiments.
Due to the sparsity of the tensor, two auxiliary matrices, the stock
quantitative feature matrix and the stock correlation matrix, are constructed
and incorporated to assist the tensor decomposition. The intuition behind is
that stocks that are highly correlated with each other tend to be affected by
the same event. Thus, instead of conducting each stock prediction task
separately and independently, we predict multiple correlated stocks
simultaneously through their commonalities, which are enabled via sharing the
collaboratively factorized low rank matrices between matrices and the tensor.
Evaluations on the China A-share stock data and the HK stock data in the year
2015 demonstrate the effectiveness of the proposed model.",low stock message
http://arxiv.org/abs/1406.7330v1,"We revisit the problem of predicting directional movements of stock prices
based on news articles: here our algorithm uses daily articles from The Wall
Street Journal to predict the closing stock prices on the same day. We propose
a unified latent space model to characterize the ""co-movements"" between stock
prices and news articles. Unlike many existing approaches, our new model is
able to simultaneously leverage the correlations: (a) among stock prices, (b)
among news articles, and (c) between stock prices and news articles. Thus, our
model is able to make daily predictions on more than 500 stocks (most of which
are not even mentioned in any news article) while having low complexity. We
carry out extensive backtesting on trading strategies based on our algorithm.
The result shows that our model has substantially better accuracy rate (55.7%)
compared to many widely used algorithms. The return (56%) and Sharpe ratio due
to a trading strategy based on our model are also much higher than baseline
indices.",low stock message
http://arxiv.org/abs/1712.02136v3,"Stock trend prediction plays a critical role in seeking maximized profit from
stock investment. However, precise trend prediction is very difficult since the
highly volatile and non-stationary nature of stock market. Exploding
information on Internet together with advancing development of natural language
processing and text mining techniques have enable investors to unveil market
trends and volatility from online content. Unfortunately, the quality,
trustworthiness and comprehensiveness of online content related to stock market
varies drastically, and a large portion consists of the low-quality news,
comments, or even rumors. To address this challenge, we imitate the learning
process of human beings facing such chaotic online news, driven by three
principles: sequential content dependency, diverse influence, and effective and
efficient learning. In this paper, to capture the first two principles, we
designed a Hybrid Attention Networks to predict the stock trend based on the
sequence of recent related news. Moreover, we apply the self-paced learning
mechanism to imitate the third principle. Extensive experiments on real-world
stock market data demonstrate the effectiveness of our approach.",low stock message
http://arxiv.org/abs/1406.1137v1,"For decades, the world of financial advisors has been dominated by large
investment banks such as Goldman Sachs. In recent years, user-contributed
investment services such as SeekingAlpha and StockTwits have grown to millions
of users. In this paper, we seek to understand the quality and impact of
content on social investment platforms, by empirically analyzing complete
datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years).
We develop sentiment analysis tools and correlate contributed content to the
historical performance of relevant stocks. While SeekingAlpha articles and
StockTwits messages provide minimal correlation to stock performance in
aggregate, a subset of authors contribute more valuable (predictive) content.
We show that these authors can be identified via both empirical methods or by
user interactions, and investments using their analysis significantly
outperform broader markets. Finally, we conduct a user survey that sheds light
on users views of SeekingAlpha content and stock manipulation.",low stock message
http://arxiv.org/abs/1708.09492v1,"Commit messages are a valuable resource in comprehension of software
evolution, since they provide a record of changes such as feature additions and
bug repairs. Unfortunately, programmers often neglect to write good commit
messages. Different techniques have been proposed to help programmers by
automatically writing these messages. These techniques are effective at
describing what changed, but are often verbose and lack context for
understanding the rationale behind a change. In contrast, humans write messages
that are short and summarize the high level rationale. In this paper, we adapt
Neural Machine Translation (NMT) to automatically ""translate"" diffs into commit
messages. We trained an NMT algorithm using a corpus of diffs and human-written
commit messages from the top 1k Github projects. We designed a filter to help
ensure that we only trained the algorithm on higher-quality commit messages.
Our evaluation uncovered a pattern in which the messages we generate tend to be
either very high or very low quality. Therefore, we created a quality-assurance
filter to detect cases in which we are unable to produce good messages, and
return a warning instead.",low stock message
http://arxiv.org/abs/1801.06541v1,"Software messaging frameworks help avoid errors and reduce engineering
efforts in building distributed systems by (1) providing an interface
definition language (IDL) to specify precisely the structure of the message
(i.e., the message schema), and (2) automatically generating the serialization
and deserialization functions that transform user data structures into binary
data for sending across the network and vice versa. Similarly, a
hardware-accelerated system, which consists of host software and multiple
FPGAs, could also benefit from a messaging framework to handle messages both
between software and FPGA and also between different FPGAs. The key challenge
for a hardware messaging framework is that it must be able to support large
messages with complex schema while meeting critical constraints such as clock
frequency, area, and throughput.
  In this paper, we present HGum, a messaging framework for hardware
accelerators that meets all the above requirements. HGum is able to generate
high-performance and low-cost hardware logic by employing a novel design that
algorithmically parses the message schema to perform serialization and
deserialization. Our evaluation of HGum shows that it not only significantly
reduces engineering efforts but also generates hardware with comparable quality
to manual implementation.",low stock message
http://arxiv.org/abs/1904.07171v1,"Atomic multicast is a communication primitive that delivers messages to
multiple groups of processes according to some total order, with each group
receiving the projection of the total order onto messages addressed to it. To
be scalable, atomic multicast needs to be genuine, meaning that only the
destination processes of a message should participate in ordering it. In this
paper we propose a novel genuine atomic multicast protocol that in the absence
of failures takes as low as 3 message delays to deliver a message when no other
messages are multicast concurrently to its destination groups, and 5 message
delays in the presence of concurrency. This improves the latencies of both the
fault-tolerant version of classical Skeen's multicast protocol (6 or 12 message
delays, depending on concurrency) and its recent improvement by Coelho et al.
(4 or 8 message delays). To achieve such low latencies, we depart from the
typical way of guaranteeing fault-tolerance by replicating each group with
Paxos. Instead, we weave Paxos and Skeen's protocol together into a single
coherent protocol, exploiting opportunities for white-box optimisations. We
experimentally demonstrate that the superior theoretical characteristics of our
protocol are reflected in practical performance pay-offs.",low stock message
http://arxiv.org/abs/1102.1475v1,"This paper considers the problem of simultaneously communicating two
messages, a high-security message and a low-security message, to a legitimate
receiver, referred to as the security embedding problem. An
information-theoretic formulation of the problem is presented. A coding scheme
that combines rate splitting, superposition coding, nested binning and channel
prefixing is considered and is shown to achieve the secrecy capacity region of
the channel in several scenarios. Specifying these results to both scalar and
independent parallel Gaussian channels (under an average individual
per-subchannel power constraint), it is shown that the high-security message
can be embedded into the low-security message at full rate (as if the
low-security message does not exist) without incurring any loss on the overall
rate of communication (as if both messages are low-security messages).
Extensions to the wiretap channel II setting of Ozarow and Wyner are also
considered, where it is shown that ""perfect"" security embedding can be achieved
by an encoder that uses a two-level coset code.",low stock message
http://arxiv.org/abs/1902.02704v3,"Stickers are popularly used in messaging apps such as Hike to visually
express a nuanced range of thoughts and utterances to convey exaggerated
emotions. However, discovering the right sticker from a large and ever
expanding pool of stickers while chatting can be cumbersome. In this paper, we
describe a system for recommending stickers in real time as the user is typing
based on the context of conversation. We decompose the sticker recommendation
problem into two steps. First, we predict the message that the user is likely
to send in the chat. Second, we substitute the predicted message with an
appropriate sticker. Majority of Hike's messages are in the form of text which
is transliterated from users' native language to the Roman script. This leads
to numerous orthographic variations of the same message and complicates message
prediction. To address this issue, we learn dense representations of chat
messages and use them to cluster the messages that have same meaning. In the
subsequent steps we predict the message cluster instead of the message. Our
model employs a character level convolution network to capture the similar
intents in orthographic variants of chats. We validate our approach using
manually labelled data on two tasks. We also propose a novel hybrid message
prediction model, which can run with low latency on low end phones that have
severe computational limitations.",low stock message
http://arxiv.org/abs/1401.5092v1,"We consider symmetric two-user Gaussian interference channel with common
messages. We derive an upper bound on the sum capacity, and show that the upper
bound is tight in the low interference regime, where the optimal transmission
scheme is to send no common messages and each receiver treats interference as
noise. Our result shows that although the availability of common messages
provides a cooperation opportunity for transmitters, in the low interference
regime the presence of common messages does not help increase the sum capacity.",low stock message
http://arxiv.org/abs/1812.01963v1,"In this report, we describe the design and implementation of Ibdxnet, a
low-latency and high-throughput transport providing the benefits of InfiniBand
networks to Java applications. Ibdxnet is part of the Java-based DXNet library,
a highly concurrent and simple to use messaging stack with transparent
serialization of messaging objects and focus on very small messages (< 64
bytes). Ibdxnet implements the transport interface of DXNet in Java and a
custom C++ library in native space using JNI. Several optimizations in both
spaces minimize context switching overhead between Java and C++ and are not
burdening message latency or throughput. Communication is implemented using the
messaging verbs of the ibverbs library complemented by an automatic connection
management in the native library. We compared DXNet with the Ibdxnet transport
to the MPI implementations FastMPJ and MVAPICH2. For small messages up to 64
bytes using multiple threads, DXNet with the Ibdxnet transport achieves a
bi-directional message rate of 10 million messages per second and surpasses
FastMPJ by a factor of 4 and MVAPICH by a factor of 2. Furthermore, DXNet
scales well on a high load all-to-all communication with up to 8 nodes
achieving a total aggregated message rate of 43.4 million messages per second
for small messages and a throughput saturation of 33.6 GB/s with only 2 kb
message size.",low stock message
http://arxiv.org/abs/1804.04406v2,"Microblogs are increasingly exploited for predicting prices and traded
volumes of stocks in financial markets. However, it has been demonstrated that
much of the content shared in microblogging platforms is created and publicized
by bots and spammers. Yet, the presence (or lack thereof) and the impact of
fake stock microblogs has never systematically been investigated before. Here,
we study 9M tweets related to stocks of the 5 main financial markets in the US.
By comparing tweets with financial data from Google Finance, we highlight
important characteristics of Twitter stock microblogs. More importantly, we
uncover a malicious practice - referred to as cashtag piggybacking -
perpetrated by coordinated groups of bots and likely aimed at promoting
low-value stocks by exploiting the popularity of high-value ones. Among the
findings of our study is that as much as 71% of the authors of suspicious
financial tweets are classified as bots by a state-of-the-art spambot detection
algorithm. Furthermore, 37% of them were suspended by Twitter a few months
after our investigation. Our results call for the adoption of spam and bot
detection techniques in all studies and applications that exploit
user-generated content for predicting the stock market.",low stock message
http://arxiv.org/abs/1804.03346v3,"We consider the problem of separating error messages generated in large
distributed data center networks into error events. In such networks, each
error event leads to a stream of messages generated by hardware and software
components affected by the event. These messages are stored in a giant message
log. We consider the unsupervised learning problem of identifying the
signatures of events that generated these messages; here, the signature of an
error event refers to the mixture of messages generated by the event. One of
the main contributions of the paper is a novel mapping of our problem which
transforms it into a problem of topic discovery in documents. Events in our
problem correspond to topics and messages in our problem correspond to words in
the topic discovery problem. However, there is no direct analog of documents.
Therefore, we use a non-parametric change-point detection algorithm, which has
linear computational complexity in the number of messages, to divide the
message log into smaller subsets called episodes, which serve as the
equivalents of documents. After this mapping has been done, we use a well-known
algorithm for topic discovery, called LDA, to solve our problem. We
theoretically analyze the change-point detection algorithm, and show that it is
consistent and has low sample complexity. We also demonstrate the scalability
of our algorithm on a real data set consisting of $97$ million messages
collected over a period of $15$ days, from a distributed data center network
which supports the operations of a large wireless service provider.",low stock message
http://arxiv.org/abs/1610.00620v1,"Excessive tail end-to-end latency occurs with conventional message brokers as
a result of having massive numbers of geographically distributed devices
communicate through a message broker. On the other hand, broker-less messaging
systems, though ensure low latency, are highly dependent on the limitation of
direct device-to-device (D2D) communication technologies, and cannot scale well
as large numbers of resource-limited devices exchange messages. In this paper,
we propose FogMQ, a cloud-based message broker system that overcomes the
limitations of conventional systems by enabling autonomous discovery,
self-deployment, and online migration of message brokers across heterogeneous
cloud platforms. For each device, FogMQ provides a high capacity device cloning
service that subscribes to device messages. The clones facilitate near-the-edge
data analytics in resourceful cloud compute nodes. Clones in FogMQ apply Flock,
an algorithm mimicking flocking-like behavior to allow clones to dynamically
select and autonomously migrate to different heterogeneous cloud platforms in a
distributed manner.",low stock message
http://arxiv.org/abs/1001.2060v3,"The security of chaotic optical communication using time-frequency (TF)
representation is analyzed in this paper. The mean scalogram ratio (MSR) of TF
representation and peak sidelobe level of MSR are defined to detect message.
Algorithm for message detection and extraction is presented in detail. Two
typical message encryption schemes, chaos masking and chaos modulation, are
analyzed. The results reveal that it is not secure to transmit message when the
message frequency locates at low power on power spectrum portrait. The proposed
method is very useful for estimating the security level of message masking in
chaotic optical communication.",low stock message
http://arxiv.org/abs/1004.1864v2,"The Low Latency Fault Tolerance (LLFT) system provides fault tolerance for
distributed applications, using the leader-follower replication technique. The
LLFT system provides application-transparent replication, with strong replica
consistency, for applications that involve multiple interacting processes or
threads. The LLFT system comprises a Low Latency Messaging Protocol, a
Leader-Determined Membership Protocol, and a Virtual Determinizer Framework.
The Low Latency Messaging Protocol provides reliable, totally ordered message
delivery by employing a direct group-to-group multicast, where the message
ordering is determined by the primary replica in the group. The
Leader-Determined Membership Protocol provides reconfiguration and recovery
when a replica becomes faulty and when a replica joins or leaves a group, where
the membership of the group is determined by the primary replica. The Virtual
Determinizer Framework captures the ordering information at the primary replica
and enforces the same ordering at the backup replicas for major sources of
non-determinism, including multi-threading, time-related operations and socket
communication. The LLFT system achieves low latency message delivery during
normal operation and low latency reconfiguration and recovery when a fault
occurs.",low stock message
http://arxiv.org/abs/1512.07782v2,"We propose a hybrid message passing method for distributed cooperative
localization and tracking of mobile agents. Belief propagation and mean field
message passing are employed for, respectively, the motion-related and
measurement-related part of the factor graph. Using a Gaussian belief
approximation, only three real values per message passing iteration have to be
broadcast to neighboring agents. Despite these very low communication
requirements, the estimation accuracy can be comparable to that of
particle-based belief propagation.",low stock message
http://arxiv.org/abs/1909.10407v1,"Noisy situations cause huge problems for suffers of hearing loss as hearing
aids often make the signal more audible but do not always restore the
intelligibility. In noisy settings, humans routinely exploit the audio-visual
(AV) nature of the speech to selectively suppress the background noise and to
focus on the target speaker. In this paper, we present a causal, language,
noise and speaker independent AV deep neural network (DNN) architecture for
speech enhancement (SE). The model exploits the noisy acoustic cues and noise
robust visual cues to focus on the desired speaker and improve the speech
intelligibility. To evaluate the proposed SE framework a first of its kind AV
binaural speech corpus, called ASPIRE, is recorded in real noisy environments
including cafeteria and restaurant. We demonstrate superior performance of our
approach in terms of objective measures and subjective listening tests over the
state-of-the-art SE approaches as well as recent DNN based SE models. In
addition, our work challenges a popular belief that a scarcity of
multi-language large vocabulary AV corpus and wide variety of noises is a major
bottleneck to build a robust language, speaker and noise independent SE
systems. We show that a model trained on synthetic mixture of Grid corpus (with
33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of
only bus, pedestrian, cafeteria, and street noises) generalise well not only on
large vocabulary corpora but also on completely unrelated languages (such as
Mandarin), wide variety of speakers and noises.",scarcity cues
http://arxiv.org/abs/1808.03926v1,"We take a practical approach to solving sequence labeling problem assuming
unavailability of domain expertise and scarcity of informational and
computational resources. To this end, we utilize a universal end-to-end
Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP
tasks and languages. The model combines morphological, semantic, and structural
cues extracted from data to arrive at informed predictions. The model's
performance is evaluated on eight benchmark datasets (covering three tasks:
POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and
Spanish). We observe state-of-the-art results on four of them: CoNLL-2012
(English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus
(German POS-tagging), and competitive performance on the rest.",scarcity cues
http://arxiv.org/abs/1812.04429v1,"Automated deception detection (ADD) from real-life videos is a challenging
task. It specifically needs to address two problems: (1) Both face and body
contain useful cues regarding whether a subject is deceptive. How to
effectively fuse the two is thus key to the effectiveness of an ADD model. (2)
Real-life deceptive samples are hard to collect; learning with limited training
data thus challenges most deep learning based ADD models. In this work, both
problems are addressed. Specifically, for face-body multimodal learning, a
novel face-focused cross-stream network (FFCSN) is proposed. It differs
significantly from the popular two-stream networks in that: (a) face detection
is added into the spatial stream to capture the facial expressions explicitly,
and (b) correlation learning is performed across the spatial and temporal
streams for joint deep feature learning across both face and body. To address
the training data scarcity problem, our FFCSN model is trained with both meta
learning and adversarial learning. Extensive experiments show that our FFCSN
model achieves state-of-the-art results. Further, the proposed FFCSN model as
well as its robust training strategy are shown to be generally applicable to
other human-centric video analysis tasks such as emotion recognition from
user-generated videos.",scarcity cues
http://arxiv.org/abs/1902.07429v1,"Practically, we are often in the dilemma that the labeled data at hand are
inadequate to train a reliable classifier, and more seriously, some of these
labeled data may be mistakenly labeled due to the various human factors.
Therefore, this paper proposes a novel semi-supervised learning paradigm that
can handle both label insufficiency and label inaccuracy. To address label
insufficiency, we use a graph to bridge the data points so that the label
information can be propagated from the scarce labeled examples to unlabeled
examples along the graph edges. To address label inaccuracy, Graph Trend
Filtering (GTF) and Smooth Eigenbase Pursuit (SEP) are adopted to filter out
the initial noisy labels. GTF penalizes the l_0 norm of label difference
between connected examples in the graph and exhibits better local adaptivity
than the traditional l_2 norm-based Laplacian smoother. SEP reconstructs the
correct labels by emphasizing the leading eigenvectors of Laplacian matrix
associated with small eigenvalues, as these eigenvectors reflect real label
smoothness and carry rich class separation cues. We term our algorithm as
`Semi-supervised learning under Inadequate and Incorrect Supervision' (SIIS).
Thorough experimental results on image classification, text categorization, and
speech recognition demonstrate that our SIIS is effective in label error
correction, leading to superior performance to the state-of-the-art methods in
the presence of label noise and label scarcity.",scarcity cues
http://arxiv.org/abs/1904.05822v3,"Deep learning techniques have enabled rapid progress in monocular depth
estimation, but their quality is limited by the ill-posed nature of the problem
and the scarcity of high quality datasets. We estimate depth from a single
camera by leveraging the dual-pixel auto-focus hardware that is increasingly
common on modern camera sensors. Classic stereo algorithms and prior
learning-based depth estimation techniques under-perform when applied on this
dual-pixel data, the former due to too-strong assumptions about RGB image
matching, and the latter due to not leveraging the understanding of optics of
dual-pixel image formation. To allow learning based methods to work well on
dual-pixel imagery, we identify an inherent ambiguity in the depth estimated
from dual-pixel cues, and develop an approach to estimate depth up to this
ambiguity. Using our approach, existing monocular depth estimation techniques
can be effectively applied to dual-pixel data, and much smaller models can be
constructed that still infer high quality depth. To demonstrate this, we
capture a large dataset of in-the-wild 5-viewpoint RGB images paired with
corresponding dual-pixel data, and show how view supervision with this data can
be used to learn depth up to the unknown ambiguities. On our new task, our
model is 30% more accurate than any prior work on learning-based monocular or
stereoscopic depth estimation.",scarcity cues
http://arxiv.org/abs/1612.03284v1,"In this paper, we establish a novel bottom-up cue named Convex Hull Overlap
(CHO), and then propose an effective approach to detect salient regions using
the combination of the CHO cue and global contrast cue. Our scheme
significantly differs from other earlier work in: 1) The hierarchical
segmentation model based on Normalized Graph-Cut fits the splitting and merging
processes in human visual perception; 2) Previous work only focuses on color
and texture cues, while our CHO cue makes up the obvious gap between the
spatial region covering and the region saliency. CHO is a kind of improved and
enhanced Gestalt cue, while other popular figure-ground cues such as convexity
and surroundedness can be regarded as the special cases of CHO. Our experiments
on a large number of public data have obtained very positive results.",scarcity cues
http://arxiv.org/abs/1901.06129v1,"In this paper, we propose a unified Multi-Object Tracking (MOT) framework
learning to make full use of long term and short term cues for handling complex
cases in MOT scenes. Besides, for better association, we propose switcher-aware
classification (SAC), which takes the potential identity-switch causer
(switcher) into consideration. Specifically, the proposed framework includes a
Single Object Tracking (SOT) sub-net to capture short term cues, a
re-identification (ReID) sub-net to extract long term cues and a switcher-aware
classifier to make matching decisions using extracted features from the main
target and the switcher. Short term cues help to find false negatives, while
long term cues avoid critical mistakes when occlusion happens, and the SAC
learns to combine multiple cues in an effective way and improves robustness.
The method is evaluated on the challenging MOT benchmarks and achieves the
state-of-the-art results.",scarcity cues
http://arxiv.org/abs/1407.8004v2,"Computer users are generally authenticated by means of a password.
Unfortunately passwords are often forgotten and replacement is expensive and
inconvenient. Some people write their passwords down but these records can
easily be lost or stolen. The option we explore is to find a way to cue
passwords securely. The specific cueing technique we report on in this paper
employs images as cues. The idea is to elicit textual descriptions of the
images, which can then be used as passwords. We have defined a set of metrics
for the kind of image that could function effectively as a password cue. We
identified five candidate image types and ran an experiment to identify the
image class with the best performance in terms of the defined metrics.
  The first experiment identified inkblot-type images as being superior. We
tested this image, called a cueblot, in a real-life environment. We allowed
users to tailor their cueblot until they felt they could describe it, and they
then entered a description of the cueblot as their password. The cueblot was
displayed at each subsequent authentication attempt to cue the password.
Unfortunately, we found that users did not exploit the cueing potential of the
cueblot, and while there were a few differences between textual descriptions of
cueblots and non-cued passwords, they were not compelling. Hence our attempts
to alleviate the difficulties people experience with passwords, by giving them
access to a tailored cue, did not have the desired effect. We have to conclude
that the password mechanism might well be unable to benefit from bolstering
activities such as this one.",scarcity cues
http://arxiv.org/abs/1903.03150v1,"Hand-held haptic devices can allow for greater freedom of motion and larger
workspaces than traditional grounded haptic devices. They can also provide more
compelling haptic sensations to the users' fingertips than many wearable haptic
devices because reaction forces can be distributed over a larger area of skin
far away from the stimulation site. This paper presents a hand-held kinesthetic
gripper that provides guidance cues in four degrees of freedom (DOF). 2-DOF
tangential forces on the thumb and index finger combine to create cues to
translate or rotate the hand. We demonstrate the device's capabilities in a
three-part user study. First, users moved their hands in response to haptic
cues before receiving instruction or training. Then, they trained on cues in
eight directions in a forced-choice task. Finally, they repeated the first
part, now knowing what each cue intended to convey. Users were able to
discriminate each cue over 90% of the time. Users moved correctly in response
to the guidance cues both before and after the training and indicated that the
cues were easy to follow. The results show promise for holdable kinesthetic
devices in haptic feedback and guidance for applications such as virtual
reality, medical training, and teleoperation.",scarcity cues
http://arxiv.org/abs/1706.09760v1,"Speaker recognition performance in emotional talking environments is not as
high as it is in neutral talking environments. This work focuses on proposing,
implementing, and evaluating a new approach to enhance the performance in
emotional talking environments. The new proposed approach is based on
identifying the unknown speaker using both his/her gender and emotion cues.
Both Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models
(SPHMMs) have been used as classifiers in this work. This approach has been
tested on our collected emotional speech database which is composed of six
emotions. The results of this work show that speaker identification performance
based on using both gender and emotion cues is higher than that based on using
gender cues only, emotion cues only, and neither gender nor emotion cues by
7.22%, 4.45%, and 19.56%, respectively. This work also shows that the optimum
speaker identification performance takes place when the classifiers are
completely biased towards suprasegmental models and no impact of acoustic
models in the emotional talking environments. The achieved average speaker
identification performance based on the new proposed approach falls within
2.35% of that obtained in subjective evaluation by human judges.",scarcity cues
http://arxiv.org/abs/1709.08126v1,"Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.",scarcity cues
http://arxiv.org/abs/1804.00155v1,"This work is dedicated to introducing, executing, and assessing a three-stage
speaker verification framework to enhance the degraded speaker verification
performance in emotional talking environments. Our framework is comprised of
three cascaded stages: gender identification stage followed by an emotion
identification stage followed by a speaker verification stage. The proposed
framework has been assessed on two distinct and independent emotional speech
datasets: our collected dataset and Emotional Prosody Speech and Transcripts
dataset. Our results demonstrate that speaker verification based on both gender
cues and emotion cues is superior to each of speaker verification based on
gender cues only, emotion cues only, and neither gender cues nor emotion cues.
The achieved average speaker verification performance based on the suggested
methodology is very similar to that attained in subjective assessment by human
listeners.",scarcity cues
http://arxiv.org/abs/1908.03000v1,"Artificial neural networks (ANNs) have become an important tool for image
classification with many applications in research and industry. However, it
remains largely unknown how relevant image features are selected and how data
properties affect this process. In particular, we are interested whether the
abstraction level of image cues correlating with class membership influences
feature selection. We perform experiments with binary images that contain a
combination of cues, representing two different levels of abstractions: one is
a pattern drawn from a random distribution where class membership correlates
with the statistics of the pattern, the other a combination of symbol-like
entities, where the symbolic code correlates with class membership. When the
network is trained with data in which both cues are equally significant, we
observe that the cues at the lower abstraction level, i.e., the pattern, is
learned, while the symbolic information is largely ignored, even in networks
with many layers. Symbol-like entities are only learned if the importance of
low-level cues is reduced compared to the high-level ones. These findings raise
important questions about the relevance of features that are learned by deep
ANNs and how learning could be shifted towards symbolic features.",scarcity cues
http://arxiv.org/abs/1710.08327v1,"Scientific knowledge is constantly subject to a variety of changes due to new
discoveries, alternative interpretations, and fresh perspectives. Understanding
uncertainties associated with various stages of scientific inquiries is an
integral part of scientists' domain expertise and it serves as the core of
their meta-knowledge of science. Despite the growing interest in areas such as
computational linguistics, systematically characterizing and tracking the
epistemic status of scientific claims and their evolution in scientific
disciplines remains a challenge. We present a unifying framework for the study
of uncertainties explicitly and implicitly conveyed in scientific publications.
The framework aims to accommodate a wide range of uncertain types, from
speculations to inconsistencies and controversies. We introduce a scalable and
adaptive method to recognize semantically equivalent cues of uncertainty across
different fields of research and accommodate individual analysts' unique
perspectives. We demonstrate how the new method can be used to expand a small
seed list of uncertainty cue words and how the validity of the expanded
candidate cue words are verified. We visualize the mixture of the original and
expanded uncertainty cue words to reveal the diversity of expressions of
uncertainty. These cue words offer a novel resource for the study of
uncertainty in scientific assertions.",scarcity cues
http://arxiv.org/abs/1404.3002v2,"The present study was aimed to create new methods for extraction and analysis
of land elevation contour lines, automatic extraction of water bodies (river
basins and lakes), from the digital elevation models (DEM) of a test area. And
extraction of villages which are fell under critical water scarcity regions for
agriculture and drinking water with respect to their elevation data and
available natural water resources.",scarcity cues
http://arxiv.org/abs/cs/9609102v1,"Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure, e.g., for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models, but also for improving upon previous
results. When compared to manually derived classification models already in the
literature, the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition, the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally, the ease of retraining makes the learning approach more scalable and
flexible than manual methods.",scarcity cues
http://arxiv.org/abs/1006.1475v2,"A quantitative condition is derived to evaluate the monocular accommodation
in holographic stereograms. We find that the reconstruction can be viewed as
true-3D image when the whole scene is located in the monocular cues area, with
compatible monocular cues and binocular cues. In contrast, it reveals incorrect
monocular cues in the visible multi-imaging area and the lacking information
area. To demonstrate our theoretical predictions, a pupil-function integral
imaging algorithm is developed to simulate the mono-eye observation, and a
holographic printing system is set up to fabricate the full-parallax
holographic stereogram. Both simulation and experimental results match our
theoretical predictions.",scarcity cues
http://arxiv.org/abs/1307.0902v3,"While introductory electricity and magnetism (E&M) has been investigated for
decades, research at the upper-division is relatively new. The University of
Colorado has developed the Colorado Upper-Division Electrostatics (CUE)
Diagnostic to test students' understanding of the content of the first semester
of an upper-division E&M course. While the questions on the CUE cover many
learning goals in an appropriate manner, we believe the rubric for the CUE is
particularly aligned to the topics and methods of teaching at the University of
Colorado. We suggest that changes to the rubric would allow for better
assessment of a wider range of teaching schemes. As an example, we highlight
one problem from the CUE involving the superposition principle. Using data from
both Oregon State University and the University of Colorado, we discuss the
limitations of the current rubric, compare results using a different analysis
scheme, and discuss the implications for assessing students' understanding.",scarcity cues
http://arxiv.org/abs/1511.06654v2,"In this paper, we present a novel method based on online target-specific
metric learning and coherent dynamics estimation for tracklet (track fragment)
association by network flow optimization in long-term multi-person tracking.
Our proposed framework aims to exploit appearance and motion cues to prevent
identity switches during tracking and to recover missed detections.
Furthermore, target-specific metrics (appearance cue) and motion dynamics
(motion cue) are proposed to be learned and estimated online, i.e. during the
tracking process. Our approach is effective even when such cues fail to
identify or follow the target due to occlusions or object-to-object
interactions. We also propose to learn the weights of these two tracking cues
to handle the difficult situations, such as severe occlusions and
object-to-object interactions effectively. Our method has been validated on
several public datasets and the experimental results show that it outperforms
several state-of-the-art tracking methods.",scarcity cues
http://arxiv.org/abs/1407.8007v1,"21st Century citizens are faced with the need to remember numbers of PINs
(Personal Identification Numbers) in order to do their daily business, and they
often have difficulties due to human memory limitations. One way of helping
them could be by providing cues during the PIN entry process. The provision of
cues that would only be helpful to the PIN owner is challenging because the cue
should only make sense to the legitimate user, and not to a random observer. In
this paper we report on an empirical study where we added colour to the PINpad
to provide an implicit memory cue to PINpad users. We compared the impact of
colour PINpads as opposed to grey ones. As expected, the ability to recall a
PIN deteriorated significantly over time irrespective of the type of PINpad
used. However, there was ultimately no improvement in the ability to recall
PINs when using colour PINpads.",scarcity cues
http://arxiv.org/abs/1606.08955v1,"The massive growth of sports videos has resulted in a need for automatic
generation of sports highlights that are comparable in quality to the
hand-edited highlights produced by broadcasters such as ESPN. Unlike previous
works that mostly use audio-visual cues derived from the video, we propose an
approach that additionally leverages contextual cues derived from the
environment that the game is being played in. The contextual cues provide
information about the excitement levels in the game, which can be ranked and
selected to automatically produce high-quality basketball highlights. We
introduce a new dataset of 25 NCAA games along with their play-by-play stats
and the ground-truth excitement data for each basket. We explore the
informativeness of five different cues derived from the video and from the
environment through user studies. Our experiments show that for our study
participants, the highlights produced by our system are comparable to the ones
produced by ESPN for the same games.",scarcity cues
http://arxiv.org/abs/1609.03213v1,"In this paper we propose a new binaural beamforming technique which can be
seen as a relaxation of the linearly constrained minimum variance (LCMV)
framework. The proposed method can achieve simultaneous noise reduction and
exact binaural cue preservation of the target source, similar to the binaural
minimum variance distortionless response (BMVDR) method. However, unlike BMVDR,
the proposed method is also able to preserve the binaural cues of multiple
interferers to a certain predefined accuracy. Specifically, it is able to
control the trade-off between noise reduction and binaural cue preservation of
the interferers by using a separate trade-off parameter per interferer.
Moreover, we provide a robust way of selecting these trade-off parameters in
such a way that the preservation accuracy for the binaural cues of the
interferers is always better than the corresponding ones of the BMVDR. The
relaxation of the constraints in the proposed method achieves approximate
binaural cue preservation of more interferers than other previously presented
LCMV-based binaural beamforming methods that use strict equality constraints.",scarcity cues
http://arxiv.org/abs/1609.04356v1,"We present a novel approach to object classification and detection which
requires minimal supervision and which combines visual texture cues and shape
information learned from freely available unlabeled web search results. The
explosion of visual data on the web can potentially make visual examples of
almost any object easily accessible via web search. Previous unsupervised
methods have utilized either large scale sources of texture cues from the web,
or shape information from data such as crowdsourced CAD models. We propose a
two-stream deep learning framework that combines these cues, with one stream
learning visual texture cues from image search data, and the other stream
learning rich shape information from 3D CAD models. To perform classification
or detection for a novel image, the predictions of the two streams are combined
using a late fusion scheme. We present experiments and visualizations for both
tasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture and
shape provide complementary information in our model. Our method outperforms
previous web image based models, 3D CAD model based approaches, and weakly
supervised models.",scarcity cues
http://arxiv.org/abs/1509.00998v1,"Causal inference in cue combination is to decide whether the cues have a
single cause or multiple causes. Although the Bayesian causal inference model
explains the problem of causal inference in cue combination successfully, how
causal inference in cue combination could be implemented by neural circuits, is
unclear. The existing method based on calculating log posterior ratio with
variable elimination has the problem of being unrealistic and task-specific. In
this paper, we take advantages of the special structure of the Bayesian causal
inference model and propose a hierarchical inference algorithm based on
importance sampling. A simple neural circuit is designed to implement the
proposed inference algorithm. Theoretical analyses and experimental results
demonstrate that our algorithm converges to the accurate value as the sample
size goes to infinite. Moreover, the neural circuit we design can be easily
generalized to implement inference for other problems, such as the
multi-stimuli cause inference and the same-different judgment.",scarcity cues
http://arxiv.org/abs/1605.04717v1,"This paper examines the cues that typically differentiate phishing emails
from genuine emails. The research is conducted in two stages. In the first
stage, we identify the cues that actually differentiate between phishing and
genuine emails. These are the consistency and personalisation of the message,
the perceived legitimacy of links and sender, and the presence of spelling or
grammatical irregularities. In the second stage, we identify the cues that
participants use to differentiate between phishing and genuine emails. This
revealed that participants often use cues that are not good indicators of
whether an email is phishing or genuine. This includes the presence of legal
disclaimers, the quality of visual presentation, and the positive consequences
emphasised in the email. This study has implications for education and training
and provides a basis for the design and development of targeted and more
relevant training and risk communication strategies.",scarcity cues
http://arxiv.org/abs/1908.10049v1,"This paper proposes the Global-Local Temporal Representation (GLTR) to
exploit the multi-scale temporal cues in video sequences for video person
Re-Identification (ReID). GLTR is constructed by first modeling the short-term
temporal cues among adjacent frames, then capturing the long-term relations
among inconsecutive frames. Specifically, the short-term temporal cues are
modeled by parallel dilated convolutions with different temporal dilation rates
to represent the motion and appearance of pedestrian. The long-term relations
are captured by a temporal self-attention model to alleviate the occlusions and
noises in video sequences. The short and long-term temporal cues are aggregated
as the final GLTR by a simple single-stream CNN. GLTR shows substantial
superiority to existing features learned with body part cues or metric learning
on four widely-used video ReID datasets. For instance, it achieves Rank-1
Accuracy of 87.02% on MARS dataset without re-ranking, better than current
state-of-the art.",scarcity cues
http://arxiv.org/abs/1411.2649v3,"With the ongoing exhaustion of free address pools at the registries serving
the global demand for IPv4 address space, scarcity has become reality. Networks
in need of address space can no longer get more address allocations from their
respective registries.
  In this work we frame the fundamentals of the IPv4 address exhaustion
phenomena and connected issues. We elaborate on how the current ecosystem of
IPv4 address space has evolved since the standardization of IPv4, leading to
the rather complex and opaque scenario we face today. We outline the evolution
in address space management as well as address space use patterns, identifying
key factors of the scarcity issues. We characterize the possible solution space
to overcome these issues and open the perspective of address blocks as virtual
resources, which involves issues such as differentiation between address
blocks, the need for resource certification, and issues arising when
transferring address space between networks.",scarcity cues
http://arxiv.org/abs/1801.05605v2,"To create a new IR test collection at minimal cost, we must carefully select
which documents merit human relevance judgments. Shared task campaigns such as
NIST TREC determine this by pooling search results from many participating
systems (and often interactive runs as well), thereby identifying the most
likely relevant documents in a given collection. While effective, it would be
preferable to be able to build a new test collection without needing to run an
entire shared task. Toward this end, we investigate multiple active learning
(AL) strategies which, without reliance on system rankings: 1) select which
documents human assessors should judge; and 2) automatically classify the
relevance of remaining unjudged documents. Because scarcity of relevant
documents tends to yield highly imbalanced training data for model estimation,
we investigate sampling strategies to mitigate class imbalance. We report
experiments on four TREC collections with varying scarcity of relevant
documents, reporting labeling accuracy achieved, as well as rank correlation
when evaluating participant systems using these labels vs. NIST judgments.
Results demonstrate the effectiveness of our approach, coupled with further
analysis showing how varying relevance scarcity, within and across collections,
impacts findings.",scarcity cues
http://arxiv.org/abs/1408.3596v1,"In this paper we consider multitarget tracking with multiple sensors for BMD.
In a previous paper multitarget tracking with a single sensor was considered
[8]. A ballistic missile may be in several pieces, presenting multiple targets.
Besides the ground based or ship sensor there is also the missile seeker. We
consider algorithms for generating and maintaining the tracks needed for BMD. A
cue of a BM from a non-organic tracking system may also be received. We
consider whether the cue is already in the local track file or is a new track.
The cue information can improve the existing local track.",scarcity cues
http://arxiv.org/abs/1904.04388v1,"Disfluencies in spontaneous speech are known to be associated with prosodic
disruptions. However, most algorithms for disfluency detection use only word
transcripts. Integrating prosodic cues has proved difficult because of the many
sources of variability affecting the acoustic correlates. This paper introduces
a new approach to extracting acoustic-prosodic cues using text-based
distributional prediction of acoustic cues to derive vector z-score features
(innovations). We explore both early and late fusion techniques for integrating
text and prosody, showing gains over a high-accuracy text-only model.",scarcity cues
http://arxiv.org/abs/1402.5443v1,"Are users who comment on a variety of matters more likely to achieve high
influence than those who delve into one focused field? Do general Twitter
hashtags, such as #lol, tend to be more popular than novel ones, such as
#instantlyinlove? Questions like these demand a way to detect topics hidden
behind messages associated with an individual or a hashtag, and a gauge of
similarity among these topics. Here we develop such an approach to identify
clusters of similar hashtags by detecting communities in the hashtag
co-occurrence network. Then the topical diversity of a user's interests is
quantified by the entropy of her hashtags across different topic clusters. A
similar measure is applied to hashtags, based on co-occurring tags. We find
that high topical diversity of early adopters or co-occurring tags implies high
future popularity of hashtags. In contrast, low diversity helps an individual
accumulate social influence. In short, diverse messages and focused messengers
are more likely to gain impact.",detect high demand message
http://arxiv.org/abs/1708.09492v1,"Commit messages are a valuable resource in comprehension of software
evolution, since they provide a record of changes such as feature additions and
bug repairs. Unfortunately, programmers often neglect to write good commit
messages. Different techniques have been proposed to help programmers by
automatically writing these messages. These techniques are effective at
describing what changed, but are often verbose and lack context for
understanding the rationale behind a change. In contrast, humans write messages
that are short and summarize the high level rationale. In this paper, we adapt
Neural Machine Translation (NMT) to automatically ""translate"" diffs into commit
messages. We trained an NMT algorithm using a corpus of diffs and human-written
commit messages from the top 1k Github projects. We designed a filter to help
ensure that we only trained the algorithm on higher-quality commit messages.
Our evaluation uncovered a pattern in which the messages we generate tend to be
either very high or very low quality. Therefore, we created a quality-assurance
filter to detect cases in which we are unable to produce good messages, and
return a warning instead.",detect high demand message
http://arxiv.org/abs/1609.08141v1,"In this paper, we propose an ad-hoc on-demand distance vector routing
algorithm for mobile ad-hoc networks taking into account node mobility.
Changeable topology of such mobile ad-hoc networks provokes overhead messages
in order to search available routes and maintain found routes. The overhead
messages impede data delivery from sources to destination and deteriorate
network performance. To overcome such a challenge, our proposed algorithm
estimates link duration based neighboring node mobility and chooses the most
reliable route. The proposed algorithm also applies the estimate for route
maintenance to lessen the number of overhead messages. Via simulations, the
proposed algorithm is verified in various mobile environments. In the low
mobility environment, by reducing route maintenance messages, the proposed
algorithm significantly improves network performance such as packet data rate
and end-to-end delay. In the high mobility environment, the reduction of route
discovery message enhances network performance since the proposed algorithm
provides more reliable routes.",detect high demand message
http://arxiv.org/abs/1704.08500v1,"Chat messages of development teams play an increasingly significant role in
software development, having replaced emails in some cases. Chat messages
contain information about discussed issues, considered alternatives and
argumentation leading to the decisions made during software development. These
elements, defined as rationale, are invaluable during software evolution for
documenting and reusing development knowledge. Rationale is also essential for
coping with changes and for effective maintenance of the software system.
However, exploiting the rationale hidden in the chat messages is challenging
due to the high volume of unstructured messages covering a wide range of
topics. This work presents the results of an exploratory study examining the
frequency of rationale in chat messages, the completeness of the available
rationale and the potential of automatic techniques for rationale extraction.
For this purpose, we apply content analysis and machine learning techniques on
more than 8,700 chat messages from three software development projects. Our
results show that chat messages are a rich source of rationale and that machine
learning is a promising technique for detecting rationale and identifying
different rationale elements.",detect high demand message
http://arxiv.org/abs/cs/0703094v1,"Geographic routing is becoming the protocol of choice for many sensor network
applications. The current state of the art is unsatisfactory: some algorithms
are very efficient, however they require a preliminary planarization of the
communication graph. Planarization induces overhead and is not realistic in
many scenarios. On the otherhand, georouting algorithms which do not rely on
planarization have fairly low success rates and either fail to route messages
around all but the simplest obstacles or have a high topology control overhead
(e.g. contour detection algorithms). To overcome these limitations, we propose
GRIC, the first lightweight and efficient on demand (i.e. all-to-all)
geographic routing algorithm which does not require planarization and has
almost 100% delivery rates (when no obstacles are added). Furthermore, the
excellent behavior of our algorithm is maintained even in the presence of large
convex obstacles. The case of hard concave obstacles is also studied; such
obstacles are hard instances for which performance diminishes.",detect high demand message
http://arxiv.org/abs/1702.08053v1,"Tremendous growing demand for high data rate services such as video, gaming
and social networking in wireless cellular systems, attracted researchers'
attention to focus on developing proximity services. In this regard,
device-to-device (D2D) communications as a promising technology for future
cellular systems, plays crucial rule. The key factor in D2D communication is
providing efficient peer discovery mechanisms in ultra dense networks. In this
paper, we propose a centralized D2D discovery scheme by employing a signaling
algorithm to exchange D2D discovery messages between network entities. In this
system, potential D2D pairs share uplink cellular users' resources with
collision detection, to initiate a D2D links. Stochastic geometry is used to
analyze system performance in terms of success probability of the transmitted
signal and minimum required time slots for the proposed discovery scheme.
Extensive simulations are used to evaluate the proposed system performance.",detect high demand message
http://arxiv.org/abs/1809.08116v2,"The problem of two-sender unicast index coding consists of two senders and a
set of receivers. Each receiver demands a unique message and possesses some of
the messages demanded by other receivers as its side-information. Every
demanded message is present with at least one of the senders. Senders avail the
knowledge of the side-information at the receivers to reduce the number of
broadcast transmissions. Solution to this problem consists of finding the
optimal number of coded transmissions from the two senders. One important class
of the two-sender problem consists of the messages at the senders and the
side-information at the receivers satisfying \emph{fully-participated
interactions}. This paper provides the optimal broadcast rates, for all the
unsolved cases of the two-sender problem with fully-participated interactions
when the associated \emph{interaction digraphs} contain cycles. The optimal
broadcast rates are provided in terms of those of the three independent
single-sender problems associated with the two-sender problem. This paper also
provides an achievable broadcast rate with $t$-bit messages for any finite $t$
and any two-sender problem with fully-participated interactions belonging to
$(i)$ any one of the six instances (classes) of the two-sender problem when the
associated interaction digraph does not contain any cycle, and $(ii)$ one of
the classes of the two-sender problem when the associated interaction digraph
contains cycles. The achievable broadcast rates are obtained by exploiting the
symmetries of the confusion graph to color the same according to the two-sender
graph coloring.",detect high demand message
http://arxiv.org/abs/1908.11151v1,"Cooperative or collective perception (or sensing) enables connected and
automated vehicles to exchange sensor information to improve their perception
of the driving environment. Standards are currently being developed by ETSI to
define collective perception message formats and generation rules. These
generation rules establish when collective perception messages should be
generated and transmitted. This study shows that current collective perception
message generation rules generate a high number of messages with information
about a small number of detected vehicles. This results in an inefficient
utilization of the communication channel that reduces the effectiveness of
collective perception. This study proposes a novel algorithm that modifies how
the information of detected vehicles is organized in collective perception
messages. The proposed algorithm improves the V2X (Vehicle to Everything)
reliability and the perception compared to current ETSI solutions for
collective perception or cooperative sensing.",detect high demand message
http://arxiv.org/abs/1109.5636v2,"We investigate the problem of distributed sensors' failure detection in
networks with a small number of defective sensors, whose measurements differ
significantly from neighboring sensor measurements. Defective sensors are
represented by non-zero values in binary sparse signals. We build on the sparse
nature of the binary sensor failure signals and propose a new distributed
detection algorithm based on Group Testing (GT). The distributed GT algorithm
estimates the set of defective sensors from a small number of linearly
independent binary messages exchanged by the sensors. The distributed GT
algorithm uses a low complexity distance decoder that is robust to noisy
messages. We first consider networks with only one defective sensor and
determine the minimal number of linearly independent messages needed for
detection of the defective sensor with high probability. We then extend our
study to the detection of multiple defective sensors by modifying appropriately
the message exchange protocol and the decoding procedure. We show through
experimentation that, for small and medium sized networks, the number of
messages required for successful detection is actually smaller than the minimal
number computed in the analysis. Simulations demonstrate that the proposed
method outperforms methods based on random walk measurements collection in
terms of detection performance and convergence rate. Finally, the proposed
method is resilient to network dynamics due to the effective gossip-based
message dissemination protocol.",detect high demand message
http://arxiv.org/abs/1302.4882v1,"A mobile ad hoc network (MANET) is a collection of autonomous nodes that
communicate with each other by forming a multi-hop radio network and
maintaining connections in a decentralized manner. Security remains a major
challenge for these networks due to their features of open medium, dynamically
changing topologies, reliance on cooperative algorithms, absence of centralized
monitoring points, and lack of clear lines of defense. Protecting the network
layer of a MANET from malicious attacks is an important and challenging
security issue, since most of the routing protocols for MANETs are vulnerable
to various types of attacks. Ad hoc on-demand distance vector routing (AODV) is
a very popular routing algorithm. However, it is vulnerable to the well-known
black hole attack, where a malicious node falsely advertises good paths to a
destination node during the route discovery process but drops all packets in
the data forwarding phase. This attack becomes more severe when a group of
malicious nodes cooperate each other. The proposed mechanism does not apply any
cryptographic primitives on the routing messages. Instead, it protects the
network by detecting and reacting to malicious activities of the nodes.
Simulation results show that the scheme has a significantly high detection rate
with moderate network traffic overhead and computation overhead in the nodes.",detect high demand message
http://arxiv.org/abs/1905.01137v1,"One of the applications of vehicular ad-hoc networks is warning message
dissemination among vehicles in dangerous situations to prevent more damage.
The only communication mechanism for message dissemination is multi-hop
broadcast; in which, forwarding a received message have to be regulated using a
scheme regarding the selection of forwarding nodes. When analyzing these
schemes, simulation-based frameworks fail to provide guaranteed analysis
results due to the high level of concurrency in this application. Therefore,
there is a need to use model checking approaches for achieving reliable
results. In this paper, we have developed a framework called VeriVANca, to
provide model checking facilities for the analysis of warning message
dissemination schemes in VANETs. To this end, an actor-based modeling language,
Rebeca, is used which is equipped with a variety of model checking engines. To
illustrate the applicability of VeriVANca, modeling and analysis of two warning
message dissemination schemes are presented. Some scenarios for these schemes
are presented to show that concurrent behaviors of the system components may
cause uncertainty in both behavior and performance which may not be detected by
simulation-based techniques. Furthermore, the scalability of VeriVANca is
examined by analyzing a middle-sized model.",detect high demand message
http://arxiv.org/abs/1509.07538v1,"Increased density of wireless devices, ever growing demands for extremely
high data rate, and spectrum scarcity at microwave bands make the millimeter
wave (mmWave) frequencies an important player in future wireless networks.
However, mmWave communication systems exhibit severe attenuation, blockage,
deafness, and may need microwave networks for coordination and fall-back
support. To compensate for high attenuation, mmWave systems exploit highly
directional operation, which in turn substantially reduces the interference
footprint. The significant differences between mmWave networks and legacy
communication technologies challenge the classical design approaches,
especially at the medium access control (MAC) layer, which has received
comparatively less attention than PHY and propagation issues in the literature
so far. In this paper, the MAC layer design aspects of short range mmWave
networks are discussed. In particular, we explain why current mmWave standards
fail to fully exploit the potential advantages of short range mmWave
technology, and argue for the necessity of new collision-aware hybrid resource
allocation frameworks with on-demand control messages, the advantages of a
collision notification message, and the potential of multihop communication to
provide reliable mmWave connections.",detect high demand message
http://arxiv.org/abs/1507.07267v3,"To meet the growing spectrum demands, future cellular systems are expected to
share the spectrum of other services such as radar. In this paper, we consider
a network multiple-input multiple-output (MIMO) with partial cooperation model
where radar stations cooperate with cellular base stations (BS)s to deliver
messages to intended mobile users. So the radar stations act as BSs in the
cellular system. However, due to the high power transmitted by radar stations
for detection of far targets, the cellular receivers could burnout when
receiving these high radar powers. Therefore, we propose a new projection
method called small singular values space projection (SSVSP) to mitigate these
harmful high power and enable radar stations to collaborate with cellular base
stations. In addition, we formulate the problem into a MIMO interference
channel with general constraints (MIMO-IFC-GC). Finally, we provide a solution
to minimize the weighted sum mean square error minimization problem (WSMMSE)
with enforcing power constraints on both radar and cellular stations.",detect high demand message
http://arxiv.org/abs/1007.4748v1,"We analyze over 500 million Twitter messages from an eight month period and
find that tracking a small number of flu-related keywords allows us to forecast
future influenza rates with high accuracy, obtaining a 95% correlation with
national health statistics. We then analyze the robustness of this approach to
spurious keyword matches, and we propose a document classification component to
filter these misleading messages. We find that this document classifier can
reduce error rates by over half in simulated false alarm experiments, though
more research is needed to develop methods that are robust in cases of
extremely high noise.",detect high demand message
http://arxiv.org/abs/1607.00490v1,"We consider the following \textit{network computation problem}. In an acyclic
network, there are multiple source nodes, each generating multiple messages,
and there are multiple sink nodes, each demanding a function of the source
messages. The network coding problem corresponds to the case in which every
demand function is equal to some source message, i.e., each sink demands some
source message. Connections between network coding problems and matroids have
been well studied. In this work, we establish a relation between network
computation problems and representable matroids. We show that a network
computation problem in which the sinks demand linear functions of source
messages admits a scalar linear solution if and only if it is matroidal with
respect to a representable matroid whose representation fulfills certain
constraints dictated by the network computation problem. Next, we obtain a
connection between network computation problems and functional dependency
relations (FD-relations) and show that FD-relations can be used to characterize
network computation problem with arbitrary (not necessarily linear) function
demands as well as nonlinear network codes.",detect high demand message
http://arxiv.org/abs/1511.09024v5,"Cloud Radio Access Network (C-RAN) is a promising architecture for
unprecedented capacity enhancement in next-generation wireless networks thanks
to the centralization and virtualization of base station processing. However,
centralized signal processing in C-RANs involves high computational complexity
that quickly becomes unaffordable when the network grows to a huge size. Among
the first, this paper endeavours to design a scalable uplink signal detection
algorithm, in the sense that both the complexity per unit network area and the
total computation time remain constant when the network size grows. To this
end, we formulate the signal detection in C-RAN as an inference problem over a
bipartite random geometric graph. By passing messages among neighboring nodes,
message passing (a.k.a. belief propagation) provides an efficient way to solve
the inference problem over a sparse graph. However, the traditional
message-passing algorithm is not guaranteed to converge, because the
corresponding bipartite random geometric graph is locally dense and contains
many short loops. As a major contribution of this paper, we propose a
randomized Gaussian message passing (RGMP) algorithm to improve the
convergence. Instead of exchanging messages simultaneously or in a fixed order,
we propose to exchange messages asynchronously in a random order. The proposed
RGMP algorithm demonstrates significantly better convergence performance than
conventional message passing. The randomness of the message update schedule
also simplifies the analysis, and allows the derivation of the convergence
conditions for the RGMP algorithm. In addition, we propose a blockwise RGMP
(B-RGMP) algorithm for practical implementation. The average computation time
of B-RGMP remains constant when the network size increases.",detect high demand message
http://arxiv.org/abs/1409.0289v2,"Fluorescent calcium imaging provides a potentially powerful tool for
inferring connectivity in neural circuits with up to thousands of neurons.
However, a key challenge in using calcium imaging for connectivity detection is
that current systems often have a temporal response and frame rate that can be
orders of magnitude slower than the underlying neural spiking process. Bayesian
inference methods based on expectation-maximization (EM) have been proposed to
overcome these limitations, but are often computationally demanding since the
E-step in the EM procedure typically involves state estimation for a
high-dimensional nonlinear dynamical system. In this work, we propose a
computationally fast method for the state estimation based on a hybrid of loopy
belief propagation and approximate message passing (AMP). The key insight is
that a neural system as viewed through calcium imaging can be factorized into
simple scalar dynamical systems for each neuron with linear interconnections
between the neurons. Using the structure, the updates in the proposed hybrid
AMP methodology can be computed by a set of one-dimensional state estimation
procedures and linear transforms with the connectivity matrix. This yields a
computationally scalable method for inferring connectivity of large neural
circuits. Simulations of the method on realistic neural networks demonstrate
good accuracy with computation times that are potentially significantly faster
than current approaches based on Markov Chain Monte Carlo methods.",detect high demand message
http://arxiv.org/abs/1903.12070v1,"This study investigates the potential effects of different Dynamic Message
Signs (DMSs) on driver behavior using a full-scale high-fidelity driving
simulator. Different DMSs are categorized by their content, structure, and type
of messages. A random forest algorithm is used for three separate behavioral
analyses; a route diversion analysis, a route choice analysis and a compliance
analysis; to identify the potential and relative influences of different DMSs
on these aspects of driver behavior. A total of 390 simulation runs are
conducted using a sample of 65 participants from diverse socioeconomic
backgrounds. Results obtained suggest that DMSs displaying lane closure and
delay information with advisory messages are most influential with regards to
diversion while color-coded DMSs and DMSs with avoid route advice are the top
contributors impacting route choice decisions and DMS compliance. In this
first-of-a-kind study, based on the responses to the pre and post simulation
surveys as well as results obtained from the analysis of
driving-simulation-session data, the authors found that color-blind-friendly,
color-coded DMSs are more effective than alphanumeric DMSs - especially in
scenarios that demand high compliance from drivers. The increased effectiveness
may be attributed to reduced comprehension time and ease with which such DMSs
are understood by a greater percentage of road users.",detect high demand message
http://arxiv.org/abs/1601.05590v1,"Inspired by the success of Google's Pregel, many systems have been developed
recently for iterative computation over big graphs. These systems provide a
user-friendly vertex-centric programming interface, where a programmer only
needs to specify the behavior of one generic vertex when developing a parallel
graph algorithm. However, most existing systems require the input graph to
reside in memories of the machines in a cluster, and the few out-of-core
systems suffer from problems such as poor efficiency for sparse computation
workload, high demand on network bandwidth, and expensive cost incurred by
external-memory join and group-by.
  In this paper, we introduce the GraphD system for a user to process very
large graphs with ordinary computing resources. GraphD fully overlaps
computation with communication, by streaming edges and messages on local disks,
while transmitting messages in parallel. For a broad class of Pregel algorithms
where message combiner is applicable, GraphD eliminates the need of any
expensive external-memory join or group-by. These key techniques allow GraphD
to achieve comparable performance to in-memory Pregel-like systems without
keeping edges and messages in memories. We prove that to process a graph G=(V,
E) with n machines using GraphD, each machine only requires O(|V|/n) memory
space, allowing GraphD to scale to very large graphs with a small cluster.
Extensive experiments show that GraphD beats existing out-of-core systems by
orders of magnitude, and achieves comparable performance to in-memory systems
running with enough memories.",detect high demand message
http://arxiv.org/abs/1906.11278v1,"In this paper, we consider the multi-server setting of Private Information
Retrieval with Private Coded Side Information (PIR-PCSI) problem. In this
problem, there is a database of $K$ messages whose copies are replicated across
$N$ servers, and there is a user who knows a random linear combination of a
random subset of $M$ messages in the database as side information. The user
wishes to download one message from the servers, while protecting the
identities of both the demand message and the messages forming the side
information. We assume that the servers know the number of messages forming the
user's side information in advance, whereas the indices of these messages and
their coefficients in the side information are not known to any of the servers
a priori.
  Our goal is to characterize (or derive a lower bound on) the capacity, i.e.,
the maximum achievable download rate, for the following two settings. In the
first setting, the set of messages forming the linear combination available to
the user as side information, does not include the user's demanded message. For
this setting, we show that the capacity is equal to
$\left(1+{1}/{N}+\dots+{1}/{N^{K-M-1}}\right)^{-1}$. In the second setting, the
demand message contributes to the linear combination available to the user as
side information, i.e., the demand message is one of the messages that form the
user's side information. For this setting, we show that the capacity is
lower-bounded by $\left(1+{1}/{N}+\dots+{1}/{N^{K-M}}\right)^{-1}$. The
proposed achievability schemes and proof techniques leverage ideas from both
our recent methods proposed for the single-server PIR-PCSI problem as well as
the techniques proposed by Sun and Jafar for multi-server private computation
problem.",detect high demand message
http://arxiv.org/abs/1808.05797v1,"We study the problem of single-server multi-message private information
retrieval with side information. One user wants to recover $N$ out of $K$
independent messages which are stored at a single server. The user initially
possesses a subset of $M$ messages as side information. The goal of the user is
to download the $N$ demand messages while not leaking any information about the
indices of these messages to the server. In this paper, we characterize the
minimum number of required transmissions. We also present the optimal linear
coding scheme which enables the user to download the demand messages and
preserves the privacy of their indices. Moreover, we show that the trivial MDS
coding scheme with $K-M$ transmissions is optimal if $N>M$ or $N^2+N \ge K-M$.
This means if one wishes to privately download more than the square-root of the
number of files in the database, then one must effectively download the full
database (minus the side information), irrespective of the amount of side
information one has available.",detect high demand message
http://arxiv.org/abs/1603.05365v1,"In contrast to the network coding problem wherein the sinks in a network
demand subsets of the source messages, in a network computation problem the
sinks demand functions of the source messages. Similarly, in the functional
index coding problem, the side information and demands of the clients include
disjoint sets of functions of the information messages held by the transmitter
instead of disjoint subsets of the messages, as is the case in the conventional
index coding problem. It is known that any network coding problem can be
transformed into an index coding problem and vice versa. In this work, we
establish a similar relationship between network computation problems and a
class of functional index coding problems, viz., those in which only the
demands of the clients include functions of messages. We show that any network
computation problem can be converted into a functional index coding problem
wherein some clients demand functions of messages and vice versa. We prove that
a solution for a network computation problem exists if and only if a functional
index code (of a specific length determined by the network computation problem)
for a suitably constructed functional index coding problem exists. And, that a
functional index coding problem admits a solution of a specified length if and
only if a suitably constructed network computation problem admits a solution.",detect high demand message
http://arxiv.org/abs/1901.09248v1,"We study the problem of single-server single-message Private Information
Retrieval with Private Coded Side Information (PIR-PCSI). In this problem,
there is a server that stores a database, and a user who knows a random linear
combination of a random subset of messages in the database. The number of
messages contributing to the user's side information is known to the server a
priori, whereas their indices and coefficients are unknown to the server a
priori. The user wants to retrieve a message from the server (with minimum
download cost), while protecting the identities of both the demand and side
information messages.
  Depending on whether the demand is part of the coded side information or not,
we consider two different models for the problem. For the model in which the
demand does not contribute to the side information, we prove a lower bound on
the minimum download cost for all (linear and non-linear) PIR protocols; and
for the other model wherein the demand is one of the messages contributing to
the side information, we prove a lower bound for all scalar-linear PIR
protocols. In addition, we propose novel PIR protocols that achieve these lower
bounds.",detect high demand message
http://arxiv.org/abs/1202.2319v2,"We study the detection performance of $M$-ary relay trees, where only the
leaves of the tree represent sensors making measurements. The root of the tree
represents the fusion center which makes an overall detection decision. Each of
the other nodes is a relay node which aggregates $M$ messages sent by its child
nodes into a new compressed message and sends the message to its parent node.
Building on previous work on the detection performance of $M$-ary relay trees
with binary messages, in this paper we study the case of non-binary relay
message alphabets. We characterize the exponent of the error probability with
respect to the message alphabet size $\mathcal D$, showing how the detection
performance increases with $\mathcal D$. Our method involves reducing a tree
with non-binary relay messages into an equivalent higher-degree tree with only
binary messages.",detect high demand message
http://arxiv.org/abs/1108.6121v1,"We consider the decentralized binary hypothesis testing problem in networks
with feedback, where some or all of the sensors have access to compressed
summaries of other sensors' observations. We study certain two-message feedback
architectures, in which every sensor sends two messages to a fusion center,
with the second message based on full or partial knowledge of the first
messages of the other sensors. We also study one-message feedback
architectures, in which each sensor sends one message to a fusion center, with
a group of sensors having full or partial knowledge of the messages from the
sensors not in that group. Under either a Neyman-Pearson or a Bayesian
formulation, we show that the asymptotically optimal (in the limit of a large
number of sensors) detection performance (as quantified by error exponents)
does not benefit from the feedback messages, if the fusion center remembers all
sensor messages. However, feedback can improve the Bayesian detection
performance in the one-message feedback architecture if the fusion center has
limited memory; for that case, we determine the corresponding optimal error
exponents.",detect high demand message
http://arxiv.org/abs/1112.4214v1,"Given the rapid increase in traffic, greater demands have been put on
research in high-speed switching systems. Such systems have to simultaneously
meet several constraints, e.g., high throughput, low delay and low complexity.
This makes it challenging to design an efficient scheduling algorithm, and has
consequently drawn considerable research interest. However, previous results
either cannot provide a 100% throughput guarantee without a speedup, or require
a complex centralized scheduler. In this paper, we design a distributed 100%
throughput algorithm for crosspoint buffered switches, called DISQUO, with very
limited message passing. We prove that DISQUO can achieve 100% throughput for
any admissible Bernoulli traffic, with a low time complexity of O(1) per port
and a few bits message exchanging in every time slot. To the best of our
knowledge, it is the first distributed algorithm that can provide a 100%
throughput for a crosspoint buffered switch.",detect high demand message
http://arxiv.org/abs/1808.10840v1,"Modern vehicles rely on scores of electronic control units (ECUs)
broadcasting messages over a few controller area networks (CANs). Bereft of
security features, in-vehicle CANs are exposed to cyber manipulation and
multiple researches have proved viable, life-threatening cyber attacks.
Complicating the issue, CAN messages lack a common mapping of functions to
commands, so packets are observable but not easily decipherable. We present a
transformational approach to CAN IDS that exploits the geometric properties of
CAN data to inform two novel detectors--one based on distance from a learned,
lower dimensional manifold and the other on discontinuities of the manifold
over time. Proof-of-concept tests are presented by implementing a potential
attack approach on a driving vehicle. The initial results suggest that (1) the
first detector requires additional refinement but does hold promise; (2) the
second detector gives a clear, strong indicator of the attack; and (3) the
algorithms keep pace with high-speed CAN messages. As our approach is
data-driven it provides a vehicle-agnostic IDS that eliminates the need to
reverse engineer CAN messages and can be ported to an after-market plugin.",detect high demand message
http://arxiv.org/abs/1402.4576v2,"For a network with one sender, $n$ receivers (users) and $m$ possible
messages (files), caching side information at the users allows to satisfy
arbitrary simultaneous demands by sending a common (multicast) coded message.
In the worst-case demand setting, explicit deterministic and random caching
strategies and explicit linear coding schemes have been shown to be order
optimal. In this work, we consider the same scenario where the user demands are
random i.i.d., according to a Zipf popularity distribution. In this case, we
pose the problem in terms of the minimum average number of equivalent message
transmissions. We present a novel decentralized random caching placement and a
coded delivery scheme which are shown to achieve order-optimal performance. As
a matter of fact, this is the first order-optimal result for the caching and
coded multicasting problem in the case of random demands.",detect high demand message
http://arxiv.org/abs/1602.02148v1,"Hash-based message authentication codes are an extremely simple yet hugely
effective construction for producing keyed message digests using shared
secrets. HMACs have seen widespread use as ad-hoc digital signatures in many
Internet applications. While messages signed with an HMAC are secure against
sender impersonation and tampering in transit, if used alone they are
susceptible to replay attacks. We propose a construction that extends HMACs to
produce a keyed message digest that has a finite validity period. We then
propose a message signature scheme that uses this time-dependent MAC along with
an unique message identifier to calculate a set of authentication factors using
which a recipient can readily detect and ignore replayed messages, thus
providing perfect resistance against replay attacks. We further analyse
time-based message authentication codes and show that they provide stronger
security guarantees than plain HMACs, even when used independently of the
aforementioned replay attack resistant message signature scheme.",detect high demand message
http://arxiv.org/abs/1312.0994v1,"A wide array of dynamic bandwidth allocation (DBA) mechanisms have recently
been proposed for improving bandwidth utilization and reducing idle times and
packets delays in passive optical networks (PONs). The DBA evaluation studies
commonly assumed that the report message for communicating the bandwidth
demands of the distributed optical network units (ONUs) to the central optical
line terminal (OLT) is scheduled for the end of an ONU's upstream transmission,
after the ONU's payload data transmissions. In this article, we conduct a
detailed investigation of the impact of the report message scheduling (RMS),
either at the beginning (i.e., before the pay load data) or the end of an ONU
upstream transmission on PON performance. We analytically characterize the
reduction in channel idle time with reporting at the beginning of an upstream
transmission compared to reporting at the end. Our extensive simulation
experiments consider both the Ethernet Passive Optical Networking (EPON)
standard and the Gigabit PON (GPON) standard. We find that for DBAs with
offline sizing and scheduling of ONU upstream transmission grants at the end of
a polling cycle, which processes requests from all ONUs, reporting at the
beginning gives substantial reductions of mean packet delay at high loads. For
high-performing DBAs with online grant sizing and scheduling, which immediately
processes individual ONU requests, or interleaving of ONUs groups, both
reporting at the beginning or end give essentially the same average packet
delays.",detect high demand message
http://arxiv.org/abs/1807.02184v1,"Datacenter applications demand both low latency and high throughput; while
interactive applications (e.g., Web Search) demand low tail latency for their
short messages due to their partition-aggregate software architecture, many
data-intensive applications (e.g., Map-Reduce) require high throughput for long
flows as they move vast amounts of data across the network. Recent proposals
improve latency of short flows and throughput of long flows by addressing the
shortcomings of existing packet scheduling and congestion control algorithms,
respectively. We make the key observation that long tails in the Flow
Completion Times (FCT) of short flows result from packets that suffer
congestion at more than one switch along their paths in the network. Our
proposal, Slytherin, specifically targets packets that suffered from congestion
at multiple points and prioritizes them in the network. Slytherin leverages ECN
mechanism which is widely used in existing datacenters to identify such tail
packets and dynamically prioritizes them using existing priority queues. As
compared to existing state-of-the-art packet scheduling proposals, Slytherin
achieves 18.6% lower 99th percentile flow completion times for short flows
without any loss of throughput. Further, Slytherin drastically reduces 99th
percentile queue length in switches by a factor of about 2x on average.",detect high demand message
http://arxiv.org/abs/1704.07014v1,"We study the problem of constructing good space-time codes for broadcasting
$K$ independent messages over a MIMO network to $L$ users, where each user
demands all the messages and already has a subset of messages as side
information. As a first attempt, we consider the $2\times 2$ case and propose
golden-coded index coding by partitioning the golden codes into $K$ subcodes,
one for each message. The proposed scheme is shown to have the property that
for any side information configuration, the minimum determinant of the code
increases exponentially with the amount of information contained in the side
information.",detect high demand message
http://arxiv.org/abs/1907.08167v1,"Smart reply systems have been developed for various messaging platforms. In
this paper, we introduce Uber's smart reply system: one-click-chat (OCC), which
is a key enhanced feature on top of the Uber in-app chat system. It enables
driver-partners to quickly respond to rider messages using smart replies. The
smart replies are dynamically selected according to conversation content using
machine learning algorithms. Our system consists of two major components:
intent detection and reply retrieval, which are very different from standard
smart reply systems where the task is to directly predict a reply. It is
designed specifically for mobile applications with short and non-canonical
messages. Reply retrieval utilizes pairings between intent and reply based on
their popularity in chat messages as derived from historical data. For intent
detection, a set of embedding and classification techniques are experimented
with, and we choose to deploy a solution using unsupervised distributed
embedding and nearest-neighbor classifier. It has the advantage of only
requiring a small amount of labeled training data, simplicity in developing and
deploying to production, and fast inference during serving and hence highly
scalable. At the same time, it performs comparably with deep learning
architectures such as word-level convolutional neural network. Overall, the
system achieves a high accuracy of 76% on intent detection. Currently, the
system is deployed in production for English-speaking countries and 71% of
in-app communications between riders and driver-partners adopted the smart
replies to speedup the communication process.",detect high demand message
http://arxiv.org/abs/1401.3842v1,"Call control features (e.g., call-divert, voice-mail) are primitive options
to which users can subscribe off-line to personalise their service. The
configuration of a feature subscription involves choosing and sequencing
features from a catalogue and is subject to constraints that prevent
undesirable feature interactions at run-time. When the subscription requested
by a user is inconsistent, one problem is to find an optimal relaxation, which
is a generalisation of the feedback vertex set problem on directed graphs, and
thus it is an NP-hard task. We present several constraint programming
formulations of the problem. We also present formulations using partial
weighted maximum Boolean satisfiability and mixed integer linear programming.
We study all these formulations by experimentally comparing them on a variety
of randomly generated instances of the feature subscription problem.",hard to cancel subscription
http://arxiv.org/abs/1611.08743v4,"Advertisements and subscriptions are tightly coupled to generate publication
routing paths in content--based publish/subscribe systems. Tight coupling
requires instantaneous updates in routing tables to generate alternative paths
which prevents offering scalable and robust dynamic routing in cyclic overlays
when link congestion is detected. We propose, OctopiA, first distributed
publish/subscribe system for content--based inter--cluster dynamic routing
using purpose--built structured cyclic overlays. OctopiA uses a novel concept
of subscription subgrouping, which divides subscriptions into disjoint sets
called subscription subgroups. The purpose--built structured cyclic overlay is
divided into identical clusters where subscriptions in each subgroup are
broadcast to an exclusive cluster. Our advertisement and subscription
forwarding algorithms use subscription subgrouping to eliminate tight coupling
to offer inter--cluster dynamic routing without requiring updates in routing
tables. Experiments on a cluster testbed with real world data show that OctopiA
reduces the number of saved advertisements in routing tables by 93%,
subscription broadcast delay by 33%, static and dynamic publication delivery
delays by 25% and 54%, respectively.",hard to cancel subscription
http://arxiv.org/abs/0705.3466v1,"Open Access to particle physics literature does not sound particularly new or
exciting, since particle physicists have been reading preprints for decades,
and arXiv.org for 15 years. However new movements in Europe are attempting to
make the peer-reviewed literature of the field fully Open Access. This is not a
new movement, nor is it restricted to this field. However, given the field's
history of preprints and eprints, it is well suited to a change to a fully Open
Access publishing model. Data shows that 90% of HEP published literature is
freely available online, meaning that HEP libraries have little need for
expensive journal subscriptions. As libraries begin to cancel journal
subscriptions, the peer review process will lose its primary source of funding.
Open Access publishing models can potentially address this issue. European
physicists and funding agencies are proposing a consortium, SCOAP3, that might
solve many of the objections to traditional Open Access publishing models in
Particle Physics. These proposed changes should be viewed as a starting point
for a serious look at the field's publication model, and are at least worthy of
attention, if not adoption.",hard to cancel subscription
http://arxiv.org/abs/1811.07088v2,"Although many scalable event matching algorithms have been proposed to
achieve scalability for large-scale content-based networks, content-based
publish/subscribe networks (especially for large-scale real time systems) still
suffer performance deterioration when subscription scale increases. While
subscription aggregation techniques can be useful to reduce the amount of
subscription dissemination traffic and the subscription table size by
exploiting the similarity among subscriptions, efficient subscription
aggregation is not a trivial task to accomplish. Previous research works have
proved that it is either a NP-Complete or a co-NP complete problem. In this
paper, we propose DLS (Discrete Label Set), a novel subscription representation
model, and design algorithms to achieve the mapping from traditional Boolean
predicate model to the DLS model. Based on the DLS model, we propose a
subscription aggregation algorithm with O(1) time complexity in most cases, and
an event matching algorithm with O(1) time complexity. The significant
performance improvement is at the cost of memory consumption and controllable
false positive rate. Our theoretical analysis shows that these algorithms are
inherently scalable and can achieve real time event matching in a large-scale
content-based publish/subscribe network. We discuss the tradeoff between
memory, false positive rate and partition granules of content space.
Experimental results show that proposed algorithms achieve expected
performance. With the increasing of computer memory capacity and the dropping
of memory price, more and more large-scale real time applications can benefit
from our proposed DLS model, such as stock quote distribution, earthquake
monitoring, and severe weather alert.",hard to cancel subscription
http://arxiv.org/abs/1307.2015v1,"We envision a publish/subscribe ontology system that is able to index
millions of user subscriptions and filter them against ontology data that
arrive in a streaming fashion. In this work, we propose a SPARQL extension
appropriate for a publish/subscribe setting; our extension builds on the
natural semantic graph matching of the language and supports the creation of
full-text subscriptions. Subsequently, we propose a main-memory subscription
indexing algorithm which performs both semantic and full-text matching at low
complexity and minimal filtering time. Thus, when ontology data are published
matching subscriptions are identified and notifications are forwarded to users.",hard to cancel subscription
http://arxiv.org/abs/1407.6968v1,"Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to
execute unmodified critical sections concurrently, even if they are protected
by the same lock. To ensure correctness, the transactions used to execute these
critical sections ""subscribe"" to the lock by reading it and checking that it is
available. A recent paper proposed using the tempting ""lazy subscription""
optimization for a similar technique in a different context, namely
transactional systems that use a single global lock (SGL) to protect all
transactional data. We identify several pitfalls that show that lazy
subscription \emph{is not safe} for TLE because unmodified critical sections
executing before subscribing to the lock may behave incorrectly in a number of
subtle ways. We also show that recently proposed compiler support for modifying
transaction code to ensure subscription occurs before any incorrect behavior
could manifest is not sufficient to avoid all of the pitfalls we identify. We
further argue that extending such compiler support to avoid all pitfalls would
add substantial complexity and would usually limit the extent to which
subscription can be deferred, undermining the effectiveness of the
optimization. Hardware extensions suggested in the recent proposal also do not
address all of the pitfalls we identify. In this extended version of our WTTM
2014 paper, we describe hardware extensions that make lazy subscription safe,
both for SGL-based transactional systems and for TLE, without the need for
special compiler support. We also explain how nontransactional loads can be
exploited, if available, to further enhance the effectiveness of lazy
subscription.",hard to cancel subscription
http://arxiv.org/abs/1512.06425v2,"Acyclic overlays used for broker-based publish/subscribe systems provide
unique paths for content-based routing from a publisher to interested
subscribers. Cyclic overlays may provide multiple paths, however, the
subscription broadcast process generates one content-based routing path per
subscription. This poses serious challenges in offering dynamic routing of
notifications when congestion is detected because instantaneous updates in
routing tables are required to generate alternative routing paths. This paper
introduces the first subscription-based publish/subscribe system, OctopiS,
which offers inter-cluster dynamic routing when congestion in the output queues
is detected. OctopiS is based on a formally defined Structured Cyclic Overlay
Topology (SCOT). SCOT is divided into homogeneous clusters where each cluster
has equal number of brokers and connects to other clusters through multiple
inter-cluster overlay links. These links are used to provide parallel routing
paths between publishers and subscribers connected to brokers in different
clusters. While aiming at deployment at data center networks, OctopiS generates
subscription-trees of shortest lengths used by Static Notification Routing
(SNR) algorithm. Dynamic Notification Routing (DNR) algorithm uses a bit-vector
mechanism to exploit the structuredness of a clustered SCOT to offer
inter-cluster dynamic routing without making updates in routing tables and
minimizing load on overwhelmed brokers and congested links. Experiments on a
cluster testbed with real world data show that OctopiS is scalable and reduces
the number of inter-broker messages in subscription delivery by 89%,
subscription delay by 77%, end-to-end notification delay in static and dynamic
routing by 47% and 58% respectively, and the lengths of output queues of
brokers in dynamic routing paths by 59%.",hard to cancel subscription
http://arxiv.org/abs/1706.10172v1,"In this paper we investigate the behavioural differences between mobile phone
customers with prepaid and postpaid subscriptions. Our study reveals that (a)
postpaid customers are more active in terms of service usage and (b) there are
strong structural correlations in the mobile phone call network as connections
between customers of the same subscription type are much more frequent than
those between customers of different subscription types. Based on these
observations we provide methods to detect the subscription type of customers by
using information about their personal call statistics, and also their
egocentric networks simultaneously. The key of our first approach is to cast
this classification problem as a problem of graph labelling, which can be
solved by max-flow min-cut algorithms. Our experiments show that, by using both
user attributes and relationships, the proposed graph labelling approach is
able to achieve a classification accuracy of $\sim 87\%$, which outperforms by
$\sim 7\%$ supervised learning methods using only user attributes. In our
second problem we aim to infer the subscription type of customers of external
operators. We propose via approximate methods to solve this problem by using
node attributes, and a two-ways indirect inference method based on observed
homophilic structural correlations. Our results have straightforward
applications in behavioural prediction and personal marketing.",hard to cancel subscription
http://arxiv.org/abs/1604.06853v1,"Cyclic or general overlays may provide multiple paths between publishers and
subscribers. However, an advertisement tree and a matching subscription
activates only one path for notifications routing in publish/subscribe systems.
This poses serious challenges in handling network conditions like congestion,
and link or broker failures. Further, content-based dynamic routing of
notifications requires instantaneous updates in routing paths, which is not a
scalable option. This paper introduces a clustering approach with a bit-vector
technique for inter-cluster dynamic routing of notifications in a structured
cyclic topology that provides multiple paths between publishers and interested
subscribers. The advertisement forwarding process exploits the structured
nature of the overlay topology to generate advertisement trees of length 1
without generating duplicate messages in the advertisement forwarding process.
Issued subscriptions are divided into multiple disjoint subgropus, where each
subscription is broadcast to a cluster, which is a limited part of the
structured cyclic overlay network. We implemented novel static and
intra-cluster dynamic routing algorithms in the proposed overlay topology for
our advertisement-based publish/subscribe system, called OctopiA. We also
performed a pragmatic comparison of our two algorithms with the
state-of-the-art. Experiments on a cluster testbed show that our approach
generates fewer inter-broker messages, and is scalable.",hard to cancel subscription
http://arxiv.org/abs/1810.00279v1,"Providing reliable and surreptitious communications is difficult in the
presence of adaptive and resourceful state level censors. In this paper we
introduce Tithonus, a framework that builds on the Bitcoin blockchain and
network to provide censorship-resistant communication mechanisms. In contrast
to previous approaches, we do not rely solely on the slow and expensive
blockchain consensus mechanism but instead fully exploit Bitcoin's peer-to-peer
gossip protocol. We develop adaptive, fast and cost effective data
communication solutions that camouflage client requests into inconspicuous
Bitcoin transactions. We propose solutions to securely request and transfer
content, with unobservability and censorship resistance, and free,
pay-per-access and subscription based payment options. When compared to
state-of-the-art Bitcoin writing solutions, Tithonus reduces the cost of
transferring data to censored clients by 2 orders of magnitude and increases
the goodput by 3 to 5 orders of magnitude. We show that Tithonus client
initiated transactions are hard to detect, while server initiated transactions
cannot be censored without creating split world problems to the Bitcoin
blockchain.",hard to cancel subscription
http://arxiv.org/abs/1611.03204v1,"With the prevalence of social media and GPS-enabled devices, a massive amount
of geo-textual data has been generated in a stream fashion, leading to a
variety of applications such as location-based recommendation and information
dissemination. In this paper, we investigate a novel real-time top-k monitoring
problem over sliding window of streaming data; that is, we continuously
maintain the top-k most relevant geo-textual messages (e.g., geo-tagged tweets)
for a large number of spatial-keyword subscriptions (e.g., registered users
interested in local events) simultaneously. To provide the most recent
information under controllable memory cost, sliding window model is employed on
the streaming geo-textual data. To the best of our knowledge, this is the first
work to study top-k spatial-keyword publish/subscribe over sliding window. A
novel centralized system, called Skype (Topk Spatial-keyword
Publish/Subscribe), is proposed in this paper. In Skype, to continuously
maintain top-k results for massive subscriptions, we devise a novel indexing
structure upon subscriptions such that each incoming message can be immediately
delivered on its arrival. To reduce the expensive top-k re-evaluation cost
triggered by message expiration, we develop a novel cost-based k-skyband
technique to reduce the number of re-evaluations in a cost-effective way.
Extensive experiments verify the great efficiency and effectiveness of our
proposed techniques. Furthermore, to support better scalability and higher
throughput, we propose a distributed version of Skype, namely, DSkype, on top
of Storm, which is a popular distributed stream processing system. With the
help of fine-tuned subscription/message distribution mechanisms, DSkype can
achieve orders of magnitude speed-up than its centralized version.",hard to cancel subscription
http://arxiv.org/abs/1802.05305v1,"Influence maximization which asks for $k$-size seed set from a social network
such that maximizing the influence over all other users (called influence
spread) has widely attracted attention due to its significant applications in
viral marketing and rumor control. In real world scenarios, people are
interested in the most influential users in particular topics, and want to
subscribe the topics-of-interests over social networks. In this paper, we
formulate the problem of influential users subscription on time-decaying social
stream, which asks for maintaining the $k$-size influential users sets for each
topic-aware subscription queries. We first analyze the widely adopted sliding
window model and propose a newly time-decaying influence model to overcome the
shortages when calculating the influence over social stream. Developed from
sieve based streaming algorithm, we propose an efficient algorithm to support
the calculation of time-decaying influence over dynamically updating social
networks. Using information among subscriptions, we then construct the Prefix
Tree Structure to allow us minimizing the times of calculating influence of
each update and easily maintained. Pruning techniques are also applied to the
Prefix Tree to optimize the performance of social stream update. Our approach
ensures a $\frac{1}{2}-\epsilon$ approximation ratio. Experimental results show
that our approach significantly outperforms the baseline approaches in
efficiency and result quality.",hard to cancel subscription
http://arxiv.org/abs/1402.2491v1,"Cloud computing allow the users to efficiently and dynamically provision
computing resource to meet their IT needs. Cloud Provider offers two
subscription plan to the customer namely reservation and on-demand. The
reservation plan is typically cheaper than on-demand plan. If the actual
computing demand is known in advance reserving the resource would be
straightforward. The challenge is how to make properly resource provisioning
and how the customers efficiently purchase the provisioning options under
reservation and on-demand. To address this issue, two-phase algorithm are
proposed to minimize service provision cost in both reservation and on-demand
plan. To reserve the correct and optimal amount of resources during
reservation, proposed a mathematical formulae in the first phase. To predict
resource demand, use kalman filter in the second phase. The evaluation result
shows that the two-phase algorithm can significantly reduce the provision cost
and the prediction is of reasonable accuracy.",hard to cancel subscription
http://arxiv.org/abs/1406.1818v1,"In this paper, we consider resource allocation optimization problem in
cellular networks for different types of users running multiple applications
simultaneously. In our proposed model, each user application is assigned a
utility function that represents the application type running on the user
equipment (UE). The network operators assign a subscription weight to each UE
based on its subscription. Each UE assigns an application weight to each of its
applications based on the instantaneous usage percentage of the application.
Additionally, UEs with higher priority assign applications target rates to
their applications. Our objective is to allocate the resources optimally among
the UEs and their applications from a single evolved node B (eNodeB) based on a
utility proportional fairness policy with priority to real-time application
users. A minimum quality of service (QoS) is guaranteed to each UE application
based on the UE subscription weight, the UE application weight and the UE
application target rate. We propose a two-stage rate allocation algorithm to
allocate the eNodeB resources among users and their applications. Finally, we
present simulation results for the performance of our rate allocation
algorithm.",hard to cancel subscription
http://arxiv.org/abs/1404.1782v2,"The goal of this paper is to provide an insight into the equilibrium of the
Internet market, when the current balance of the market is disrupted, and one
of the ISPs switches to a non-neutral regime. We consider a content provider
with a subscription revenue model and a continuum of end-users. The CP is also
non-neutral, in the sense that she can charge users of different ISPs different
subscription fees, and use this ""leverage"" to control the equilibrium outcome.
Results reveal that the CP is able to control the non-neutral ISP to some
extend. However, switching to a non-neutral regime by an ISP tips the balance
of the market in favor of this ISP.",hard to cancel subscription
http://arxiv.org/abs/1504.03719v3,"This paper lists some new directions for research related to the Algebra of
Communicating Processes (ACP). Most of these directions have been inspired by
work on SubScript, an ACP based extension to the programming language Scala.
SubScript applies several new ideas that build on ACP, but currently these lack
formal treatment. Some of these new ideas are rather fundamental. E.g. it
appears that the theory of ACP may well apply to structures of any kind of
items, rather than to just processes. The aim of this list is to raise
awareness of the research community about these new ideas; this could help both
the research area and the programming language SubScript.",hard to cancel subscription
http://arxiv.org/abs/1808.10569v3,"Recently, the mobile network operators (MNOs) are exploring more time
flexibility with the rollover data plan, which allows the unused data from the
previous month to be used in the current month. Motivated by this industry
trend, we propose a general framework for designing and optimizing the mobile
data plan with time flexibility. Such a framework includes the traditional data
plan, two existing rollover data plans, and a new credit data plan as special
cases. Under this framework, we formulate a monopoly MNO's optimal data plan
design as a three-stage Stackelberg game: In Stage I, the MNO decides the data
mechanism; In Stage II, the MNO further decides the corresponding data cap,
subscription fee, and the per-unit fee; Finally in Stage III, users make
subscription decisions based on their own characteristics. Through backward
induction, we analytically characterize the MNO's profit-maximizing data plan
and the corresponding users' subscriptions. Furthermore, we conduct a market
survey to estimate the distribution of users' two-dimensional characteristics,
and evaluate the performance of different data mechanisms using the real data.
We find that a more time-flexible data mechanism increases MNO's profit and
users' payoffs, hence improves the social welfare.",hard to cancel subscription
http://arxiv.org/abs/1112.1314v2,"A fundamental aspect in performance engineering of wireless networks is
optimizing the set of links that can be concurrently activated to meet given
signal-to-interference-and-noise ratio (SINR) thresholds. The solution of this
combinatorial problem is the key element in scheduling and cross-layer resource
management. Previous works on link activation assume single-user decoding
receivers, that treat interference in the same way as noise. In this paper, we
assume multiuser decoding receivers, which can cancel strongly interfering
signals. As a result, in contrast to classical spatial reuse, links being close
to each other are more likely to be active simultaneously. Our goal here is to
deliver a comprehensive theoretical and numerical study on optimal link
activation under this novel setup, in order to provide insight into the gains
from adopting interference cancellation. We therefore consider the optimal
problem setting of successive interference cancellation (SIC), as well as the
simpler, yet instructive, case of parallel interference cancellation (PIC). We
prove that both problems are NP-hard and develop compact integer linear
programming formulations that enable us to approach the global optimum
solutions. We provide an extensive numerical performance evaluation, indicating
that for low to medium SINR thresholds the improvement is quite substantial,
especially with SIC, whereas for high SINR thresholds the improvement
diminishes and both schemes perform equally well.",hard to cancel subscription
http://arxiv.org/abs/1501.04705v1,"Polar codes are of great interests because they provably achieve the capacity
of both discrete and continuous memoryless channels while having an explicit
construction. Most existing decoding algorithms of polar codes are based on
bit-wise hard or soft decisions. In this paper, we propose symbol-decision
successive cancellation (SC) and successive cancellation list (SCL) decoders
for polar codes, which use symbol-wise hard or soft decisions for higher
throughput or better error performance. First, we propose to use a recursive
channel combination to calculate symbol-wise channel transition probabilities,
which lead to symbol decisions. Our proposed recursive channel combination also
has a lower complexity than simply combining bit-wise channel transition
probabilities. The similarity between our proposed method and Arikan's channel
transformations also helps to share hardware resources between calculating bit-
and symbol-wise channel transition probabilities. Second, a two-stage list
pruning network is proposed to provide a trade-off between the error
performance and the complexity of the symbol-decision SCL decoder. Third, since
memory is a significant part of SCL decoders, we propose a pre-computation
memory-saving technique to reduce memory requirement of an SCL decoder.
Finally, to evaluate the throughput advantage of our symbol-decision decoders,
we design an architecture based on a semi-parallel successive cancellation list
decoder. In this architecture, different symbol sizes, sorting implementations,
and message scheduling schemes are considered. Our synthesis results show that
in terms of area efficiency, our symbol-decision SCL decoders outperform both
bit- and symbol-decision SCL decoders.",hard to cancel subscription
http://arxiv.org/abs/cs/0603033v1,"Modern frameworks for development of graphical interfaces are using the
native controls of the operating system. Because of that they are using
operating system events model for inter-component communication. We consider a
method to increase inter-component communication speed by sending messages from
one component to the other passing over the operating system. Besides the
messages subscription helps to avoid receiving of unnecessary messages.",hard to cancel subscription
http://arxiv.org/abs/1004.0728v1,"Modern large-scale date centres, such as those used for cloud computing
service provision, are becoming ever-larger as the operators of those data
centres seek to maximise the benefits from economies of scale. With these
increases in size comes a growth in system complexity, which is usually
problematic. There is an increased desire for automated ""self-star""
configuration, management, and failure-recovery of the data-centre
infrastructure, but many traditional techniques scale much worse than linearly
as the number of nodes to be managed increases. As the number of nodes in a
median-sized data-centre looks set to increase by two or three orders of
magnitude in coming decades, it seems reasonable to attempt to explore and
understand the scaling properties of the data-centre middleware before such
data-centres are constructed. In [1] we presented SPECI, a simulator that
predicts aspects of large-scale data-centre middleware performance,
concentrating on the influence of status changes such as policy updates or
routine node failures. [...]. In [1] we used a first-approximation assumption
that such subscriptions are distributed wholly at random across the data
centre. In this present paper, we explore the effects of introducing more
realistic constraints to the structure of the internal network of
subscriptions. We contrast the original results [...] exploring the effects of
making the data-centre's subscription network have a regular lattice-like
structure, and also semi-random network structures resulting from parameterised
network generation functions that create ""small-world"" and ""scale-free""
networks. We show that for distributed middleware topologies, the structure and
distribution of tasks carried out in the data centre can significantly
influence the performance overhead imposed by the middleware.",hard to cancel subscription
http://arxiv.org/abs/1112.0416v2,"This paper analyzes the adoption of unstructured P2P overlay networks to
build publish-subscribe systems. We consider a very simple distributed
communication protocol, based on gossip and on the local knowledge each node
has about subscriptions made by its neighbours. In particular, upon reception
(or generation) of a novel event, a node sends it to those neighbours whose
subscriptions match that event. Moreover, the node gossips the event to its
""non-interested"" neighbours, so that the event can be spread through the
overlay. A mathematical analysis is provided to estimate the number of nodes
receiving the event, based on the network topology, the amount of subscribers
and the gossip probability. These outcomes are compared to those obtained via
simulation. Results show even when the amount of subscribers represents a very
small (yet non-negligible) portion of network nodes, by tuning the gossip
probability the event can percolate through the overlay. Hence, the use of
unstructured networks. coupled with simple dissemination protocols, represents
a viable approach to build peer-to-peer publish-subscribe applications.",hard to cancel subscription
http://arxiv.org/abs/0903.1555v1,"Methods are presented for measuring thrust using common force sensors and
data acquisition to construct a dynamic force plate. A spreadsheet can be used
to compute trajectory by integrating the equations of motion numerically. These
techniques can be used in college physics courses, and have also been used with
high school students concurrently enrolled in algebra 2.",forced enrollment websites
http://arxiv.org/abs/1804.07016v3,"The widespread usage of password authentication in online websites leads to
an ever-increasing concern, especially when considering the possibility for an
attacker to recover the user password by leveraging the loopholes in the
password recovery mechanisms. Indeed, if a website adopts a poor password
management system, this choice makes useless even the most robust password
chosen by its users. In this paper, we first provide a survey of currently
adopted password recovery mechanisms. Later, we model an attacker with
different capabilities and we show how current password recovery mechanisms can
be exploited in our attacker model. Then, we provide a thorough analysis of the
password management of some of the Alexa's top 200 websites in different
countries, including England, France, Germany, Spain and Italy. Of these 1,000
websites, 722 do not require authentication -- and hence are excluded by our
study -- while out of the remaining 278 we focused on 174, since 104 demanded a
complex registration procedure. Of these 174, almost 25% of the them have
critical vulnerabilities, while 44% have some form of vulnerability. Finally,
we propose some effective countermeasures and we point out that, by considering
the entry into force of the General Data Protection Regulation (GDPR) in May,
2018, most of websites are not compliant with the legislation and may incur in
heavy fines. This study, other than being important on its own since it
highlights some severe current vulnerabilities and proposes corresponding
remedies, has the potential to also have a relevant impact on the EU industrial
ecosystem.",forced enrollment websites
http://arxiv.org/abs/1211.7194v1,"Undergraduate research is widely regarded as a high impact practice. However,
usually only the highest achieving students are rewarded with undergraduate
research opportunities. This paper reports on the successful implementation of
a student research program offering the weakest 10% of incoming freshmen
opportunities to conduct original research in one of several science or
engineering disciplines with the possibility of publication if the research and
report meet a suitable standard, defined as earning an A on the final research
project report in the introductory math course. The opportunity has been
offered now for two years to incoming cadets at the United States Air Force
Academy who are placed in Basic Math. The cadets placed in this course score in
the bottom 5% of incoming cadets on the math placement exam. During the second
semester of their freshman year, cadets enrolled in Calculus 1 are also offered
a similar research opportunity. About 10% of cadets are enrolled in this course
each Spring, the 5% who began in Basic Math and matriculate to Calculus 1 and
the 5% who failed Calculus 1 in their first attempt. During the first four
semesters, the program has yielded 22 cadet papers which have been published or
are currently under review and expected to be published. This represents
approximately 38% of the projects in the program, because the majority of the
projects do not earn As and are not suitable for publication. Over 80% of the
cadet co-authors on the publication quality papers are minorities, women,
and/or intercollegiate athletes.",forced enrollment websites
http://arxiv.org/abs/1901.05520v1,"We analyze the changes in the training and educational efforts of the SciNet
HPC Consortium, a Canadian academic High Performance Computing center, in the
areas of Scientific Computing and High-Performance Computing, over the last six
years. Initially, SciNet offered isolated training events on how to use HPC
systems and write parallel code, but the training program now consists of a
broad range of workshops and courses that users can take toward certificates in
scientific computing, data science, or high-performance computing. Using data
on enrollment, attendence, and certificate numbers from SciNet's education
website, used by almost 1800 users so far, we extract trends on the growth,
demand, and breadth of SciNet's training program. Among the results are a
steady overall growth, a sharp and steady increase in the demand for data
science training, and a wider participation of 'non-traditional' computing
disciplines, which has motivated an increasingly broad spectrum of training
offerings. Of interest is also that many of the training initiatives have
evolved into courses that can be taken as part of the graduate curriculum at
the University of Toronto.",forced enrollment websites
http://arxiv.org/abs/1602.06423v1,"Learning physics requires understanding the applicability of fundamental
principles in a variety of contexts that share deep features. One way to help
students learn physics is via analogical reasoning. Students can be taught to
make an analogy between situations that are more familiar or easier to
understand and another situation where the same physics principle is involved
but that is more difficult to handle. Here, we examine introductory physics
students' ability to use analogies in solving problems involving Newton's
second law. Students enrolled in an algebra-based introductory physics course
were given a solved problem involving tension in a rope and were then asked to
solve another problem for which the physics is very similar but involved a
frictional force. They were asked to point out the similarities between the two
problems and then use the analogy to solve the friction problem.",forced enrollment websites
http://arxiv.org/abs/1312.7511v1,"In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.",forced enrollment websites
http://arxiv.org/abs/1509.05358v1,"This study investigates how faculty, student, and course features are linked
to student outcomes in Learning Assistant (LA) supported courses. Over 4,500
students and 17 instructors from 13 LA Alliance member institutions
participated in the study. Each participating student completed an online
concept inventory at the start (pre) and end (post) of their term. The physics
concept inventories included Force and Motion Concept Evaluation (FMCE) and the
Brief Electricity and Magnetism Assessment (BEMA). Concepts inventories from
the fields of biology and chemistry were also included. Our analyses utilize
hierarchical linear models that nest student level data (e.g. pre/post scores
and gender) within course level data (e.g. discipline and course enrollment) to
build models that examine student outcomes across institutions and disciplines.
We report findings on the connections between students' outcomes and their
gender, race, and time spent working with LAs as well as instructors'
experiences with LAs.",forced enrollment websites
http://arxiv.org/abs/physics/0702213v1,"An algorithm for fast calculation of the Coulombic forces and energies of
point particles with free boundary conditions is proposed. Its calculation time
scales as N log N for N particles. This novel method has lower crossover point
with the full O(N^2) direct summation than the Fast Multipole Method. The
forces obtained by our algorithm are analytical derivatives of the energy which
guarantees energy conservation during a molecular dynamics simulation. Our
algorithm is very simple. An MPI parallelised version of the code can be
downloaded under the GNU General Public License from the website of our group.",forced enrollment websites
http://arxiv.org/abs/1908.02706v1,"In this paper, we present a novel architecture that integrates a deep hashing
framework with a neural network decoder (NND) for application to face template
protection. It improves upon existing face template protection techniques to
provide better matching performance with one-shot and multi-shot enrollment. A
key novelty of our proposed architecture is that the framework can also be used
with zero-shot enrollment. This implies that our architecture does not need to
be re-trained even if a new subject is to be enrolled into the system. The
proposed architecture consists of two major components: a deep hashing (DH)
component, which is used for robust mapping of face images to their
corresponding intermediate binary codes, and a NND component, which corrects
errors in the intermediate binary codes that are caused by differences in the
enrollment and probe biometrics due to factors such as variation in pose,
illumination, and other factors. The final binary code generated by the NND is
then cryptographically hashed and stored as a secure face template in the
database. The efficacy of our approach with zero-shot, one-shot, and multi-shot
enrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE
face databases. With zero-shot enrollment, the system achieves approximately
85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with
one-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at
0.01% FAR, while providing a high level of template security.",forced enrollment websites
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",forced enrollment websites
http://arxiv.org/abs/1902.01663v2,"In this study, we investigate fundamental trade-off among identification,
secrecy, template, and privacy-leakage rates in biometric identification
system. Ignatenko and Willems (2015) studied this system assuming that the
channel in the enrollment process of the system is noiseless. In the enrollment
process, however, it is highly considerable that noise occurs when bio-data is
scanned. In this paper, we impose a noisy channel in the enrollment process and
characterize the capacity region of the rate tuples. The obtained result shows
that the characterization reduces to the one given by Ignatenko and Willems
(2015) as a special case where the enrollment channel is noiseless and there is
no constraint on the template rate.",forced enrollment websites
http://arxiv.org/abs/1905.03598v1,"We investigate fundamental limits among identification, secrecy, template,
and privacy-leakage rates in biometric identification system with secret
binding from an information theoretic perspective. Ignatenko and Willems (2015)
studied the system with estimation of individuals and clarified the fundamental
limit among identification, secrecy, and privacy-leakage rates provided that
the enrollment channel of the system is noiseless. In the enrollment process,
however, it is highly considerable that noise occurs when bio-data is scanned.
Recently, G\""unl\""u and Kramer (2018) analyzed the system assuming that the
channel is noisy when there is only one individual. In this paper, we analyze
the model considering the estimation of individuals under noisy enrollment and
characterize the capacity region of identification, secrecy, template, and
privacy-leakage rates in the biometric identification system with secret
binding. The obtained result shows that it reduces to the one given by
Ignatenko and Willems (2015) as a special case where the enrollment channel is
noiseless.",forced enrollment websites
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",forced enrollment websites
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",forced enrollment websites
http://arxiv.org/abs/1005.5060v1,"The use of agile principles and practices in software development is becoming
a powerful force in today's workplace. In our quest to develop better products,
therefore, it is imperative that we strive to learn and understand the
application of Agile methods, principles and techniques to the software
development enterprise. Unfortunately, in many educational institutions courses
and projects that emphasize Agile Software Development are minimal. At best,
students have only limited exposure to the agile philosophy, principles and
practices at the graduate and undergraduate levels of education. In an effort
to address this concern, we offered a graduate-level course entitled ""Agile
Software Engineering"" in the Department of Computer Science at Virginia Tech in
Fall 2009. The primary objectives of the class were to introduce the values,
principles and practices underlying the agile philosophy, and to do so in an
atmosphere that encourages debate and critical thinking. The course was
designed around three central components: (1) teaching the essentials of how
one develops a product within an Agile framework, (2) having invited
presentation by notable industry experts, and (3) having students present and
discuss current research topics and issues. This paper describes our
experiences during the offering of that course, and in particular, the unique
perspectives of the class instructor, the teaching assistant and a student who
was enrolled in the class.",forced enrollment websites
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",forced enrollment websites
http://arxiv.org/abs/1403.5501v1,"We have collected and analyzed the relevant data from public schools in
greater Houston area of Texas. Based and analyzed. Since the data is only
limited to a few school, we are still working on getting more data so that we
can compare and contrast the results adequately and understand the core of the
enrollment issue at the national level. However, based on the raw data and
partial analysis, we propose a few recommendations towards the improvement of
science education in Texas Schools, in general, and greater Houston area
schools in particular. Our results indicate that the quality of science
education can be improved significantly if we focus on the improvement of high
school education or even intermediate schools when students are first time
exposed to science in a little technical way. Simply organizing teacher
training programs at K-12 level as school education plays a pivotal role in the
decrease in physics enrollment at the higher level. Similar analysis can
actually be generalized to other states to find out the best way to increase
the physics enrollment.",forced enrollment websites
http://arxiv.org/abs/1608.05131v1,"Massive Open Online Courses (MOOCs) have the potential to democratize
education by providing learners with access to rich sources of information.
However, evidence supporting this democratization across countries is limited.
We explored the question of democratization by investigating whether females
from different countries were more likely to enroll in and complete STEM MOOCs
compared with males. We found that whereas females were less likely to enroll
in STEM MOOCs, they were equally likely to complete them. We found smaller
gender gaps in STEM MOOC enrollment in less economically developed countries.
Further, females were more likely than males to complete STEM MOOCs in
countries identified as having a high potential to become the largest economies
in the 21st century.",forced enrollment websites
http://arxiv.org/abs/1702.01167v1,"Iris recognition systems are a mature technology that is widely used
throughout the world. In identification (as opposed to verification) mode, an
iris to be recognized is typically matched against all N enrolled irises. This
is the classic ""1-to-N search"". In order to improve the speed of large-scale
identification, a modified ""1-to-First"" search has been used in some
operational systems. A 1-to-First search terminates with the first
below-threshold match that is found, whereas a 1-to-N search always finds the
best match across all enrollments. We know of no previous studies that evaluate
how the accuracy of 1-to-First search differs from that of 1-to-N search. Using
a dataset of over 50,000 iris images from 2,800 different irises, we perform
experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search.
We evaluate how the accuracy difference changes with larger numbers of enrolled
irises, and with larger ranges of rotational difference allowed between iris
images. We find that False Match error rate for 1-to-First is higher than for
1-to-N, and the the difference grows with larger number of enrolled irises and
with larger range of rotation.",forced enrollment websites
http://arxiv.org/abs/1905.13383v1,"Understanding large-scale patterns in student course enrollment is a problem
of great interest to university administrators and educational researchers. Yet
important decisions are often made without a good quantitative framework of the
process underlying student choices. We propose a probabilistic approach to
modelling course enrollment decisions, drawing inspiration from multilabel
classification and mixture models. We use ten years of anonymized student
transcripts from a large university to construct a Gaussian latent variable
model that learns the joint distribution over course enrollments. The models
allow for a diverse set of inference queries and robustness to data sparsity.
We demonstrate the efficacy of this approach in comparison to others, including
deep learning architectures, and demonstrate its ability to infer the
underlying student interests that guide enrollment decisions.",forced enrollment websites
http://arxiv.org/abs/1910.05171v1,"A keyword spotting (KWS) system determines the existence of, usually
predefined, keyword in a continuous speech stream. This paper presents a
query-by-example on-device KWS system which is user-specific. The proposed
system consists of two main steps: query enrollment and testing. In query
enrollment step, phonetic posteriors are output by a small-footprint automatic
speech recognition model based on connectionist temporal classification. Using
the phonetic-level posteriorgram, hypothesis graph of finite-state transducer
(FST) is built, thus can enroll any keywords thus avoiding an out-of-vocabulary
problem. In testing, a log-likelihood is scored for input audio using the FST.
We propose a threshold prediction method while using the user-specific keyword
hypothesis only. The system generates query-specific negatives by rearranging
each query utterance in waveform. The threshold is decided based on the
enrollment queries and generated negatives. We tested two keywords in English,
and the proposed work shows promising performance while preserving simplicity.",forced enrollment websites
http://arxiv.org/abs/1205.2494v7,"In a recent article, this author proposed a program for physics beyond the
Standard Model, solely based on modifying the twin pillars of fundamental
physics by replacing Lorentz structure with Euclidean Jordan algebra while
keeping quantum theory. This program predicts not only quarks and leptons but
also a short-range 5th fundamental force accompanying gravity.
  This 5th force predicts quark mixing and the related CP violation, which in
fact was a phenomena observed in labs about fifty years ago. Thus, there are
two conflicting theories as of now, the one based on the 5th force which {\em
predicts} this phenomena and the established Cabibbo-Kobayashi-Maskawa (CKM)
theory which was invented to {\em explain} this phenomena. In this article a
test of these two theories against the recent experimental data is presented.
It is found in this test that the CKM theory fares poorly, whereas the one
based on the 5th force withstands the test well, in both accuracy and
precision. For example, for the CKM matrix entry $V_{cd}$, we have
$$|V_{cb}^{\tiny \mbox{experiment}}|=0.0409 \pm 0.0011, \quad |V_{cb}^{\tiny
\mbox{CKM}}|= 2.37\pm 1.82,\quad |V_{cb}^{\tiny \mbox{5th force}}|=0.0408 \pm
0.0028. $$",forced enrollment websites
http://arxiv.org/abs/1105.6361v1,"Many countries around the world have initiated national ID card programs in
the last decade. These programs are considered of strategic value to
governments due to its contribution in enhancing existing identity management
systems. Considering the total cost of such programs which goes up to billions
of dollars, the success in attaining their objectives is a crucial element in
the agendas of political systems in countries worldwide. Our experience in the
field shows that many of such projects have been challenged to deliver their
primary objectives of population enrolment, and therefore resulted in failing
to meet deadlines and keeping up with budgetary constraints. The purpose of
this paper is to explain the finding of a case study action research aimed to
introduce a new approach to how population are enrolled in national ID
programs. This is achieved through presenting a case study of a business
process reengineering initiative undertaken in the UAE national ID program. The
scope of this research is limited to the enrolment process within the program.
This article also intends to explore the possibilities of significant results
with the new proposed enrolment approach with the application of BPR. An
overview of the ROI study has been developed to illustrate such efficiencies.",forced enrollment websites
http://arxiv.org/abs/1507.06955v5,"A fundamental assumption in software security is that a memory location can
only be modified by processes that may write to this memory location. However,
a recent study has shown that parasitic effects in DRAM can change the content
of a memory cell without accessing it, but by accessing other memory locations
in a high frequency. This so-called Rowhammer bug occurs in most of today's
memory modules and has fatal consequences for the security of all affected
systems, e.g., privilege escalation attacks.
  All studies and attacks related to Rowhammer so far rely on the availability
of a cache flush instruction in order to cause accesses to DRAM modules at a
sufficiently high frequency. We overcome this limitation by defeating complex
cache replacement policies. We show that caches can be forced into fast cache
eviction to trigger the Rowhammer bug with only regular memory accesses. This
allows to trigger the Rowhammer bug in highly restricted and even scripting
environments.
  We demonstrate a fully automated attack that requires nothing but a website
with JavaScript to trigger faults on remote hardware. Thereby we can gain
unrestricted access to systems of website visitors. We show that the attack
works on off-the-shelf systems. Existing countermeasures fail to protect
against this new Rowhammer attack.",forced enrollment websites
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",forced enrollment websites
http://arxiv.org/abs/1605.07750v1,"Atomic force microscope (AFM) users often calibrate the spring constants of
cantilevers using functionality built into individual instruments. This is
performed without reference to a global standard, which hinders robust
comparison of force measurements reported by different laboratories. In this
article, we describe a virtual instrument (an internet-based initiative)
whereby users from all laboratories can instantly and quantitatively compare
their calibration measurements to those of others - standardising AFM force
measurements - and simultaneously enabling non-invasive calibration of AFM
cantilevers of any geometry. This global calibration initiative requires no
additional instrumentation or data processing on the part of the user. It
utilises a single website where users upload currently available data. A
proof-of-principle demonstration of this initiative is presented using measured
data from five independent laboratories across three countries, which also
allows for an assessment of current calibration.",forced enrollment websites
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",forced enrollment websites
http://arxiv.org/abs/1811.00923v1,"Shared Web Hosting service enables hosting multitude of websites on a single
powerful server. It is a well-known solution as many people share the overall
cost of server maintenance and also, website owners do not need to deal with
administration issues is not necessary for website owners. In this paper, we
illustrate how shared web hosting service works and demonstrate the security
weaknesses rise due to the lack of proper isolation between different websites,
hosted on the same server. We exhibit two new server-side attacks against the
log file whose objectives are revealing information of other hosted websites
which are considered to be private and arranging other complex attacks. In the
absence of isolated log files among websites, an attacker controlling a website
can inspect and manipulate contents of the log file. These attacks enable an
attacker to disclose file and directory structure of other websites and launch
other sorts of attacks. Finally, we propose several countermeasures to secure
shared web hosting servers against the two attacks subsequent to the separation
of log files for each website.",forced enrollment websites
http://arxiv.org/abs/1305.4018v1,"Many online video websites provide the shortcut links to facilitate the video
sharing to other websites especially to the online social networks (OSNs). Such
video sharing behavior greatly changes the interplays between the two types of
websites. For example, users in OSNs may watch and re-share videos shared by
their friends from online video websites, and this can also boost the
popularity of videos in online video websites and attract more people to watch
and share them. Characterizing these interplays can provide great insights for
understanding the relationships among online video websites, OSNs, ISPs and so
on. In this paper we conduct empirical experiments to study the interplays
between video sharing websites and OSNs using three totally different data
sources: online video websites, OSNs, and campus network traffic. We find that,
a) there are many factors that can affect the external sharing probability of
videos in online video websites. b) The popularity of a video itself in online
video websites can greatly impact on its popularity in OSNs. Videos in Renren,
Qzone (the top two most popular Chinese OSNs) usually attract more viewers than
in Sina and Tencent Weibo (the top two most popular Chinese microblogs), which
indicates the different natures of the two kinds of OSNs. c) The analysis based
on real traffic data illustrates that 10\% of video flows are related to OSNs,
and they account for 25\% of traffic generated by all videos.",forced enrollment websites
http://arxiv.org/abs/1304.6146v1,"We begin this paper by presenting our approach to robot manipulation, which
emphasizes the benefits of making contact with the world across the entire
manipulator. We assume that low contact forces are benign, and focus on the
development of robots that can control their contact forces during
goal-directed motion. Inspired by biology, we assume that the robot has
low-stiffness actuation at its joints, and tactile sensing across the entire
surface of its manipulator. We then describe a novel controller that exploits
these assumptions. The controller only requires haptic sensing and does not
need an explicit model of the environment prior to contact. It also handles
multiple contacts across the surface of the manipulator. The controller uses
model predictive control (MPC) with a time horizon of length one, and a linear
quasi-static mechanical model that it constructs at each time step. We show
that this controller enables both real and simulated robots to reach goal
locations in high clutter with low contact forces. Our experiments include
tests using a real robot with a novel tactile sensor array on its forearm
reaching into simulated foliage and a cinder block. In our experiments, robots
made contact across their entire arms while pushing aside movable objects,
deforming compliant objects, and perceiving the world.",forced enrollment websites
http://arxiv.org/abs/physics/0509206v1,"We discuss and analyze the fact that physics is generally losing its ability
to captivate students who may possess the potential to enhance the quality of
our future in this age of technology. We have tried to investigate the reasons
behind this low enrollment in the light of the results of a few surveys with
the undergraduate students in different physics courses and in current relevant
college programs. It is not an exclusively descriptive issue, so our analysis
is a way to delineate the details of the matter leading to the suggestions for
future improvements.",forced enrollment websites
http://arxiv.org/abs/1104.2518v1,"The post-enrolment course timetabling (PE-CTT) is one of the most studied
timetabling problems, for which many instances and results are available. In
this work we design a metaheuristic approach based on Simulated Annealing to
solve the PE-CTT. We consider all the different variants of the problem that
have been proposed in the literature and we perform a comprehensive
experimental analysis on all the public instances available. The outcome is
that our solver, properly engineered and tuned, performs very well on all
cases, providing the new best known results on many instances and
state-of-the-art values for the others.",forced enrollment websites
http://arxiv.org/abs/1405.3729v1,"Data Mining is the process of extracting useful patterns from the huge amount
of database and many data mining techniques are used for mining these patterns.
Recently, one of the remarkable facts in higher educational institute is the
rapid growth data and this educational data is expanding quickly without any
advantage to the educational management. The main aim of the management is to
refine the education standard; therefore by applying the various data mining
techniques on this data one can get valuable information. This research study
proposed the ""classification model for the student's enrollment process in
higher educational courses using data mining techniques"". Additionally, this
study contributes to finding some patterns that are meaningful to management.",forced enrollment websites
http://arxiv.org/abs/1510.04160v2,"Aadhaar is the world's largest biometric database with a billion records,
being compiled as an identity platform to deliver social services to residents
of India.Aadhaar processes streams of biometric data as residents are enrolled
and updated.Besides 1 million enrolments and updates per day,up to 100 million
daily biometric authentications are expected during delivery of various public
services.These form critical Big Data applications,with large volumes and high
velocity of data.",forced enrollment websites
http://arxiv.org/abs/1405.6442v1,"Physics community is generally concerned about the decrease in Physics
enrollment. Due to the deficiency of specially qualified Physics teachers, high
school Physics is sometimes taught by general science teachers who may not be
trained to motivate students to study Physics. These students will neither be
inclined to register into Physics courses nor plan to teach Physics. They will
be unable to find out the relevance of Physics with daily life, its application
in different disciplines and even the job market. In this situation, we have to
carefully make the existing Physics programs more attractive, instead of
closing them down. We discuss teaching methodology and course requirements that
will help to make Physics programs more attractive and preferable for incoming
students. However, we emphasize to set our goals and plan to increase
enrollment in parallel steps such as proper information about the program, help
in developing a required mathematics background, offering scholarships,
teaching assistantship or internships and involving them in research. Also we
need to make Physics programs more interesting with the interdisciplinary
courses and other electives which students may like. We will have to work on
the retention rate to maintain enrollment without compromising on standards.
However, we still have to develop a complete understanding of the problem and
keep looking for a better solution.",forced enrollment websites
http://arxiv.org/abs/0903.1555v1,"Methods are presented for measuring thrust using common force sensors and
data acquisition to construct a dynamic force plate. A spreadsheet can be used
to compute trajectory by integrating the equations of motion numerically. These
techniques can be used in college physics courses, and have also been used with
high school students concurrently enrolled in algebra 2.",forced enrollment detection
http://arxiv.org/abs/1602.06967v1,"Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art
method for modeling $i$-vector space in speaker recognition task. However the
performance degradation is observed if enrollment data size differs from one
speaker to another. This paper presents a solution to such problem by
introducing new PLDA scoring normalization technique. Normalization parameters
are derived in a blind way, so that, unlike traditional \textit{ZT-norm}, no
extra development data is required. Moreover, proposed method has shown to be
optimal in terms of detection cost function. The experiments conducted on NIST
SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number
condition.",forced enrollment detection
http://arxiv.org/abs/1211.7194v1,"Undergraduate research is widely regarded as a high impact practice. However,
usually only the highest achieving students are rewarded with undergraduate
research opportunities. This paper reports on the successful implementation of
a student research program offering the weakest 10% of incoming freshmen
opportunities to conduct original research in one of several science or
engineering disciplines with the possibility of publication if the research and
report meet a suitable standard, defined as earning an A on the final research
project report in the introductory math course. The opportunity has been
offered now for two years to incoming cadets at the United States Air Force
Academy who are placed in Basic Math. The cadets placed in this course score in
the bottom 5% of incoming cadets on the math placement exam. During the second
semester of their freshman year, cadets enrolled in Calculus 1 are also offered
a similar research opportunity. About 10% of cadets are enrolled in this course
each Spring, the 5% who began in Basic Math and matriculate to Calculus 1 and
the 5% who failed Calculus 1 in their first attempt. During the first four
semesters, the program has yielded 22 cadet papers which have been published or
are currently under review and expected to be published. This represents
approximately 38% of the projects in the program, because the majority of the
projects do not earn As and are not suitable for publication. Over 80% of the
cadet co-authors on the publication quality papers are minorities, women,
and/or intercollegiate athletes.",forced enrollment detection
http://arxiv.org/abs/1206.4261v1,"In recent decades it has been detected in Italy a decrease in enrollment in
basic sciences, i.e. Mathematics, Physics and Chemistry. The increase in
specific orientation is strategically crucial to achieve the goal of
maintaining and increasing the number of motivated and capable students who
enroll in these courses. With the purpose of increasing scientific vocations,
workshops were organized in high schools and teachers involved in planning and
implementation of laboratories, conferences for scientific outreach, thematic
exhibitions, guided tours of research laboratories, summer's schools for
students and courses for teachers were realized for developing a cultural
enhancement in teaching basic sciences. Particularly significant is the case of
activities organized by the Department of Physics of the University of Siena
for students and teachers in Southern Tuscany. The methods used in cultural
enhancement of teachers and activities designed to support schools with limited
laboratory facilities, together with stimulating activities for motivated
students are allowed to take root for some good practices in physics teaching
and orientation to scientific degrees. Beyond describing the main activities
for orientation to Physics, activities done in partnership with chemists,
biologists and geologists are reported, as well as an activity in which the
Departments of Mathematical Sciences and Physics are both involved in looking
for introducing new interdisciplinary methodologies to increase students'
understanding in high school of some selected topics in which both disciplines
give a contribution in the construction of important and mutually reinforcing
basic concepts.",forced enrollment detection
http://arxiv.org/abs/1802.09990v2,"Face recognition (FR) systems for video surveillance (VS) applications
attempt to accurately detect the presence of target individuals over a
distributed network of cameras. In video-based FR systems, facial models of
target individuals are designed a priori during enrollment using a limited
number of reference still images or video data. These facial models are not
typically representative of faces being observed during operations due to large
variations in illumination, pose, scale, occlusion, blur, and to camera
inter-operability. Specifically, in still-to-video FR application, a single
high-quality reference still image captured with still camera under controlled
conditions is employed to generate a facial model to be matched later against
lower-quality faces captured with video cameras under uncontrolled conditions.
Current video-based FR systems can perform well on controlled scenarios, while
their performance is not satisfactory in uncontrolled scenarios mainly because
of the differences between the source (enrollment) and the target (operational)
domains. Most of the efforts in this area have been toward the design of robust
video-based FR systems in unconstrained surveillance environments. This chapter
presents an overview of recent advances in still-to-video FR scenario through
deep convolutional neural networks (CNNs). In particular, deep learning
architectures proposed in the literature based on triplet-loss function (e.g.,
cross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and
supervised autoencoders (e.g., canonical face representation CNN) are reviewed
and compared in terms of accuracy and computational complexity.",forced enrollment detection
http://arxiv.org/abs/1602.06423v1,"Learning physics requires understanding the applicability of fundamental
principles in a variety of contexts that share deep features. One way to help
students learn physics is via analogical reasoning. Students can be taught to
make an analogy between situations that are more familiar or easier to
understand and another situation where the same physics principle is involved
but that is more difficult to handle. Here, we examine introductory physics
students' ability to use analogies in solving problems involving Newton's
second law. Students enrolled in an algebra-based introductory physics course
were given a solved problem involving tension in a rope and were then asked to
solve another problem for which the physics is very similar but involved a
frictional force. They were asked to point out the similarities between the two
problems and then use the analogy to solve the friction problem.",forced enrollment detection
http://arxiv.org/abs/1312.7511v1,"In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.",forced enrollment detection
http://arxiv.org/abs/1509.05358v1,"This study investigates how faculty, student, and course features are linked
to student outcomes in Learning Assistant (LA) supported courses. Over 4,500
students and 17 instructors from 13 LA Alliance member institutions
participated in the study. Each participating student completed an online
concept inventory at the start (pre) and end (post) of their term. The physics
concept inventories included Force and Motion Concept Evaluation (FMCE) and the
Brief Electricity and Magnetism Assessment (BEMA). Concepts inventories from
the fields of biology and chemistry were also included. Our analyses utilize
hierarchical linear models that nest student level data (e.g. pre/post scores
and gender) within course level data (e.g. discipline and course enrollment) to
build models that examine student outcomes across institutions and disciplines.
We report findings on the connections between students' outcomes and their
gender, race, and time spent working with LAs as well as instructors'
experiences with LAs.",forced enrollment detection
http://arxiv.org/abs/1908.02706v1,"In this paper, we present a novel architecture that integrates a deep hashing
framework with a neural network decoder (NND) for application to face template
protection. It improves upon existing face template protection techniques to
provide better matching performance with one-shot and multi-shot enrollment. A
key novelty of our proposed architecture is that the framework can also be used
with zero-shot enrollment. This implies that our architecture does not need to
be re-trained even if a new subject is to be enrolled into the system. The
proposed architecture consists of two major components: a deep hashing (DH)
component, which is used for robust mapping of face images to their
corresponding intermediate binary codes, and a NND component, which corrects
errors in the intermediate binary codes that are caused by differences in the
enrollment and probe biometrics due to factors such as variation in pose,
illumination, and other factors. The final binary code generated by the NND is
then cryptographically hashed and stored as a secure face template in the
database. The efficacy of our approach with zero-shot, one-shot, and multi-shot
enrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE
face databases. With zero-shot enrollment, the system achieves approximately
85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with
one-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at
0.01% FAR, while providing a high level of template security.",forced enrollment detection
http://arxiv.org/abs/1804.08659v1,"We open source fingerprint Match in Box, a complete end-to-end fingerprint
recognition system embedded within a 4 inch cube. Match in Box stands in
contrast to a typical bulky and expensive proprietary fingerprint recognition
system which requires sending a fingerprint image to an external host for
processing and subsequent spoof detection and matching. In particular, Match in
Box is a first of a kind, portable, low-cost, and easy-to-assemble fingerprint
reader with an enrollment database embedded within the reader's memory and open
source fingerprint spoof detector, feature extractor, and matcher all running
on the reader's internal vision processing unit (VPU). An onboard touch screen
and rechargeable battery pack make this device extremely portable and ideal for
applying both fingerprint authentication (1:1 comparison) and fingerprint
identification (1:N search) to applications (vaccination tracking, food and
benefit distribution programs, human trafficking prevention) in rural
communities, especially in developing countries. We also show that Match in Box
is suited for capturing neonate fingerprints due to its high resolution (1900
ppi) cameras.",forced enrollment detection
http://arxiv.org/abs/1809.11068v1,"The task of spoken pass-phrase verification is to decide whether a test
utterance contains the same phrase as given enrollment utterances. Beside other
applications, pass-phrase verification can complement an independent speaker
verification subsystem in text-dependent speaker verification. It can also be
used for liveness detection by verifying that the user is able to correctly
respond to a randomly prompted phrase. In this paper, we build on our previous
work on i-vector based text-dependent speaker verification, where we have shown
that i-vectors extracted using phrase specific Hidden Markov Models (HMMs) or
using Deep Neural Network (DNN) based bottle-neck (BN) features help to reject
utterances with wrong pass-phrases. We apply the same i-vector extraction
techniques to the stand-alone task of speaker-independent spoken pass-phrase
classification and verification. The experiments on RSR2015 and RedDots
databases show that very simple scoring techniques (e.g. cosine distance
scoring) applied to such i-vectors can provide results superior to those
previously published on the same data.",forced enrollment detection
http://arxiv.org/abs/1902.01663v2,"In this study, we investigate fundamental trade-off among identification,
secrecy, template, and privacy-leakage rates in biometric identification
system. Ignatenko and Willems (2015) studied this system assuming that the
channel in the enrollment process of the system is noiseless. In the enrollment
process, however, it is highly considerable that noise occurs when bio-data is
scanned. In this paper, we impose a noisy channel in the enrollment process and
characterize the capacity region of the rate tuples. The obtained result shows
that the characterization reduces to the one given by Ignatenko and Willems
(2015) as a special case where the enrollment channel is noiseless and there is
no constraint on the template rate.",forced enrollment detection
http://arxiv.org/abs/1905.03598v1,"We investigate fundamental limits among identification, secrecy, template,
and privacy-leakage rates in biometric identification system with secret
binding from an information theoretic perspective. Ignatenko and Willems (2015)
studied the system with estimation of individuals and clarified the fundamental
limit among identification, secrecy, and privacy-leakage rates provided that
the enrollment channel of the system is noiseless. In the enrollment process,
however, it is highly considerable that noise occurs when bio-data is scanned.
Recently, G\""unl\""u and Kramer (2018) analyzed the system assuming that the
channel is noisy when there is only one individual. In this paper, we analyze
the model considering the estimation of individuals under noisy enrollment and
characterize the capacity region of identification, secrecy, template, and
privacy-leakage rates in the biometric identification system with secret
binding. The obtained result shows that it reduces to the one given by
Ignatenko and Willems (2015) as a special case where the enrollment channel is
noiseless.",forced enrollment detection
http://arxiv.org/abs/1908.10240v1,"Background: The role of neonatal pain on the developing nervous system is not
completely understood, but evidence suggests that sensory pathways are
influenced by an infants pain experience. Research has shown that an infants
previous pain experiences lead to an increased, and likely abnormal, response
to subsequent painful stimuli. We are working to improve neonatal pain
detection through automated devices that continuously monitor an infant. The
current study outlines some of the initial steps we have taken to evaluate Near
Infrared Spectroscopy (NIRS) as a technology to detect neonatal pain. Our
findings may provide neonatal intensive care unit (NICU) practitioners with the
data necessary to monitor and perhaps better manage an abnormal pain response.
Methods: A prospective pilot study was conducted to evaluate nociceptive evoked
cortical activity in preterm infants. NIRS data were recorded for approximately
10 minutes prior to an acute painful procedure and for approximately 10 minutes
after the procedure. Individual data collection events were performed at a
weekly maximum frequency. Eligible infants included those admitted to the Tampa
General Hospital (TGH) NICU with a birth gestational age of less than 37 weeks.
Results: A total of 15 infants were enrolled and 25 individual studies were
completed. Analysis demonstrated a statistically significant difference between
the median of the pre- and post-painful procedure data sets in each infants
first NIRS collection (p value = 0.01). Conclusions: Initial analysis shows
NIRS may be useful in detecting acute pain. An acute painful procedure is
typically followed by a negative deflection in NIRS readings.",forced enrollment detection
http://arxiv.org/abs/1005.5060v1,"The use of agile principles and practices in software development is becoming
a powerful force in today's workplace. In our quest to develop better products,
therefore, it is imperative that we strive to learn and understand the
application of Agile methods, principles and techniques to the software
development enterprise. Unfortunately, in many educational institutions courses
and projects that emphasize Agile Software Development are minimal. At best,
students have only limited exposure to the agile philosophy, principles and
practices at the graduate and undergraduate levels of education. In an effort
to address this concern, we offered a graduate-level course entitled ""Agile
Software Engineering"" in the Department of Computer Science at Virginia Tech in
Fall 2009. The primary objectives of the class were to introduce the values,
principles and practices underlying the agile philosophy, and to do so in an
atmosphere that encourages debate and critical thinking. The course was
designed around three central components: (1) teaching the essentials of how
one develops a product within an Agile framework, (2) having invited
presentation by notable industry experts, and (3) having students present and
discuss current research topics and issues. This paper describes our
experiences during the offering of that course, and in particular, the unique
perspectives of the class instructor, the teaching assistant and a student who
was enrolled in the class.",forced enrollment detection
http://arxiv.org/abs/1201.3417v1,"The main objective of higher education institutions is to provide quality
education to its students. One way to achieve highest level of quality in
higher education system is by discovering knowledge for prediction regarding
enrolment of students in a particular course, alienation of traditional
classroom teaching model, detection of unfair means used in online examination,
detection of abnormal values in the result sheets of the students, prediction
about students' performance and so on. The knowledge is hidden among the
educational data set and it is extractable through data mining techniques.
Present paper is designed to justify the capabilities of data mining techniques
in context of higher education by offering a data mining model for higher
education system in the university. In this research, the classification task
is used to evaluate student's performance and as there are many approaches that
are used for data classification, the decision tree method is used here. By
this task we extract knowledge that describes students' performance in end
semester examination. It helps earlier in identifying the dropouts and students
who need special attention and allow the teacher to provide appropriate
advising/counseling. Keywords-Educational Data Mining (EDM); Classification;
Knowledge Discovery in Database (KDD); ID3 Algorithm.",forced enrollment detection
http://arxiv.org/abs/1901.11291v2,"In this work, we address a novel, but potentially emerging, problem of
discriminating the natural human voices and those played back by any kind of
audio devices in the context of interactions with in-house voice user
interface. The tackled problem may find relevant applications in (1) the
far-field voice interactions of vocal interfaces such as Amazon Echo, Google
Home, Facebook Portal, etc, and (2) the replay spoofing attack detection. The
detection of loudspeaker emitted speech will help avoid false wake-ups or
unintended interactions with the devices in the first application, while
eliminating attacks involve the replay of recordings collected from enrolled
speakers in the second one. At first we collect a real-world dataset under
well-controlled conditions containing two classes: recorded speeches directly
spoken by numerous people (considered as the natural speech), and recorded
speeches played back from various loudspeakers (considered as the loudspeaker
emitted speech). Then from this dataset, we build prediction models based on
Deep Neural Network (DNN) for which different combination of audio features
have been considered. Experiment results confirm the feasibility of the task
where the combination of audio embeddings extracted from SoundNet and VGGish
network yields the classification accuracy up to about 90%.",forced enrollment detection
http://arxiv.org/abs/1403.5501v1,"We have collected and analyzed the relevant data from public schools in
greater Houston area of Texas. Based and analyzed. Since the data is only
limited to a few school, we are still working on getting more data so that we
can compare and contrast the results adequately and understand the core of the
enrollment issue at the national level. However, based on the raw data and
partial analysis, we propose a few recommendations towards the improvement of
science education in Texas Schools, in general, and greater Houston area
schools in particular. Our results indicate that the quality of science
education can be improved significantly if we focus on the improvement of high
school education or even intermediate schools when students are first time
exposed to science in a little technical way. Simply organizing teacher
training programs at K-12 level as school education plays a pivotal role in the
decrease in physics enrollment at the higher level. Similar analysis can
actually be generalized to other states to find out the best way to increase
the physics enrollment.",forced enrollment detection
http://arxiv.org/abs/1608.05131v1,"Massive Open Online Courses (MOOCs) have the potential to democratize
education by providing learners with access to rich sources of information.
However, evidence supporting this democratization across countries is limited.
We explored the question of democratization by investigating whether females
from different countries were more likely to enroll in and complete STEM MOOCs
compared with males. We found that whereas females were less likely to enroll
in STEM MOOCs, they were equally likely to complete them. We found smaller
gender gaps in STEM MOOC enrollment in less economically developed countries.
Further, females were more likely than males to complete STEM MOOCs in
countries identified as having a high potential to become the largest economies
in the 21st century.",forced enrollment detection
http://arxiv.org/abs/1702.01167v1,"Iris recognition systems are a mature technology that is widely used
throughout the world. In identification (as opposed to verification) mode, an
iris to be recognized is typically matched against all N enrolled irises. This
is the classic ""1-to-N search"". In order to improve the speed of large-scale
identification, a modified ""1-to-First"" search has been used in some
operational systems. A 1-to-First search terminates with the first
below-threshold match that is found, whereas a 1-to-N search always finds the
best match across all enrollments. We know of no previous studies that evaluate
how the accuracy of 1-to-First search differs from that of 1-to-N search. Using
a dataset of over 50,000 iris images from 2,800 different irises, we perform
experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search.
We evaluate how the accuracy difference changes with larger numbers of enrolled
irises, and with larger ranges of rotational difference allowed between iris
images. We find that False Match error rate for 1-to-First is higher than for
1-to-N, and the the difference grows with larger number of enrolled irises and
with larger range of rotation.",forced enrollment detection
http://arxiv.org/abs/1905.13383v1,"Understanding large-scale patterns in student course enrollment is a problem
of great interest to university administrators and educational researchers. Yet
important decisions are often made without a good quantitative framework of the
process underlying student choices. We propose a probabilistic approach to
modelling course enrollment decisions, drawing inspiration from multilabel
classification and mixture models. We use ten years of anonymized student
transcripts from a large university to construct a Gaussian latent variable
model that learns the joint distribution over course enrollments. The models
allow for a diverse set of inference queries and robustness to data sparsity.
We demonstrate the efficacy of this approach in comparison to others, including
deep learning architectures, and demonstrate its ability to infer the
underlying student interests that guide enrollment decisions.",forced enrollment detection
http://arxiv.org/abs/1910.05171v1,"A keyword spotting (KWS) system determines the existence of, usually
predefined, keyword in a continuous speech stream. This paper presents a
query-by-example on-device KWS system which is user-specific. The proposed
system consists of two main steps: query enrollment and testing. In query
enrollment step, phonetic posteriors are output by a small-footprint automatic
speech recognition model based on connectionist temporal classification. Using
the phonetic-level posteriorgram, hypothesis graph of finite-state transducer
(FST) is built, thus can enroll any keywords thus avoiding an out-of-vocabulary
problem. In testing, a log-likelihood is scored for input audio using the FST.
We propose a threshold prediction method while using the user-specific keyword
hypothesis only. The system generates query-specific negatives by rearranging
each query utterance in waveform. The threshold is decided based on the
enrollment queries and generated negatives. We tested two keywords in English,
and the proposed work shows promising performance while preserving simplicity.",forced enrollment detection
http://arxiv.org/abs/1105.6361v1,"Many countries around the world have initiated national ID card programs in
the last decade. These programs are considered of strategic value to
governments due to its contribution in enhancing existing identity management
systems. Considering the total cost of such programs which goes up to billions
of dollars, the success in attaining their objectives is a crucial element in
the agendas of political systems in countries worldwide. Our experience in the
field shows that many of such projects have been challenged to deliver their
primary objectives of population enrolment, and therefore resulted in failing
to meet deadlines and keeping up with budgetary constraints. The purpose of
this paper is to explain the finding of a case study action research aimed to
introduce a new approach to how population are enrolled in national ID
programs. This is achieved through presenting a case study of a business
process reengineering initiative undertaken in the UAE national ID program. The
scope of this research is limited to the enrolment process within the program.
This article also intends to explore the possibilities of significant results
with the new proposed enrolment approach with the application of BPR. An
overview of the ROI study has been developed to illustrate such efficiencies.",forced enrollment detection
http://arxiv.org/abs/1811.03026v2,"We present a novel method for learning hybrid force/position control from
demonstration. We learn a dynamic constraint frame aligned to the direction of
desired force using Cartesian Dynamic Movement Primitives. In contrast to
approaches that utilize a fixed constraint frame, our approach easily
accommodates tasks with rapidly changing task constraints over time. We
activate only one degree of freedom for force control at any given time,
ensuring motion is always possible orthogonal to the direction of desired
force. Since we utilize demonstrated forces to learn the constraint frame, we
are able to compensate for forces not detected by methods that learn only from
the demonstrated kinematic motion, such as frictional forces between the
end-effector and the contact surface. We additionally propose novel extensions
to the Dynamic Movement Primitive (DMP) framework that encourage robust
transition from free-space motion to in-contact motion in spite of environment
uncertainty. We incorporate force feedback and a dynamically shifting goal to
reduce forces applied to the environment and retain stable contact while
enabling force control. Our methods exhibit low impact forces on contact and
low steady-state tracking error.",forced enrollment detection
http://arxiv.org/abs/1612.04718v2,"Oscillations in a power system can be categorized into free oscillations and
forced oscillations. Many algorithms have been developed to estimate the modes
of free oscillations in a power system. Recently, forced oscillations caught
many attentions. Techniques are proposed to detect forced oscillations and
locate their sources. In addition, forced oscillations may have negative impact
on the estimation of mode and mode-shape if they are not properly accounted
for. To improve the power system reliability and dynamic properties, it is
important to first distinguish forced oscillations from free oscillations and
then locate the sources of forced oscillations in timely manner. The negative
impact of forced oscillation can be mitigated when they are detected and
located. This paper provides an overview on the analysis technique of forced
oscillations in power systems. In addition, some future opportunities are
discussed on forced oscillation studies.",forced enrollment detection
http://arxiv.org/abs/1901.05861v1,"Nanometer-scale structures with high aspect ratio such as nanowires and
nanotubes combine low mechanical dissipation with high resonance frequencies,
making them ideal force transducers and scanning probes in applications
requiring the highest sensitivity. Such structures promise record force
sensitivities combined with ease of use in scanning probe microscopes. A wide
variety of possible material compositions and functionalizations is available,
allowing for the sensing of various kinds of forces with optimized sensitivity.
In addition, nanowires possess quasi-degenerate mechanical mode doublets, which
has allowed the demonstration of sensitive vectorial force and mass detection.
These developments have driven researchers to use nanowire cantilevers in
various force sensing applications, which include imaging of sample surface
topography, detection of optomechanical, electrical, and magnetic forces, and
magnetic resonance force microscopy. In this review, we discuss the motivation
behind using nanowires as force transducers, explain the methods of force
sensing with nanowire cantilevers, and give an overview of the experimental
progress and future prospects of the field.",forced enrollment detection
http://arxiv.org/abs/1605.03163v1,"We demonstrate the measurement of laterally induced optical forces using an
Atomic Force Microscope (AFM). The lateral electric field distribution between
a gold coated AFM probe and a nano-aperture in a gold film is mapped by
measuring the lateral optical force between the apex of the AFM probe and the
nano-aperture. Torsional eigenmodes of an AFM cantilever probe were used to
detect the laterally induced optical forces. We engineered the cantilever shape
using a focused ion beam to enhance the torsional eigenmode resonance. The
measured lateral optical force agrees well with simulations. This technique can
be extended to simultaneously detect both lateral and longitudinal optical
forces at the nanoscale by using an AFM cantilever as a multichannel detector.
This will enable simultaneous Photon Induced Force Microscopy (PIFM) detection
of molecular responses with different incident field polarizations. The
technique can be implemented on both cantilever and tuning fork based AFMs.",forced enrollment detection
http://arxiv.org/abs/physics/0211035v1,"Gravitomagnetic and gravitoelectric forces have been studied for sometime and
tests for detecting such forces arising from the earth, are under way. We apply
similar considerations at the level of elementary particles in a formulation
using General Relativity, and deduce the presence of short range forces. A
possible candidate could be the somewhat recently detected but otherwise
mysterious short range $B_{(3)}$ force, mediated by massive ""photons"".",forced enrollment detection
http://arxiv.org/abs/1310.5793v1,"Intelligent Transportation System in case of cities is controlling traffic
congestion and regulating the traffic flow. This paper presents three modules
that will help in managing city traffic issues and ultimately gives advanced
development in transportation system. First module, Congestion Detection and
Management will provide user real time information about congestion on the road
towards his destination, Second module, Intelligent Public Transport System
will provide user real time public transport information,i.e, local buses, and
the third module, Signal Synchronization will help in controlling congestion at
signals, with real time adjustments of signal timers according to the
congestion. All the information that user is getting about the traffic or
public transportation will be provided on users day to day device that is
mobile through Android application or SMS. Moreover, communication can also be
done via Website for Clients having internet access. And all these modules will
be fully automated without any human intervention at server side.",website timer
http://arxiv.org/abs/0909.1241v1,"Timer-based mechanisms are often used to help a given (sink) node select the
best helper node among many available nodes. Specifically, a node transmits a
packet when its timer expires, and the timer value is a monotone non-increasing
function of its local suitability metric. The best node is selected
successfully if no other node's timer expires within a 'vulnerability' window
after its timer expiry, and so long as the sink can hear the available nodes.
In this paper, we show that the optimal metric-to-timer mapping that (i)
maximizes the probability of success or (ii) minimizes the average selection
time subject to a minimum constraint on the probability of success, maps the
metric into a set of discrete timer values. We specify, in closed-form, the
optimal scheme as a function of the maximum selection duration, the
vulnerability window, and the number of nodes. An asymptotic characterization
of the optimal scheme turns out to be elegant and insightful. For any
probability distribution function of the metric, the optimal scheme is
scalable, distributed, and performs much better than the popular inverse metric
timer mapping. It even compares favorably with splitting-based selection, when
the latter's feedback overhead is accounted for.",website timer
http://arxiv.org/abs/1706.04252v1,"A great deal of effort has gone into trying to model social influence ---
including the spread of behavior, norms, and ideas --- on networks. Most models
of social influence tend to assume that individuals react to changes in the
states of their neighbors without any time delay, but this is often not true in
social contexts, where (for various reasons) different agents can have
different response times. To examine such situations, we introduce the idea of
a timer into threshold models of social influence. The presence of timers on
nodes delays the adoption --- i.e., change of state --- of each agent, which in
turn delays the adoptions of its neighbors. With a homogeneous-distributed
timer, in which all nodes exhibit the same amount of delay, adoption delays are
also homogeneous, so the adoption order of nodes remains the same. However,
heterogeneously-distributed timers can change the adoption order of nodes and
hence the ""adoption paths"" through which state changes spread in a network.
Using a threshold model of social contagions, we illustrate that heterogeneous
timers can either accelerate or decelerate the spread of adoptions compared to
an analogous situation with homogeneous timers, and we investigate the
relationship of such acceleration or deceleration with respect to timer
distribution and network structure. We derive an analytical approximation for
the temporal evolution of the fraction of adopters by modifying a pair
approximation of the Watts threshold model, and we find good agreement with
numerical computations. We also examine our new timer model on networks
constructed from empirical data.",website timer
http://arxiv.org/abs/0910.1217v1,"A feature of current membrane systems is the fact that objects and membranes
are persistent. However, this is not true in the real world. In fact, cells and
intracellular proteins have a well-defined lifetime. Inspired from these
biological facts, we define a model of systems of mobile membranes in which
each membrane and each object has a timer representing their lifetime. We show
that systems of mutual mobile membranes with and without timers have the same
computational power. An encoding of timed safe mobile ambients into systems of
mutual mobile membranes with timers offers a relationship between two
formalisms used in describing biological systems.",website timer
http://arxiv.org/abs/1710.09494v2,"Watchdog timers are devices that are commonly used to monitor the health of
safety-critical hardware and software systems. Their primary function is to
raise an alarm if the monitored systems fail to emit periodic ""heartbeats"" that
signal their well-being. In this paper we design and verify a molecular
watchdog timer for monitoring the health of programmed molecular nanosystems.
This raises new challenges because our molecular watchdog timer and the system
that it monitors both operate in the probabilistic environment of chemical
kinetics, where many failures are certain to occur and it is especially hard to
detect the absence of a signal.
  Our molecular watchdog timer is the result of an incremental design process
that uses goal-oriented requirements engineering, simulation, stochastic
analysis, and software verification tools. We demonstrate the molecular
watchdog's functionality by having it monitor a molecular oscillator. Both the
molecular watchdog timer and the oscillator are implemented as chemical
reaction networks, which are the current programming language of choice for
many molecular programming applications.",website timer
http://arxiv.org/abs/1201.5190v1,"Ubiquitously during experiments one encounters a situation where time lapse
between two events has to measured. For example during the oscillations of a
pendulum or a vibrating reed, the powering of a lamp and achieving of its full
intensity. The powering of a relay and the closure of its contacts etc.
Situations like these call for a time measuring device between two events.
Hence this article describes a general Bi-Event timer that can be used in a
physics lab for ubiquitous time lapse measurements during experiments. These
measurements in turn can be used to interpret other parameters like velocity,
acceleration etc. The timer described here is simple to build and accurate in
performance. The Bi-event occurence can be applied as a signal to the inputs of
the timer either on separate lines or along a single path in series as voltage
pulses.",website timer
http://arxiv.org/abs/0908.1437v1,"Experiments in mechanics can often be timed by the sounds they produce. In
such cases, digital audio recordings provide a simple way of measuring time
intervals with an accuracy comparable to that of photogate timers. We
illustrate this with an experiment in the physics of sports: to measure the
speed of a hard-kicked soccer ball.",website timer
http://arxiv.org/abs/1104.0064v1,"I detail applications of timer interrupts in a popular micro-controller
family to time critical applications in laser-cooling type experiments. I
demonstrate a low overhead 1-bit frequency locking scheme and a multichannel
experimental sequencer using the timer-counter intterrupts to achieve accurate
timing along with flexible interfaces. The general purpose nature of
micro-controllers can offer unique functionality compared with commercial
solutions due to the flexibility of a computer controlled interface without the
poor latencies associated with computer timing.",website timer
http://arxiv.org/abs/1108.1361v1,"In this article, we examine the Location Management costs in mobile
communication networks utilizing the timer-based method. From the study of the
probabilities that a mobile terminal changes a number of Location Areas between
two calls, we identify a threshold value of 0.7 for the Call-to-Mobility Ratio
(CMR) below which the application of the timer-based method is most
appropriate. We characterize the valley appearing in the evolution of the costs
with the timeout period, showing that the time interval required to reach 90%
of the stabilized costs grows with the mobility index, the paging cost per
Location Area and the movement dimension, in opposition to the behavior
presented by the time interval that achieves the minimum of the costs. The
results obtained for CMRs below the suggested 0.7 threshold show that the
valley appearing in the costs tends to disappear for CMRs within [0.001, 0.7]
in onedimensional movements and within [0.2, 0.7] in two-dimensional ones, and
when the normalized paging cost per Location Area is below 0.3.",website timer
http://arxiv.org/abs/1902.06040v1,"Due to a hard dependency between time steps, large-scale simulations of gas
using the Direct Simulation Monte Carlo (DSMC) method proceed at the pace of
the slowest processor. Scalability is therefore achievable only by ensuring
that the work done each time step is as evenly apportioned among the processors
as possible. Furthermore, as the simulated system evolves, the load shifts, and
thus this load-balancing typically needs to be performed multiple times over
the course of a simulation. Common methods generally use either crude
performance models or processor-level timers. We combine both to create a
timer-augmented cost function which both converges quickly and yields
well-balanced processor decompositions. When compared to a particle-based
performance model alone, our method achieves 2x speedup at steady-state on up
to 1024 processors for a test case consisting of a Mach 9 argon jet impacting a
solid wall.",website timer
http://arxiv.org/abs/1906.10860v2,"As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases.",website timer
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",website timer
http://arxiv.org/abs/1809.07686v1,"This paper explores how to analyze empirically a network of website visitors
from several countries in the world. While exploring this huge network of
website visitors worldwide, this paper shows an empirical data analysis with a
visualization of how data has been analyzed and interpreted. By evaluating the
methods used in analyzing and interpreting these data, this paper provides the
required knowledge to empirically analyze a set of various obtained data from
website visitors with different browsers and IP-addresses. Keywords: Website
Data Analysis, Website Communities, Visualization",website timer
http://arxiv.org/abs/1701.01654v2,"Washing machine is of great domestic necessity as it frees us from the burden
of washing our clothes and saves ample of our time. This paper will cover the
aspect of designing and developing of Fuzzy Logic based, Smart Washing Machine.
The regular washing machine (timer based) makes use of multi-turned timer based
start-stop mechanism which is mechanical as is prone to breakage. In addition
to its starting and stopping issues, the mechanical timers are not efficient
with respect of maintenance and electricity usage. Recent developments have
shown that merger of digital electronics in optimal functionality of this
machine is possible and nowadays in practice. A number of international
renowned companies have developed the machine with the introduction of smart
artificial intelligence. Such a machine makes use of sensors and smartly
calculates the amount of run-time (washing time) for the main machine motor.
Realtime calculations and processes are also catered in optimizing the run-time
of the machine. The obvious result is smart time management, better economy of
electricity and efficiency of work. This paper deals with the indigenization of
FLC (Fuzzy Logic Controller) based Washing Machine, which is capable of
automating the inputs and getting the desired output (wash-time).",website timer
http://arxiv.org/abs/1711.03941v3,"Caching algorithms are usually described by the eviction method and analyzed
using a metric of hit probability. Since contents have different importance
(e.g. popularity), the utility of a high hit probability, and the cost of
transmission can vary across contents. In this paper, we consider timer-based
(TTL) policies across a cache network, where contents have differentiated
timers over which we optimize. Each content is associated with a utility
measured in terms of the corresponding hit probability. We start our analysis
from a linear cache network: we propose a utility maximization problem where
the objective is to maximize the sum of utilities and a cost minimization
problem where the objective is to minimize the content transmission cost across
the network. These frameworks enable us to design online algorithms for cache
management, for which we prove achieving optimal performance. Informed by the
results of our analysis, we formulate a non-convex optimization problem for a
general cache network. We show that the duality gap is zero, hence we can
develop a distributed iterative primal-dual algorithm for content management in
the network. Numerical evaluations show that our algorithm significant
outperforms path replication with traditional caching algorithms over some
network topologies. Finally, we consider a direct application of our cache
network model to content distribution.",website timer
http://arxiv.org/abs/1902.10369v3,"We consider the task of measuring time with probabilistic threshold gates
implemented by bio-inspired spiking neurons. In the model of spiking neural
networks, network evolves in discrete rounds, where in each round, neurons fire
in pulses in response to a sufficiently high membrane potential. This potential
is induced by spikes from neighboring neurons that fired in the previous round,
which can have either an excitatory or inhibitory effect. We first consider a
deterministic implementation of a neural timer and show that $\Theta(\log t)$
(deterministic) threshold gates are both sufficient and necessary. This raised
the question of whether randomness can be leveraged to reduce the number of
neurons. We answer this question in the affirmative by considering neural
timers with spiking neurons where the neuron $y$ is required to fire for $t$
consecutive rounds with probability at least $1-\delta$, and should stop firing
after at most $2t$ rounds with probability $1-\delta$ for some input parameter
$\delta \in (0,1)$. Our key result is a construction of a neural timer with
$O(\log\log 1/\delta)$ spiking neurons. Interestingly, this construction uses
only one spiking neuron, while the remaining neurons can be deterministic
threshold gates. We complement this construction with a matching lower bound of
$\Omega(\min\{\log\log 1/\delta, \log t\})$ neurons. This provides the first
separation between deterministic and randomized constructions in the setting of
spiking neural networks. Finally, we demonstrate the usefulness of compressed
counting networks for synchronizing neural networks.",website timer
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",website timer
http://arxiv.org/abs/1811.00923v1,"Shared Web Hosting service enables hosting multitude of websites on a single
powerful server. It is a well-known solution as many people share the overall
cost of server maintenance and also, website owners do not need to deal with
administration issues is not necessary for website owners. In this paper, we
illustrate how shared web hosting service works and demonstrate the security
weaknesses rise due to the lack of proper isolation between different websites,
hosted on the same server. We exhibit two new server-side attacks against the
log file whose objectives are revealing information of other hosted websites
which are considered to be private and arranging other complex attacks. In the
absence of isolated log files among websites, an attacker controlling a website
can inspect and manipulate contents of the log file. These attacks enable an
attacker to disclose file and directory structure of other websites and launch
other sorts of attacks. Finally, we propose several countermeasures to secure
shared web hosting servers against the two attacks subsequent to the separation
of log files for each website.",website timer
http://arxiv.org/abs/1305.4018v1,"Many online video websites provide the shortcut links to facilitate the video
sharing to other websites especially to the online social networks (OSNs). Such
video sharing behavior greatly changes the interplays between the two types of
websites. For example, users in OSNs may watch and re-share videos shared by
their friends from online video websites, and this can also boost the
popularity of videos in online video websites and attract more people to watch
and share them. Characterizing these interplays can provide great insights for
understanding the relationships among online video websites, OSNs, ISPs and so
on. In this paper we conduct empirical experiments to study the interplays
between video sharing websites and OSNs using three totally different data
sources: online video websites, OSNs, and campus network traffic. We find that,
a) there are many factors that can affect the external sharing probability of
videos in online video websites. b) The popularity of a video itself in online
video websites can greatly impact on its popularity in OSNs. Videos in Renren,
Qzone (the top two most popular Chinese OSNs) usually attract more viewers than
in Sina and Tencent Weibo (the top two most popular Chinese microblogs), which
indicates the different natures of the two kinds of OSNs. c) The analysis based
on real traffic data illustrates that 10\% of video flows are related to OSNs,
and they account for 25\% of traffic generated by all videos.",website timer
http://arxiv.org/abs/cs/0701015v2,"We consider the problem of failure detection in dynamic networks such as
MANETs. Unreliable failure detectors are classical mechanisms which provide
information about process failures. However, most of current implementations
consider that the network is fully connected and that the initial number of
nodes of the system is known. This assumption is not applicable to dynamic
environments. Furthermore, such implementations are usually timer-based while
in dynamic networks there is no upper bound for communication delays since
nodes can move. This paper presents an asynchronous implementation of a failure
detector for unknown and mobile networks. Our approach does not rely on timers
and neither the composition nor the number of nodes in the system are known. We
prove that our algorithm can implement failure detectors of class <>S when
behavioral properties and connectivity conditions are satisfied by the
underlying system.",website timer
http://arxiv.org/abs/0705.3015v1,"Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.",website timer
http://arxiv.org/abs/0910.0316v1,"Rate based transport protocol determines the rate of data transmission
between the sender and receiver and then sends the data according to that rate.
To notify the rate to the sender, the receiver sends ACKplusRate packet based
on epoch timer expiry. In this paper, through detailed arguments and simulation
it is shown that the transmission of ACKplusRate packet based on epoch timer
expiry consumes more energy in network with low mobility. To overcome this
problem, a new technique called Dynamic Rate Feedback (DRF) is proposed. DRF
sends ACKplusRate whenever there is a change in rate of (plus or minus) 25
percent than the previous rate. Based on ns2 simulation DRF is compared with a
reliable transport protocol for ad hoc network (ATP)",website timer
http://arxiv.org/abs/1009.4992v1,"The main objective of this work is to design and construct a microcomputer
based system: to control electric appliances such as light, fan, heater,
washing machine, motor, TV, etc. The paper discusses two major approaches to
control home appliances. The first involves controlling home appliances using
timer option. The second approach is to control home appliances using voice
command. Moreover, it is also possible to control appliances using Graphical
User Interface. The parallel port is used to transfer data from computer to the
particular device to be controlled. An interface box is designed to connect the
high power loads to the parallel port. This system will play an important role
for the elderly and physically disable people to control their home appliances
in intuitive and flexible way. We have developed a system, which is able to
control eight electric appliances properly in these three modes.",website timer
http://arxiv.org/abs/0709.2618v2,"Wireless Sensor Networks research and demand are now in full expansion, since
people came to understand these are the key to a large number of issues in
industry, commerce, home automation, healthcare, agriculture and environment,
monitoring, public safety etc. One of the most challenging research problems in
sensor networks research is power awareness and power-saving techniques. In
this master's thesis, we have studied one particular power-saving technique,
i.e. frequency scaling. In particular, we analysed the close relationship
between clock frequencies in a microcontroller and several types of constraints
imposed on these frequencies, e.g. by other components of the microcontroller,
by protocol specifications, by external factors etc. Among these constraints,
we were especially interested in the ones imposed by the timer service and by
the serial ports' transmission rates. Our efforts resulted in a microcontroller
configuration management tool which aims at assisting application programmers
in choosing microcontroller configurations, in function of the particular needs
and constraints of their application.",website timer
http://arxiv.org/abs/1502.00050v1,"To circumvent the FLP impossibility result in a deterministic way several
protocols have been proposed on top of an asynchronous distributed system
enriched with additional assumptions. In the context of Byzantine failures for
systems where at most t processes may exhibit a Byzantine behavior, two
approaches have been investigated to solve the consensus problem.The first,
relies on the addition of synchrony, called Timer-Based, but the second is
based on the pattern of the messages that are exchanged, called Time-Free. This
paper shows that both types of assumptions are not antagonist and can be
combined to solve authenticated Byzantine consensus. This combined assumption
considers a correct process pi, called 2t-BW, and a set X of 2t processes such
that, eventually, for each query broadcasted by a correct process pj of X, pj
receives a response from pi 2 X among the (n- t) first responses to that query
or both links connecting pi and pj are timely. Based on this combination, a
simple hybrid authenticated Byzantine consensus protocol,benefiting from the
best of both worlds, is proposed. Whereas many hybrid protocols have been
designed for the consensus problem in the crash model, this is, to our
knowledge, the first hybrid deterministic solution to the Byzantine consensus
problem.",website timer
http://arxiv.org/abs/cs/0512069v1,"Backup or preservation of websites is often not considered until after a
catastrophic event has occurred. In the face of complete website loss, ""lazy""
webmasters or concerned third parties may be able to recover some of their
website from the Internet Archive. Other pages may also be salvaged from
commercial search engine caches. We introduce the concept of ""lazy
preservation""- digital preservation performed as a result of the normal
operations of the Web infrastructure (search engines and caches). We present
Warrick, a tool to automate the process of website reconstruction from the
Internet Archive, Google, MSN and Yahoo. Using Warrick, we have reconstructed
24 websites of varying sizes and composition to demonstrate the feasibility and
limitations of website reconstruction from the public Web infrastructure. To
measure Warrick's window of opportunity, we have profiled the time required for
new Web resources to enter and leave search engine caches.",website timer
http://arxiv.org/abs/cs/0608034v1,"Technical security is only part of E-Commerce security operations; human
usability and security perception play major and sometimes dominating factors.
For instance, slick websites with impressive security icons but no real
technical security are often perceived by users to be trustworthy (and thus
more profitable) than plain vanilla websites that use powerful encryption for
transmission and server protection. We study one important type of E-Commerce
transaction website, E-Tax Filing, that is exposed to large populations. We
assess a large number of international (5), Federal (USA), and state E-Tax
filing websites (38) for both technical security protection and human
perception of security. As a result of this assessment, we identify security
best practices across these E-Tax Filing websites and recommend additional
security techniques that have not been found in current use by E-Tax Filing
websites.",website timer
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",website timer
http://arxiv.org/abs/1711.03656v2,"Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.",website timer
http://arxiv.org/abs/1801.04829v2,"This paper presents a pilot study on developing an instrument to predict the
quality of e-commerce websites. The 8C model was adopted as the reference model
of the heuristic evaluation. Each dimension of the 8C was mapped into a set of
quantitative website elements, selected websites were scraped to get the
quantitative website elements, and the score of each dimension was calculated.
A software was developed in PHP for the experiments. In the training process,
10 experiments were conducted and quantitative analyses were regressively
conducted between the experiments. The conversion rate was used to verify the
heuristic evaluation of an e-commerce website after each experiment. The
results showed that the mapping revisions between the experiments improved the
performance of the evaluation instrument, therefore the experiment process and
the quantitative mapping revision guideline proposed was on the right track.
The software resulted from the experiment 10 can serve as the aimed e-commerce
website evaluation instrument. The experiment results and the future work have
been discussed.",website timer
http://arxiv.org/abs/1904.05738v1,"Common privacy enhancing technologies fail to effectively hide certain
statistical aspects of encrypted traffic, namely individual packets length,
packets direction and, packets timing. Recent researches have shown that using
such attributes, an adversary is able to extract various information from the
encrypted traffic such as the visited website and used protocol. Such attacks
are called traffic analysis. Proposed countermeasures attempt to change the
distribution of such features. however, either they fail to effectively reduce
attacker's accuracy or do so while enforcing high bandwidth overhead and timing
delay. In this paper, through the use of a predefined set of clustered traces
of websites and a greedy packet morphing algorithm, we introduce a website
fingerprinting countermeasure called TG-PSM. Firstly, this method clusters
websites based on their behavior in different phases of loading. Secondly, it
finds a suitable target site for any visiting website based on user indicated
importance degree; thus providing dynamic tunability. Thirdly, this method
morphs the given website to the target website using a greedy algorithm
considering the distance and the resulted overhead. Our evaluations show that
TG-PSM outperforms previous countermeasures regarding attacker accuracy
reduction and enforced bandwidth, e.g., reducing bandwidth overhead over 40%
while maintaining attacker's accuracy.",website timer
http://arxiv.org/abs/1202.3987v1,"Malware spread among websites and between websites and clients is an
increasing problem. Search engines play an important role in directing users to
websites and are a natural control point for intervening, using mechanisms such
as blacklisting. The paper presents a simple Markov model of malware spread
through large populations of websites and studies the effect of two
interventions that might be deployed by a search provider: blacklisting
infected web pages by removing them from search results entirely and a
generalization of blacklisting, called depreferencing, in which a website's
ranking is decreased by a fixed percentage each time period the site remains
infected. We analyze and study the trade-offs between infection exposure and
traffic loss due to false positives (the cost to a website that is incorrectly
blacklisted) for different interventions. As expected, we find that
interventions are most effective when websites are slow to remove infections.
Surprisingly, we also find that low infection or recovery rates can increase
traffic loss due to false positives. Our analysis also shows that heavy-tailed
distributions of website popularity, as documented in many studies, leads to
high sample variance of all measured outcomes. These result implies that it
will be difficult to determine empirically whether certain website
interventions are effective, and it suggests that theoretical models such as
the one described in this paper have an important role to play in improving web
security.",website timer
http://arxiv.org/abs/1111.4692v1,"The European Spreadsheet Risks Interest Group (EuSpRIG) has maintained a
website almost since its inception in 2000. We present here longitudinal and
cross-sectional statistics from the website log in order to shed some light
upon end-user activity in the EuSpRIG domain.",website timer
http://arxiv.org/abs/1908.07753v1,"City authorities need to analyze urban geospatial data to improve
transportation and infrastructure. Current tools do not address the exploratory
and interactive nature of these analyses and in many cases consult the raw data
to compute query results. While pre-aggregation and materializing intermediate
query results is common practice in many OLAP settings, it is rarely used to
speed up geospatial queries. We introduce GeoBlocks, a pre-aggregating,
query-driven storage layout for geospatial point data that can provide
approximate, yet precision-bounded aggregation results over arbitrary query
polygons. GeoBlocks adapt to the skew naturally present in query workloads to
improve query performance over time. In summary, GeoBlocks outperform
on-the-fly aggregation by up to several orders of magnitude, providing the
sub-second query latencies required for interactive analytics.",geoblocking
http://arxiv.org/abs/1805.03741v1,"We consider the 4-block $n$-fold integer programming (IP), in which the
constraint matrix consists of $n$ copies of small matrices $A$, $B$, $D$ and
one copy of $C$ in a specific block structure. We prove that, the
$\ell_{\infty}$-norm of the Graver basis elements of 4-block $n$-fold IP is
upper bounded by $O_{FPT}(n^{s_c})$ where $s_c$ is the number of rows of matrix
$C$ and $O_{FPT}$ hides a multiplicative factor that is only dependent on the
parameters of the small matrices $A,B,C,D$ (i.e., the number of rows and
columns, and the largest absolute value among the entries). This improves upon
the existing upper bound of $O_{FPT}(n^{2^{s_c}})$. We provide a matching lower
bounded of $\Omega(n^{s_c})$, which even holds for an arbitrary non-zero
integral element in the kernel space. We then consider a special case of
4-block $n$-fold in which $C$ is a zero matrix (called 3-block $n$-fold IP). We
show that, surprisingly, 3-block $n$-fold IP admits a Hilbert basis whose
$\ell_{\infty}$-norm is bounded by $O_{FPT}(1)$, despite the fact that the
$\ell_{\infty}$-norm of its Graver basis elements is still $\Omega(n)$.
Finally, we provide upper bounds on the $\ell_{\infty}$-norm of Graver basis
elements for 3-block $n$-fold IP. Based on these upper bounds, we establish
algorithms for 3-block $n$-fold IP and provide improved algorithms for 4-block
$n$-fold IP.",ip blocking
http://arxiv.org/abs/0812.2559v1,"Maximum Likelihood (ML) decoding is the optimal decoding algorithm for
arbitrary linear block codes and can be written as an Integer Programming (IP)
problem. Feldman et al. relaxed this IP problem and presented Linear
Programming (LP) based decoding algorithm for linear block codes. In this
paper, we propose a new IP formulation of the ML decoding problem and solve the
IP with generic methods. The formulation uses indicator variables to detect
violated parity checks. We derive Gomory cuts from our formulation and use them
in a separation algorithm to find ML codewords. We further propose an efficient
method of finding cuts induced by redundant parity checks (RPC). Under certain
circumstances we can guarantee that these RPC cuts are valid and cut off the
fractional optimal solutions of LP decoding. We demonstrate on two LDPC codes
and one BCH code that our separation algorithm performs significantly better
than LP decoding.",ip blocking
http://arxiv.org/abs/1205.4487v1,"The Network-on-chip (NoC) designs consisting of large pack of Intellectual
Property (IP) blocks (cores) on the same silicon die is becoming technically
possible nowadays. But, the communication between the IP Cores is the main
issue in recent years. This paper presents the design of a Code Division
Multiple Access (CDMA) based wrapper interconnect as a component of System on
programmable chip (SOPC) builder to communicate between IP cores. In the
proposal, only bus lines that carry address and data signals are CDMA coded.
CDMA technology has better data integrity, channel continuity, channel
isolation, and also mainly it reduces the no.of lines in the bus architecture
for transmitting the data from master to slave.",ip blocking
http://arxiv.org/abs/1901.01135v2,"We consider so called $2$-stage stochastic integer programs (IPs) and their
generalized form of multi-stage stochastic IPs. A $2$-stage stochastic IP is an
integer program of the form $\max \{ c^T x \mid Ax = b, l \leq x \leq u, x \in
\mathbb{Z}^{nt + s} \}$ where the constraint matrix $A \in \mathbb{Z}^{r \times
s}$ consists roughly of $n$ repetition of a block matrix $A$ on the vertical
line and $n$ repetitions of a matrix $B \in \mathbb{Z}^{r \times t}$ on the
diagonal. In this paper we improve upon an algorithmic result by Hemmecke and
Schultz form 2003 to solve $2$-stage stochastic IPs. The algorithm is based on
the Graver augmentation framework where our main contribution is to give an
explicit doubly exponential bound on the size of the augmenting steps. The
previous bound for the size of the augmenting steps relied on non-constructive
finiteness arguments from commutative algebra and therefore only an implicit
bound was known that depends on parameters $r,s,t$ and $\Delta$, where $\Delta$
is the largest entry of the constraint matrix. Our new improved bound however
is obtained by a novel theorem which argues about the intersection of paths in
a vector space. As a result of our new bound we obtain an algorithm to solve
$2$-stage stochastic IPs in time $poly(n,t) \cdot f(r,s,\Delta)$, where $f$ is
a doubly exponential function. To complement our result, we also prove a doubly
exponential lower bound for the size of the augmenting steps.",ip blocking
http://arxiv.org/abs/0811.3828v1,"How can we protect the network infrastructure from malicious traffic, such as
scanning, malicious code propagation, and distributed denial-of-service (DDoS)
attacks? One mechanism for blocking malicious traffic is filtering: access
control lists (ACLs) can selectively block traffic based on fields of the IP
header. Filters (ACLs) are already available in the routers today but are a
scarce resource because they are stored in the expensive ternary content
addressable memory (TCAM).
  In this paper, we develop, for the first time, a framework for studying
filter selection as a resource allocation problem. Within this framework, we
study five practical cases of source address/prefix filtering, which correspond
to different attack scenarios and operator's policies. We show that filter
selection optimization leads to novel variations of the multidimensional
knapsack problem and we design optimal, yet computationally efficient,
algorithms to solve them. We also evaluate our approach using data from
Dshield.org and demonstrate that it brings significant benefits in practice.
Our set of algorithms is a building block that can be immediately used by
operators and manufacturers to block malicious traffic in a cost-efficient way.",ip blocking
http://arxiv.org/abs/1809.09086v2,"Tor and I2P are well-known anonymity networks used by many individuals to
protect their online privacy and anonymity. Tor's centralized directory
services facilitate the understanding of the Tor network, as well as the
measurement and visualization of its structure through the Tor Metrics project.
In contrast, I2P does not rely on centralized directory servers, and thus
obtaining a complete view of the network is challenging. In this work, we
conduct an empirical study of the I2P network, in which we measure properties
including population, churn rate, router type, and the geographic distribution
of I2P peers. We find that there are currently around 32K active I2P peers in
the network on a daily basis. Of these peers, 14K are located behind NAT or
firewalls.
  Using the collected network data, we examine the blocking resistance of I2P
against a censor that wants to prevent access to I2P using address-based
blocking techniques. Despite the decentralized characteristics of I2P, we
discover that a censor can block more than 95% of peer IP addresses known by a
stable I2P client by operating only 10 routers in the network. This amounts to
severe network impairment: a blocking rate of more than 70% is enough to cause
significant latency in web browsing activities, while blocking more than 90% of
peer IP addresses can make the network unusable. Finally, we discuss the
security consequences of the network being blocked, and directions for
potential approaches to make I2P more resistant to blocking.",ip blocking
http://arxiv.org/abs/1107.5372v1,"Both IP lookup and packet classification in IP routers can be implemented by
some form of tree traversal. SRAM-based Pipelining can improve the throughput
dramatically. However, previous pipelining schemes result in unbalanced memory
allocation over the pipeline stages. This has been identified as a major
challenge for scalable pipelined solutions. This paper proposes a flexible
bidirectional linear pipeline architecture based on widely-used dual-port
SRAMs. A search tree is partitioned, and then mapped onto pipeline stages by a
bidirectional fine-grained mapping scheme. We introduce the notion of inversion
factor and several heuristics to invert subtrees for memory balancing. Due to
its linear structure, the architecture maintains packet input order, and
supports non-blocking route updates. Our experiments show that, the
architecture can achieve a perfectly balanced memory distribution over the
pipeline stages, for both trie-based IP lookup and tree-based multi-dimensional
packet classification. For IP lookup, it can store a full backbone routing
table with 154419 entries using 2MB of memory, and sustain a high throughput of
1.87 billion packets per second (GPPS), i.e. 0.6 Tbps for the minimum size (40
bytes) packets. The throughput can be improved further to be 2.4 Tbps, by
employing caching to exploit the Internet traffic locality.",ip blocking
http://arxiv.org/abs/1110.1753v1,"The challenging number is used for the detection of Spoofing attack. The IP
Spoofing is considered to be one of the potentially brutal attack which acts as
a tool for the DDoS attack which is considered to be a major threat among
security problems in today's internet. These kinds of attack are extremely
severe. They bring down business of company drastically. DDoS attack can easily
exhaust the computing and communication resources of its victim within a short
period of time. There are attacks exploiting some vulnerability or
implementation bug in the software implementation of a service to bring that
down and some attacks will use all the available resources at the target
machine. This deals on attacks that consume all the bandwidth available to the
victim machine. While concentrating on the bandwidth attack the TCP SYN flood
is the more prominent attack. TCP/IP protocol suite is the most widely used
protocol suite for data communication. The TCP SYN flood works by exhausting
the TCP connection queue of the host and thus denying legitimate connection
request. There are various methods used to detect and prevent this attack, one
of which is to block the packet based on SYN flag count from the same IP
address. This kind of prevention methods becomes unsuitable when the attackers
use the Spoofed IP address. The SYN spoofing becomes a major tool the TCP SYN
flooding. For the prevention of this kind of attacks, the TCP specific probing
is used in the proposed scheme where the client is requested challenging number
while sending the ACK in the three way hand shake. This is very useful to find
the Spoofed IP Packets/TCP SYN flood and preventing them.",ip blocking
http://arxiv.org/abs/1910.01519v1,"A massive threat to the modern and complex IC production chain is the use of
untrusted off-shore foundries which are able to infringe valuable hardware
design IP or to inject hardware Trojans causing severe loss of safety and
security. Similarly, market dominating SRAM-based FPGAs are vulnerable to both
attacks since the crucial gate-level netlist can be retrieved even in field for
the majority of deployed device series. In order to perform IP infringement or
Trojan injection, reverse engineering (parts of) the hardware design is
necessary to understand its internal workings. Even though IP protection and
obfuscation techniques exist to hinder both attacks, the security of most
techniques is doubtful since realistic capabilities of reverse engineering are
often neglected. The contribution of our work is twofold: first, we carefully
review an IP watermarking scheme tailored to FPGAs and improve its security by
using opaque predicates. In addition, we show novel reverse engineering
strategies on proposed opaque predicate implementations that again enables to
automatically detect and alter watermarks. Second, we demonstrate automatic
injection of hardware Trojans specifically tailored for third-party
cryptographic IP gate-level netlists. More precisely, we extend our
understanding of adversary's capabilities by presenting how block and stream
cipher implementations can be surreptitiously weakened.",ip blocking
http://arxiv.org/abs/1802.06289v1,"We consider integer programming problems $\max \{ c^T x : \mathcal{A} x = b,
l \leq x \leq u, x \in \mathbb{Z}^{nt}\}$ where $\mathcal{A}$ has a (recursive)
block-structure generalizing ""$n$-fold integer programs"" which recently
received considerable attention in the literature. An $n$-fold IP is an integer
program where $\mathcal{A}$ consists of $n$ repetitions of submatrices $A \in
\mathbb{Z}^{r \times t}$ on the top horizontal part and $n$ repetitions of a
matrix $B \in \mathbb{Z}^{s \times t}$ on the diagonal below the top part.
Instead of allowing only two types of block matrices, one for the horizontal
line and one for the diagonal, we generalize the $n$-fold setting to allow for
arbitrary matrices in every block. We show that such an integer program can be
solved in time $n^2 t^2 {\phi} \cdot (rs{\Delta})^{\mathcal{O}(rs^2+ sr^2)}$
(ignoring logarithmic factors). Here ${\Delta}$ is an upper bound on the
largest absolute value of an entry of $\mathcal{A}$ and ${\phi}$ is the largest
binary encoding length of a coefficient of $c$. This improves upon the
previously best algorithm of Hemmecke, Onn and Romanchuk that runs in time
$n^3t^3 {\phi} \cdot {\Delta}^{\mathcal{O}(t^2s)}$. In particular, our
algorithm is not exponential in the number $t$ of columns of $A$ and $B$.
  Our algorithm is based on a new upper bound on the $l_1$-norm of an element
of the ""Graver basis"" of an integer matrix and on a proximity bound between the
LP and IP optimal solutions tailored for IPs with block structure. These new
bounds rely on the ""Steinitz Lemma"".
  Furthermore, we extend our techniques to the recently introduced ""tree-fold
IPs"", where we again present a more efficient algorithm in a generalized
setting.",ip blocking
http://arxiv.org/abs/1105.1967v1,"An algebra-logical repair method for FPGA functional logic blocks on the
basis of solving the coverage problem is proposed. It is focused on
implementation into Infrastructure IP for system-on-a chip and
system-in-package. A method is designed for providing the operability of FPGA
blocks and digital system as a whole. It enables to obtain exact and optimal
solution associated with the minimum number of spares needed to repair the FPGA
logic components with multiple faults.",ip blocking
http://arxiv.org/abs/0710.4805v1,"The idea of design domain specific Mother Model of IP block family as a base
of modeling of system integration is presented here. A common reconfigurable
Mother Model for ten different standardized digital OFDM transmitters has been
developed. By means of a set of parameters, the mother model can be
reconfigured to any of the ten selected standards. So far the applicability of
the proposed reconfiguration and analog-digital co-modeling methods have been
proved by modeling the function of the digital parts of three, 802.11a, ADSL
and DRM, transmitters in an RF system simulator. The model is intended to be
used as signal source template in RF system simulations. The concept is not
restricted to signal sources, it can be applied to any IP block development.
The idea of the Mother Model will be applied in other design domains to prove
that in certain application areas, OFDM transceivers in this case, the design
process can progress simultaneously in different design domains - mixed signal,
system and RTL-architectural - without the need of high-level synthesis. Only
the Mother Models of three design domains are needed to be formally proved to
function as specified.",ip blocking
http://arxiv.org/abs/1112.4018v1,"Mobile IP is an open standard, defined by the Internet Engineering Task Force
(IETF) RFC 3220. By using Mobile IP, you can keep the same IP address, stay
connected, and maintain ongoing applications while roaming between IP networks.
Mobile IP is scalable for the Internet because it is based on IP - any media
that can support IP can support Mobile IP.",ip blocking
http://arxiv.org/abs/1404.3465v2,"Mobile devices are in roles where the integrity and confidentiality of their
apps and data are of paramount importance. They usually contain a
System-on-Chip (SoC), which integrates microprocessors and peripheral
Intellectual Property (IP) connected by a Network-on-Chip (NoC). Malicious IP
or software could compromise critical data. Some types of attacks can be
blocked by controlling data transfers on the NoC using Memory Management Units
(MMUs) and other access control mechanisms. However, commodity processors do
not provide strong assurances regarding the correctness of such mechanisms, and
it is challenging to verify that all access control mechanisms in the system
are correctly configured. We propose a NoC Firewall (NoCF) that provides a
single locus of control and is amenable to formal analysis. We demonstrate an
initial analysis of its ability to resist malformed NoC commands, which we
believe is the first effort to detect vulnerabilities that arise from NoC
protocol violations perpetrated by erroneous or malicious IP.",ip blocking
http://arxiv.org/abs/1203.1673v2,"A key challenge in censorship-resistant web browsing is being able to direct
legitimate users to redirection proxies while preventing censors, posing as
insiders, from discovering their addresses and blocking them. We propose a new
framework for censorship-resistant web browsing called {\it CensorSpoofer} that
addresses this challenge by exploiting the asymmetric nature of web browsing
traffic and making use of IP spoofing. CensorSpoofer de-couples the upstream
and downstream channels, using a low-bandwidth indirect channel for delivering
outbound requests (URLs) and a high-bandwidth direct channel for downloading
web content. The upstream channel hides the request contents using
steganographic encoding within email or instant messages, whereas the
downstream channel uses IP address spoofing so that the real address of the
proxies is not revealed either to legitimate users or censors. We built a
proof-of-concept prototype that uses encrypted VoIP for this downstream channel
and demonstrated the feasibility of using the CensorSpoofer framework in a
realistic environment.",ip blocking
http://arxiv.org/abs/1401.6370v1,"By using the dynamic reconfigurable transceiver in high speed interface
design, designer can solve critical technology problems such as ensuring signal
integrity conveniently, with lower error binary rate. In this paper, we
designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to
transparently extend the physical reach of the XGMII. The following points are
focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two
transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design
which is featured of dynamic reconfiguration performance, that is,
ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by
changing settings of the devices without power off. These two blocks can
accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium
Attachment), however, with higher efficiency and reliability. (2) 1+1
protection. In our design, two ALTGX IP cores are used to work in parallel,
which named XAUI0 and XAUI1. The former works as the main channel while the
latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1
will start to work to keep the business. (3) RTL (Register Transfer Level)
coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and
ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then
connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the
design was simulated on VCS simulator. The validated result indicates that the
packets are transferred efficiently. FPGA makes high-speed optical
communication system design simplified.",ip blocking
http://arxiv.org/abs/1707.09791v1,"A few years after standardization of the High Efficiency Video Coding (HEVC),
now the Joint Video Exploration Team (JVET) group is exploring post-HEVC video
compression technologies. In the intra prediction domain, this effort has
resulted in an algorithm with 67 internal modes, new filters and tools which
significantly improve HEVC. However, the improved algorithm still suffers from
the long distance prediction inaccuracy problem. In this paper, we propose an
In-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes
inner-block reconstructed pixels as references to reduce the distance from
predicted pixels. This is done by using the ILR signal for partially
reconstructing each pixel, right after its prediction and before its
block-level out-loop residual calculation. The ILR signal is decided in the
rate-distortion sense, by a brute-force search on a QP-dependent finite
codebook that is known to the decoder. Experiments show that the proposed
ILR-IP algorithm improves the existing method in the Joint Exploration Model
(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at
the decoder side.",ip blocking
http://arxiv.org/abs/0706.2824v1,"The re-use of pre-designed blocks is a well-known concept of the software
development. This technique has been applied to System-on-Chip (SoC) design
whose complexity and heterogeneity are growing. The re-use is made thanks to
high level components, called virtual components (IP), available in more or
less flexible forms. These components are dedicated blocks: digital signal
processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These
blocks rest on a model of fixed architecture with very few degrees of
personalization. This rigidity is particularly true for the communication
interface whose orders of acquisition and production of data, the temporal
behavior and protocols of exchanges are fixed. The successful integration of
such an IP requires that the designer (1) synchronizes the components (2)
converts the protocols between ""incompatible"" blocks (3) temporizes the data to
guarantee the temporal constraints and the order of the data. This phase
remains however very manual and source of errors. Our approach proposes a
formal modeling, based on an original Ressource Compatibility Graph. The
synthesis flow is based on a set of transformations of the initial graph to
lead to an interface architecture allowing the space-time adaptation of the
data exchanges between several components.",ip blocking
http://arxiv.org/abs/1904.04324v2,"Like many other user-generated content sites, Wikipedia blocks contributions
through open proxies like Tor, because of a perception among the Wikipedia
community that privacy enhancing tools are a source of vandalism, spam, and
abuse. While blocking editors that use these tools is done in an attempt to
stem abuse, collateral damage in the form of unrealized valuable contributions
from IP-anonymity-seeking editors is nearly invisible. Although Wikipedia has
taken steps to block contributions from Tor users since as early as 2005, we
demonstrate that these blocks have been imperfect and that thousands of
attempts to edit on Wikipedia through Tor have been successful. We draw upon
several data sources and analytical techniques to measure and describe the
history of Tor editing on Wikipedia over time and to compare contributions from
Tor users to those from other groups of Wikipedia users. Our analysis suggests
that Tor users who manage to slip through Wikipedia's ban contribute content
that is similar in quality to unregistered Wikipedia contributors and to the
initial contributions of registered users.",ip blocking
http://arxiv.org/abs/1802.09007v2,"In recent years, algorithmic breakthroughs in stringology, computational
social choice, scheduling, etc., were achieved by applying the theory of
so-called $n$-fold integer programming. An $n$-fold integer program (IP) has a
highly uniform block structured constraint matrix. Hemmecke, Onn, and Romanchuk
[Math. Programming, 2013] showed an algorithm with runtime $a^{O(rst + r^2s)}
n^3$, where $a$ is the largest coefficient, $r,s$, and $t$ are dimensions of
blocks of the constraint matrix and $n$ is the total dimension of the IP; thus,
an algorithm efficient if the blocks are of small size and with small
coefficients. The algorithm works by iteratively improving a feasible solution
with augmenting steps, and $n$-fold IPs have the special property that
augmenting steps are guaranteed to exist in a not-too-large neighborhood.
  We have implemented the algorithm and learned the following along the way.
The original algorithm is practically unusable, but we discover a series of
improvements which make its evaluation possible. Crucially, we observe that a
certain constant in the algorithm can be treated as a tuning parameter, which
yields an efficient heuristic (essentially searching in a
smaller-than-guaranteed neighborhood). Furthermore, the algorithm uses an
overly expensive strategy to find a ""best"" step, while finding only an
""approximatelly best"" step is much cheaper, yet sufficient for quick
convergence. Using this insight, we improve the asymptotic dependence on $n$
from $n^3$ to $n^2 \log n$.
  We show that decreasing the tuning parameter initially leads to an increased
number of iterations needed for convergence and eventually to getting stuck in
local optima, as expected. However, surprisingly small values of the parameter
already exhibit good behavior. Second, our new strategy for finding
""approximatelly best"" steps wildly outperforms the original construction.",ip blocking
http://arxiv.org/abs/1207.2683v2,"Open communication over the Internet poses a serious threat to countries with
repressive regimes, leading them to develop and deploy network-based censorship
mechanisms within their networks. Existing censorship circumvention systems
face different difficulties in providing unobservable communication with their
clients; this limits their availability and poses threats to their users. To
provide the required unobservability, several recent circumvention systems
suggest modifying Internet routers running outside the censored region to
intercept and redirect packets to censored destinations. However, these
approaches require modifications to ISP networks, and hence requires
cooperation from ISP operators and/or network equipment vendors, presenting a
substantial deployment challenge. In this report we propose a deployable and
unobservable censorship-resistant infrastructure, called FreeWave. FreeWave
works by modulating a client's Internet connections into acoustic signals that
are carried over VoIP connections. Such VoIP connections are targeted to a
server, FreeWave server, that extracts the tunneled traffic of clients and
proxies them to the uncensored Internet. The use of actual VoIP connections, as
opposed to traffic morphing, allows FreeWave to relay its VoIP connections
through oblivious VoIP nodes, hence keeping itself unblockable from censors
that perform IP address blocking. Also, the use of end-to-end encryption
prevents censors from identifying FreeWave's VoIP connections using packet
content filtering technologies, like deep-packet inspection. We prototype the
designed FreeWave system over the popular VoIP system of Skype. We show that
FreeWave is able to reliably achieve communication bandwidths that are
sufficient for web browsing, even when clients are far distanced from the
FreeWave server.",ip blocking
http://arxiv.org/abs/1412.5052v2,"The vulnerability of the Internet has been demonstrated by prominent IP
prefix hijacking events. Major outages such as the China Telecom incident in
2010 stimulate speculations about malicious intentions behind such anomalies.
Surprisingly, almost all discussions in the current literature assume that
hijacking incidents are enabled by the lack of security mechanisms in the
inter-domain routing protocol BGP. In this paper, we discuss an attacker model
that accounts for the hijacking of network ownership information stored in
Regional Internet Registry (RIR) databases. We show that such threats emerge
from abandoned Internet resources (e.g., IP address blocks, AS numbers). When
DNS names expire, attackers gain the opportunity to take resource ownership by
re-registering domain names that are referenced by corresponding RIR database
objects. We argue that this kind of attack is more attractive than conventional
hijacking, since the attacker can act in full anonymity on behalf of a victim.
Despite corresponding incidents have been observed in the past, current
detection techniques are not qualified to deal with these attacks. We show that
they are feasible with very little effort, and analyze the risk potential of
abandoned Internet resources for the European service region: our findings
reveal that currently 73 /24 IP prefixes and 7 ASes are vulnerable to be
stealthily abused. We discuss countermeasures and outline research directions
towards preventive solutions.",ip blocking
http://arxiv.org/abs/1205.4011v1,"We present practical poisoning and name-server block- ing attacks on standard
DNS resolvers, by off-path, spoofing adversaries. Our attacks exploit large DNS
responses that cause IP fragmentation; such long re- sponses are increasingly
common, mainly due to the use of DNSSEC. In common scenarios, where DNSSEC is
partially or incorrectly deployed, our poisoning attacks allow 'com- plete'
domain hijacking. When DNSSEC is fully de- ployed, attacker can force use of
fake name server; we show exploits of this allowing off-path traffic analy- sis
and covert channel. When using NSEC3 opt-out, attacker can also create fake
subdomains, circumvent- ing same origin restrictions. Our attacks circumvent
resolver-side defenses, e.g., port randomisation, IP ran- domisation and query
randomisation. The (new) name server (NS) blocking attacks force re- solver to
use specific name server. This attack allows Degradation of Service,
traffic-analysis and covert chan- nel, and also facilitates DNS poisoning. We
validated the attacks using standard resolver soft- ware and standard DNS name
servers and zones, e.g., org.",ip blocking
http://arxiv.org/abs/1202.4530v1,"The Internet Threat Monitoring (ITM) is an efficient monitoring system used
globally to measure, detect, characterize and track threats such as denial of
service (DoS) and distributed Denial of Service (DDoS) attacks and worms. . To
block the monitoring system in the internet the attackers are targeted the ITM
system. In this paper we address the flooding attack of DDoS against ITM
monitors to exhaust the network resources, such as bandwidth, computing power,
or operating system data structures by sending the malicious traffic. We
propose an information-theoretic frame work that models the flooding attacks
using Botnet on ITM. One possible way to counter DDoS attacks is to trace the
attack sources and punish the perpetrators. we propose a novel traceback method
for DDoS using Honeypots. IP tracing through honeypot is a single packet
tracing method and is more efficient than commonly used packet marking
techniques.",ip blocking
http://arxiv.org/abs/1611.03252v1,"Current tools and systems of detecting vulnerabilities simply alert the
administrator of attempted attacks against his network or system. However,
generally, the huge number of alerts to analyze and the amount time required to
update security rules after analyzing alerts provides time and opportunity for
the attacker to inflict damages. Moreover, most of these tools generate
positive and negative falses, which may be important to the attacked network.
Otherwise, many solutions exist such as IPS, but it shows a great defect due,
fundamentally, to false positives. Indeed, attackers often make IPS block a
legitimate traffic when they detect its presence in the attacked network. In
this paper we describe an automated algorithm that gives the ability to detect
attacks before they occurrence, then reduce positive and negative falses rates.
Moreover, we use a set of data related to malicious traffic captured using a
network of honeypots to recognize potential threats sources.",ip blocking
http://arxiv.org/abs/1705.02257v2,"We present a sorting algorithm that works in-place, executes in parallel, is
cache-efficient, avoids branch-mispredictions, and performs work O(n log n) for
arbitrary inputs with high probability. The main algorithmic contributions are
new ways to make distribution-based algorithms in-place: On the practical side,
by using coarse-grained block-based permutations, and on the theoretical side,
we show how to eliminate the recursion stack. Extensive experiments show that
our algorithm IPS$^4$o scales well on a variety of multi-core machines. We
outperform our closest in-place competitor by a factor of up to 3. Even as a
sequential algorithm, we are up to 1.5 times faster than the closest sequential
competitor, BlockQuicksort.",ip blocking
http://arxiv.org/abs/1809.06207v1,"Galois Field arithmetic blocks are the key components in many security
applications, such as Elliptic Curve Cryptography (ECC) and the S-Boxes of the
Advanced Encryption Standard (AES) cipher. This paper introduces a novel
hardware intellectual property (IP) protection technique by obfuscating
arithmetic functions over Galois Field (GF), specifically, focusing on
obfuscation of GF multiplication that underpins complex GF arithmetic and
elliptic curve point arithmetic functions. Obfuscating GF multiplication
circuits is important because the choice of irreducible polynomials in GF
multiplication has the great impact on the performance of the hardware designs,
and because the significant effort is spent on finding an optimum irreducible
polynomial for a given field, which can provide one company a competitive
advantage over another.",ip blocking
http://arxiv.org/abs/1602.01956v3,"The Hospitals / Residents problem with Couples (HRC) models the allocation of
intending junior doctors to hospitals where couples are allowed to submit joint
preference lists over pairs of (typically geographically close) hospitals. It
is known that a stable matching need not exist, so we consider MIN BP HRC, the
problem of finding a matching that admits the minimum number of blocking pairs
(i.e., is ""as stable as possible""). We show that this problem is NP-hard and
difficult to approximate even in the highly restricted case that each couple
finds only one hospital pair acceptable. However if we further assume that the
preference list of each single resident and hospital is of length at most 2, we
give a polynomial-time algorithm for this case. We then present the first
Integer Programming (IP) and Constraint Programming (CP) models for MIN BP HRC.
Finally, we discuss an empirical evaluation of these models applied to
randomly-generated instances of MIN BP HRC. We find that on average, the CP
model is about 1.15 times faster than the IP model, and when presolving is
applied to the CP model, it is on average 8.14 times faster. We further observe
that the number of blocking pairs admitted by a solution is very small, i.e.,
usually at most 1, and never more than 2, for the (28,000) instances
considered.",ip blocking
http://arxiv.org/abs/1006.1165v1,"In this paper, we consider the problem of blocking malicious traffic on the
Internet, via source-based filtering. In particular, we consider filtering via
access control lists (ACLs): these are already available at the routers today
but are a scarce resource because they are stored in the expensive ternary
content addressable memory (TCAM). Aggregation (by filtering source prefixes
instead of individual IP addresses) helps reduce the number of filters, but
comes also at the cost of blocking legitimate traffic originating from the
filtered prefixes. We show how to optimally choose which source prefixes to
filter, for a variety of realistic attack scenarios and operators' policies. In
each scenario, we design optimal, yet computationally efficient, algorithms.
Using logs from Dshield.org, we evaluate the algorithms and demonstrate that
they bring significant benefit in practice.",ip blocking
http://arxiv.org/abs/1410.6146v1,"Resource management is one of the most indispensable components of
cluster-level infrastructure layers. Users of such systems should be able to
specify their job requirements as a configuration parameter (CPU, RAM, disk
I/O, network I/O) and have the scheduler translate those into an appropriate
reservation and allocation of resources. YARN is an emerging resource
management in the Hadoop ecosystem, which supports only RAM and CPU reservation
at present.
  In this paper, we propose a solution that takes into account the operation of
the Hadoop Distributed File System to control the data rate of applications in
the framework of a Hadoop compute platform. We utilize the property that a data
pipe between a container and a DataNode consists of a disk I/O subpipe and a
TCP/IP subpipe. We have implemented building block software components to
control the data rate of data pipes between containers and DataNodes and
provide a proof-of-concept with measurement results.",ip blocking
http://arxiv.org/abs/1705.02505v2,"As online fraudsters invest more resources, including purchasing large pools
of fake user accounts and dedicated IPs, fraudulent attacks become less obvious
and their detection becomes increasingly challenging. Existing approaches such
as average degree maximization suffer from the bias of including more nodes
than necessary, resulting in lower accuracy and increased need for manual
verification. Hence, we propose HoloScope, which uses information from graph
topology and temporal spikes to more accurately detect groups of fraudulent
users. In terms of graph topology, we introduce ""contrast suspiciousness,"" a
dynamic weighting approach, which allows us to more accurately detect
fraudulent blocks, particularly low-density blocks. In terms of temporal
spikes, HoloScope takes into account the sudden bursts and drops of fraudsters'
attacking patterns. In addition, we provide theoretical bounds for how much
this increases the time cost needed for fraudsters to conduct adversarial
attacks. Additionally, from the perspective of ratings, HoloScope incorporates
the deviation of rating scores in order to catch fraudsters more accurately.
Moreover, HoloScope has a concise framework and sub-quadratic time complexity,
making the algorithm reproducible and scalable. Extensive experiments showed
that HoloScope achieved significant accuracy improvements on synthetic and real
data, compared with state-of-the-art fraud detection methods.",ip blocking
http://arxiv.org/abs/1708.01732v2,"Organised crime, as well as individual criminals, is benefiting from the
protection of private browsers provide to those who would carry out illegal
activity, such as money laundering, drug trafficking, the online exchange of
child-abuse material, etc. The protection afforded to users of the Epic Privacy
Browser illustrates these benefits. This browser is currently in use in
approximately 180 countries worldwide. This paper outlines the location and
type of evidence available through live and post-mortem state analyses of the
Epic Privacy Browser. This study identifies the manner in which the browser
functions during use, where evidence can be recovered after use, as well as the
tools and effective presentation of the recovered material.",privacy browser
http://arxiv.org/abs/1703.05066v2,"Browsers and their users can be tracked even in the absence of a persistent
IP address or cookie. Unique and hence identifying pieces of information,
making up what is known as a fingerprint, can be collected from browsers by a
visited website, e.g. using JavaScript. However, browsers vary in precisely
what information they make available, and hence their fingerprintability may
also vary. In this paper, we report on the results of experiments examining the
fingerprintable attributes made available by a range of modern browsers. We
tested the most widely used browsers for both desktop and mobile platforms. The
results reveal significant differences between browsers in terms of their
fingerprinting potential, meaning that the choice of browser has significant
privacy implications.",privacy browser
http://arxiv.org/abs/1905.09581v1,"Browser fingerprinting is a relatively new method of uniquely identifying
browsers that can be used to track web users. In some ways it is more
privacy-threatening than tracking via cookies, as users have no direct control
over it. A number of authors have considered the wide variety of techniques
that can be used to fingerprint browsers; however, relatively little
information is available on how widespread browser fingerprinting is, and what
information is collected to create these fingerprints in the real world. To
help address this gap, we crawled the 10,000 most popular websites; this gave
insights into the number of websites that are using the technique, which
websites are collecting fingerprinting information, and exactly what
information is being retrieved. We found that approximately 69\% of websites
are, potentially, involved in first-party or third-party browser
fingerprinting. We further found that third-party browser fingerprinting, which
is potentially more privacy-damaging, appears to be predominant in practice. We
also describe \textit{FingerprintAlert}, a freely available browser extension
we developed that detects and, optionally, blocks fingerprinting attempts by
visited websites.",privacy browser
http://arxiv.org/abs/1710.09598v1,"With the advance of technology, Criminal Justice agencies are being
confronted with an increased need to investigate crimes perpetuated partially
or entirely over the Internet. These types of crime are known as cybercrimes.
In order to conceal illegal online activity, criminals often use private
browsing features or browsers designed to provide total browsing privacy. The
use of private browsing is a common challenge faced in for example child
exploitation investigations, which usually originate on the Internet. Although
private browsing features are not designed specifically for criminal activity,
they have become a valuable tool for criminals looking to conceal their online
activity. As such, Technological Crime units often focus their forensic
analysis on thoroughly examining the web history on a computer. Private
browsing features and browsers often require a more in-depth, post mortem
analysis. This often requires the use of multiple tools, as well as different
forensic approaches to uncover incriminating evidence. This evidence may be
required in a court of law, where analysts are often challenged both on their
findings and on the tools and approaches used to recover evidence. However,
there are very few research on evaluating of private browsing in terms of
privacy preserving as well as forensic acquisition and analysis of privacy
preserving internet browsers. Therefore in this chapter, we firstly review the
private mode of popular internet browsers. Next, we describe the forensic
acquisition and analysis of Browzar, a privacy preserving internet browser and
compare it with other popular internet browsers",privacy browser
http://arxiv.org/abs/1708.06774v1,"Timing attacks have been a continuous threat to users' privacy in modern
browsers. To mitigate such attacks, existing approaches, such as Tor Browser
and Fermata, add jitters to the browser clock so that an attacker cannot
accurately measure an event. However, such defenses only raise the bar for an
attacker but do not fundamentally mitigate timing attacks, i.e., it just takes
longer than previous to launch a timing attack. In this paper, we propose a
novel approach, called deterministic browser, which can provably prevent timing
attacks in modern browsers. Borrowing from Physics, we introduce several
concepts, such as an observer and a reference frame. Specifically, a snippet of
JavaScript, i.e., an observer in JavaScript reference frame, will always obtain
the same, fixed timing information so that timing attacks are prevented; at
contrast, a user, i.e., an oracle observer, will perceive the JavaScript
differently and do not experience the performance slowdown. We have implemented
a prototype called DeterFox and our evaluation shows that the prototype can
defend against browser-related timing attacks.",privacy browser
http://arxiv.org/abs/1802.03367v1,"We evaluate Tencent's QQ Browser, a popular mobile browser in China with
hundreds of millions of users---including 16 million overseas, with respect to
the threat model of a man-in-the-middle attacker with state actor capabilities.
This is motivated by information in the Snowden revelations suggesting that
another Chinese mobile browser, UC Browser, was being used to track users by
Western nation-state adversaries.
  Among the many issues we found in QQ Browser that are presented in this
paper, the use of ""textbook RSA""---that is, RSA implemented as shown in
textbooks, with no padding---is particularly interesting because it affords us
the opportunity to contextualize existing research in breaking textbook RSA. We
also present a novel attack on QQ Browser's use of textbook RSA that is
distinguished from previous research by its simplicity. We emphasize that
although QQ Browser's cryptography and our attacks on it are very simple, the
impact is serious. Thus, research into how to break very poor cryptography
(such as textbook RSA) has both pedagogical value and real-world impact.",privacy browser
http://arxiv.org/abs/1901.03397v1,"Browser extensions are third party programs, tightly integrated to browsers,
where they execute with elevated privileges in order to provide users with
additional functionalities. Unlike web applications, extensions are not subject
to the Same Origin Policy (SOP) and therefore can read and write user data on
any web application. They also have access to sensitive user information
including browsing history, bookmarks, cookies and list of installed
extensions. Extensions have a permanent storage in which they can store data
and can trigger the download of arbitrary files on the user's device. For
security reasons, browser extensions and web applications are executed in
separate contexts. Nonetheless, in all major browsers, extensions and web
applications can interact by exchanging messages. Through these communication
channels, a web application can exploit extension privileged capabilities and
thereby access and exfiltrate sensitive user information. In this work, we
analyzed the communication interfaces exposed to web applications by Chrome,
Firefox and Opera browser extensions. As a result, we identified many
extensions that web applications can exploit to access privileged capabilities.
Through extensions' APIS, web applications can bypass SOP, access user cookies,
browsing history, bookmarks, list of installed extensions, extensions storage,
and download arbitrary files on the user's device. Our results demonstrate that
the communications between browser extensions and web applications pose serious
security and privacy threats to browsers, web applications and more importantly
to users. We discuss countermeasures and proposals, and believe that our study
and in particular the tool we used to detect and exploit these threats, can be
used as part of extensions review process by browser vendors to help them
identify and fix the aforementioned problems in extensions.",privacy browser
http://arxiv.org/abs/1808.01718v1,"Harm to the privacy of users through data leakage is not an unknown issue,
however, it has not been studied in the context of the crash reporting system.
Automatic Crash Reporting Systems (ACRS) are used by applications to report
information about the errors happening during a software failure. Although
crash reports are valuable to diagnose errors, they may contain users'
sensitive information. In this paper, we study such a privacy leakage vis-a-vis
browsers' crash reporting systems. As a case study, we mine a dataset
consisting of crash reports collected over the period of six years. Our
analysis shows the presence of more than 20,000 sessions and token IDs, 600
passwords, 9,000 email addresses, an enormous amount of contact information,
and other sensitive data. Our analysis sheds light on an important security and
privacy issue in the current state-of-the-art browser crash reporting systems.
Further, we propose a hotfix to enhance users' privacy and security in ACRS by
removing sensitive data from the crash report prior to submit the report to the
server. Our proposed hotfix can be easily integrated into the current
implementation of ACRS and has no impact on the process of fixing bugs while
maintaining the reports' readability.",privacy browser
http://arxiv.org/abs/1703.02209v4,"Certificate transparency (CT) is an elegant mechanism designed to detect when
a certificate authority (CA) has issued a certificate incorrectly. Many CAs now
support CT and it is being actively deployed in browsers. However, a number of
privacy-related challenges remain. In this paper we propose practical solutions
to two issues. First, we develop a mechanism that enables web browsers to audit
a CT log without violating user privacy. Second, we extend CT to support
non-public subdomains.",privacy browser
http://arxiv.org/abs/1709.05395v1,"The introduction of the WebRTC API to modern browsers has brought about a new
threat to user privacy. This API causes a range of client IP addresses to
become available to a visited website via JavaScript even if a VPN is in use.
This a potentially serious problem for users utilizing VPN services for
anonymity. In order to better understand the magnitude of this issue, we tested
widely used browsers and VPN services to discover which client IP addresses can
be revealed and in what circumstances. In most cases, at least one of the
client addresses is leaked. The number and type of leaked IP addresses are
affected by the choices of browser and VPN service, meaning that
privacy-sensitive users should choose their browser and their VPN provider with
care. We conclude by proposing countermeasures which can be used to help
mitigate this issue.",privacy browser
http://arxiv.org/abs/1812.03920v1,"We measure how effective Privacy Enhancing Technologies (PETs) are at
protecting users from website fingerprinting. Our measurements use both
experimental and observational methods. Experimental methods allow control,
precision, and use on new PETs that currently lack a user base. Observational
methods enable scale and drawing from the browsers currently in real-world use.
By applying experimentally created models of a PET's behavior to an
observational data set, our novel hybrid method offers the best of both worlds.
We find the Tor Browser Bundle to be the most effective PET amongst the set we
tested. We find that some PETs have inconsistent behaviors, which can do more
harm than good.",privacy browser
http://arxiv.org/abs/1705.04437v1,"The browser history reveals highly sensitive information about users, such as
financial status, health conditions, or political views. Private browsing modes
and anonymity networks are consequently important tools to preserve the privacy
not only of regular users but in particular of whistleblowers and dissidents.
Yet, in this work we show how a malicious application can infer opened websites
from Google Chrome in Incognito mode and from Tor Browser by exploiting
hardware performance events (HPEs). In particular, we analyze the browsers'
microarchitectural footprint with the help of advanced Machine Learning
techniques: k-th Nearest Neighbors, Decision Trees, Support Vector Machines,
and in contrast to previous literature also Convolutional Neural Networks. We
profile 40 different websites, 30 of the top Alexa sites and 10 whistleblowing
portals, on two machines featuring an Intel and an ARM processor. By monitoring
retired instructions, cache accesses, and bus cycles for at most 5 seconds, we
manage to classify the selected websites with a success rate of up to 86.3%.
The results show that hardware performance events can clearly undermine the
privacy of web users. We therefore propose mitigation strategies that impede
our attacks and still allow legitimate use of HPEs.",privacy browser
http://arxiv.org/abs/1809.04774v3,"Users regularly enter sensitive data, such as passwords, credit card numbers,
or tax information, into the browser window. While modern browsers provide
powerful client-side privacy measures to protect this data, none of these
defenses prevent a browser compromised by malware from stealing it. In this
work, we present Fidelius, a new architecture that uses trusted hardware
enclaves integrated into the browser to enable protection of user secrets
during web browsing sessions, even if the entire underlying browser and OS are
fully controlled by a malicious attacker.
  Fidelius solves many challenges involved in providing protection for browsers
in a fully malicious environment, offering support for integrity and privacy
for form data, JavaScript execution, XMLHttpRequests, and protected web
storage, while minimizing the TCB. Moreover, interactions between the enclave
and the browser, the keyboard, and the display all require new protocols, each
with their own security considerations. Finally, Fidelius takes into account UI
considerations to ensure a consistent and simple interface for both developers
and users.
  As part of this project, we develop the first open source system that
provides a trusted path from input and output peripherals to a hardware enclave
with no reliance on additional hypervisor security assumptions. These
components may be of independent interest and useful to future projects.
  We implement and evaluate Fidelius to measure its performance overhead,
finding that Fidelius imposes acceptable overhead on page load and user
interaction for secured pages and has no impact on pages and page components
that do not use its enhanced security features.",privacy browser
http://arxiv.org/abs/1803.02887v1,"In this paper, we examine the recent trend towards in-browser mining of
cryptocurrencies; in particular, the mining of Monero through Coinhive and
similar code- bases. In this model, a user visiting a website will download a
JavaScript code that executes client-side in her browser, mines a
cryptocurrency, typically without her consent or knowledge, and pays out the
seigniorage to the website. Websites may consciously employ this as an
alternative or to supplement advertisement revenue, may offer premium content
in exchange for mining, or may be unwittingly serving the code as a result of a
breach (in which case the seigniorage is collected by the attacker). The
cryptocurrency Monero is preferred seemingly for its unfriendliness to
large-scale ASIC mining that would drive browser-based efforts out of the
market, as well as for its purported privacy features. In this paper, we survey
this landscape, conduct some measurements to establish its prevalence and
profitability, outline an ethical framework for considering whether it should
be classified as an attack or business opportunity, and make suggestions for
the detection, mitigation and/or prevention of browser-based mining for non-
consenting users.",privacy browser
http://arxiv.org/abs/1708.08510v2,"Modern web browsers have accrued an incredibly broad set of features since
being invented for hypermedia dissemination in 1990. Many of these features
benefit users by enabling new types of web applications. However, some features
also bring risk to users' privacy and security, whether through implementation
error, unexpected composition, or unintended use. Currently there is no general
methodology for weighing these costs and benefits. Restricting access to only
the features which are necessary for delivering desired functionality on a
given website would allow users to enforce the principle of lease privilege on
use of the myriad APIs present in the modern web browser.
  However, security benefits gained by increasing restrictions must be balanced
against the risk of breaking existing websites. This work addresses this
problem with a methodology for weighing the costs and benefits of giving
websites default access to each browser feature. We model the benefit as the
number of websites that require the feature for some user-visible benefit, and
the cost as the number of CVEs, lines of code, and academic attacks related to
the functionality. We then apply this methodology to 74 Web API standards
implemented in modern browsers. We find that allowing websites default access
to large parts of the Web API poses significant security and privacy risks,
with little corresponding benefit.
  We also introduce a configurable browser extension that allows users to
selectively restrict access to low-benefit, high-risk features on a per site
basis. We evaluated our extension with two hardened browser configurations, and
found that blocking 15 of the 74 standards avoids 52.0% of code paths related
to previous CVEs, and 50.0% of implementation code identified by our metric,
without affecting the functionality of 94.7% of measured websites.",privacy browser
http://arxiv.org/abs/1410.4500v1,"JavaScript engines inside modern browsers are capable of running
sophisticated multi-player games, rendering impressive 3D scenes, and
supporting complex, interactive visualizations. Can this processing power be
harnessed for information retrieval? This paper explores the feasibility of
building a JavaScript search engine that runs completely self-contained on the
client side within the browser---this includes building the inverted index,
gathering terms statistics for scoring, and performing query evaluation. The
design takes advantage of the IndexDB API, which is implemented by the LevelDB
key-value store inside Google's Chrome browser. Experiments show that although
the performance of the JavaScript prototype falls far short of the open-source
Lucene search engine, it is sufficiently responsive for interactive
applications. This feasibility demonstration opens the door to interesting
applications in offline and private search across multiple platforms as well as
hybrid split-execution architectures whereby clients and servers
collaboratively perform query evaluation. One possible future scenario is the
rise of an online search marketplace in which commercial search engine
companies and individual users participate as rational economic actors,
balancing privacy, resource usage, latency, and other factors based on
customizable utility profiles.",privacy browser
http://arxiv.org/abs/1910.07455v1,"Websites are capable of learning a wide range of information about the
platform on which a browser is executing. One major source of such information
is the set of standardised Application Programming Interfaces (APIs) provided
within the browser, which can be accessed by JavaScript downloaded by a
website; this information can then either be used by the JavaScript or sent
back to the originating site. As has been widely discussed, much of this
information can threaten user privacy. The main purpose of this paper is to
document a publicly available platform designed to enable further investigation
of one class of such threats, namely those based on analysing user behavioural
data. The platform has two main components: a Chrome extension that gathers
user keystroke and mouse data via browser APIs, and server software that
collects and stores this data for subsequent experimentation.",privacy browser
http://arxiv.org/abs/1506.04104v1,"We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.",privacy browser
http://arxiv.org/abs/1907.10279v1,"The increasing use of encrypted data within file storage and in network
communications leaves investigators with many challenges. One of the most
challenging is the Tor protocol, as its main focus is to protect the privacy of
the user, in both its local footprint within a host and over a network
connection. The Tor browser, though, can leave behind digital artefacts which
can be used by an investigator. This paper outlines an experimental methodology
and provides results for evidence trails which can be used within real-life
investigations.",privacy browser
http://arxiv.org/abs/1905.07444v2,"Online advertising has been a long-standing concern for user privacy and
overall web experience. Several techniques have been proposed to block ads,
mostly based on filter-lists and manually-written rules. While a typical ad
blocker relies on manually-curated block lists, these inevitably get
out-of-date, thus compromising the ultimate utility of this ad blocking
approach. In this paper we present Percival, a browser-embedded, lightweight,
deep learning-powered ad blocker. Percival embeds itself within the browser's
image rendering pipeline, which makes it possible to intercept every image
obtained during page execution and to perform blocking based on applying
machine learning for image classification to flag potential ads. Our
implementation inside both Chromium and Brave browsers shows only a minor
rendering performance overhead of 4.55%, demonstrating the feasibility of
deploying traditionally heavy models (i.e. deep neural networks) inside the
critical path of the rendering engine of a browser. We show that our
image-based ad blocker can replicate EasyList rules with an accuracy of 96.76%.
To show the versatility of the Percival's approach we present case studies that
demonstrate that Percival 1) does surprisingly well on ads in languages other
than English; 2) Percival also performs well on blocking first-party Facebook
ads, which have presented issues for other ad blockers. Percival proves that
image-based perceptual ad blocking is an attractive complement to today's
dominant approach of block lists",privacy browser
http://arxiv.org/abs/1109.4677v1,"Most search engines can potentially infer the preferences and interests of a
user based on her history of search queries. While search engines can use these
inferences for a variety of tasks, including targeted advertisements, such
tasks do impose an serious threat to user privacy. In 2006, after AOL disclosed
the search queries of 650,000 users, TrackMeNot was released as a simple
browser extension that sought to hide user search preferences in a cloud of
queries. The first versions of TrackMeNot, though used extensively in the past
three years, was fairly simplistic in design and did not provide any strong
privacy guarantees. In this paper, we present the new design and implementation
of TrackMeNot, which address many of the limitations of the first release.
TrackMeNot addresses two basic problems. First, using a model for
characterizing search queries, TrackMeNot provides a mechanism for obfuscating
the search preferences of a user from a search engine. Second, TrackMeNot
prevents the leakage of information revealing the use of obfuscation to a
search engine via several potential side channels in existing browsers such as
clicks, cookies etc. Finally, we show that TrackMeNot cannot be detected by
current search bot detection mechanisms and demonstrate the effectiveness of
TrackMeNot in obfuscating user interests by testing its efficiency on a major
search engine.",privacy browser
http://arxiv.org/abs/1806.05426v1,"It is important to design browser security and privacy alerts so as to
maximise their value to the end user, and their efficacy in terms of
communicating risk. We derived a list of design guidelines from the research
literature by carrying out a systematic review. We analysed the papers both
quantitatively and qualitatively to arrive at a comprehensive set of
guidelines. Our findings aim to to provide designers and developers with
guidance as to how to construct privacy and security alerts. We conclude by
providing an alert template,highlighting its adherence to the derived
guidelines.",privacy browser
http://arxiv.org/abs/1510.00651v1,"In April 2015, BitTorrent Inc. released their distributed peer-to-peer
powered browser, Project Maelstrom, into public beta. The browser facilitates a
new alternative website distribution paradigm to the traditional HTTP-based,
client-server model. This decentralised web is powered by each of the visitors
accessing each Maelstrom hosted website. Each user shares their copy of the
website's source code and multimedia content with new visitors. As a result, a
Maelstrom hosted website cannot be taken offline by law enforcement or any
other parties. Due to this open distribution model, a number of interesting
censorship, security and privacy considerations are raised. This paper explores
the application, its protocol, sharing Maelstrom content and its new visitor
powered ""web-hosting"" paradigm.",privacy browser
http://arxiv.org/abs/1905.05224v2,"Comprehensive case studies on malicious code mostly focus on botnets and
worms (recently revived with IoT devices), prominent pieces of malware or
Advanced Persistent Threats, exploit kits, and ransomware. However, adware
seldom receives such attention. Previous studies on ""unwanted"" Windows
applications, including adware, favored breadth of analysis, uncovering ties
between different actors and distribution methods. In this paper, we
demonstrate the capabilities, privacy and security risks, and prevalence of a
particularly successful and active adware business: Wajam, by tracking its
evolution over nearly six years. We first study its multi-layer antivirus
evasion capabilities, a combination of known and newly adapted techniques, that
ensure low detection rates of its daily variants, along with prominent
features, e.g., traffic interception and browser process injection. Then, we
look at the privacy and security implications for infected users, including
plaintext leaks of browser histories and keyword searches on highly popular
websites, along with arbitrary content injection on HTTPS webpages and remote
code execution vulnerabilities. Finally, we study Wajam's prevalence through
the popularity of its domains. Once considered as seriously as spyware, adware
is now merely called ""not-a-virus"", ""optional"" or ""unwanted"" although its
negative impact is growing. We emphasize that the adware problem has been
overlooked for too long, which can reach (or even surplus) the complexity and
impact of regular malware, and pose both privacy and security risks to users,
more so than many well-known and thoroughly-analyzed malware families.",privacy browser
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",privacy browser
http://arxiv.org/abs/1602.04115v1,"Conforming to W3C specifications, mobile web browsers allow JavaScript code
in a web page to access motion and orientation sensor data without the user's
permission. The associated risks to user security and privacy are however not
considered in W3C specifications. In this work, for the first time, we show how
user security can be compromised using these sensor data via browser, despite
that the data rate is 3 to 5 times slower than what is available in app. We
examine multiple popular browsers on Android and iOS platforms and study their
policies in granting permissions to JavaScript code with respect to access to
motion and orientation sensor data. Based on our observations, we identify
multiple vulnerabilities, and propose TouchSignatures which implements an
attack where malicious JavaScript code on an attack tab listens to such sensor
data measurements. Based on these streams, TouchSignatures is able to
distinguish the user's touch actions (i.e., tap, scroll, hold, and zoom) and
her PINs, allowing a remote website to learn the client-side user activities.
We demonstrate the practicality of this attack by collecting data from real
users and reporting high success rates using our proof-of-concept
implementations. We also present a set of potential solutions to address the
vulnerabilities. The W3C community and major mobile browser vendors including
Mozilla, Google, Apple and Opera have acknowledge our work and are implementing
some of our proposed countermeasures.",privacy browser
http://arxiv.org/abs/1508.01719v1,"Single sign-on (SSO) systems, such as OpenID and OAuth, allow web sites,
so-called relying parties (RPs), to delegate user authentication to identity
providers (IdPs), such as Facebook or Google. These systems are very popular,
as they provide a convenient means for users to log in at RPs and move much of
the burden of user authentication from RPs to IdPs.
  There is, however, a downside to current systems, as they do not respect
users' privacy: IdPs learn at which RP a user logs in. With one exception,
namely Mozilla's BrowserID system (a.k.a. Mozilla Persona), current SSO systems
were not even designed with user privacy in mind. Unfortunately, recently
discovered attacks, which exploit design flaws of BrowserID, show that
BrowserID does not provide user privacy either.
  In this paper, we therefore propose the first privacy-respecting SSO system
for the web, called SPRESSO (for Secure Privacy-REspecting Single Sign-On). The
system is easy to use, decentralized, and platform independent. It is based
solely on standard HTML5 and web features and uses no browser extensions,
plug-ins, or other executables.
  Existing SSO systems and the numerous attacks on such systems illustrate that
the design of secure SSO systems is highly non-trivial. We therefore also carry
out a formal analysis of SPRESSO based on an expressive model of the web in
order to formally prove that SPRESSO enjoys strong authentication and privacy
properties.",privacy browser
http://arxiv.org/abs/1506.04107v1,"The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.",privacy browser
http://arxiv.org/abs/1906.01276v1,"There was a time when the word security was only confined to the physical
protection of things that were valuable which must be guarded against all the
odds. Today, in a world where people can do things virtually have emerged the
necessity to protect the virtual world. Every single facet of our life is being
controlled by the internet one way or another. There is no privacy in the
cyberspace as the data which we are browsing on the internet is being monitored
on the other side by someone. Each work we are doing on the internet is getting
tracked or the data are getting leaked without consent. To browse the internet
securely we developed a router named Aranea which relates to the browser Tor.
Tor gives traffic anonymity and security. The Tor browser can be used in both
positive and negative purpose. Tor encrypts data, it hides the location and
identity of the user, it hides the IP address of the device, it hides the
network traffic and many more. By using Tor browser each user can browse the
internet safely in the cyber world. Our goal is to create an additional
security bridge through the router Aranea for every user so that each user can
simply browse the internet anonymously.",privacy browser
http://arxiv.org/abs/1907.10331v1,"Recent work has demonstrated that by monitoring the Real Time Bidding (RTB)
protocol, one can estimate the monetary worth of different users for the
programmatic advertising ecosystem, even when the so-called winning bids are
encrypted. In this paper we describe how to implement the above techniques in a
practical and privacy preserving manner. Specifically, we study the privacy
consequences of reporting back to a centralized server, features that are
necessary for estimating the value of encrypted winning bids. We show that by
appropriately modulating the granularity of the necessary information and by
scrambling the communication channel to the server, one can increase the
privacy performance of the system in terms of K-anonymity. We've implemented
the above ideas on a browser extension and disseminated it to some 200 users.
Analyzing the results from 6 months of deployment, we show that the average
value of users for the programmatic advertising ecosystem has grown more than
75% in the last 3 years.",privacy browser
http://arxiv.org/abs/1808.07359v1,"Recent works showed that websites can detect browser extensions that users
install and websites they are logged into. This poses significant privacy
risks, since extensions and Web logins that reflect user's behavior, can be
used to uniquely identify users on the Web. This paper reports on the first
large-scale behavioral uniqueness study based on 16,393 users who visited our
website. We test and detect the presence of 16,743 Chrome extensions, covering
28% of all free Chrome extensions. We also detect whether the user is connected
to 60 different websites.
  We analyze how unique users are based on their behavior, and find out that
54.86% of users that have installed at least one detectable extension are
unique; 19.53% of users are unique among those who have logged into one or more
detectable websites; and 89.23% are unique among users with at least one
extension and one login. We use an advanced fingerprinting algorithm and show
that it is possible to identify a user in less than 625 milliseconds by
selecting the most unique combinations of extensions.
  Because privacy extensions contribute to the uniqueness of users, we study
the trade-off between the amount of trackers blocked by such extensions and how
unique the users of these extensions are. We have found that privacy extensions
should be considered more useful than harmful. The paper concludes with
possible countermeasures.",privacy browser
http://arxiv.org/abs/1905.03518v1,"Small TCP flows make up the majority of web flows. For them, the TCP
three-way handshake represents a significant delay overhead. The TCP Fast Open
(TFO) protocol provides zero round-trip time (0-RTT) handshakes for subsequent
TCP connections to the same host. In this paper, we present real-world privacy
and performance limitations of TFO. We investigated its deployment on popular
websites and browsers. We found that a client revisiting a web site for the
first time fails to use an abbreviated TFO handshake about 40% of the time due
to web server load-balancing. Our analysis further reveals significant privacy
problems in the protocol design and implementation. Network-based attackers and
online trackers can exploit these shortcomings to track the online activities
of users. As a countermeasure, we introduce a novel protocol called TCP Fast
Open Privacy (FOP). It overcomes the performance and privacy limitations of TLS
over TFO by utilizing a custom TLS extension. TCP FOP prevents tracking by
network attackers and impedes third-party tracking, while still allowing for
0-RTT handshakes as in TFO. As a proof-of-concept, we have implemented the
proposed protocol. Our measurements indicate that TCP FOP outperforms TLS over
TFO when websites are served from multiple IP addresses.",privacy browser
http://arxiv.org/abs/1705.06805v1,"The increasing popularity of specialized Internet-connected devices and
appliances, dubbed the Internet-of-Things (IoT), promises both new conveniences
and new privacy concerns. Unlike traditional web browsers, many IoT devices
have always-on sensors that constantly monitor fine-grained details of users'
physical environments and influence the devices' network communications.
Passive network observers, such as Internet service providers, could
potentially analyze IoT network traffic to infer sensitive details about users.
Here, we examine four IoT smart home devices (a Sense sleep monitor, a Nest Cam
Indoor security camera, a WeMo switch, and an Amazon Echo) and find that their
network traffic rates can reveal potentially sensitive user interactions even
when the traffic is encrypted. These results indicate that a technological
solution is needed to protect IoT device owner privacy, and that IoT-specific
concerns must be considered in the ongoing policy debate around ISP data
collection and usage.",privacy browser
http://arxiv.org/abs/1412.2432v2,"With few exceptions, the field of Machine Learning (ML) research has largely
ignored the browser as a computational engine. Beyond an educational resource
for ML, the browser has vast potential to not only improve the state-of-the-art
in ML research, but also, inexpensively and on a massive scale, to bring
sophisticated ML learning and prediction to the public at large. This paper
introduces MLitB, a prototype ML framework written entirely in JavaScript,
capable of performing large-scale distributed computing with heterogeneous
classes of devices. The development of MLitB has been driven by several
underlying objectives whose aim is to make ML learning and usage ubiquitous (by
using ubiquitous compute devices), cheap and effortlessly distributed, and
collaborative. This is achieved by allowing every internet capable device to
run training algorithms and predictive models with no software installation and
by saving models in universally readable formats. Our prototype library is
capable of training deep neural networks with synchronized, distributed
stochastic gradient descent. MLitB offers several important opportunities for
novel ML research, including: development of distributed learning algorithms,
advancement of web GPU algorithms, novel field and mobile applications, privacy
preserving computing, and green grid-computing. MLitB is available as open
source software.",privacy browser
http://arxiv.org/abs/1506.04110v1,"Website publishers can derive enormous performance benefits and cost savings
by directing traffic to their sites through content distribution networks
(CDNs). However, publishers who use CDNs today must trust their CDN not to
modify the site's JavaScript, CSS, images or other media en route to end users.
A CDN that violates this trust could inject ads into websites, downsample media
to save bandwidth or, worse, inject malicious JavaScript code to steal user
secrets it could not otherwise access. We present Stickler, a system for
website publishers that guarantees the end-to-end authenticity of content
served to end users while simultaneously allowing publishers to reap the
benefits of CDNs. Crucially, Stickler achieves these guarantees without
requiring modifications to the browser.",privacy browser
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",privacy browser
http://arxiv.org/abs/1804.08959v2,"Online tracking has become of increasing concern in recent years, however our
understanding of its extent to date has been limited to snapshots from web
crawls. Previous at-tempts to measure the tracking ecosystem, have been done
using instrumented measurement platforms, which are notable to accurately
capture how people interact with the web. In this work we present a method for
the measurement of tracking in the web through a browser extension, as well as
a method for the aggregation and collection of this information which protects
the privacy of participants. We deployed this extension to more than 5 million
users, enabling measurement across multiple countries, ISPs and browser
configurations, to give an accurate picture of real-world tracking. The result
is the largest and longest measurement of online tracking to date based on real
users, covering 1.5 billion page loads gathered over 12 months. The data,
detailing tracking behaviour over a year, is made publicly available to help
drive transparency around online tracking practices.",privacy browser
http://arxiv.org/abs/1809.09292v1,"With mobiles overtaking desktops as the primary vehicle of Internet
consumption, mobile web performance has become a crucial factor for websites as
it directly impacts their revenue. In principle, improving web performance
entails squeezing out every millisecond of the webpage delivery, loading, and
rendering. However, on a practical note, an illusion of faster websites
suffices. This paper presents DriveShaft, a system envisioned to be deployed in
Content Delivery Networks, which improves the perceived web performance on
mobile devices by reducing the time taken to show visually complete web pages,
without requiring any changes in websites, browsers, or any actions from
end-user. DriveShaft employs (i) crowdsourcing, (ii) on-the-fly JavaScript
injection, (iii) privacy preserving desensitization, and (iv) automatic HTML
generation to achieve its goals. Experimental evaluations using 200
representative websites on different networks (Wi-Fi and 4G), different devices
(high-end and low-end phones) and different browsers, show a reduction of 5x in
the time required to see a visually complete website while giving a perception
of 5x-6x faster page loading.",privacy browser
http://arxiv.org/abs/1506.04112v1,"In this paper we investigate the Web interfaces of several DSL home routers
that can be used to manage their settings via a Web browser. Our goal is to
change these settings by using primary XSS and UI redressing attacks. This
study evaluates routers from 10 different manufacturers (TP-Link, Netgear,
Huawei, D-Link, Linksys, LogiLink, Belkin, Buffalo, Fritz!Box, and Asus). We
were able to circumvent the security of all of them. To demonstrate how all
devices are able to be attacked, we show how to do fast fingerprinting attacks.
Furthermore, we provide countermeasures to make administration interfaces and
therefore the use of routers more secure.",privacy browser
http://arxiv.org/abs/1307.1542v1,"The investigation of the browsing behavior of users provides useful
information to optimize web site design, web browser design, search engines
offerings, and online advertisement. This has been a topic of active research
since the Web started and a large body of work exists. However, new online
services as well as advances in Web and mobile technologies clearly changed the
meaning behind ""browsing the Web"" and require a fresh look at the problem and
research, specifically in respect to whether the used models are still
appropriate. Platforms such as YouTube, Netflix or last.fm have started to
replace the traditional media channels (cinema, television, radio) and media
distribution formats (CD, DVD, Blu-ray). Social networks (e.g., Facebook) and
platforms for browser games attracted whole new, particularly less tech-savvy
audiences. Furthermore, advances in mobile technologies and devices made
browsing ""on-the-move"" the norm and changed the user behavior as in the mobile
case browsing is often being influenced by the user's location and context in
the physical world. Commonly used datasets, such as web server access logs or
search engines transaction logs, are inherently not capable of capturing the
browsing behavior of users in all these facets. DOBBS (DERI Online Behavior
Study) is an effort to create such a dataset in a non-intrusive, completely
anonymous and privacy-preserving way. To this end, DOBBS provides a browser
add-on that users can install, which keeps track of their browsing behavior
(e.g., how much time they spent on the Web, how long they stay on a website,
how often they visit a website, how they use their browser, etc.). In this
paper, we outline the motivation behind DOBBS, describe the add-on and captured
data in detail, and present some first results to highlight the strengths of
DOBBS.",privacy browser
http://arxiv.org/abs/1605.02065v1,"""Concentrated differential privacy"" was recently introduced by Dwork and
Rothblum as a relaxation of differential privacy, which permits sharper
analyses of many privacy-preserving computations. We present an alternative
formulation of the concept of concentrated differential privacy in terms of the
Renyi divergence between the distributions obtained by running an algorithm on
neighboring inputs. With this reformulation in hand, we prove sharper
quantitative results, establish lower bounds, and raise a few new questions. We
also unify this approach with approximate differential privacy by giving an
appropriate definition of ""approximate concentrated differential privacy.""",privacy extension
http://arxiv.org/abs/1810.09152v1,"Location privacy-preserving mechanisms (LPPMs) have been extensively studied
for protecting a user's location at each time point or a sequence of locations
with different timestamps (i.e., a trajectory). We argue that existing LPPMs
are not capable of protecting the sensitive information in user's
spatiotemporal activities, such as ""visited hospital in the last week"" or
""regularly commuting between Address 1 and Address 2 every morning and
afternoon"" (it is easy to infer that Addresses 1 and 2 may be home and office).
We define such privacy as \textit{Spatiotemporal Event Privacy}, which can be
formalized as Boolean expressions between location and time predicates. To
understand how much spatiotemporal event privacy that existing LPPMs can
provide, we first formally define spatiotemporal event privacy by extending the
notion of differential privacy, and then provide a framework for calculating
the spatiotemporal event privacy loss of a given LPPM under attackers who have
knowledge of user's mobility pattern. We also show a case study of utilizing
our framework to convert the state-of-the-art mechanism for location privacy,
i.e., Planner Laplace Mechanism for Geo-indistinguishability, into one
protecting spatiotemporal event privacy. Our experiments on real-life and
synthetic data verified that the proposed method is effective and efficient.",privacy extension
http://arxiv.org/abs/1906.07902v2,"With the prevalence of machine learning services, crowdsourced data
containing sensitive information poses substantial privacy challenges. Existing
work focusing on protecting against membership inference attacks under the
rigorous framework of differential privacy are vulnerable to attribute
inference attacks. In light of the current gap between theory and practice, we
develop a novel theoretical framework for privacy-preservation under the attack
of attribute inference. Under our framework, we propose a minimax optimization
formulation to protect the given attribute and analyze its privacy guarantees
against arbitrary adversaries. On the other hand, it is clear that privacy
constraint may cripple utility when the protected attribute is correlated with
the target variable. To this end, we also prove an information-theoretic lower
bound to precisely characterize the fundamental trade-off between utility and
privacy. Empirically, we extensively conduct experiments to corroborate our
privacy guarantee and validate the inherent trade-offs in different privacy
preservation algorithms. Our experimental results indicate that the adversarial
representation learning approaches achieve the best trade-off in terms of
privacy preservation and utility maximization.",privacy extension
http://arxiv.org/abs/1601.01500v1,"Privacy-preservation for sensitive data has become a challenging issue in
cloud computing. Threat modeling as a part of requirements engineering in
secure software development provides a structured approach for identifying
attacks and proposing countermeasures against the exploitation of
vulnerabilities in a system . This paper describes an extension of Cloud
Privacy Threat Modeling (CPTM) methodology for privacy threat modeling in
relation to processing sensitive data in cloud computing environments. It
describes the modeling methodology that involved applying Method Engineering to
specify characteristics of a cloud privacy threat modeling methodology,
different steps in the proposed methodology and corresponding products. We
believe that the extended methodology facilitates the application of a
privacy-preserving cloud software development approach from requirements
engineering to design.",privacy extension
http://arxiv.org/abs/1311.6230v7,"Recently, a novel class of incentive mechanisms is proposed to attract
extensive users to truthfully participate in crowd sensing applications with a
given budget constraint. The class mechanisms also bring good service quality
for the requesters in crowd sensing applications. Although it is so important,
there still exists many verification and privacy challenges, including users'
bids and subtask information privacy and identification privacy, winners' set
privacy of the platform, and the security of the payment outcomes. In this
paper, we present a privacy-preserving verifiable incentive mechanism for crowd
sensing applications with the budget constraint, not only to explore how to
protect the privacies of users and the platform, but also to make the
verifiable payment correct between the platform and users for crowd sensing
applications. Results indicate that our privacy-preserving verifiable incentive
mechanism achieves the same results as the generic one without privacy
preservation.",privacy extension
http://arxiv.org/abs/1612.04350v2,"High-dimensional crowdsourced data collected from a large number of users
produces rich knowledge for our society. However, it also brings unprecedented
privacy threats to participants. Local privacy, a variant of differential
privacy, is proposed as a means to eliminate the privacy concern.
Unfortunately, achieving local privacy on high-dimensional crowdsourced data
raises great challenges on both efficiency and effectiveness. Here, based on EM
and Lasso regression, we propose efficient multi-dimensional joint distribution
estimation algorithms with local privacy. Then, we develop a Locally
privacy-preserving high-dimensional data Publication algorithm, LoPub, by
taking advantage of our distribution estimation techniques. In particular, both
correlations and joint distribution among multiple attributes can be identified
to reduce the dimension of crowdsourced data, thus achieving both efficiency
and effectiveness in locally private high-dimensional data publication.
Extensive experiments on real-world datasets demonstrated that the efficiency
of our multivariate distribution estimation scheme and confirm the
effectiveness of our LoPub scheme in generating approximate datasets with local
privacy.",privacy extension
http://arxiv.org/abs/1601.04740v4,"There is often a fundamental mismatch between programmable privacy
frameworks, on the one hand, and the ever shifting privacy expectations of
computer system users, on the other hand. Based on the theory of contextual
integrity (CI), our paper addresses this problem by proposing a privacy
framework that translates users' privacy expectations (norms) into a set of
actionable privacy rules that are rooted in the language of CI. These norms are
then encoded using Datalog logic specification to develop an information system
that is able to verify whether information flows are appropriate and the
privacy of users thus preserved. A particular benefit of our framework is that
it can automatically adapt as users' privacy expectations evolve over time.
  To evaluate our proposed framework, we conducted an extensive survey
involving more than 450 participants and 1400 questions to derive a set of
privacy norms in the educational context. Based on the crowdsourced responses,
we demonstrate that our framework can derive a compact Datalog encoding of the
privacy norms which can in principle be directly used for enforcing privacy of
information flows within this context. In addition, our framework can
automatically detect logical inconsistencies between individual users' privacy
expectations and the derived privacy logic.",privacy extension
http://arxiv.org/abs/1410.5919v5,"Concerns on location privacy frequently arise with the rapid development of
GPS enabled devices and location-based applications. While spatial
transformation techniques such as location perturbation or generalization have
been studied extensively, most techniques rely on syntactic privacy models
without rigorous privacy guarantee. Many of them only consider static scenarios
or perturb the location at single timestamps without considering temporal
correlations of a moving user's locations, and hence are vulnerable to various
inference attacks. While differential privacy has been accepted as a standard
for privacy protection, applying differential privacy in location based
applications presents new challenges, as the protection needs to be enforced on
the fly for a single user and needs to incorporate temporal correlations
between a user's locations.
  In this paper, we propose a systematic solution to preserve location privacy
with rigorous privacy guarantee. First, we propose a new definition,
""$\delta$-location set"" based differential privacy, to account for the temporal
correlations in location data. Second, we show that the well known
$\ell_1$-norm sensitivity fails to capture the geometric sensitivity in
multidimensional space and propose a new notion, sensitivity hull, based on
which the error of differential privacy is bounded. Third, to obtain the
optimal utility we present a planar isotropic mechanism (PIM) for location
perturbation, which is the first mechanism achieving the lower bound of
differential privacy. Experiments on real-world datasets also demonstrate that
PIM significantly outperforms baseline approaches in data utility.",privacy extension
http://arxiv.org/abs/1710.11571v1,"Context: System Theoretic Process Analysis for Privacy (STPA-Priv) is a novel
privacy risk elicitation method using a top down approach. It has not gotten
very much attention but may offer a convenient structured approach and
generation of additional artifacts compared to other methods. Aim: The aim of
this exploratory study is to find out what benefits the privacy risk
elicitation method STPA-Priv has and to explain how the method can be used.
Method: Therefore we apply STPA-Priv to a real world health scenario that
involves a smart glucose measurement device used by children. Different kinds
of data from the smart device including location data should be shared with the
parents, physicians, and urban planners. This makes it a sociotechnical system
that offers adequate and complex privacy risks to be found. Results: We find
out that STPA-Priv is a structured method for privacy analysis and finds
complex privacy risks. The method is supported by a tool called XSTAMPP which
makes the analysis and its results more profound. Additionally, we learn that
an iterative application of the steps might be necessary to find more privacy
risks when more information about the system is available later. Conclusions:
STPA-Priv helps to identify complex privacy risks that are derived from
sociotechnical interactions in a system. It also outputs privacy constraints
that are to be enforced by the system to ensure privacy.",privacy extension
http://arxiv.org/abs/1703.02382v3,"Demand response (DR) programs have emerged as a potential key enabling
ingredient in the context of smart grid (SG). Nevertheless, the rising concerns
over privacy issues raised by customers subscribed to these programs constitute
a major threat towards their effective deployment and utilization. This has
driven extensive research to resolve the hindrance confronted, resulting in a
number of methods being proposed for preserving customers' privacy. While these
methods provide stringent privacy guarantees, only limited attention has been
paid to their computational efficiency and performance quality. Under the
paradigm of differential privacy, this paper initiates a systematic empirical
study on quantifying the trade-off between privacy and optimality in
centralized DR systems for maximizing cumulative customer utility. Aiming to
elucidate the factors governing this trade-off, we analyze the cost of privacy
in terms of the effect incurred on the objective value of the DR optimization
problem when applying the employed privacy-preserving strategy based on Laplace
mechanism. The theoretical results derived from the analysis are complemented
with empirical findings, corroborated extensively by simulations on a 4-bus MG
system with up to thousands of customers. By evaluating the impact of privacy,
this pilot study serves DR practitioners when considering the social and
economic implications of deploying privacy-preserving DR programs in practice.
Moreover, it stimulates further research on exploring more efficient approaches
with bounded performance guarantees for optimizing energy procurement of MGs
without infringing the privacy of customers on demand side.",privacy extension
http://arxiv.org/abs/1604.06787v1,"Privacy has been a major motivation for distributed problem optimization.
However, even though several methods have been proposed to evaluate it, none of
them is widely used. The Distributed Constraint Optimization Problem (DCOP) is
a fundamental model used to approach various families of distributed problems.
As privacy loss does not occur when a solution is accepted, but when it is
proposed, privacy requirements cannot be interpreted as a criteria of the
objective function of the DCOP. Here we approach the problem by letting both
the optimized costs found in DCOPs and the privacy requirements guide the
agents' exploration of the search space. We introduce Utilitarian Distributed
Constraint Optimization Problem (UDCOP) where the costs and the privacy
requirements are used as parameters to a heuristic modifying the search
process. Common stochastic algorithms for decentralized constraint optimization
problems are evaluated here according to how well they preserve privacy.
Further, we propose some extensions where these solvers modify their search
process to take into account their privacy requirements, succeeding in
significantly reducing their privacy loss without significant degradation of
the solution quality.",privacy extension
http://arxiv.org/abs/1909.07637v1,"As an effective resource allocation approach, double auctions (DAs) have been
extensively studied in electronic commerce. Most previous studies have focused
on how to design strategy-proof DA mechanisms, while not much research effort
has been done concerning privacy and security issues. However, security,
especially privacy issues have become such a public concern that the European
governments lay down the law to enforce the privacy guarantees recently. In
this paper, to address the privacy issue in electronic auctions, we concentrate
on how to design a privacy-preserving mechanism for double auctions by
employing Goldwasser-Micali homomorphic encryption and sorting networks. We
achieve provable privacy such that the auctions do not reveal any bid
information except the auction results, resulting in a strict privacy
guarantee. Moreover, to achieve practical system performance, we compare
different sorting algorithms, and suggest using the faster ones. Experimental
results show that different sorting algorithms may have great effect on the
performance of our mechanism, and demonstrate the practicality of our protocol
for real-world applications in electronic commerce.",privacy extension
http://arxiv.org/abs/1904.02200v4,"Deep learning techniques based on neural networks have shown significant
success in a wide range of AI tasks. Large-scale training datasets are one of
the critical factors for their success. However, when the training datasets are
crowdsourced from individuals and contain sensitive information, the model
parameters may encode private information and bear the risks of privacy
leakage. The recent growing trend of the sharing and publishing of pre-trained
models further aggravates such privacy risks. To tackle this problem, we
propose a differentially private approach for training neural networks. Our
approach includes several new techniques for optimizing both privacy loss and
model accuracy. We employ a generalization of differential privacy called
concentrated differential privacy(CDP), with both a formal and refined privacy
loss analysis on two different data batching methods. We implement a dynamic
privacy budget allocator over the course of training to improve model accuracy.
Extensive experiments demonstrate that our approach effectively improves
privacy loss accounting, training efficiency and model quality under a given
privacy budget.",privacy extension
http://arxiv.org/abs/1506.04105v1,"In this paper we present the Privacy Dashboard -- a tool designed to inform
and empower the people using mobile devices, by introducing features such as
Remote Privacy Protection, Backup, Adjustable Location Accuracy, Permission
Control and Secondary-User Mode. We have implemented our solution on FirefoxOS
and conducted user studies to verify the usefulness and usability of our tool.
The paper starts with a discussion of different aspects of mobile privacy, how
users perceive it and how much they are willing to give up for better
usability. Then we describe the tool in detail, presenting what incentives
drove us to certain design decisions. During our studies we tried to understand
how users interact with the system and what are their priorities. We have
verified our hypothesis, and the impact of the educational aspects on the
decisions about the privacy settings. We show that by taking a user-centric
development of privacy extensions we can reduce the gap between protection and
usability.",privacy extension
http://arxiv.org/abs/1610.03577v3,"Preserving privacy of continuous and/or high-dimensional data such as images,
videos and audios, can be challenging with syntactic anonymization methods
which are designed for discrete attributes. Differential privacy, which
provides a more formal definition of privacy, has shown more success in
sanitizing continuous data. However, both syntactic and differential privacy
are susceptible to inference attacks, i.e., an adversary can accurately infer
sensitive attributes from sanitized data. The paper proposes a novel
filter-based mechanism which preserves privacy of continuous and
high-dimensional attributes against inference attacks. Finding the optimal
utility-privacy tradeoff is formulated as a min-diff-max optimization problem.
The paper provides an ERM-like analysis of the generalization error and also a
practical algorithm to perform the optimization. In addition, the paper
proposes an extension that combines minimax filter and differentially-private
noisy mechanism. Advantages of the method over purely noisy mechanisms is
explained and demonstrated with examples. Experiments with several real-world
tasks including facial expression classification, speech emotion
classification, and activity classification from motion, show that the minimax
filter can simultaneously achieve similar or better target task accuracy and
lower inference accuracy, often significantly lower than previous methods.",privacy extension
http://arxiv.org/abs/1809.04579v1,"Mobile healthcare system integrating wearable sensing and wireless
communication technologies continuously monitors the users' health status.
However, the mHealth system raises a severe privacy concern as the data it
collects are private information, such as heart rate and blood pressure. In
this paper, we propose an efficient and privacy-preserving mHealth data release
approach for the statistic data with the objectives to preserve the unique
patterns in the original data bins. The proposed approach adopts the bucket
partition algorithm and the differential privacy algorithm for privacy
preservation. A customized bucket partition algorithm is proposed to combine
the database value bins into buckets according to certain conditions and
parameters such that the patterns are preserved. The differential privacy
algorithm is then applied to the buckets to prevent an attacker from being able
to identify the small changes at the original data. We prove that the proposed
approach achieves differential privacy. We also show the accuracy of the
proposed approach through extensive simulations on real data. Real experiments
show that our partitioning algorithm outperforms the state-of-the-art in
preserving the patterns of the original data by a factor of 1.75.",privacy extension
http://arxiv.org/abs/1810.08451v1,"Auction is widely regarded as an effective way in dynamic spectrum
redistribution. Recently, considerable research efforts have been devoted to
designing privacy-preserving spectrum auctions in a variety of auction
settings. However, none of existing work has addressed the privacy issue in the
most generic scenario, double spectrum auctions where each seller sells
multiple channels and each buyer buys multiple channels. To fill this gap, in
this paper we propose PP-MCSA, a Privacy Preserving mechanism for Multi-Channel
double Spectrum Auctions. Technically, by leveraging garbled circuits, we
manage to protect the privacy of both sellers' requests and buyers' bids in
multi-channel double spectrum auctions. As far as we know, PP-MCSA is the first
privacy-preserving solution for multi-channel double spectrum auctions. We
further theoretically demonstrate the privacy guarantee of PP-MCSA, and
extensively evaluate its performance via experiments. Experimental results show
that PP-MCSA incurs only moderate communication and computation overhead.",privacy extension
http://arxiv.org/abs/1901.10892v1,"Privacy by design (PbD) is the principle that privacy should be considered at
every stage of the software engineering process. It is increasingly both viewed
as best practice and required by law. It is therefore desirable to have formal
methods that provide guarantees that certain privacy-relevant properties hold.
We propose an approach that can be used to design a privacy-compliant
architecture without needing to know the source code or internal structure of
any individual component. We model an architecture as a set of agents or
components that pass messages to each other. We present in this paper
algorithms that take as input an architecture and a set of privacy constraints,
and output an extension of the original architecture that satisfies the privacy
constraints.",privacy extension
http://arxiv.org/abs/1902.07337v1,"Although Bitcoin in its original whitepaper stated that it offers anonymous
transactions, de-anonymization techniques have found otherwise. Therefore,
alternative cryptocurrencies, like Dash, Monero, and Zcash, were developed to
provide better privacy. As Edward Snowden stated, ""Zcash's privacy tech makes
it the most interesting Bitcoin alternative (...) because the privacy
properties of it are truly unique"". Zcash's privacy is based on peer-reviewed
cryptographic constructions, hence it is considered to provide the foundations
for the best anonymity. However, even Zcash makes some privacy concessions. It
does not protect users' privacy in the presence of a global adversary who is
able to observe the whole network, and hence correlate the parties exchanging
money, by using their network addresses. The recent empirical analysis of Zcash
shows, that users often choose naive ways while performing the protocol
operations, not realizing that it degrades their anonymity. In this talk, we
will discuss an extension of Zcash using mix networks to enhance the privacy
guarantees of users that choose to remain anonymous by tackling two major
security challenges: one at the application layer of the scheme and one at its
network layer.",privacy extension
http://arxiv.org/abs/1904.12620v1,"With billions of personal images being generated from social media and
cameras of all sorts on a daily basis, security and privacy are unprecedentedly
challenged. Although extensive attempts have been made, existing face image
de-identification techniques are either insufficient in photo-reality or
incapable of balancing privacy and usability qualitatively and quantitatively,
i.e., they fail to answer counterfactual questions such as ""is it private
now?"", ""how private is it?"", and ""can it be more private?"" In this paper, we
propose a novel framework called AnonymousNet, with an effort to address these
issues systematically, balance usability, and enhance privacy in a natural and
measurable manner. The framework encompasses four stages: facial attribute
estimation, privacy-metric-oriented face obfuscation, directed natural image
synthesis, and adversarial perturbation. Not only do we achieve the
state-of-the-arts in terms of image quality and attribute prediction accuracy,
we are also the first to show that facial privacy is measurable, can be
factorized, and accordingly be manipulated in a photo-realistic fashion to
fulfill different requirements and application scenarios. Experiments further
demonstrate the effectiveness of the proposed framework.",privacy extension
http://arxiv.org/abs/1905.13264v1,"Social graphs are widely used in research (e.g., epidemiology) and business
(e.g., recommender systems). However, sharing these graphs poses privacy risks
because they contain sensitive information about individuals. Graph
anonymization techniques aim to protect individual users in a graph, while
graph de-anonymization aims to re-identify users. The effectiveness of
anonymization and de-anonymization algorithms is usually evaluated with privacy
metrics. However, it is unclear how strong existing privacy metrics are when
they are used in graph privacy. In this paper, we study 26 privacy metrics for
graph anonymization and de-anonymization and evaluate their strength in terms
of three criteria: monotonicity indicates whether the metric indicates lower
privacy for stronger adversaries; for within-scenario comparisons, evenness
indicates whether metric values are spread evenly; and for between-scenario
comparisons, shared value range indicates whether metrics use a consistent
value range across scenarios. Our extensive experiments indicate that no single
metric fulfills all three criteria perfectly. We therefore use methods from
multi-criteria decision analysis to aggregate multiple metrics in a metrics
suite, and we show that these metrics suites improve monotonicity compared to
the best individual metric. This important result enables more monotonic, and
thus more accurate, evaluations of new graph anonymization and de-anonymization
algorithms.",privacy extension
http://arxiv.org/abs/1907.10218v1,"The state-of-the-art federated learning brings a new direction for the data
privacy protection of mobile crowdsensing machine learning applications.
However, besides being vulnerable to GAN based user data construction attack,
the existing gradient descent based federate learning schemes are lack of
consideration for how to preserve the model privacy. In this paper, we propose
a secret sharing based federated extreme boosting learning frame-work (FedXGB)
to achieve privacy-preserving model training for mobile crowdsensing. First, a
series of protocols are designed to implement privacy-preserving extreme
gradient boosting of classification and regression tree. The protocols preserve
the user data privacy protection feature of federated learning that XGBoost is
trained without revealing plaintext user data. Then, in consideration of the
high commercial value of a well-trained model, a secure prediction protocol is
developed to protect the model privacy for the crowdsensing sponsor.
Additionally, we operate comprehensive theoretical analysis and extensive
experiments to evaluate the security, effectiveness and efficiency of FedXGB.
The results show that FedXGB is secure in the honest-but-curious model, and
attains approximate accuracy and convergence rate with the original model in
low runtime.",privacy extension
http://arxiv.org/abs/1512.06000v1,"The extensive collection and processing of personal information in big data
analytics has given rise to serious privacy concerns, related to wide scale
electronic surveillance, profiling, and disclosure of private data. To reap the
benefits of analytics without invading the individuals' private sphere, it is
essential to draw the limits of big data processing and integrate data
protection safeguards in the analytics value chain. ENISA, with the current
report, supports this approach and the position that the challenges of
technology (for big data) should be addressed by the opportunities of
technology (for privacy).
  We first explain the need to shift from ""big data versus privacy"" to ""big
data with privacy"". In this respect, the concept of privacy by design is key to
identify the privacy requirements early in the big data analytics value chain
and in subsequently implementing the necessary technical and organizational
measures.
  After an analysis of the proposed privacy by design strategies in the
different phases of the big data value chain, we review privacy enhancing
technologies of special interest for the current and future big data landscape.
In particular, we discuss anonymization, the ""traditional"" analytics technique,
the emerging area of encrypted search and privacy preserving computations,
granular access control mechanisms, policy enforcement and accountability, as
well as data provenance issues. Moreover, new transparency and access tools in
big data are explored, together with techniques for user empowerment and
control.
  Achieving ""big data with privacy"" is no easy task and a lot of research and
implementation is still needed. Yet, it remains a possible task, as long as all
the involved stakeholders take the necessary steps to integrate privacy and
data protection safeguards in the heart of big data, by design and by default.",privacy extension
http://arxiv.org/abs/1512.05110v2,"k-Anonymity and {\epsilon}-differential privacy are two mainstream privacy
models, the former introduced to anonymize data sets and the latter to limit
the knowledge gain that results from including one individual in the data set.
Whereas basic k-anonymity only protects against identity disclosure,
t-closeness was presented as an extension of k-anonymity that also protects
against attribute disclosure. We show here that, if not quite equivalent,
t-closeness and {\epsilon}-differential privacy are strongly related to one
another when it comes to anonymizing data sets. Specifically, k-anonymity for
the quasi-identifiers combined with {\epsilon}-differential privacy for the
confidential attributes yields stochastic t-closeness (an extension of
t-closeness), with t a function of k and {\epsilon}. Conversely, t-closeness
can yield {\epsilon}- differential privacy when t = exp({\epsilon}/2) and the
assumptions made by t-closeness about the prior and posterior views of the data
hold",privacy extension
http://arxiv.org/abs/1902.01580v1,"AI intensive systems that operate upon user data face the challenge of
balancing data utility with privacy concerns. We propose the idea and present
the prototype of an open-source tool called Privacy Utility Trade-off (PUT)
Workbench which seeks to aid software practitioners to take such crucial
decisions. We pick a simple privacy model that doesn't require any background
knowledge in Data Science and show how even that can achieve significant
results over standard and real-life datasets. The tool and the source code is
made freely available for extensions and usage.",privacy extension
http://arxiv.org/abs/1904.01711v1,"Perfect data privacy seems to be in fundamental opposition to the economical
and scientific opportunities associated with extensive data exchange. Defying
this intuition, this paper develops a framework that allows the disclosure of
collective properties of datasets without compromising the privacy of
individual data samples. We present an algorithm to build an optimal disclosure
strategy/mapping, and discuss it fundamental limits on finite and
asymptotically large datasets. Furthermore, we present explicit expressions to
the asymptotic performance of this scheme in some scenarios, and study cases
where our approach attains maximal efficiency. We finally discuss suboptimal
schemes to provide sample privacy guarantees to large datasets with a reduced
computational cost.",privacy extension
http://arxiv.org/abs/1808.01010v1,"Mobile location-based services (LBSs) empowered by mobile crowdsourcing
provide users with context-aware intelligent services based on user locations.
As smartphones are capable of collecting and disseminating massive user
location-embedded sensing information, privacy preservation for mobile users
has become a crucial issue. This paper proposes a metric called privacy
exposure to quantify the notion of privacy, which is subjective and qualitative
in nature, in order to support mobile LBSs to evaluate the effectiveness of
privacy-preserving solutions. This metric incorporates activity coverage and
activity uniformity to address two primary privacy threats, namely activity
hotspot disclosure and activity transition disclosure. In addition, we propose
an algorithm to minimize privacy exposure for mobile LBSs. We evaluate the
proposed metric and the privacy-preserving sensing algorithm via extensive
simulations. Moreover, we have also implemented the algorithm in an
Android-based mobile system and conducted real-world experiments. Both our
simulations and experimental results demonstrate that (1) the proposed metric
can properly quantify the privacy exposure level of human activities in the
spatial domain and (2) the proposed algorithm can effectively cloak users'
activity hotspots and transitions at both high and low user-mobility levels.",privacy extension
http://arxiv.org/abs/1311.5417v3,"In crowdsourcing markets, there are two different type jobs, i.e. homogeneous
jobs and heterogeneous jobs, which need to be allocated to workers. Incentive
mechanisms are essential to attract extensive user participating for achieving
good service quality, especially under a given budget constraint condition. To
this end, recently, Singer et al. propose a novel class of auction mechanisms
for determining near-optimal prices of tasks for crowdsourcing markets
constrained by the given budget. Their mechanisms are very useful to motivate
extensive user to truthfully participate in crowdsourcing markets. Although
they are so important, there still exist many security and privacy challenges
in real-life environments. In this paper, we present a general
privacy-preserving verifiable incentive mechanism for crowdsourcing markets
with the budget constraint, not only to exploit how to protect the bids and
assignments' privacy, and the chosen winners' privacy in crowdsourcing markets
with homogeneous jobs and heterogeneous jobs and identity privacy from users,
but also to make the verifiable payment between the platform and users for
crowdsourcing applications. Results show that our general privacy-preserving
verifiable incentive mechanisms achieve the same results as the generic one
without privacy preservation.",privacy extension
http://arxiv.org/abs/1809.08396v3,"The EU General Data Protection Regulation (GDPR) is one of the most demanding
and comprehensive privacy regulations of all time. A year after it went into
effect, we study its impact on the landscape of privacy policies online. We
conduct the first longitudinal, in-depth, and at-scale assessment of privacy
policies before and after the GDPR. We gauge the complete consumption cycle of
these policies, from the first user impressions until the compliance
assessment. We create a diverse corpus of two sets of 6,278 unique
English-language privacy policies from inside and outside the EU, covering
their pre-GDPR and the post-GDPR versions. The results of our tests and
analyses suggest that the GDPR has been a catalyst for a major overhaul of the
privacy policies inside and outside the EU. This overhaul of the policies,
manifesting in extensive textual changes, especially for the EU-based websites,
comes at mixed benefits to the users. While the privacy policies have become
considerably longer, our user study with 470 participants on Amazon MTurk
indicates a significant improvement in the visual representation of privacy
policies from the users' perspective for the EU websites. We further develop a
new workflow for the automated assessment of requirements in privacy policies.
Using this workflow, we show that privacy policies cover more data practices
and are more consistent with seven compliance requirements post the GDPR. We
also assess how transparent the organizations are with their privacy practices
by performing specificity analysis. In this analysis, we find evidence for
positive changes triggered by the GDPR, with the specificity level improving on
average. Still, we find the landscape of privacy policies to be in a
transitional phase; many policies still do not meet several key GDPR
requirements or their improved coverage comes with reduced specificity.",privacy extension
http://arxiv.org/abs/1906.01562v2,"With the rapid development of artificial intelligence (AI), ethical issues
surrounding AI have attracted increasing attention. In particular, autonomous
vehicles may face moral dilemmas in accident scenarios, such as staying the
course resulting in hurting pedestrians or swerving leading to hurting
passengers. To investigate such ethical dilemmas, recent studies have adopted
preference aggregation, in which each voter expresses her/his preferences over
decisions for the possible ethical dilemma scenarios, and a centralized system
aggregates these preferences to obtain the winning decision. Although a useful
methodology for building ethical AI systems, such an approach can potentially
violate the privacy of voters since moral preferences are sensitive information
and their disclosure can be exploited by malicious parties. In this paper, we
report a first-of-its-kind privacy-preserving crowd-guided AI decision-making
approach in ethical dilemmas. We adopt the notion of differential privacy to
quantify privacy and consider four granularities of privacy protection by
taking voter-/record-level privacy protection and centralized/distributed
perturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and
RLDP. Moreover, we propose different algorithms to achieve these privacy
protection granularities, while retaining the accuracy of the learned moral
preference model. Specifically, VLCP and RLCP are implemented with the data
aggregator setting a universal privacy parameter and perturbing the averaged
moral preference to protect the privacy of voters' data. VLDP and RLDP are
implemented in such a way that each voter perturbs her/his local moral
preference with a personalized privacy parameter. Extensive experiments on both
synthetic and real data demonstrate that the proposed approach can achieve high
accuracy of preference aggregation while protecting individual voter's privacy.",privacy extension
http://arxiv.org/abs/0909.5530v1,"Privacy preserving data publishing has attracted considerable research
interest in recent years. Among the existing solutions, {\em
$\epsilon$-differential privacy} provides one of the strongest privacy
guarantees. Existing data publishing methods that achieve
$\epsilon$-differential privacy, however, offer little data utility. In
particular, if the output dataset is used to answer count queries, the noise in
the query answers can be proportional to the number of tuples in the data,
which renders the results useless.
  In this paper, we develop a data publishing technique that ensures
$\epsilon$-differential privacy while providing accurate answers for {\em
range-count queries}, i.e., count queries where the predicate on each attribute
is a range. The core of our solution is a framework that applies {\em wavelet
transforms} on the data before adding noise to it. We present instantiations of
the proposed framework for both ordinal and nominal data, and we provide a
theoretical analysis on their privacy and utility guarantees. In an extensive
experimental study on both real and synthetic data, we show the effectiveness
and efficiency of our solution.",privacy extension
http://arxiv.org/abs/1109.6883v1,"With the growing use of location-based services, location privacy attracts
increasing attention from users, industry, and the research community. While
considerable effort has been devoted to inventing techniques that prevent
service providers from knowing a user's exact location, relatively little
attention has been paid to enabling so-called peer-wise privacy--the protection
of a user's location from unauthorized peer users. This paper identifies an
important efficiency problem in existing peer-privacy approaches that simply
apply a filtering step to identify users that are located in a query range, but
that do not want to disclose their location to the querying peer. To solve this
problem, we propose a novel, privacy-policy enabled index called the PEB-tree
that seamlessly integrates location proximity and policy compatibility. We
propose efficient algorithms that use the PEB-tree for processing privacy-aware
range and kNN queries. Extensive experiments suggest that the PEB-tree enables
efficient query processing.",privacy extension
http://arxiv.org/abs/1506.00242v1,"Motivated by tensions between data privacy for individual citizens, and
societal priorities such as counterterrorism and the containment of infectious
disease, we introduce a computational model that distinguishes between parties
for whom privacy is explicitly protected, and those for whom it is not (the
targeted subpopulation). The goal is the development of algorithms that can
effectively identify and take action upon members of the targeted subpopulation
in a way that minimally compromises the privacy of the protected, while
simultaneously limiting the expense of distinguishing members of the two groups
via costly mechanisms such as surveillance, background checks, or medical
testing. Within this framework, we provide provably privacy-preserving
algorithms for targeted search in social networks. These algorithms are natural
variants of common graph search methods, and ensure privacy for the protected
by the careful injection of noise in the prioritization of potential targets.
We validate the utility of our algorithms with extensive computational
experiments on two large-scale social network datasets.",privacy extension
http://arxiv.org/abs/1604.00666v1,"Privacy-preservation for sensitive data has become a challenging issue in
cloud computing. Threat modeling as a part of requirements engineering in
secure software development provides a structured approach for identifying
attacks and proposing countermeasures against the exploitation of
vulnerabilities in a system . This paper describes an extension of Cloud
Privacy Threat Modeling (CPTM) methodology for privacy threat modeling in
relation to processing sensitive data in cloud computing environments. It
describes the modeling methodology that involved applying Method Engineering to
specify characteristics of a cloud privacy threat modeling methodology,
different steps in the proposed methodology and corresponding products. In
addition, a case study has been implemented as a proof of concept to
demonstrate the usability of the proposed methodology. We believe that the
extended methodology facilitates the application of a privacy-preserving cloud
software development approach from requirements engineering to design.",privacy extension
http://arxiv.org/abs/1807.05756v1,"Privacy is a well-understood concept in the physical world, with us all
desiring some escape from the public gaze. However, while individuals might
recognise locking doors as protecting privacy, they have difficulty practising
equivalent actions online. Privacy salience considers the tangibility of this
important principle; one which is often obscured in digital environments.
Through extensively surveying a range of studies, we construct the first
taxonomies of privacy salience. After coding articles and identifying
commonalities, we categorise works by their methodologies, platforms and
underlying themes. While web browsing appears to be frequently analysed, the
Internet-of-Things has received little attention. Through our use of category
tuples and frequency matrices, we then explore those research opportunities
which might have been overlooked. These include studies of targeted advertising
and its affect on salience in social networks. It is through refining our
understanding of this important topic that we can better highlight the subject
of privacy.",privacy extension
http://arxiv.org/abs/1812.02292v1,"Machine learning (ML) classifiers are invaluable building blocks that have
been used in many fields. High quality training dataset collected from multiple
data providers is essential to train accurate classifiers. However, it raises
concern about data privacy due to potential leakage of sensitive information in
training dataset. Existing studies have proposed many solutions to
privacy-preserving training of ML classifiers, but it remains a challenging
task to strike a balance among accuracy, computational efficiency, and
security. In this paper, we propose Heda, an efficient privacypreserving scheme
for training ML classifiers. By combining homomorphic cryptosystem (HC) with
differential privacy (DP), Heda obtains the tradeoffs between efficiency and
accuracy, and enables flexible switch among different tradeoffs by parameter
tuning. In order to make such combination efficient and feasible, we present
novel designs based on both HC and DP: A library of building blocks based on
partially HC are proposed to construct complex training algorithms without
introducing a trusted thirdparty or computational relaxation; A set of
theoretical methods are proposed to determine appropriate privacy budget and to
reduce sensitivity. Security analysis demonstrates that our solution can
construct complex ML training algorithm securely. Extensive experimental
results show the effectiveness and efficiency of the proposed scheme.",privacy extension
http://arxiv.org/abs/1903.09315v1,"This paper proposes a privacy protocol for distributed average consensus
algorithms on bounded real-valued inputs that guarantees statistical privacy of
honest agents' inputs against colluding (passive adversarial) agents, if the
set of colluding agents is not a vertex cut in the underlying communication
network. This implies that privacy of agents' inputs is preserved against $t$
number of arbitrary colluding agents if the connectivity of the communication
network is at least $(t+1)$. A similar privacy protocol has been proposed for
the case of bounded integral inputs in our previous
paper~\cite{gupta2018information}. However, many applications of distributed
consensus concerning distributed control or state estimation deal with
real-valued inputs. Thus, in this paper we propose an extension of the privacy
protocol in~\cite{gupta2018information}, for bounded real-valued agents'
inputs, where bounds are known apriori to all the agents.",privacy extension
http://arxiv.org/abs/1906.06589v2,"Large capacity machine learning models are prone to membership inference
attacks in which an adversary aims to infer whether a particular data sample is
a member of the target model's training dataset. Such membership inferences can
lead to serious privacy violations as machine learning models are often trained
using privacy-sensitive data such as medical records and controversial user
opinions. Recently, defenses against membership inference attacks are
developed, in particular, based on differential privacy and adversarial
regularization; unfortunately, such defenses highly impact the classification
accuracy of the underlying machine learning models. In this work, we present a
new defense against membership inference attacks that preserves the utility of
the target machine learning models significantly better than prior defenses.
Our defense, called distillation for membership privacy (DMP), leverages
knowledge distillation to train machine learning models with membership
privacy. We analyze the key requirements for membership privacy and provide a
novel criterion to select data used for knowledge transfer, in order to improve
membership privacy of the final models. DMP works effectively against the
attackers with either a whitebox or blackbox access to the target model. We
evaluate DMP's performance through extensive experiments on different deep
neural networks and using various benchmark datasets. We show that DMP provides
a significantly better tradeoff between inference resistance and classification
performance than state-of-the-art membership inference defenses. For instance,
a DMP-trained DenseNet provides a classification accuracy of 65.3% for a 54.4%
blackbox membership inference attack accuracy, while an adversarially
regularized DenseNet provides a classification accuracy of only 53.7% for a
(much worse) 68.7% blackbox membership inference attack accuracy.",privacy extension
http://arxiv.org/abs/1608.04411v1,"VPN service providers (VSP) and IP-VPN customers have traditionally
maintained service demarcation boundaries between their routing and signaling
entities. This has resulted in the VPNs viewing the VSP network as an opaque
entity and therefore limiting any meaningful interaction between the VSP and
the VPNs. A key challenge is to expose each VPN to information about available
network resources through an abstraction (TA) [1] which is both accurate and
fair. In [2] we proposed three decentralized schemes assuming that all the
border nodes performing the abstraction have access to the entire core network
topology. This assumption likely leads to over- or under-subscription. In this
paper we develop centralized schemes to partition the core network capacities,
and assign each partition to a specific VPN for applying the decentralized
abstraction schemes presented in [2]. First, we present two schemes based on
the maximum concurrent flow and the maximum multicommodity flow (MMCF)
formulations. We then propose approaches to address the fairness concerns that
arise when MMCF formulation is used. We present results based on extensive
simulations on several topologies, and provide a comparative evaluation of the
different schemes in terms of abstraction efficiency, fairness to VPNs and call
performance characteristics achieved.",vpn alternative
http://arxiv.org/abs/1907.04023v1,"Anecdotal evidence suggests an increasing number of people are turning to VPN
services for the properties of privacy, anonymity and free communication over
the internet. Despite this, there is little research into what these services
are actually being used for. We use DNS cache snooping to determine what
domains people are accessing through VPNs. This technique is used to discover
whether certain queries have been made against a particular DNS server. Some
VPNs operate their own DNS servers, ensuring that any cached queries were made
by users of the VPN. We explore 3 methods of DNS cache snooping and briefly
discuss their strengths and limitations. Using the most reliable of the
methods, we perform a DNS cache snooping scan against the DNS servers of
several major VPN providers. With this we discover which domains are actually
accessed through VPNs. We run this technique against popular domains, as well
as those known to be censored in certain countries; China, Indonesia, Iran, and
Turkey. Our work gives a glimpse into what users use VPNs for, and provides a
technique for discovering the frequency with which domain records are accessed
on a DNS server.",vpn alternative
http://arxiv.org/abs/1001.2575v1,"Centralized Virtual Private Networks (VPNs) when used in distributed systems
have performance constraints as all traffic must traverse through a central
server. In recent years, there has been a paradigm shift towards the use of P2P
in VPNs to alleviate pressure placed upon the central server by allowing
participants to communicate directly with each other, relegating the server to
handling session management and supporting NAT traversal using relays when
necessary. Another, less common, approach uses unstructured P2P systems to
remove all centralization from the VPN. These approaches currently lack the
depth in security options provided by other VPN solutions, and their
scalability constraints have not been well studied.
  In this paper, we propose and implement a novel VPN architecture, which uses
a structured P2P system for peer discovery, session management, NAT traversal,
and autonomic relay selection and a central server as a partially-automated
public key infrastructure (PKI) via a user-friendly web interface. Our model
also provides the first design and implementation of a P2P VPN with full
tunneling support, whereby all non-P2P based Internet traffic routes through a
trusted third party and does so in a way that is more secure than existing full
tunnel techniques. To verify our model, we evaluate our reference
implementation by comparing it quantitatively to other VPN technologies
focusing on latency, bandwidth, and memory usage. We also discuss some of our
experiences with developing, maintaining, and deploying a P2P VPN.",vpn alternative
http://arxiv.org/abs/1709.05395v1,"The introduction of the WebRTC API to modern browsers has brought about a new
threat to user privacy. This API causes a range of client IP addresses to
become available to a visited website via JavaScript even if a VPN is in use.
This a potentially serious problem for users utilizing VPN services for
anonymity. In order to better understand the magnitude of this issue, we tested
widely used browsers and VPN services to discover which client IP addresses can
be revealed and in what circumstances. In most cases, at least one of the
client addresses is leaked. The number and type of leaked IP addresses are
affected by the choices of browser and VPN service, meaning that
privacy-sensitive users should choose their browser and their VPN provider with
care. We conclude by proposing countermeasures which can be used to help
mitigate this issue.",vpn alternative
http://arxiv.org/abs/1206.3365v1,"We experimentally demonstrate a novel optical physical-layer network coding
(PNC) scheme over time-division multiplexing (TDM) passive optical network
(PON). Full-duplex error-free communications between optical network units
(ONUs) at 2.5 Gb/s are shown for all-optical virtual private network (VPN)
applications. Compared to the conventional half-duplex communications set-up,
our scheme can increase the capacity by 100% with power penalty smaller than 3
dB. Synchronization of two ONUs is not required for the proposed VPN scheme",vpn alternative
http://arxiv.org/abs/1002.1152v1,"A Virtual Private Network (VPN) provides private network connections over a
publicly accessible shared network. The effective allocation of bandwidth for
VPNs assumes significance in the present scenario due to varied traffic. Each
VPN endpoint specifies bounds on the total amount of traffic that it is likely
to send or receive at any time. The network provider tailors the VPN so that
there is sufficient bandwidth for any traffic matrix that is consistent with
these bounds. The approach incorporates the use of Ad-hoc On demand Distance
Vector (AODV) protocol, with a view to accomplish an enhancement in the
performance of the mobile networks. The NS2 based simulation results are
evaluated in terms of its metrics for different bandwidth allocations, besides
analyzing its performance in the event of exigencies such as link failures. The
results highlight the suitability of the proposed strategy in the context of
real time applications.",vpn alternative
http://arxiv.org/abs/1201.0428v1,"Like most advances, wireless LAN poses both opportunities and risks. The
evolution of wireless networking in recent years has raised many serious
security issues. These security issues are of great concern for this technology
as it is being subjected to numerous attacks. Because of the free-space radio
transmission in wireless networks, eavesdropping becomes easy and consequently
a security breach may result in unauthorized access, information theft,
interference and service degradation. Virtual Private Networks (VPNs) have
emerged as an important solution to security threats surrounding the use of
public networks for private communications. While VPNs for wired line networks
have matured in both research and commercial environments, the design and
deployment of VPNs for WLAN is still an evolving field. This paper presents an
approach to secure IEEE 802.11g WLAN using OpenVPN, a transport layer VPN
solution and its impact on performance of IEEE 802.11g WLAN.",vpn alternative
http://arxiv.org/abs/1602.03706v1,"BGP/MPLS IP VPN and VPLS services are considered to be widely used in IP/MPLS
networks for connecting customers' remote sites. However, service providers
struggle with many challenges to provide these services. Management complexity,
equipment costs, and last but not least, scalability issues emerging as the
customers increase in number, are just some of these problems. Software-defined
networking (SDN) is an emerging paradigm that can solve aforementioned issues
using a logically centralized controller for network devices. In this paper, we
propose a SDN-based solution called SDxVPN which considerably lowers the
complexity of VPN service definition and management. Our method eliminates
complex and costly device interactions that used to be done through several
control plane protocols and enables customers to determine their service
specifications, define restriction policies and even interconnect with other
customers automatically without operator's intervention. We describe our
prototype implementation of SDxVPN and its scalability evaluations under
several representative scenarios. The results indicate the effectiveness of the
proposed solution for deployment to provide large scale VPN services.",vpn alternative
http://arxiv.org/abs/1707.03497v2,"This paper proposes a novel deep reinforcement learning (RL) architecture,
called Value Prediction Network (VPN), which integrates model-free and
model-based RL methods into a single neural network. In contrast to typical
model-based RL methods, VPN learns a dynamics model whose abstract states are
trained to make option-conditional predictions of future values (discounted sum
of rewards) rather than of future observations. Our experimental results show
that VPN has several advantages over both model-free and model-based baselines
in a stochastic environment where careful planning is required but building an
accurate observation-prediction model is difficult. Furthermore, VPN
outperforms Deep Q-Network (DQN) on several Atari games even with
short-lookahead planning, demonstrating its potential as a new way of learning
a good state representation.",vpn alternative
http://arxiv.org/abs/1001.4200v1,"This is an Internet era. Most of the organizations try to establish their
development centers and branch offices across the World. Employees working from
their homes are also becoming very popular and organizations benefit
financially by utilizing less office space, and reducing total expenses
incurred by having office workers on site. To meet such requirements
organizations develop a need to communicate with these offices over highly
secure, confidential and reliable connections regardless of the location of the
office. Here the VPN plays a vital role in establishing a distributed business
model.",vpn alternative
http://arxiv.org/abs/1509.00236v1,"Though objectives of trusted routing and virtual private networks (VPN) data
transfer methods are to guarantee data transfer securely to from senders to
receivers over public networks like Internet yet there are paramount
differences between the two methods. This paper analyses their differences.",vpn alternative
http://arxiv.org/abs/1604.08799v1,"This paper proposes a PKINIT_AS Kerberos V5 authentication system to use
public key cryptography and a method to implement the gssapi_krb authentication
method and secured Internet service using it in IPSec VPN",vpn alternative
http://arxiv.org/abs/1610.00527v1,"We propose a probabilistic video model, the Video Pixel Network (VPN), that
estimates the discrete joint distribution of the raw pixel values in a video.
The model and the neural architecture reflect the time, space and color
structure of video tensors and encode it as a four-dimensional dependency
chain. The VPN approaches the best possible performance on the Moving MNIST
benchmark, a leap over the previous state of the art, and the generated videos
show only minor deviations from the ground truth. The VPN also produces
detailed samples on the action-conditional Robotic Pushing benchmark and
generalizes to the motion of novel objects.",vpn alternative
http://arxiv.org/abs/1611.04268v2,"In this paper, we present a complete system for on-device passive monitoring,
collection, and analysis of fine grained, large-scale packet measurements from
mobile devices. First, we describe the design and implementation of AntMonitor
as a userspace mobile app based on a VPN-service but only on the device
(without the need to route through a remote VPN server) and using only the
minimum resources required. We evaluate our prototype and show that it
significantly outperforms prior state-of-the-art approaches: it achieves
throughput of over 90 Mbps downlink and 65 Mbps uplink, which is 2x and 8x
faster than mobile-only baselines and is 94% of the throughput without VPN,
while using 2-12x less energy. Second, we show that AntMonitor is uniquely
positioned to serve as a platform for passive on-device mobile network
monitoring and to enable a number of applications, including: (i) real-time
detection and prevention of private information leakage from the device to the
network; (ii) passive network performance monitoring; and (iii) application
classification and user profiling. We showcase preliminary results from a pilot
user study at a university campus.",vpn alternative
http://arxiv.org/abs/1811.10228v1,"We propose a semi-supervised model for detecting anomalies in videos
inspiredby the Video Pixel Network [van den Oord et al., 2016]. VPN is a
probabilisticgenerative model based on a deep neural network that estimates the
discrete jointdistribution of raw pixels in video frames. Our model extends the
Convolutional-LSTM video encoder part of the VPN with a novel convolutional
based attentionmechanism. We also modify the Pixel-CNN decoder part of the VPN
to a frameinpainting task where a partially masked version of the frame to
predict is given asinput. The frame reconstruction error is used as an anomaly
indicator. We test ourmodel on a modified version of the moving mnist dataset
[Srivastava et al., 2015]. Our model is shown to be effective in detecting
anomalies in videos. This approachcould be a component in applications
requiring visual common sense.",vpn alternative
http://arxiv.org/abs/1904.11423v1,"Secure communication is an integral feature of many Internet services. The
widely deployed TLS protects reliable transport protocols. DTLS extends TLS
security services to protocols relying on plain UDP packet transport, such as
VoIP or IoT applications. In this paper, we construct a model to determine the
performance of generic DTLS-enabled applications. Our model considers basic
network characteristics, e.g., number of connections, and the chosen security
parameters, e.g., the encryption algorithm in use. Measurements are presented
demonstrating the applicability of our model. These experiments are performed
using a high-performance DTLS-enabled VPN gateway built on top of the
well-established libraries DPDK and OpenSSL. This VPN solution represents the
most essential parts of DTLS, creating a DTLS performance baseline. Using this
baseline the model can be extended to predict even more complex DTLS protocols
besides the measured VPN. Code and measured data used in this paper are
publicly available at https://git.io/MoonSec and https://git.io/Sdata.",vpn alternative
http://arxiv.org/abs/1906.03560v1,"Sensing surroundings is ubiquitous and effortless to humans: It takes a
single glance to extract the spatial configuration of objects and the free
space from the scene. To help machine vision with spatial understanding
capabilities, we introduce the View Parsing Network (VPN) for cross-view
semantic segmentation. In this framework, the first-view observations are
parsed into a top-down-view semantic map indicating precise object locations.
VPN contains a view transformer module, designed to aggregate the first-view
observations taken from multiple angles and modalities, in order to draw a
bird-view semantic map. We evaluate the VPN framework for cross-view
segmentation on two types of environments, indoors and driving-traffic scenes.
Experimental results show that our model accurately predicts the top-down-view
semantic mask of the visible objects from the first-view observations, as well
as infer the location of contextually-relevant objects even if they are
invisible.",vpn alternative
http://arxiv.org/abs/1910.00159v1,"Distributed Virtual Private Networks (dVPNs) are new VPN solutions aiming to
solve the trust-privacy concern of a VPN's central authority by leveraging a
distributed architecture. In this paper, we first review the existing dVPN
ecosystem and debate on its privacy requirements. Then, we present VPN0, a dVPN
with strong privacy guarantees and minimal performance impact on its users.
VPN0 guarantees that a dVPN node only carries traffic it has ""whitelisted"",
without revealing its whitelist or knowing the traffic it tunnels. This is
achieved via three main innovations. First, an attestation mechanism which
leverages TLS to certify a user visit to a specific domain. Second, a zero
knowledge proof to certify that some incoming traffic is authorized, e.g.,
falls in a node's whitelist, without disclosing the target domain. Third, a
dynamic chain of VPN tunnels to both increase privacy and guarantee service
continuation while traffic certification is in place. The paper demonstrates
VPN0 functioning when integrated with several production systems, namely
BitTorrent DHT and ProtonVPN.",vpn alternative
http://arxiv.org/abs/1709.02656v3,"Internet traffic classification has become more important with rapid growth
of current Internet network and online applications. There have been numerous
studies on this topic which have led to many different approaches. Most of
these approaches use predefined features extracted by an expert in order to
classify network traffic. In contrast, in this study, we propose a \emph{deep
learning} based approach which integrates both feature extraction and
classification phases into one system. Our proposed scheme, called ""Deep
Packet,"" can handle both \emph{traffic characterization} in which the network
traffic is categorized into major classes (\eg, FTP and P2P) and application
identification in which end-user applications (\eg, BitTorrent and Skype)
identification is desired. Contrary to most of the current methods, Deep Packet
can identify encrypted traffic and also distinguishes between VPN and non-VPN
network traffic. After an initial pre-processing phase on data, packets are fed
into Deep Packet framework that embeds stacked autoencoder and convolution
neural network in order to classify network traffic. Deep packet with CNN as
its classification model achieved recall of $0.98$ in application
identification task and $0.94$ in traffic categorization task. To the best of
our knowledge, Deep Packet outperforms all of the proposed classification
methods on UNB ISCX VPN-nonVPN dataset.",vpn alternative
http://arxiv.org/abs/1406.7841v1,"Robust network design refers to a class of optimization problems that occur
when designing networks to efficiently handle variable demands. The notion of
""hierarchical hubbing"" was introduced (in the narrow context of a specific
robust network design question), by Olver and Shepherd [2010]. Hierarchical
hubbing allows for routings with a multiplicity of ""hubs"" which are connected
to the terminals and to each other in a treelike fashion. Recently, Fr\'echette
et al. [2013] explored this notion much more generally, focusing on its
applicability to an extension of the well-studied hose model that allows for
upper bounds on individual point-to-point demands. In this paper, we consider
hierarchical hubbing in the context of a previously studied (and extremely
natural) generalization of the hose model, and prove that the optimal
hierarchical hubbing solution can be found efficiently. This result is relevant
to a recently proposed generalization of the ""VPN Conjecture"".",vpn alternative
http://arxiv.org/abs/1907.03593v1,"In this paper we propose P4-IPsec which follows the software-defined
networking (SDN) paradigm. It comprises a P4-based implementation of an IPsec
gateway, a client agent, and a controller-based, IKE-less signalling between
them. P4-IPsec features the Encapsulation Security Payload (ESP) protocol,
tunnel mode, and various cipher suites for host-to-site virtual private
networks (VPNs). We consider the use case of a roadwarrior and multiple IPsec
gateways steered by the same controller. P4-IPsec supports on-demand VPN which
sets up tunnels to appropriate resources within these sites when requested by
applications. To validate the P4-based approach for IPsec gateways, we provide
three prototypes leveraging the software switch BMv2, the NetFPGA SUME card,
and the Edgecore Wedge 100BF-32X switch as P4 targets. For the latter, we
perform a performance evaluation giving experimental results on throughput and
delay.",vpn alternative
http://arxiv.org/abs/cs/0606123v1,"To demonstrate the result of researches in laboratory with the focus in
exhibiting the real impact of the use of the technology MPLS in LAN. Through
these researches we will verify that the investment in this technology is
shown, of the point of view cost/benefit, very interesting, being necessary,
however, the adoption of another measured, in order to settle down a
satisfactory level in the items Quality and safety in the sending of packages
in VPN but assisting to the requirement latency of the net very well being
shown in the tests that it consumes on average one Tuesday leaves of the time
spend for the same function in routing IP.",vpn alternative
http://arxiv.org/abs/1009.2491v1,"This paper reviews the requirements for the security mechanisms that are
currently being developed in the framework of the European research project
INDECT. An overview of features for integrated technologies such as Virtual
Private Networks (VPNs), Cryptographic Algorithms, Quantum Cryptography,
Federated ID Management and Secure Mobile Ad-hoc networking are described
together with their expected use in INDECT.",vpn alternative
http://arxiv.org/abs/1906.11288v1,"In this article, we provide a summary of recent efforts towards achieving
Internet geolocation securely, \ie without allowing the entity being geolocated
to cheat about its own geographic location. Cheating motivations arise from
many factors, including impersonation (in the case locations are used to
reinforce authentication), and gaining location-dependent benefits. In
particular, we provide a technical overview of Client Presence Verification
(CPV) and Server Location Verification (SLV)---two recently proposed techniques
designed to verify the geographic locations of clients and servers in realtime
over the Internet. Each technique addresses a wide range of adversarial tactics
to manipulate geolocation, including the use of IP-hiding technologies like
VPNs and anonymizers, as we now explain.",vpn alternative
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",vpn alternative
http://arxiv.org/abs/1409.2261v1,"Traditionally, 802.11-based networks that relied on wired equivalent protocol
(WEP) were especially vulnerable to packet sniffing. Today, wireless networks
are more prolific, and the monitoring devices used to find them are mobile and
easy to access. Securing wireless networks can be difficult because these
networks consist of radio transmitters and receivers, and anybody can listen,
capture data and attempt to compromise it. In recent years, a range of
technologies and mechanisms have helped make networking more secure. This paper
holistically evaluated various enhanced protocols proposed to solve WEP related
authentication, confidentiality and integrity problems. It discovered that
strength of each solution depends on how well the encryption, authentication
and integrity techniques work. The work suggested using a Defence-in-Depth
Strategy and integration of biometric solution in 802.11i. Comprehensive
in-depth comparative analysis of each of the security mechanisms is driven by
review of related work in WLAN security solutions.",vpn alternative
http://arxiv.org/abs/1206.1748v1,"VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system",vpn alternative
http://arxiv.org/abs/1206.5469v1,"Quality of Service (QoS) techniques are applied in IP networks to utilize
available network resources in the most efficient manner to minimize delays and
delay variations (jitters) in network traffic having multiple type of services.
Multimedia services may include voice, video and database. Researchers have
done considerable work on queuing disciplines to analyze and improve QoS
performance in wired and wireless IP networks. This paper highlights QoS
analysis in a wired IP network with more realistic enterprise modeling and
presents simulation results of a few statistics not presented and discussed
before. Four different applications are used i.e. FTP, Database, Voice over IP
(VoIP) and Video Conferencing (VC). Two major queuing disciplines are evaluated
i.e. 'Priority Queuing' and 'Weighted Fair Queuing' for packet identification
under Differentiated Services Code Point (DSCP). The simulation results show
that WFQ has an edge over PQ in terms of queuing delays and jitters experienced
by low priority services. For high priority traffic, dependency of 'Traffic
Drop', 'Buffer Usage' and 'Packet Delay Variation' on selected buffer sizes is
simulated and discussed to evaluate QoS deeper. In the end, it is also analyzed
how network's database service with applied Quality of Service may be affected
in terms of throughput (average rate of data received) for internal network
users when the server is also accessed by external user(s) through Virtual
Private Network (VPN).",vpn alternative
http://arxiv.org/abs/0905.1362v1,"We focus in this paper on the problem of configuring and managing network
security devices, such as Firewalls, Virtual Private Network (VPN) tunnels, and
Intrusion Detection Systems (IDSs). Our proposal is the following. First, we
formally specify the security requirements of a given system by using an
expressive access control model. As a result, we obtain an abstract security
policy, which is free of ambiguities, redundancies or unnecessary details.
Second, we deploy such an abstract policy through a set of automatic
compilations into the security devices of the system. This proposed deployment
process not only simplifies the security administrator's job, but also
guarantees a resulting configuration free of anomalies and/or inconsistencies.",vpn alternative
http://arxiv.org/abs/0908.0175v1,"The Border Gateway Protocol (BGP) is an important component in today's IP
network infrastructure. As the main routing protocol of the Internet, clear
understanding of its dynamics is crucial for configuring, diagnosing and
debugging Internet routing problems. Despite the increase in the services that
BGP provide such as MPLS VPNs, there is no much progress achieved in automating
the BGP management tasks. In this paper we discuss some of the problems
encountered by network engineers when managing BGP networks. We also describe
some of the open source tools and methods that attempt to resolve these issues.
Then we present some of the features that, if implemented, will ease BGP
management related tasks.",vpn alternative
http://arxiv.org/abs/0909.4858v1,"Mobile IPv6 will be an integral part of the next generation Internet
protocol. The importance of mobility in the Internet gets keep on increasing.
Current specification of Mobile IPv6 does not provide proper support for
reliability in the mobile network and there are other problems associated with
it. In this paper, we propose Virtual Private Network (VPN) based Home Agent
Reliability Protocol (VHAHA) as a complete system architecture and extension to
Mobile IPv6 that supports reliability and offers solutions to the security
problems that are found in Mobile IP registration part. The key features of
this protocol over other protocols are: better survivability, transparent
failure detection and recovery, reduced complexity of the system and workload,
secure data transfer and improved overall performance.",vpn alternative
http://arxiv.org/abs/0910.3511v1,"We define stealth Man-in-the-Middle adversaries, and analyse their ability to
launch denial and degradation of service (DoS) attacks on secure channels. We
show realistic attacks, disrupting TCP communication over secure VPNs using
IPsec. We present:
  First amplifying DoS attack on IPsec, when deployed without anti-replay
window.
  First amplifying attack on IPsec, when deployed with a `small' anti-replay
window, and analysis of `sufficient' window size.
  First amplifying attack on IPsec, when deployed with `sufficient' window
size. This attack (as the previous) is realistic: attacker needs only to
duplicate and speed-up few packets.
  We also suggest a solution designed to prevent the presented attacks, and to
provide secure channel immune to degradation and other DoS attacks. Our
solution involves changes (only) to the two gateway machines running IPsec.
  In addition to their practical importance, our results also raise the
challenge of formally defining secure channels immune to DoS and degradation
attacks, and providing provably-secure implementations.",vpn alternative
http://arxiv.org/abs/1204.1245v1,"Multi-Protocol Label Switching (MPLS) had been deployed by many data
networking service providers, including the next-generation mobile backhaul
networks, because of its undeniable potential in terms of virtual private
network (VPN) management, traffic engineering, etc. In MPLS networks, IP
packets are transmitted along a Label Switched Path (LSP) established between
edge nodes. To improve the efficiency of resource use in MPLS networks, it is
essential to utilize the LSPs efficiently.
  This paper proposes a method of selecting the optimal LSP pair from among
multiple LSP pairs which are established between the same pair of edge nodes,
on the assumption that both the upward and downward LSPs are established as a
pair (both-way operation). It is supposed that both upward and downward
bandwidths are allocated simultaneously in the selected LSP pair for each
service request. It is demonstrated by simulation evaluations that the proposal
method could reduce the total amount of the bandwidth required by up to 15%
compared with the conventional selection method. The proposed method can also
reuse the know-how and management tools in many existing networks which are
based on both-way operation.",vpn alternative
http://arxiv.org/abs/1403.6644v1,"If simplicity is a key strategy for success as a network protocol OpenFlow is
not winning. At its core OpenFlow presents a simple idea, which is a network
switch data plane abstraction along with a control protocol for manipulating
that abstraction. The result of this idea has been far from simple: a new
version released each year, five active versions, com- plex feature
dependencies, unstable version negotiation, lack of state machine definition,
etc. This complexity represents roadblocks for network, software, and hardware
engineers.
  We have distilled the core abstractions present in 5 existing versions of
OpenFlow and refactored them into a simple API called tinyNBI. Our work does
not provide high-level network abstractions (address pools, VPN maps, etc.),
instead it focuses on providing a clean low level interface that supports the
development of these higher layer abstractions. The goal of tinyNBI is to allow
configuration of all existing OpenFlow abstractions without having to deal with
the unique personalities of each version of OpenFlow or their level of support
in target switches.",vpn alternative
http://arxiv.org/abs/1404.2153v1,"As tablet devices continue to gain market share at the expense of the
traditional PC, they become a more integral part of the corporate landscape.
Tablets are no longer being utilized only by sales executives for presentation
purposes, or as addition to the traditional laptop. Users are attempting to
perform significant amounts of their daily work on tablet devices, some even
abandoning the ubiquitous laptop or desktop entirely. Operating exclusively
from a tablet device, specifically Apple IOS tablet devices creates unique
challenges in a corporate environment traditionally dominated by Microsoft
Windows operating systems. Interactions with file shares, presentation media,
VPN, and remote access present barriers that users and helpdesk support are
unfamiliar with in a relation to an iPad or iPhone. Many solutions are being
offered to these challenges some of which are analyzed by this manuscript.",vpn alternative
http://arxiv.org/abs/1410.2087v3,"We introduce an attack against encrypted web traffic that makes use only of
packet timing information on the uplink. This attack is therefore impervious to
existing packet padding defences. In addition, unlike existing approaches this
timing-only attack does not require knowledge of the start/end of web fetches
and so is effective against traffic streams. We demonstrate the effectiveness
of the attack against both wired and wireless traffic, achieving mean success
rates in excess of 90%. In addition to being of interest in its own right, this
timing-only attack serves to highlight deficiencies in existing defences and so
to areas where it would be beneficial for VPN designers to focus further
attention.",vpn alternative
http://arxiv.org/abs/1907.04023v1,"Anecdotal evidence suggests an increasing number of people are turning to VPN
services for the properties of privacy, anonymity and free communication over
the internet. Despite this, there is little research into what these services
are actually being used for. We use DNS cache snooping to determine what
domains people are accessing through VPNs. This technique is used to discover
whether certain queries have been made against a particular DNS server. Some
VPNs operate their own DNS servers, ensuring that any cached queries were made
by users of the VPN. We explore 3 methods of DNS cache snooping and briefly
discuss their strengths and limitations. Using the most reliable of the
methods, we perform a DNS cache snooping scan against the DNS servers of
several major VPN providers. With this we discover which domains are actually
accessed through VPNs. We run this technique against popular domains, as well
as those known to be censored in certain countries; China, Indonesia, Iran, and
Turkey. Our work gives a glimpse into what users use VPNs for, and provides a
technique for discovering the frequency with which domain records are accessed
on a DNS server.",free vpn
http://arxiv.org/abs/1206.3365v1,"We experimentally demonstrate a novel optical physical-layer network coding
(PNC) scheme over time-division multiplexing (TDM) passive optical network
(PON). Full-duplex error-free communications between optical network units
(ONUs) at 2.5 Gb/s are shown for all-optical virtual private network (VPN)
applications. Compared to the conventional half-duplex communications set-up,
our scheme can increase the capacity by 100% with power penalty smaller than 3
dB. Synchronization of two ONUs is not required for the proposed VPN scheme",free vpn
http://arxiv.org/abs/1707.03497v2,"This paper proposes a novel deep reinforcement learning (RL) architecture,
called Value Prediction Network (VPN), which integrates model-free and
model-based RL methods into a single neural network. In contrast to typical
model-based RL methods, VPN learns a dynamics model whose abstract states are
trained to make option-conditional predictions of future values (discounted sum
of rewards) rather than of future observations. Our experimental results show
that VPN has several advantages over both model-free and model-based baselines
in a stochastic environment where careful planning is required but building an
accurate observation-prediction model is difficult. Furthermore, VPN
outperforms Deep Q-Network (DQN) on several Atari games even with
short-lookahead planning, demonstrating its potential as a new way of learning
a good state representation.",free vpn
http://arxiv.org/abs/1201.0428v1,"Like most advances, wireless LAN poses both opportunities and risks. The
evolution of wireless networking in recent years has raised many serious
security issues. These security issues are of great concern for this technology
as it is being subjected to numerous attacks. Because of the free-space radio
transmission in wireless networks, eavesdropping becomes easy and consequently
a security breach may result in unauthorized access, information theft,
interference and service degradation. Virtual Private Networks (VPNs) have
emerged as an important solution to security threats surrounding the use of
public networks for private communications. While VPNs for wired line networks
have matured in both research and commercial environments, the design and
deployment of VPNs for WLAN is still an evolving field. This paper presents an
approach to secure IEEE 802.11g WLAN using OpenVPN, a transport layer VPN
solution and its impact on performance of IEEE 802.11g WLAN.",free vpn
http://arxiv.org/abs/1906.03560v1,"Sensing surroundings is ubiquitous and effortless to humans: It takes a
single glance to extract the spatial configuration of objects and the free
space from the scene. To help machine vision with spatial understanding
capabilities, we introduce the View Parsing Network (VPN) for cross-view
semantic segmentation. In this framework, the first-view observations are
parsed into a top-down-view semantic map indicating precise object locations.
VPN contains a view transformer module, designed to aggregate the first-view
observations taken from multiple angles and modalities, in order to draw a
bird-view semantic map. We evaluate the VPN framework for cross-view
segmentation on two types of environments, indoors and driving-traffic scenes.
Experimental results show that our model accurately predicts the top-down-view
semantic mask of the visible objects from the first-view observations, as well
as infer the location of contextually-relevant objects even if they are
invisible.",free vpn
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",free vpn
http://arxiv.org/abs/1608.04411v1,"VPN service providers (VSP) and IP-VPN customers have traditionally
maintained service demarcation boundaries between their routing and signaling
entities. This has resulted in the VPNs viewing the VSP network as an opaque
entity and therefore limiting any meaningful interaction between the VSP and
the VPNs. A key challenge is to expose each VPN to information about available
network resources through an abstraction (TA) [1] which is both accurate and
fair. In [2] we proposed three decentralized schemes assuming that all the
border nodes performing the abstraction have access to the entire core network
topology. This assumption likely leads to over- or under-subscription. In this
paper we develop centralized schemes to partition the core network capacities,
and assign each partition to a specific VPN for applying the decentralized
abstraction schemes presented in [2]. First, we present two schemes based on
the maximum concurrent flow and the maximum multicommodity flow (MMCF)
formulations. We then propose approaches to address the fairness concerns that
arise when MMCF formulation is used. We present results based on extensive
simulations on several topologies, and provide a comparative evaluation of the
different schemes in terms of abstraction efficiency, fairness to VPNs and call
performance characteristics achieved.",free vpn
http://arxiv.org/abs/1001.2575v1,"Centralized Virtual Private Networks (VPNs) when used in distributed systems
have performance constraints as all traffic must traverse through a central
server. In recent years, there has been a paradigm shift towards the use of P2P
in VPNs to alleviate pressure placed upon the central server by allowing
participants to communicate directly with each other, relegating the server to
handling session management and supporting NAT traversal using relays when
necessary. Another, less common, approach uses unstructured P2P systems to
remove all centralization from the VPN. These approaches currently lack the
depth in security options provided by other VPN solutions, and their
scalability constraints have not been well studied.
  In this paper, we propose and implement a novel VPN architecture, which uses
a structured P2P system for peer discovery, session management, NAT traversal,
and autonomic relay selection and a central server as a partially-automated
public key infrastructure (PKI) via a user-friendly web interface. Our model
also provides the first design and implementation of a P2P VPN with full
tunneling support, whereby all non-P2P based Internet traffic routes through a
trusted third party and does so in a way that is more secure than existing full
tunnel techniques. To verify our model, we evaluate our reference
implementation by comparing it quantitatively to other VPN technologies
focusing on latency, bandwidth, and memory usage. We also discuss some of our
experiences with developing, maintaining, and deploying a P2P VPN.",free vpn
http://arxiv.org/abs/0905.1362v1,"We focus in this paper on the problem of configuring and managing network
security devices, such as Firewalls, Virtual Private Network (VPN) tunnels, and
Intrusion Detection Systems (IDSs). Our proposal is the following. First, we
formally specify the security requirements of a given system by using an
expressive access control model. As a result, we obtain an abstract security
policy, which is free of ambiguities, redundancies or unnecessary details.
Second, we deploy such an abstract policy through a set of automatic
compilations into the security devices of the system. This proposed deployment
process not only simplifies the security administrator's job, but also
guarantees a resulting configuration free of anomalies and/or inconsistencies.",free vpn
http://arxiv.org/abs/1709.05395v1,"The introduction of the WebRTC API to modern browsers has brought about a new
threat to user privacy. This API causes a range of client IP addresses to
become available to a visited website via JavaScript even if a VPN is in use.
This a potentially serious problem for users utilizing VPN services for
anonymity. In order to better understand the magnitude of this issue, we tested
widely used browsers and VPN services to discover which client IP addresses can
be revealed and in what circumstances. In most cases, at least one of the
client addresses is leaked. The number and type of leaked IP addresses are
affected by the choices of browser and VPN service, meaning that
privacy-sensitive users should choose their browser and their VPN provider with
care. We conclude by proposing countermeasures which can be used to help
mitigate this issue.",free vpn
http://arxiv.org/abs/1002.1152v1,"A Virtual Private Network (VPN) provides private network connections over a
publicly accessible shared network. The effective allocation of bandwidth for
VPNs assumes significance in the present scenario due to varied traffic. Each
VPN endpoint specifies bounds on the total amount of traffic that it is likely
to send or receive at any time. The network provider tailors the VPN so that
there is sufficient bandwidth for any traffic matrix that is consistent with
these bounds. The approach incorporates the use of Ad-hoc On demand Distance
Vector (AODV) protocol, with a view to accomplish an enhancement in the
performance of the mobile networks. The NS2 based simulation results are
evaluated in terms of its metrics for different bandwidth allocations, besides
analyzing its performance in the event of exigencies such as link failures. The
results highlight the suitability of the proposed strategy in the context of
real time applications.",free vpn
http://arxiv.org/abs/1602.03706v1,"BGP/MPLS IP VPN and VPLS services are considered to be widely used in IP/MPLS
networks for connecting customers' remote sites. However, service providers
struggle with many challenges to provide these services. Management complexity,
equipment costs, and last but not least, scalability issues emerging as the
customers increase in number, are just some of these problems. Software-defined
networking (SDN) is an emerging paradigm that can solve aforementioned issues
using a logically centralized controller for network devices. In this paper, we
propose a SDN-based solution called SDxVPN which considerably lowers the
complexity of VPN service definition and management. Our method eliminates
complex and costly device interactions that used to be done through several
control plane protocols and enables customers to determine their service
specifications, define restriction policies and even interconnect with other
customers automatically without operator's intervention. We describe our
prototype implementation of SDxVPN and its scalability evaluations under
several representative scenarios. The results indicate the effectiveness of the
proposed solution for deployment to provide large scale VPN services.",free vpn
http://arxiv.org/abs/1904.07088v1,"We propose P4-MACsec to protect network links between P4 switches through
automated deployment of MACsec, a widespread IEEE standard for securing Layer 2
infrastructures. It is supported by switches and routers from major
manufacturers and has only little performance limitations compared to VPN
technologies such as IPsec. P4-MACsec introduces a data plane implementation of
MACsec including AES-GCM encryption and decryption directly on P4 switches.
P4-MACsec features a two-tier control plane structure where local controllers
running on the P4 switches interact with a central controller. We propose a
novel secure link discovery mechanism that leverages protected LLDP frames and
the two-tier control plane structure for secure and efficient management of a
global link map. Automated deployment of MACsec creates secure channel,
generates keying material, and configures the P4 switches for each detected
link between two P4 switches. It detects link changes and performs rekeying to
provide a secure, configuration-free operation of MACsec. In this paper, we
review the technological background of P4-MACsec and explain its architecture.
To demonstrate the feasibility of P4-MACsec, we implement it on the BMv2 P4
software switch and validate the prototype through experiments. We evaluate its
performance through experiments that focus on TCP throughput and round-trip
time. We publish the prototype and experiment setups on Github.",free vpn
http://arxiv.org/abs/1907.07120v1,"The prevalence of Internet censorship has prompted the creation of several
measurement platforms for monitoring filtering activities. An important
challenge faced by these platforms revolves around the trade-off between depth
of measurement and breadth of coverage. In this paper, we present an
opportunistic censorship measurement infrastructure built on top of a network
of distributed VPN servers run by volunteers, which we used to measure the
extent to which the I2P anonymity network is blocked around the world. This
infrastructure provides us with not only numerous and geographically diverse
vantage points, but also the ability to conduct in-depth measurements across
all levels of the network stack. Using this infrastructure, we measured at a
global scale the availability of four different I2P services: the official
homepage, its mirror site, reseed servers, and active relays in the network.
Within a period of one month, we conducted a total of 54K measurements from
1.7K network locations in 164 countries. With different techniques for
detecting domain name blocking, network packet injection, and block pages, we
discovered I2P censorship in five countries: China, Iran, Oman, Qatar, and
Kuwait. Finally, we conclude by discussing potential approaches to circumvent
censorship on I2P.",free vpn
http://arxiv.org/abs/1001.4200v1,"This is an Internet era. Most of the organizations try to establish their
development centers and branch offices across the World. Employees working from
their homes are also becoming very popular and organizations benefit
financially by utilizing less office space, and reducing total expenses
incurred by having office workers on site. To meet such requirements
organizations develop a need to communicate with these offices over highly
secure, confidential and reliable connections regardless of the location of the
office. Here the VPN plays a vital role in establishing a distributed business
model.",free vpn
http://arxiv.org/abs/1509.00236v1,"Though objectives of trusted routing and virtual private networks (VPN) data
transfer methods are to guarantee data transfer securely to from senders to
receivers over public networks like Internet yet there are paramount
differences between the two methods. This paper analyses their differences.",free vpn
http://arxiv.org/abs/1604.08799v1,"This paper proposes a PKINIT_AS Kerberos V5 authentication system to use
public key cryptography and a method to implement the gssapi_krb authentication
method and secured Internet service using it in IPSec VPN",free vpn
http://arxiv.org/abs/1610.00527v1,"We propose a probabilistic video model, the Video Pixel Network (VPN), that
estimates the discrete joint distribution of the raw pixel values in a video.
The model and the neural architecture reflect the time, space and color
structure of video tensors and encode it as a four-dimensional dependency
chain. The VPN approaches the best possible performance on the Moving MNIST
benchmark, a leap over the previous state of the art, and the generated videos
show only minor deviations from the ground truth. The VPN also produces
detailed samples on the action-conditional Robotic Pushing benchmark and
generalizes to the motion of novel objects.",free vpn
http://arxiv.org/abs/1611.04268v2,"In this paper, we present a complete system for on-device passive monitoring,
collection, and analysis of fine grained, large-scale packet measurements from
mobile devices. First, we describe the design and implementation of AntMonitor
as a userspace mobile app based on a VPN-service but only on the device
(without the need to route through a remote VPN server) and using only the
minimum resources required. We evaluate our prototype and show that it
significantly outperforms prior state-of-the-art approaches: it achieves
throughput of over 90 Mbps downlink and 65 Mbps uplink, which is 2x and 8x
faster than mobile-only baselines and is 94% of the throughput without VPN,
while using 2-12x less energy. Second, we show that AntMonitor is uniquely
positioned to serve as a platform for passive on-device mobile network
monitoring and to enable a number of applications, including: (i) real-time
detection and prevention of private information leakage from the device to the
network; (ii) passive network performance monitoring; and (iii) application
classification and user profiling. We showcase preliminary results from a pilot
user study at a university campus.",free vpn
http://arxiv.org/abs/1811.10228v1,"We propose a semi-supervised model for detecting anomalies in videos
inspiredby the Video Pixel Network [van den Oord et al., 2016]. VPN is a
probabilisticgenerative model based on a deep neural network that estimates the
discrete jointdistribution of raw pixels in video frames. Our model extends the
Convolutional-LSTM video encoder part of the VPN with a novel convolutional
based attentionmechanism. We also modify the Pixel-CNN decoder part of the VPN
to a frameinpainting task where a partially masked version of the frame to
predict is given asinput. The frame reconstruction error is used as an anomaly
indicator. We test ourmodel on a modified version of the moving mnist dataset
[Srivastava et al., 2015]. Our model is shown to be effective in detecting
anomalies in videos. This approachcould be a component in applications
requiring visual common sense.",free vpn
http://arxiv.org/abs/1904.11423v1,"Secure communication is an integral feature of many Internet services. The
widely deployed TLS protects reliable transport protocols. DTLS extends TLS
security services to protocols relying on plain UDP packet transport, such as
VoIP or IoT applications. In this paper, we construct a model to determine the
performance of generic DTLS-enabled applications. Our model considers basic
network characteristics, e.g., number of connections, and the chosen security
parameters, e.g., the encryption algorithm in use. Measurements are presented
demonstrating the applicability of our model. These experiments are performed
using a high-performance DTLS-enabled VPN gateway built on top of the
well-established libraries DPDK and OpenSSL. This VPN solution represents the
most essential parts of DTLS, creating a DTLS performance baseline. Using this
baseline the model can be extended to predict even more complex DTLS protocols
besides the measured VPN. Code and measured data used in this paper are
publicly available at https://git.io/MoonSec and https://git.io/Sdata.",free vpn
http://arxiv.org/abs/1910.00159v1,"Distributed Virtual Private Networks (dVPNs) are new VPN solutions aiming to
solve the trust-privacy concern of a VPN's central authority by leveraging a
distributed architecture. In this paper, we first review the existing dVPN
ecosystem and debate on its privacy requirements. Then, we present VPN0, a dVPN
with strong privacy guarantees and minimal performance impact on its users.
VPN0 guarantees that a dVPN node only carries traffic it has ""whitelisted"",
without revealing its whitelist or knowing the traffic it tunnels. This is
achieved via three main innovations. First, an attestation mechanism which
leverages TLS to certify a user visit to a specific domain. Second, a zero
knowledge proof to certify that some incoming traffic is authorized, e.g.,
falls in a node's whitelist, without disclosing the target domain. Third, a
dynamic chain of VPN tunnels to both increase privacy and guarantee service
continuation while traffic certification is in place. The paper demonstrates
VPN0 functioning when integrated with several production systems, namely
BitTorrent DHT and ProtonVPN.",free vpn
http://arxiv.org/abs/1709.02656v3,"Internet traffic classification has become more important with rapid growth
of current Internet network and online applications. There have been numerous
studies on this topic which have led to many different approaches. Most of
these approaches use predefined features extracted by an expert in order to
classify network traffic. In contrast, in this study, we propose a \emph{deep
learning} based approach which integrates both feature extraction and
classification phases into one system. Our proposed scheme, called ""Deep
Packet,"" can handle both \emph{traffic characterization} in which the network
traffic is categorized into major classes (\eg, FTP and P2P) and application
identification in which end-user applications (\eg, BitTorrent and Skype)
identification is desired. Contrary to most of the current methods, Deep Packet
can identify encrypted traffic and also distinguishes between VPN and non-VPN
network traffic. After an initial pre-processing phase on data, packets are fed
into Deep Packet framework that embeds stacked autoencoder and convolution
neural network in order to classify network traffic. Deep packet with CNN as
its classification model achieved recall of $0.98$ in application
identification task and $0.94$ in traffic categorization task. To the best of
our knowledge, Deep Packet outperforms all of the proposed classification
methods on UNB ISCX VPN-nonVPN dataset.",free vpn
http://arxiv.org/abs/1406.7841v1,"Robust network design refers to a class of optimization problems that occur
when designing networks to efficiently handle variable demands. The notion of
""hierarchical hubbing"" was introduced (in the narrow context of a specific
robust network design question), by Olver and Shepherd [2010]. Hierarchical
hubbing allows for routings with a multiplicity of ""hubs"" which are connected
to the terminals and to each other in a treelike fashion. Recently, Fr\'echette
et al. [2013] explored this notion much more generally, focusing on its
applicability to an extension of the well-studied hose model that allows for
upper bounds on individual point-to-point demands. In this paper, we consider
hierarchical hubbing in the context of a previously studied (and extremely
natural) generalization of the hose model, and prove that the optimal
hierarchical hubbing solution can be found efficiently. This result is relevant
to a recently proposed generalization of the ""VPN Conjecture"".",free vpn
http://arxiv.org/abs/1907.03593v1,"In this paper we propose P4-IPsec which follows the software-defined
networking (SDN) paradigm. It comprises a P4-based implementation of an IPsec
gateway, a client agent, and a controller-based, IKE-less signalling between
them. P4-IPsec features the Encapsulation Security Payload (ESP) protocol,
tunnel mode, and various cipher suites for host-to-site virtual private
networks (VPNs). We consider the use case of a roadwarrior and multiple IPsec
gateways steered by the same controller. P4-IPsec supports on-demand VPN which
sets up tunnels to appropriate resources within these sites when requested by
applications. To validate the P4-based approach for IPsec gateways, we provide
three prototypes leveraging the software switch BMv2, the NetFPGA SUME card,
and the Edgecore Wedge 100BF-32X switch as P4 targets. For the latter, we
perform a performance evaluation giving experimental results on throughput and
delay.",free vpn
http://arxiv.org/abs/cs/0606123v1,"To demonstrate the result of researches in laboratory with the focus in
exhibiting the real impact of the use of the technology MPLS in LAN. Through
these researches we will verify that the investment in this technology is
shown, of the point of view cost/benefit, very interesting, being necessary,
however, the adoption of another measured, in order to settle down a
satisfactory level in the items Quality and safety in the sending of packages
in VPN but assisting to the requirement latency of the net very well being
shown in the tests that it consumes on average one Tuesday leaves of the time
spend for the same function in routing IP.",free vpn
http://arxiv.org/abs/1009.2491v1,"This paper reviews the requirements for the security mechanisms that are
currently being developed in the framework of the European research project
INDECT. An overview of features for integrated technologies such as Virtual
Private Networks (VPNs), Cryptographic Algorithms, Quantum Cryptography,
Federated ID Management and Secure Mobile Ad-hoc networking are described
together with their expected use in INDECT.",free vpn
http://arxiv.org/abs/1906.11288v1,"In this article, we provide a summary of recent efforts towards achieving
Internet geolocation securely, \ie without allowing the entity being geolocated
to cheat about its own geographic location. Cheating motivations arise from
many factors, including impersonation (in the case locations are used to
reinforce authentication), and gaining location-dependent benefits. In
particular, we provide a technical overview of Client Presence Verification
(CPV) and Server Location Verification (SLV)---two recently proposed techniques
designed to verify the geographic locations of clients and servers in realtime
over the Internet. Each technique addresses a wide range of adversarial tactics
to manipulate geolocation, including the use of IP-hiding technologies like
VPNs and anonymizers, as we now explain.",free vpn
http://arxiv.org/abs/1409.2261v1,"Traditionally, 802.11-based networks that relied on wired equivalent protocol
(WEP) were especially vulnerable to packet sniffing. Today, wireless networks
are more prolific, and the monitoring devices used to find them are mobile and
easy to access. Securing wireless networks can be difficult because these
networks consist of radio transmitters and receivers, and anybody can listen,
capture data and attempt to compromise it. In recent years, a range of
technologies and mechanisms have helped make networking more secure. This paper
holistically evaluated various enhanced protocols proposed to solve WEP related
authentication, confidentiality and integrity problems. It discovered that
strength of each solution depends on how well the encryption, authentication
and integrity techniques work. The work suggested using a Defence-in-Depth
Strategy and integration of biometric solution in 802.11i. Comprehensive
in-depth comparative analysis of each of the security mechanisms is driven by
review of related work in WLAN security solutions.",free vpn
http://arxiv.org/abs/1206.1748v1,"VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system",free vpn
http://arxiv.org/abs/1206.5469v1,"Quality of Service (QoS) techniques are applied in IP networks to utilize
available network resources in the most efficient manner to minimize delays and
delay variations (jitters) in network traffic having multiple type of services.
Multimedia services may include voice, video and database. Researchers have
done considerable work on queuing disciplines to analyze and improve QoS
performance in wired and wireless IP networks. This paper highlights QoS
analysis in a wired IP network with more realistic enterprise modeling and
presents simulation results of a few statistics not presented and discussed
before. Four different applications are used i.e. FTP, Database, Voice over IP
(VoIP) and Video Conferencing (VC). Two major queuing disciplines are evaluated
i.e. 'Priority Queuing' and 'Weighted Fair Queuing' for packet identification
under Differentiated Services Code Point (DSCP). The simulation results show
that WFQ has an edge over PQ in terms of queuing delays and jitters experienced
by low priority services. For high priority traffic, dependency of 'Traffic
Drop', 'Buffer Usage' and 'Packet Delay Variation' on selected buffer sizes is
simulated and discussed to evaluate QoS deeper. In the end, it is also analyzed
how network's database service with applied Quality of Service may be affected
in terms of throughput (average rate of data received) for internal network
users when the server is also accessed by external user(s) through Virtual
Private Network (VPN).",free vpn
http://arxiv.org/abs/0908.0175v1,"The Border Gateway Protocol (BGP) is an important component in today's IP
network infrastructure. As the main routing protocol of the Internet, clear
understanding of its dynamics is crucial for configuring, diagnosing and
debugging Internet routing problems. Despite the increase in the services that
BGP provide such as MPLS VPNs, there is no much progress achieved in automating
the BGP management tasks. In this paper we discuss some of the problems
encountered by network engineers when managing BGP networks. We also describe
some of the open source tools and methods that attempt to resolve these issues.
Then we present some of the features that, if implemented, will ease BGP
management related tasks.",free vpn
http://arxiv.org/abs/0909.4858v1,"Mobile IPv6 will be an integral part of the next generation Internet
protocol. The importance of mobility in the Internet gets keep on increasing.
Current specification of Mobile IPv6 does not provide proper support for
reliability in the mobile network and there are other problems associated with
it. In this paper, we propose Virtual Private Network (VPN) based Home Agent
Reliability Protocol (VHAHA) as a complete system architecture and extension to
Mobile IPv6 that supports reliability and offers solutions to the security
problems that are found in Mobile IP registration part. The key features of
this protocol over other protocols are: better survivability, transparent
failure detection and recovery, reduced complexity of the system and workload,
secure data transfer and improved overall performance.",free vpn
http://arxiv.org/abs/0910.3511v1,"We define stealth Man-in-the-Middle adversaries, and analyse their ability to
launch denial and degradation of service (DoS) attacks on secure channels. We
show realistic attacks, disrupting TCP communication over secure VPNs using
IPsec. We present:
  First amplifying DoS attack on IPsec, when deployed without anti-replay
window.
  First amplifying attack on IPsec, when deployed with a `small' anti-replay
window, and analysis of `sufficient' window size.
  First amplifying attack on IPsec, when deployed with `sufficient' window
size. This attack (as the previous) is realistic: attacker needs only to
duplicate and speed-up few packets.
  We also suggest a solution designed to prevent the presented attacks, and to
provide secure channel immune to degradation and other DoS attacks. Our
solution involves changes (only) to the two gateway machines running IPsec.
  In addition to their practical importance, our results also raise the
challenge of formally defining secure channels immune to DoS and degradation
attacks, and providing provably-secure implementations.",free vpn
http://arxiv.org/abs/1204.1245v1,"Multi-Protocol Label Switching (MPLS) had been deployed by many data
networking service providers, including the next-generation mobile backhaul
networks, because of its undeniable potential in terms of virtual private
network (VPN) management, traffic engineering, etc. In MPLS networks, IP
packets are transmitted along a Label Switched Path (LSP) established between
edge nodes. To improve the efficiency of resource use in MPLS networks, it is
essential to utilize the LSPs efficiently.
  This paper proposes a method of selecting the optimal LSP pair from among
multiple LSP pairs which are established between the same pair of edge nodes,
on the assumption that both the upward and downward LSPs are established as a
pair (both-way operation). It is supposed that both upward and downward
bandwidths are allocated simultaneously in the selected LSP pair for each
service request. It is demonstrated by simulation evaluations that the proposal
method could reduce the total amount of the bandwidth required by up to 15%
compared with the conventional selection method. The proposed method can also
reuse the know-how and management tools in many existing networks which are
based on both-way operation.",free vpn
http://arxiv.org/abs/1403.6644v1,"If simplicity is a key strategy for success as a network protocol OpenFlow is
not winning. At its core OpenFlow presents a simple idea, which is a network
switch data plane abstraction along with a control protocol for manipulating
that abstraction. The result of this idea has been far from simple: a new
version released each year, five active versions, com- plex feature
dependencies, unstable version negotiation, lack of state machine definition,
etc. This complexity represents roadblocks for network, software, and hardware
engineers.
  We have distilled the core abstractions present in 5 existing versions of
OpenFlow and refactored them into a simple API called tinyNBI. Our work does
not provide high-level network abstractions (address pools, VPN maps, etc.),
instead it focuses on providing a clean low level interface that supports the
development of these higher layer abstractions. The goal of tinyNBI is to allow
configuration of all existing OpenFlow abstractions without having to deal with
the unique personalities of each version of OpenFlow or their level of support
in target switches.",free vpn
http://arxiv.org/abs/1608.04411v1,"VPN service providers (VSP) and IP-VPN customers have traditionally
maintained service demarcation boundaries between their routing and signaling
entities. This has resulted in the VPNs viewing the VSP network as an opaque
entity and therefore limiting any meaningful interaction between the VSP and
the VPNs. A key challenge is to expose each VPN to information about available
network resources through an abstraction (TA) [1] which is both accurate and
fair. In [2] we proposed three decentralized schemes assuming that all the
border nodes performing the abstraction have access to the entire core network
topology. This assumption likely leads to over- or under-subscription. In this
paper we develop centralized schemes to partition the core network capacities,
and assign each partition to a specific VPN for applying the decentralized
abstraction schemes presented in [2]. First, we present two schemes based on
the maximum concurrent flow and the maximum multicommodity flow (MMCF)
formulations. We then propose approaches to address the fairness concerns that
arise when MMCF formulation is used. We present results based on extensive
simulations on several topologies, and provide a comparative evaluation of the
different schemes in terms of abstraction efficiency, fairness to VPNs and call
performance characteristics achieved.",change ip vpn
http://arxiv.org/abs/1709.05395v1,"The introduction of the WebRTC API to modern browsers has brought about a new
threat to user privacy. This API causes a range of client IP addresses to
become available to a visited website via JavaScript even if a VPN is in use.
This a potentially serious problem for users utilizing VPN services for
anonymity. In order to better understand the magnitude of this issue, we tested
widely used browsers and VPN services to discover which client IP addresses can
be revealed and in what circumstances. In most cases, at least one of the
client addresses is leaked. The number and type of leaked IP addresses are
affected by the choices of browser and VPN service, meaning that
privacy-sensitive users should choose their browser and their VPN provider with
care. We conclude by proposing countermeasures which can be used to help
mitigate this issue.",change ip vpn
http://arxiv.org/abs/1602.03706v1,"BGP/MPLS IP VPN and VPLS services are considered to be widely used in IP/MPLS
networks for connecting customers' remote sites. However, service providers
struggle with many challenges to provide these services. Management complexity,
equipment costs, and last but not least, scalability issues emerging as the
customers increase in number, are just some of these problems. Software-defined
networking (SDN) is an emerging paradigm that can solve aforementioned issues
using a logically centralized controller for network devices. In this paper, we
propose a SDN-based solution called SDxVPN which considerably lowers the
complexity of VPN service definition and management. Our method eliminates
complex and costly device interactions that used to be done through several
control plane protocols and enables customers to determine their service
specifications, define restriction policies and even interconnect with other
customers automatically without operator's intervention. We describe our
prototype implementation of SDxVPN and its scalability evaluations under
several representative scenarios. The results indicate the effectiveness of the
proposed solution for deployment to provide large scale VPN services.",change ip vpn
http://arxiv.org/abs/1509.00236v1,"Though objectives of trusted routing and virtual private networks (VPN) data
transfer methods are to guarantee data transfer securely to from senders to
receivers over public networks like Internet yet there are paramount
differences between the two methods. This paper analyses their differences.",change ip vpn
http://arxiv.org/abs/1206.3365v1,"We experimentally demonstrate a novel optical physical-layer network coding
(PNC) scheme over time-division multiplexing (TDM) passive optical network
(PON). Full-duplex error-free communications between optical network units
(ONUs) at 2.5 Gb/s are shown for all-optical virtual private network (VPN)
applications. Compared to the conventional half-duplex communications set-up,
our scheme can increase the capacity by 100% with power penalty smaller than 3
dB. Synchronization of two ONUs is not required for the proposed VPN scheme",change ip vpn
http://arxiv.org/abs/1206.5469v1,"Quality of Service (QoS) techniques are applied in IP networks to utilize
available network resources in the most efficient manner to minimize delays and
delay variations (jitters) in network traffic having multiple type of services.
Multimedia services may include voice, video and database. Researchers have
done considerable work on queuing disciplines to analyze and improve QoS
performance in wired and wireless IP networks. This paper highlights QoS
analysis in a wired IP network with more realistic enterprise modeling and
presents simulation results of a few statistics not presented and discussed
before. Four different applications are used i.e. FTP, Database, Voice over IP
(VoIP) and Video Conferencing (VC). Two major queuing disciplines are evaluated
i.e. 'Priority Queuing' and 'Weighted Fair Queuing' for packet identification
under Differentiated Services Code Point (DSCP). The simulation results show
that WFQ has an edge over PQ in terms of queuing delays and jitters experienced
by low priority services. For high priority traffic, dependency of 'Traffic
Drop', 'Buffer Usage' and 'Packet Delay Variation' on selected buffer sizes is
simulated and discussed to evaluate QoS deeper. In the end, it is also analyzed
how network's database service with applied Quality of Service may be affected
in terms of throughput (average rate of data received) for internal network
users when the server is also accessed by external user(s) through Virtual
Private Network (VPN).",change ip vpn
http://arxiv.org/abs/1404.4806v3,"The introduction of SDN in IP backbones requires the coexistence of regular
IP forwarding and SDN based forwarding. The former is typically applied to best
effort Internet traffic, the latter can be used for different types of advanced
services (VPNs, Virtual Leased Lines, Traffic Engineering...). In this paper we
first introduce the architecture and the services of an ""hybrid"" IP/SDN
networking scenario. Then we describe the design and implementation of an Open
Source Hybrid IP/SDN (OSHI) node. It combines Quagga for OSPF routing and Open
vSwitch for OpenFlow based switching on Linux. The availability of tools for
experimental validation and performance evaluation of SDN solutions is
fundamental for the evolution of SDN. We provide a set of open source tools
that allow to facilitate the design of hybrid IP/SDN experimental networks,
their deployment on Mininet or on distributed SDN research testbeds and their
test. Finally, using the provided tools, we evaluate key performance aspects of
the proposed solutions. The OSHI development and test environment is available
in a VirtualBox VM image that can be downloaded.",change ip vpn
http://arxiv.org/abs/cs/0606123v1,"To demonstrate the result of researches in laboratory with the focus in
exhibiting the real impact of the use of the technology MPLS in LAN. Through
these researches we will verify that the investment in this technology is
shown, of the point of view cost/benefit, very interesting, being necessary,
however, the adoption of another measured, in order to settle down a
satisfactory level in the items Quality and safety in the sending of packages
in VPN but assisting to the requirement latency of the net very well being
shown in the tests that it consumes on average one Tuesday leaves of the time
spend for the same function in routing IP.",change ip vpn
http://arxiv.org/abs/1906.11288v1,"In this article, we provide a summary of recent efforts towards achieving
Internet geolocation securely, \ie without allowing the entity being geolocated
to cheat about its own geographic location. Cheating motivations arise from
many factors, including impersonation (in the case locations are used to
reinforce authentication), and gaining location-dependent benefits. In
particular, we provide a technical overview of Client Presence Verification
(CPV) and Server Location Verification (SLV)---two recently proposed techniques
designed to verify the geographic locations of clients and servers in realtime
over the Internet. Each technique addresses a wide range of adversarial tactics
to manipulate geolocation, including the use of IP-hiding technologies like
VPNs and anonymizers, as we now explain.",change ip vpn
http://arxiv.org/abs/1206.1748v1,"VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system",change ip vpn
http://arxiv.org/abs/0908.0175v1,"The Border Gateway Protocol (BGP) is an important component in today's IP
network infrastructure. As the main routing protocol of the Internet, clear
understanding of its dynamics is crucial for configuring, diagnosing and
debugging Internet routing problems. Despite the increase in the services that
BGP provide such as MPLS VPNs, there is no much progress achieved in automating
the BGP management tasks. In this paper we discuss some of the problems
encountered by network engineers when managing BGP networks. We also describe
some of the open source tools and methods that attempt to resolve these issues.
Then we present some of the features that, if implemented, will ease BGP
management related tasks.",change ip vpn
http://arxiv.org/abs/0909.4858v1,"Mobile IPv6 will be an integral part of the next generation Internet
protocol. The importance of mobility in the Internet gets keep on increasing.
Current specification of Mobile IPv6 does not provide proper support for
reliability in the mobile network and there are other problems associated with
it. In this paper, we propose Virtual Private Network (VPN) based Home Agent
Reliability Protocol (VHAHA) as a complete system architecture and extension to
Mobile IPv6 that supports reliability and offers solutions to the security
problems that are found in Mobile IP registration part. The key features of
this protocol over other protocols are: better survivability, transparent
failure detection and recovery, reduced complexity of the system and workload,
secure data transfer and improved overall performance.",change ip vpn
http://arxiv.org/abs/1204.1245v1,"Multi-Protocol Label Switching (MPLS) had been deployed by many data
networking service providers, including the next-generation mobile backhaul
networks, because of its undeniable potential in terms of virtual private
network (VPN) management, traffic engineering, etc. In MPLS networks, IP
packets are transmitted along a Label Switched Path (LSP) established between
edge nodes. To improve the efficiency of resource use in MPLS networks, it is
essential to utilize the LSPs efficiently.
  This paper proposes a method of selecting the optimal LSP pair from among
multiple LSP pairs which are established between the same pair of edge nodes,
on the assumption that both the upward and downward LSPs are established as a
pair (both-way operation). It is supposed that both upward and downward
bandwidths are allocated simultaneously in the selected LSP pair for each
service request. It is demonstrated by simulation evaluations that the proposal
method could reduce the total amount of the bandwidth required by up to 15%
compared with the conventional selection method. The proposed method can also
reuse the know-how and management tools in many existing networks which are
based on both-way operation.",change ip vpn
http://arxiv.org/abs/0911.4034v2,"IPSec is a protocol that allows to make secure connections between branch
offices and allows secure VPN accesses. However, the efforts to improve IPSec
are still under way; one aspect of this improvement is to take Quality of
Service (QoS) requirements into account. QoS is the ability of the network to
provide a service at an assured service level while optimizing the global usage
of network resources. The QoS level that a flow receives depends on a six-bit
identifier in the IP header; the so-called Differentiated Services code point
(DSCP). Basically, Multi-Field classifiers classify a packet by inspecting
IP/TCP headers, to decide how the packet should be processed. The current IPSec
standard does hardly offer any guidance to do this, because the existing IPSec
ESP security protocol hides much of this information in its encrypted payloads,
preventing network control devices such as routers and switches from utilizing
this information in performing classification appropriately. To solve this
problem, we propose a QoS-friendly Encapsulated Security Payload (Q-ESP) as a
new IPSec security protocol that provides both security and QoS supports. We
also present our NetBSD kernel-based implementation as well as our evaluation
results of Q-ESP.",change ip vpn
http://arxiv.org/abs/1907.04023v1,"Anecdotal evidence suggests an increasing number of people are turning to VPN
services for the properties of privacy, anonymity and free communication over
the internet. Despite this, there is little research into what these services
are actually being used for. We use DNS cache snooping to determine what
domains people are accessing through VPNs. This technique is used to discover
whether certain queries have been made against a particular DNS server. Some
VPNs operate their own DNS servers, ensuring that any cached queries were made
by users of the VPN. We explore 3 methods of DNS cache snooping and briefly
discuss their strengths and limitations. Using the most reliable of the
methods, we perform a DNS cache snooping scan against the DNS servers of
several major VPN providers. With this we discover which domains are actually
accessed through VPNs. We run this technique against popular domains, as well
as those known to be censored in certain countries; China, Indonesia, Iran, and
Turkey. Our work gives a glimpse into what users use VPNs for, and provides a
technique for discovering the frequency with which domain records are accessed
on a DNS server.",change ip vpn
http://arxiv.org/abs/1011.2324v1,"The availability of end-hosts and their assigned routable IP addresses has
impact on the ability to fight spammers and attackers, and on peer-to-peer
application performance. Previous works study the availability of hosts mostly
by using either active pinging or by studying access to a mail service, both
approaches suffer from inherent inaccuracies. We take a different approach by
measuring the IP addresses periodically reported by a uniquely identified group
of the hosts running the DIMES agent. This fresh approach provides a chance to
measure the true availability of end-hosts and the dynamics of their assigned
routable IP addresses. Using a two month study of 1804 hosts, we find that over
60% of the hosts have a fixed IP address and 90% median availability, while
some of the remaining hosts have more than 30 different IPs. For those that
have periodically changing IP addresses, we find that the median average period
per AS is roughly 24 hours, with a strong relation between the offline time and
the probability of altering IP address.",change ip vpn
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",change ip vpn
http://arxiv.org/abs/1804.07511v2,"Information-centric networking (ICN) has long been advocating for radical
changes to the IP-based Internet. However, the upgrade challenges that this
entails have hindered ICN adoption. To break this loop, the POINT project
proposed a hybrid, IP-over-ICN, architecture: IP networks are preserved at the
edge, connected to each other over an ICN core. This exploits the key benefits
of ICN, enabling individual network operators to improve the performance of
their IP-based services, without changing the rest of the Internet. We provide
an overview of POINT and outline how it improves upon IP in terms of
performance and resilience. Our focus is on the successful trial of the POINT
prototype in a production network, where real users operated actual IP-based
applications.",change ip vpn
http://arxiv.org/abs/1001.2575v1,"Centralized Virtual Private Networks (VPNs) when used in distributed systems
have performance constraints as all traffic must traverse through a central
server. In recent years, there has been a paradigm shift towards the use of P2P
in VPNs to alleviate pressure placed upon the central server by allowing
participants to communicate directly with each other, relegating the server to
handling session management and supporting NAT traversal using relays when
necessary. Another, less common, approach uses unstructured P2P systems to
remove all centralization from the VPN. These approaches currently lack the
depth in security options provided by other VPN solutions, and their
scalability constraints have not been well studied.
  In this paper, we propose and implement a novel VPN architecture, which uses
a structured P2P system for peer discovery, session management, NAT traversal,
and autonomic relay selection and a central server as a partially-automated
public key infrastructure (PKI) via a user-friendly web interface. Our model
also provides the first design and implementation of a P2P VPN with full
tunneling support, whereby all non-P2P based Internet traffic routes through a
trusted third party and does so in a way that is more secure than existing full
tunnel techniques. To verify our model, we evaluate our reference
implementation by comparing it quantitatively to other VPN technologies
focusing on latency, bandwidth, and memory usage. We also discuss some of our
experiences with developing, maintaining, and deploying a P2P VPN.",change ip vpn
http://arxiv.org/abs/1804.07509v2,"The efficient provision of IPTV services requires support for IP multicasting
and IGMP snooping, limiting such services to single operator networks.
Information-Centric Networking (ICN), with its native support for multicast
seems ideal for such services, but it requires operators and users to overhaul
their networks and applications. The POINT project has proposed a hybrid,
IP-over-ICN, architecture, preserving IP devices and applications at the edge,
but interconnecting them via an SDN-based ICN core. This allows individual
operators to exploit the benefits of ICN, without expecting the rest of the
Internet to change. In this paper, we first outline the POINT approach and show
how it can handle multicast-based IPTV services in a more efficient and
resilient manner than IP. We then describe a successful trial of the POINT
prototype in a production network, where real users tested actual IPTV services
over both IP and POINT under regular and exceptional conditions. Results from
the trial show that the POINT prototype matched or improved upon the services
offered via plain IP.",change ip vpn
http://arxiv.org/abs/1608.04551v1,"Interplanetary (IP) shock plays a key role in causing the global dynamic
changes of the geospace environment. For the perspective of Solar-Terrestrial
relationship, it will be of great importance to estimate the properties of
post-shock solar wind simply and accurately. Motivated by this, we performed a
statistical analysis of IP shocks during 1998-2008, focusing on the
significantly different responses of two well-used geomagnetic indices (SYMH
and AL) to the passive of two types of IP shocks. For the IP shocks with
northward IMF (91 cases), the SYMH index keeps on the high level after the
sudden impulses (SI) for a long time. Meanwhile, the change of AL index is
relative small, with an mean value of only -29 nT. However, for the IP shocks
with southward IMF (92 cases), the SYMH index suddenly decreases at a certain
rate after SI, and the change of AL index is much significant, of -316 nT.
Furthermore, the change rate of SYMH index after SI is found to be linearly
correlated with the post-shock reconnection E-field (E$_{KL}$). Based on these
facts, an inversion model of post-shock IMF orientation and E$_{KL}$ is
developed. The model validity is also confirmed by studying 68 IP shocks in the
period of 2009-2013. The inversion accuracy of IMF orientation is 88.24%, and
the inversion efficiency of E$_{KL}$ is as high as 78%.",change ip vpn
http://arxiv.org/abs/1906.10478v1,"IP headers include a 16-bit ID field. Our work examines the generation of
this field in Windows (versions 8 and higher), Linux and Android, and shows
that the IP ID field enables remote servers to assign a unique ID to each
device and thus be able to identify subsequent transmissions sent from that
device. This identification works across all browsers and over network changes.
In modern Linux and Android versions, this field leaks a kernel address, thus
we also break KASLR.
  Our work includes reverse-engineering of the Windows IP ID generation code,
and a cryptanalysis of this code and of the Linux kernel IP ID generation code.
It provides practical techniques to partially extract the key used by each of
these algorithms, overcoming different implementation issues, and observing
that this key can identify individual devices. We deployed a demo (for Windows)
showing that key extraction and machine fingerprinting works in the wild, and
tested it from networks around the world.",change ip vpn
http://arxiv.org/abs/1403.7371v1,"Denial-of-Service attacks continue to be a serious problem for the Internet
community despite the fact that a large number of defense approaches has been
proposed by the research community. In this paper we introduce IP Fast Hopping,
easily deployable and effective network-layer architecture against DDoS
attacks. Our approach also provides an easy way for clients to hide content and
destination server of theirs communication sessions. We describe a method of
dynamic server IP address change and all modules necessary to implement the
approach.",change ip vpn
http://arxiv.org/abs/0910.3511v1,"We define stealth Man-in-the-Middle adversaries, and analyse their ability to
launch denial and degradation of service (DoS) attacks on secure channels. We
show realistic attacks, disrupting TCP communication over secure VPNs using
IPsec. We present:
  First amplifying DoS attack on IPsec, when deployed without anti-replay
window.
  First amplifying attack on IPsec, when deployed with a `small' anti-replay
window, and analysis of `sufficient' window size.
  First amplifying attack on IPsec, when deployed with `sufficient' window
size. This attack (as the previous) is realistic: attacker needs only to
duplicate and speed-up few packets.
  We also suggest a solution designed to prevent the presented attacks, and to
provide secure channel immune to degradation and other DoS attacks. Our
solution involves changes (only) to the two gateway machines running IPsec.
  In addition to their practical importance, our results also raise the
challenge of formally defining secure channels immune to DoS and degradation
attacks, and providing provably-secure implementations.",change ip vpn
http://arxiv.org/abs/1902.08937v1,"Driven by their quest to improve web performance, Content Delivery Networks
(CDNs) are known adaptors of performance optimizations. In this regard, TCP
congestion control and particularly its initial congestion window (IW) size is
one long-debated topic that can influence CDN performance. Its size is,
however, assumed to be static by IETF recommendations---despite being network-
and application-dependent---and only infrequently changed in its history. To
understand if the standardization and research perspective still meets Internet
reality, we study the IW configurations of major CDNs. Our study uses a
globally distributed infrastructure of VPNs giving access to residential access
links that enable to shed light on network-dependent configurations. We observe
that most CDNs are well aware of the IW's impact and find a high amount of
customization that is beyond current Internet standards. Further, we find CDNs
that utilize different IWs for different customers and content while others
resort to fixed values. We find various initial window configurations, most
below 50 segments yet with exceptions of up to 100 segments---the tenfold of
current standards. Our study highlights that Internet reality drifted away from
recommended and standardized practices.",change ip vpn
http://arxiv.org/abs/1402.3401v5,"Internet censorship is enforced by numerous governments worldwide, however,
due to the lack of publicly available information, as well as the inherent
risks of performing active measurements, it is often hard for the research
community to investigate censorship practices in the wild. Thus, the leak of
600GB worth of logs from 7 Blue Coat SG-9000 proxies, deployed in Syria to
filter Internet traffic at a country scale, represents a unique opportunity to
provide a detailed snapshot of a real-world censorship ecosystem. This paper
presents the methodology and the results of a measurement analysis of the
leaked Blue Coat logs, revealing a relatively stealthy, yet quite targeted,
censorship. We find that traffic is filtered in several ways: using IP
addresses and domain names to block subnets or websites, and keywords or
categories to target specific content. We show that keyword-based censorship
produces some collateral damage as many requests are blocked even if they do
not relate to sensitive content. We also discover that Instant Messaging is
heavily censored, while filtering of social media is limited to specific pages.
Finally, we show that Syrian users try to evade censorship by using web/socks
proxies, Tor, VPNs, and BitTorrent. To the best of our knowledge, our work
provides the first analytical look into Internet filtering in Syria.",change ip vpn
http://arxiv.org/abs/1112.4018v1,"Mobile IP is an open standard, defined by the Internet Engineering Task Force
(IETF) RFC 3220. By using Mobile IP, you can keep the same IP address, stay
connected, and maintain ongoing applications while roaming between IP networks.
Mobile IP is scalable for the Internet because it is based on IP - any media
that can support IP can support Mobile IP.",change ip vpn
http://arxiv.org/abs/1410.4695v1,"The routes in IP networks are determined by the IP destination address and
the routing tables in each router on the path to the destination. Hence all the
IP packets follow the same route until the route is changes due to congestion,
link failure or topology updates. IPv4 tried using Type of Service (TOS) field
in the IP header to classify traffic and that did not succeed as it was based
on fair self-classification of applications in comparison to the network
traffic of other applications. As multimedia applications were quite foreign at
the initial IPv4 stage, TOS field was not used uniformly. As there are
different existing Quality of Service (QoS) paradigms available, IPv6 QoS
approach was designed to be more flexible. The IPv6 protocol thus has
QoS-specific elements in Base header and Extension headers which can be used in
different ways to enhance multimedia application performance. In this paper, we
plan to survey these options and other QoS architectures and discuss their
strengths and weaknesses. Some basic simulation for various queuing schemes is
presented for comparison and a new queuing scheme prioritized WFQ with RR is
proposed.",change ip vpn
http://arxiv.org/abs/1002.1152v1,"A Virtual Private Network (VPN) provides private network connections over a
publicly accessible shared network. The effective allocation of bandwidth for
VPNs assumes significance in the present scenario due to varied traffic. Each
VPN endpoint specifies bounds on the total amount of traffic that it is likely
to send or receive at any time. The network provider tailors the VPN so that
there is sufficient bandwidth for any traffic matrix that is consistent with
these bounds. The approach incorporates the use of Ad-hoc On demand Distance
Vector (AODV) protocol, with a view to accomplish an enhancement in the
performance of the mobile networks. The NS2 based simulation results are
evaluated in terms of its metrics for different bandwidth allocations, besides
analyzing its performance in the event of exigencies such as link failures. The
results highlight the suitability of the proposed strategy in the context of
real time applications.",change ip vpn
http://arxiv.org/abs/1201.0428v1,"Like most advances, wireless LAN poses both opportunities and risks. The
evolution of wireless networking in recent years has raised many serious
security issues. These security issues are of great concern for this technology
as it is being subjected to numerous attacks. Because of the free-space radio
transmission in wireless networks, eavesdropping becomes easy and consequently
a security breach may result in unauthorized access, information theft,
interference and service degradation. Virtual Private Networks (VPNs) have
emerged as an important solution to security threats surrounding the use of
public networks for private communications. While VPNs for wired line networks
have matured in both research and commercial environments, the design and
deployment of VPNs for WLAN is still an evolving field. This paper presents an
approach to secure IEEE 802.11g WLAN using OpenVPN, a transport layer VPN
solution and its impact on performance of IEEE 802.11g WLAN.",change ip vpn
http://arxiv.org/abs/1707.03497v2,"This paper proposes a novel deep reinforcement learning (RL) architecture,
called Value Prediction Network (VPN), which integrates model-free and
model-based RL methods into a single neural network. In contrast to typical
model-based RL methods, VPN learns a dynamics model whose abstract states are
trained to make option-conditional predictions of future values (discounted sum
of rewards) rather than of future observations. Our experimental results show
that VPN has several advantages over both model-free and model-based baselines
in a stochastic environment where careful planning is required but building an
accurate observation-prediction model is difficult. Furthermore, VPN
outperforms Deep Q-Network (DQN) on several Atari games even with
short-lookahead planning, demonstrating its potential as a new way of learning
a good state representation.",change ip vpn
http://arxiv.org/abs/1603.04865v5,"Desktops and laptops can be maliciously exploited to violate privacy. There
are two main types of attack scenarios: active and passive. In this paper, we
consider the passive scenario where the adversary does not interact actively
with the device, but he is able to eavesdrop on the network traffic of the
device from the network side. Most of the Internet traffic is encrypted and
thus passive attacks are challenging. In this paper, we show that an external
attacker can robustly identify the operating system, browser and application of
HTTP encrypted traffic (HTTPS). We provide a large dataset of more than 20,000
examples for this task. We present a comprehensive evaluation of traffic
features including new ones and machine learning algorithms. We run a
comprehensive set of experiments, which shows that our classification accuracy
is 96.06%. Due to the adaptive nature of the problem, we also investigate the
robustness and resilience to changes of features due to different network
conditions (e.g., VPN) at test time and the effect of small training set on the
accuracy. We show that our proposed solution is robust to these changes.",change ip vpn
http://arxiv.org/abs/1904.07088v1,"We propose P4-MACsec to protect network links between P4 switches through
automated deployment of MACsec, a widespread IEEE standard for securing Layer 2
infrastructures. It is supported by switches and routers from major
manufacturers and has only little performance limitations compared to VPN
technologies such as IPsec. P4-MACsec introduces a data plane implementation of
MACsec including AES-GCM encryption and decryption directly on P4 switches.
P4-MACsec features a two-tier control plane structure where local controllers
running on the P4 switches interact with a central controller. We propose a
novel secure link discovery mechanism that leverages protected LLDP frames and
the two-tier control plane structure for secure and efficient management of a
global link map. Automated deployment of MACsec creates secure channel,
generates keying material, and configures the P4 switches for each detected
link between two P4 switches. It detects link changes and performs rekeying to
provide a secure, configuration-free operation of MACsec. In this paper, we
review the technological background of P4-MACsec and explain its architecture.
To demonstrate the feasibility of P4-MACsec, we implement it on the BMv2 P4
software switch and validate the prototype through experiments. We evaluate its
performance through experiments that focus on TCP throughput and round-trip
time. We publish the prototype and experiment setups on Github.",change ip vpn
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",cloned firms scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",cloned firms scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",cloned firms scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",cloned firms scam
http://arxiv.org/abs/1110.3939v1,"In elections, a set of candidates ranked consecutively (though possibly in
different order) by all voters is called a clone set, and its members are
called clones. A clone structure is a family of all clone sets of a given
election. In this paper we study properties of clone structures. In particular,
we give an axiomatic characterization of clone structures, show their
hierarchical structure, and analyze clone structures in single-peaked and
single-crossing elections. We give a polynomial-time algorithm that finds a
minimal collection of clones that need to be collapsed for an election to
become single-peaked, and we show that this problem is NP-hard for
single-crossing elections.",cloned firms scam
http://arxiv.org/abs/1611.08005v1,"Background: Code cloning - copying and reusing pieces of source code - is a
common phenomenon in software development in practice. There have been several
empirical studies on the effects of cloning, but there are contradictory
results regarding the connection of cloning and faults. Objective: Our aim is
to clarify the relationship between code clones and faults. In particular, we
focus on inconsistent (or type-3) clones in this work. Method: We conducted a
case study with TWT GmbH where we detected the code clones in three Java
systems, set them into relation to information from issue tracking and version
control and interviewed three key developers. Results: Of the type-3 clones, 17
% contain faults. Developers modified most of the type-3 clones simultaneously
and thereby fixed half of the faults in type-3 clones consistently. Type-2
clones with faults all evolved to fixed type-3 clones. Clone length is only
weakly correlated with faultiness. Conclusion: There are indications that the
developers in two cases have been aware of clones. It might be a reason for the
weak relationship between type-3 clones and faults. Hence, it seems important
to keep developers aware of clones, potentially with new tool support. Future
studies need to investigate if the rate of faults in type-3 clones justifies
using them as cues in defect detection.",cloned firms scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",cloned firms scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",cloned firms scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",cloned firms scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",cloned firms scam
http://arxiv.org/abs/1301.2447v1,"Cloned code is one of the most important obstacles against consistent
software maintenance and evolution. Although today's clone detection tools find
a variety of clones, they do not offer any advice how to remove such clones. We
explain the problems involved in finding a sequence of changes for clone
removal and suggest to view this problem as a process of stepwise unification
of the clone instances. Consequently the problem can be solved by backtracking
over the possible unification steps.",cloned firms scam
http://arxiv.org/abs/1903.04958v1,"In coal-fired power plants, it is critical to improve the operational
efficiency of boilers for sustainability. In this work, we formulate real-time
boiler control as an optimization problem that looks for the best distribution
of temperature in different zones and oxygen content from the flue to improve
the boiler's stability and energy efficiency. We employ an efficient algorithm
by integrating appropriate machine learning and optimization techniques. We
obtain a large dataset collected from a real boiler for more than two months
from our industry partner, and conduct extensive experiments to demonstrate the
effectiveness and efficiency of the proposed algorithm.",boiler room scam
http://arxiv.org/abs/1302.5942v1,"Low temperature heating panel systems offer distinctive advantages in terms
of thermal comfort and energy consumption, allowing work with low exergy
sources. The purpose of this paper is to compare floor, wall, ceiling, and
floor-ceiling panel heating systems in terms of energy, exergy and CO2
emissions. Simulation results for each of the analyzed panel system are given
by its energy (the consumption of gas for heating, electricity for pumps and
primary energy) and exergy consumption, the price of heating, and its carbon
dioxide emission. Then, the values of the air temperatures of rooms are
investigated and that of the surrounding walls and floors. It is found that the
floor-ceiling heating system has the lowest energy, exergy, CO2 emissions,
operating costs, and uses boiler of the lowest power. The worst system by all
these parameters is the classical ceiling heating",boiler room scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",boiler room scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",boiler room scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",boiler room scam
http://arxiv.org/abs/1303.2619v1,"Modern distributed systems use names everywhere. Lockservices such as Chubby
and ZooKeeper provide an effective mechanism for mapping from application names
to server instances, but proper usage of them requires a large amount of
error-prone boiler-plate code.
  Application programmers often try to write wrappers to abstract away this
logic, but it turns out there is a more general and easier way of handling the
issue. We show that by extending the existing name resolution capabilities of
RPC libraries, we can remove the need for such annoying boiler-plate code while
at the same time making our services more robust.",boiler room scam
http://arxiv.org/abs/1606.00827v1,"The article discusses a possibility of removing smog particles from a boiler
smoke. To do this, the boiler smoke is passed through a flow of gamma
radiation, formed by interaction of the microtron beam with a heavy target. The
energy of the microtron electrons twenty five megaelectronvolts, the beam
current one hundred microamperes. Smog particles are ionized with gamma
radiation and then sat down on the plates of the electrostatic filter. The
height of the filter plates is one m, the electric field between the plates one
kilovolt per centimeter. The smog particles on the plates should be removed
regularly to a specialized dust collector.",boiler room scam
http://arxiv.org/abs/1803.05362v1,"With the advancement of software engineering in recent years, the model
checking techniques are widely applied in various areas to do the verification
for the system model. However, it is difficult to apply the model checking to
verify requirements due to lacking the details of the design. Unlike other
model checking tools, LTSA provides the structure diagram, which can bridge the
gap between the requirements and the design. In this paper, we demonstrate the
abilities of LTSA shipped with the classic case study of the steam boiler
system. The structure diagram of LTSA can specify the interactions between the
controller and the steam boiler, which can be derived from UML requirements
model such as system sequence diagram of the steam boiler system. The start-up
design model of LTSA can be generated from the structure diagram. Furthermore,
we provide a variation law of the steam rate to avoid the issue of state space
explosion and show how explicitly and implicitly model the time that reflects
the difference between system modeling and the physical world. Finally, the
derived model is verified against the required properties. Our work
demonstrates the potential power of integrating UML with model checking tools
in requirement elicitation, system design, and verification.",boiler room scam
http://arxiv.org/abs/1910.05118v1,"Accurate prediction of mercury content emitted from fossil fueled power
stations is of utmost important for environmental pollution assessment and
hazard mitigation. In this paper, mercury content in the output gas of power
stations boilers was predicted using adaptive neuro fuzzy inference system
method integrated with particle swarm optimization. The input parameters of the
model include coal characteristics and the operational parameters of the
boilers. The dataset has been collected from a number of power plants and
employed to educate and examine the proposed model. To evaluate the performance
of the proposed ANFIS PSO model the statistical meter of MARE was implemented.
Furthermore, relative errors between acquired data and predicted values
presented, which confirm the accuracy of the model to deal nonlinearity and
representing the dependency of flue gas mercury content into the specifications
of coal and the boiler type.",boiler room scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",boiler room scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",boiler room scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",boiler room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",boiler room scam
http://arxiv.org/abs/1811.09406v1,"This paper proposes a detailed optimal scheduling model of an exemplar
multi-energy system comprising combined cycle power plants (CCPPs), battery
energy storage systems, renewable energy sources, boilers, thermal energy
storage systems,electric loads and thermal loads. The proposed model considers
the detailed start-up and shutdown power trajectories of the gas turbines,
steam turbines and boilers. Furthermore, a practical,multi-energy load
management scheme is proposed within the framework of the optimal scheduling
problem. The proposed load management scheme utilizes the flexibility offered
by system components such as flexible electrical pump loads, electrical
interruptible loads and a flexible thermal load to reduce the overall energy
cost of the system. The efficacy of the proposed model in reducing the energy
cost of the system is demonstrated in the context of a day-ahead scheduling
problem using four illustrative scenarios.",boiler room scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",boiler room scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",boiler room scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",boiler room scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",boiler room scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",boiler room scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",boiler room scam
http://arxiv.org/abs/1405.1512v1,"This set of theories presents an Isabelle/HOL+Isar formalisation of stream
processing components introduces in Focus, a framework for formal specification
and development of interactive systems. This is an extended and updated version
of the formalisation, which was elaborated within the methodology 'Focus on
Isabelle'. In addition, we also applied the formalisation on three case studies
that cover different application areas: process control (Steam Boiler System),
data transmission (FlexRay communication protocol), memory and processing
components (Automotive-Gateway System).",boiler room scam
http://arxiv.org/abs/1610.07884v1,"In this technical report we summarise the spatio-temporal features and
present the core operators of FocusST specification framework. We present the
general idea of these operators, using a Steam Boiler System example to
illustrate how the specification framework can be applied.
  FocusST was inspired by Focus, a framework for formal specification and
development of interactive systems. In contrast to Focus, FocusST is devoted to
specify and to analyse spatial (S) and timing (T) aspects of the systems, which
is also reflected in the name of the framework: the extension ST highlights the
spatio-temporal nature of the specifications.",boiler room scam
http://arxiv.org/abs/1704.08323v1,"This paper will cover several studies and design changes that will eventually
be implemented to the Fermi National Accelerator Laboratory (FNAL) magnetron
ion source. The topics include tungsten cathode insert, solenoid gas valves,
current controlled arc pulser, cesium boiler redesign, gas mixtures of hydrogen
and nitrogen, and duty factor reduction. The studies were performed on the FNAL
test stand, with the aim to improve source lifetime, stability, and reducing
the amount of tuning needed.",boiler room scam
http://arxiv.org/abs/1808.09418v1,"The results of research pipes diffusion hardening as an effective method for
increasing the durability and reliability of power equipment are presented. The
experience of commercial operation of pipes manufactured from diffusion
chromized and heat-hardened steel 14 MoV6-3; on the heating surfaces of the
supercritical pressure boilers are generalized. The possibility and
effectiveness of this method on the example of capacities of Trypil'ska CHP are
shown.",boiler room scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",boiler room scam
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",boiler room scam
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",boiler room scam
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",boiler room scam
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",boiler room scam
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",boiler room scam
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",boiler room scam
http://arxiv.org/abs/1101.3661v1,"A numerical model was established to investigate the lateral mass transfer as
well as the mechanism of bed-inventory overturn inside a pant-leg circulating
fluidized bed (CFB), which are of great importance to maintain safe and
efficient operation of the CFB. Results show that the special flow structure in
which the solid particle volume fraction along the central line of the pant-leg
CFB is relative high enlarges the lateral mass transfer rate and make it more
possible for bed inventory overturn. Although the lateral pressure difference
generated from lateral mass transfer inhibits continuing lateral mass transfer,
providing the pant-leg CFB with self-balancing ability to some extent, the
primary flow rate change due to the outlet pressure change often disable the
self-balancing ability by continually enhancing the flow rate difference. As
the flow rate of the primary air fan is more sensitive to its outlet pressure,
it is easier to lead to bed inventory overturn. While when the solid particle
is easier to change its flow patter to follow the surrounding air flow,the
self-balancing ability is more active.",boiler room scam
http://arxiv.org/abs/1507.04300v1,"Networked Automation Systems (NAS) have to meet stringent response time
during operation. Verifying response time of automation is an important step
during design phase before deployment. Timing discrepancies due to hardware,
software and communication components of NAS affect the response time. This
investigation uses model templates for verifying the response time in NAS.
First, jitter bounds model the timing fluctuations of NAS components. These
jitter bounds are the inputs to model templates that are formal models of
timing fluctuations. The model templates are atomic action patterns composed of
three composition operators- sequential, alternative, and parallel and embedded
in time wrapper that specifies clock driven activation conditions. Model
templates in conjunction with formal model of technical process offer an easier
way to verify the response time. The investigation demonstrates the proposed
verification method using an industrial steam boiler with typical NAS
components in plant floor.",boiler room scam
http://arxiv.org/abs/1804.01574v1,"Thermoelectric generation (TEG) has increasingly drawn attention for being
environmentally friendly. A few researches have focused on improving TEG
efficiency at the system level on vehicle radiators. The most recent
reconfiguration algorithm shows improvement in performance but suffers from
major drawback on computational time and energy overhead, and non-scalability
in terms of array size and processing frequency. In this paper, we propose a
novel TEG array reconfiguration algorithm that determines near-optimal
configuration with an acceptable computational time. More precisely, with
$O(N)$ time complexity, our prediction-based fast TEG reconfiguration algorithm
enables all modules to work at or near their maximum power points (MPP).
Additionally, we incorporate prediction methods to further reduce the runtime
and switching overhead during the reconfiguration process. Experimental results
present $30\%$ performance improvement, almost $100\times$ reduction on
switching overhead and $13\times$ enhancement on computational speed compared
to the baseline and prior work. The scalability of our algorithm makes it
applicable to larger scale systems such as industrial boilers and heat
exchangers.",boiler room scam
http://arxiv.org/abs/1210.6766v1,"We tackle the multi-party speech recovery problem through modeling the
acoustic of the reverberant chambers. Our approach exploits structured sparsity
models to perform room modeling and speech recovery. We propose a scheme for
characterizing the room acoustic from the unknown competing speech sources
relying on localization of the early images of the speakers by sparse
approximation of the spatial spectra of the virtual sources in a free-space
model. The images are then clustered exploiting the low-rank structure of the
spectro-temporal components belonging to each source. This enables us to
identify the early support of the room impulse response function and its unique
map to the room geometry. To further tackle the ambiguity of the reflection
ratios, we propose a novel formulation of the reverberation model and estimate
the absorption coefficients through a convex optimization exploiting joint
sparsity model formulated upon spatio-spectral sparsity of concurrent speech
representation. The acoustic parameters are then incorporated for separating
individual speech signals through either structured sparse recovery or inverse
filtering the acoustic channels. The experiments conducted on real data
recordings demonstrate the effectiveness of the proposed approach for
multi-party speech recovery and recognition.",recovery room scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",recovery room scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",recovery room scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",recovery room scam
http://arxiv.org/abs/1307.4894v1,"We study two cases of acoustic source localization in a reverberant room,
from a number of point-wise narrowband measurements. In the first case, the
room is perfectly known. We show that using a sparse recovery algorithm with a
dictionary of sources computed a priori requires measurements at multiple
frequencies. Furthermore, we study the choice of frequencies for these
measurements, and show that one should avoid the modal frequencies of the room.
In the second case, when the shape and the boundary conditions of the room are
unknown, we propose a model of the acoustical field based on the Vekua theory,
still allowing the localization of sources, at the cost of an increased number
of measurements. Numerical results are given, using simple adaptations of
standard sparse recovery methods.",recovery room scam
http://arxiv.org/abs/1704.02420v1,"We analyze the list-decodability, and related notions, of random linear
codes. This has been studied extensively before: there are many different
parameter regimes and many different variants. Previous works have used
complementary styles of arguments---which each work in their own parameter
regimes but not in others---and moreover have left some gaps in our
understanding of the list-decodability of random linear codes. In particular,
none of these arguments work well for list-recovery, a generalization of
list-decoding that has been useful in a variety of settings.
  In this work, we present a new approach, which works across parameter regimes
and further generalizes to list-recovery. Our main theorem can establish better
list-decoding and list-recovery results for low-rate random linear codes over
large fields; list-recovery of high-rate random linear codes; and it can
recover the rate bounds of Guruswami, Hastad, and Kopparty for constant-rate
random linear codes (although with large list sizes).",recovery room scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",recovery room scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",recovery room scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",recovery room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",recovery room scam
http://arxiv.org/abs/1612.05793v1,"This paper considers the problem of simultaneous 2-D room shape
reconstruction and self-localization without the requirement of any
pre-established infrastructure. A mobile device equipped with co-located
microphone and loudspeaker as well as internal motion sensors is used to emit
acoustic pulses and collect echoes reflected by the walls. Using only first
order echoes, room shape recovery and self-localization is feasible when
auxiliary information is obtained using motion sensors. In particular, it is
established that using echoes collected at three measurement locations and the
two distances between consecutive measurement points, unique localization and
mapping can be achieved provided that the three measurement points are not
collinear. Practical algorithms for room shape reconstruction and
self-localization in the presence of noise and higher order echoes are proposed
along with experimental results to demonstrate the effectiveness of the
proposed approach.",recovery room scam
http://arxiv.org/abs/1806.08294v1,"In this paper, we propose a novel procedure for 3D layout recovery of indoor
scenes from single 360 degrees panoramic images. With such images, all scene is
seen at once, allowing to recover closed geometries. Our method combines
strategically the accuracy provided by geometric reasoning (lines and vanishing
points) with the higher level of data abstraction and pattern recognition
achieved by deep learning techniques (edge and normal maps). Thus, we extract
structural corners from which we generate layout hypotheses of the room
assuming Manhattan world. The best layout model is selected, achieving good
performance on both simple rooms (box-type) and complex shaped rooms (with more
than four walls). Experiments of the proposed approach are conducted within two
public datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of
estimating layouts by combining geometry and deep learning and the
effectiveness of our proposal with respect to the state of the art.",recovery room scam
http://arxiv.org/abs/1411.7889v1,"Single particle 3D imaging with ultrashort X-ray laser pulses is based on
collecting and combining the information content of 2D scattering patterns of
an object at different orientations. Typical sample-delivery schemes leave
little or no room for controlling the orientations. As such, the orientation
associated with a given snapshot should be estimated after the experiment. Here
we present an open-source code for the most rigorous technique having been
reported in this context. Some practical issues along with proposed solutions
are also discussed.",recovery room scam
http://arxiv.org/abs/1604.00408v1,"Generation of photon pairs from compact, manufacturable and inexpensive
silicon (Si) photonic devices at room temperature may help develop practical
applications of quantum photonics. An important characteristic of photon-pair
generation is the two-photon joint spectral intensity (JSI), which describes
the frequency correlations of the photon pair. In particular, heralded
single-photon generation requires uncorrelated photons, rather than the highly
anti-correlated photons conventionally obtained under continuous-wave (CW)
pumping. Recent attempts to achieve such a factorizable JSI have used short
optical pulses from mode-locked lasers, which are much more expensive and
bigger table-top or rack-sized instruments compared to the Si microchip pair
generator, dominate the cost and inhibit the miniaturization of the source.
Here, we generate photon pairs from a Si microring resonator by using an
electronic step-recovery diode to drive an electro-optic modulator which carves
the pump light from a CW optical diode into pulses of the appropriate width,
thus potentially eliminating the need for optical mode-locked lasers.",recovery room scam
http://arxiv.org/abs/1605.07469v1,"Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In applications
such as source separation, the phase recovery for each extracted component is a
major issue since it often leads to audible artifacts. In this paper, we
present a methodology for evaluating various NMF-based source separation
techniques involving phase reconstruction. For each model considered, a
comparison between two approaches (blind separation without prior information
and oracle separation with supervised model learning) is performed, in order to
inquire about the room for improvement for the estimation methods. Experimental
results show that the High Resolution NMF (HRNMF) model is particularly
promising, because it is able to take phases and correlations over time into
account with a great expressive power.",recovery room scam
http://arxiv.org/abs/1711.06503v1,"Location fingerprinting locates devices based on pattern matching signal
observations to a pre-defined signal map. This paper introduces a technique to
enable fast signal map creation given a dedicated surveyor with a smartphone
and floorplan. Our technique (PFSurvey) uses accelerometer, gyroscope and
magnetometer data to estimate the surveyor's trajectory post-hoc using
Simultaneous Localisation and Mapping and particle filtering to incorporate a
building floorplan. We demonstrate conventional methods can fail to recover the
survey path robustly and determine the room unambiguously. To counter this we
use a novel loop closure detection method based on magnetic field signals and
propose to incorporate the magnetic loop closures and straight-line constraints
into the filtering process to ensure robust trajectory recovery. We show this
allows room ambiguities to be resolved.
  An entire building can be surveyed by the proposed system in minutes rather
than days. We evaluate in a large office space and compare to state-of-the-art
approaches. We achieve trajectories within 1.1 m of the ground truth 90% of the
time. Output signal maps well approximate those built from conventional,
laborious manual survey. We also demonstrate that the signal maps built by
PFSurvey provide similar or even better positioning performance than the manual
signal maps.",recovery room scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",recovery room scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",recovery room scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",recovery room scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",recovery room scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",recovery room scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",recovery room scam
http://arxiv.org/abs/physics/0703199v1,"The electromagnetic calorimeter of PANDA at the FAIR facility will rely on an
operation of lead tungstate (PWO) scintillation crystals at temperatures near
-25 deg.C to provide sufficient resolution for photons in the energy range from
8 GeV down to 10 MeV. Radiation hardness of PWO crystals was studied at the
IHEP (Protvino) irradiation facility in the temperature range from room
temperature down to -25 deg.C. These studies have indicated a significantly
different behaviour in the time evolution of the damaging processes well below
room temperature. Different signal loss levels at the same dose rate, but at
different temperatures were observed. The effect of a deep suppression of the
crystal recovery process at temperatures below
  0 deg.C has been seen.",recovery room scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",recovery room scam
http://arxiv.org/abs/physics/0604102v1,"Eight PbWO4 crystals produced for the electromagnetic calorimeter of the CMS
experiment at LHC have been irradiated in a 20 GeV/c proton beam up to fluences
of 5.4 E13 p/cm2. The damage recovery in these crystals, stored in the dark at
room temperature, has been followed for over a year. Comparative irradiations
with 60Co photons have been performed on seven other crystals using a dose rate
of 1 kGy/h. The issue whether hadrons cause a specific damage to the
scintillation mechanism has been studied through light output measurements on
the irradiated crystals using cosmic rays. The correlation between light output
changes and light transmission changes is measured to be the same for
proton-irradiated crystals and for gamma-irradiated crystals. Thus, within the
precision of the measurements and for the explored range of proton fluences, no
additional, hadron-specific damage to the scintillation mechanism is observed.",recovery room scam
http://arxiv.org/abs/0709.4409v1,"A Lead Tungstate crystal produced for the electromagnetic calorimeter of the
CMS experiment at the LHC was cut into three equal-length sections. The central
one was irradiated with 290 MeV/c positive pions up to a fluence of (5.67 +-
0.46)x10^13 /cm^2, while the other two were exposed to a 24 GeV/c proton
fluence of (1.17 +- 0.11) x 10^13/ cm^2. The damage recovery in these crystals,
stored in the dark at room temperature, has been followed over two years. The
comparison of the radiation-induced changes in light transmission for these
crystals shows that damage is proportional to the star densities produced by
the irradiation.",recovery room scam
http://arxiv.org/abs/1502.02245v1,"Structures play a significant role in the field of signal processing. As a
representative of structural data, low rank matrix along with its restricted
isometry property (RIP) has been an important research topic in compressive
signal processing. Subspace projection matrix is a kind of low rank matrix with
additional structure, which allows for further reduction of its intrinsic
dimension. This leaves room for improving its own RIP, which could work as the
foundation of compressed subspace projection matrix recovery. In this work, we
study the RIP of subspace projection matrix under random orthonormal
compression. Considering the fact that subspace projection matrices of $s$
dimensional subspaces in $\mathbb{R}^N$ form an $s(N-s)$ dimensional
submanifold in $\mathbb{R}^{N\times N}$, our main concern is transformed to the
stable embedding of such submanifold into $\mathbb{R}^{N\times N}$. The result
is that by $O(s(N-s)\log N)$ number of random measurements the RIP of subspace
projection matrix is guaranteed.",recovery room scam
http://arxiv.org/abs/1309.3872v2,"Lutetium-Yttrium Orthosilicate doped with Cerium (LYSO:Ce), as a bright
scintillating crystal, is a candidate for calorimetry applications in strong
ionizing-radiation fields and large high-energy hadron fluences as are expected
at the CERN Large Hadron Collider after the planned High-Luminosity upgrade.
There, proton-proton collisions will produce fast hadron fluences up to
~5E14/cm^2 in the large-rapidity regions of the calorimeters. The performance
of LYSO:Ce has been investigated, after exposure to different fluences of 24
GeV/c protons. Measured changes in optical transmission as a function of proton
fluence are presented, and the evolution over time due to spontaneous recovery
at room temperature is studied. The activation of materials will also be an
issue in the described environment. Studies of the ambient dose induced by LYSO
and its evolution with time, in comparison with other scintillating crystals,
have also been performed through measurements and FLUKA simulations.",recovery room scam
http://arxiv.org/abs/1811.09097v1,"Supercapacitors are electric devices able to deliver a large power, enabling
their use e.g. for the recovery of breaking energy in cars. This is achieved by
using two carbon electrodes and an electrolyte solution or a pure ionic liquid
(Room Temperature Ionic Liquid, RTIL). Energy is stored by the adsorption of
ions at the surface of the electrodes, but the microscopic mechanism underlying
the exceptional performance of Carbide Derived Carbon (CDC) electrodes remained
unknown. Using molecular simulation with realistic electrode structures and
under constant voltage conditions, we investigate the effect of confinement and
solvation on the microscopic charging mechanism. We further analyse the
dynamics of the charging process and make the link with equivalent circuit
models used by electrochemists.",recovery room scam
http://arxiv.org/abs/1910.07058v1,"In recent years the concept of the Internet of Things (IoT) has evolved to
connect commercial gadgets together with the medical field to facilitate an
unprecedented range of accessibility. The development of medical devices
connected to internet of things has been praised for the potential of
alleviating the strain on the modern healthcare system by giving users the
opportunity to reside in the home during treatment or recovery. With the IoT
becoming more prevalent and available at a commercial level, there exists room
for integration into emerging, intelligent environments such as smart homes.
When used in tandem with conventional healthcare, the IoT offers a vast range
of custom-tailored treatment options. This paper studies recent
state-of-the-art research on the field of IoT for health monitoring and smart
homes, examines several potential use-cases of blending the technology, and
proposes integration with an existing smart home testbed for further study.
Challenges of adoption and future research on the topic are also discussed.",recovery room scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",credit scam
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",credit scam
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",credit scam
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",credit scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",credit scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",credit scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",credit scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",credit scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",credit scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",credit scam
http://arxiv.org/abs/1408.3455v1,"Collaboration among researchers is an essential component of the modern
scientific enterprise, playing a particularly important role in
multidisciplinary research. However, we continue to wrestle with allocating
credit to the coauthors of publications with multiple authors, since the
relative contribution of each author is difficult to determine. At the same
time, the scientific community runs an informal field-dependent credit
allocation process that assigns credit in a collective fashion to each work.
Here we develop a credit allocation algorithm that captures the coauthors'
contribution to a publication as perceived by the scientific community,
reproducing the informal collective credit allocation of science. We validate
the method by identifying the authors of Nobel-winning papers that are credited
for the discovery, independent of their positions in the author list. The
method can also compare the relative impact of researchers working in the same
field, even if they did not publish together. The ability to accurately measure
the relative credit of researchers could affect many aspects of credit
allocation in science, potentially impacting hiring, funding, and promotion
decisions.",credit scam
http://arxiv.org/abs/1312.7740v1,"Today, with respect to the increasing growth of demand to get credit from the
customers of banks and finance and credit institutions, using an effective and
efficient method to decrease the risk of non-repayment of credit given is very
necessary. Assessment of customers' credit is one of the most important and the
most essential duties of banks and institutions, and if an error occurs in this
field, it would leads to the great losses for banks and institutions. Thus,
using the predicting computer systems has been significantly progressed in
recent decades. The data that are provided to the credit institutions' managers
help them to make a straight decision for giving the credit or not-giving it.
In this paper, we will assess the customer credit through a combined
classification using artificial neural networks, genetics algorithm and
Bayesian probabilities simultaneously, and the results obtained from three
methods mentioned above would be used to achieve an appropriate and final
result. We use the K_folds cross validation test in order to assess the method
and finally, we compare the proposed method with the methods such as
Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well
as GA+SVM where the genetics algorithm has been used to improve them.",credit scam
http://arxiv.org/abs/0812.0956v2,"EcoTRADE is a multi player network game of a virtual biodiversity credit
market. Each player controls the land use of a certain amount of parcels on a
virtual landscape. The biodiversity credits of a particular parcel depend on
neighboring parcels, which may be owned by other players. The game can be used
to study the strategies of players in experiments or classroom games and also
as a communication tool for stakeholders participating in credit markets that
include spatially interdependent credits.",credit scam
http://arxiv.org/abs/1901.04957v3,"In Time-Sensitive Networking (TSN), it is important to formally prove per
flow latency and backlog bounds. To this end, recent works apply network
calculus and obtain latency bounds from service curves. The latency component
of such service curves is directly derived from upper bounds on the values of
the credit counters used by the Credit-Based Shaper (CBS), an essential
building-block of TSN. In this paper, we derive and formally prove credit upper
bounds for CBS, which improve on existing bounds.",credit scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",credit scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",credit scam
http://arxiv.org/abs/1805.09104v2,"We present a secure approach for maintaining and reporting credit history
records on the Blockchain. Our approach removes third-parties such as credit
reporting agencies from the lending process and replaces them with smart
contracts. This allows customers to interact directly with the lenders or banks
while ensuring the integrity, unmalleability and privacy of their credit data.
Most importantly, each customer is given full control over complete or
selective disclosure of her credit records, eliminating the risk of privacy
violations or data breaches such as the one that happened to Equifax in 2017.
Moreover, our approach provides strong guarantees for the lenders as well. A
lender can check both correctness and completeness of the credit data disclosed
to her. This is the first approach that is able to perform all real-world
credit reporting tasks without a central authority or changing the financial
mechanisms.",credit scam
http://arxiv.org/abs/1611.06439v1,"Credit card plays a very important rule in today's economy. It becomes an
unavoidable part of household, business and global activities. Although using
credit cards provides enormous benefits when used carefully and
responsibly,significant credit and financial damages may be caused by
fraudulent activities. Many techniques have been proposed to confront the
growth in credit card fraud. However, all of these techniques have the same
goal of avoiding the credit card fraud; each one has its own drawbacks,
advantages and characteristics. In this paper, after investigating difficulties
of credit card fraud detection, we seek to review the state of the art in
credit card fraud detection techniques, data sets and evaluation criteria.The
advantages and disadvantages of fraud detection methods are enumerated and
compared.Furthermore, a classification of mentioned techniques into two main
fraud detection approaches, namely, misuses (supervised) and anomaly detection
(unsupervised) is presented. Again, a classification of techniques is proposed
based on capability to process the numerical and categorical data sets.
Different data sets used in literature are then described and grouped into real
and synthesized data and the effective and common attributes are extracted for
further usage.Moreover, evaluation employed criterions in literature are
collected and discussed.Consequently, open issues for credit card fraud
detection are explained as guidelines for new researchers.",credit scam
http://arxiv.org/abs/1905.01943v1,"A fair assignment of credit for multi-authored publications is a
long-standing issue in scientometrics. In the calculation of the $h$-index, for
instance, all co-authors receive equal credit for a given publication,
independent of a given author's contribution to the work or of the total number
of co-authors. Several attempts have been made to distribute the credit in a
more appropriate manner. In a recent paper, Hirsch has suggested a new way of
credit assignment that is fundamentally different from the previous ones: All
credit for a multi-author paper goes to a single author, the called
``$\alpha$-author'', defined as the person with the highest current $h$-index
not the highest $h$-index at the time of the paper's publication) (J. E.
Hirsch, Scientometrics 118, 673 (2019)). The collection of papers this author
has received credit for as $\alpha$-author is then used to calculate a new
index, $h_{\alpha}$, following the same recipe as for the usual $h$ index. The
objective of this new assignment is not a fairer distribution of credit, but
rather the determination of an altogether different property, the degree of a
person's scientific leadership. We show that given the complex time dependence
of $h$ for individual scientists, the approach of using the current $h$ value
instead of the historic one is problematic, and we argue that it would be
feasible to determine the $\alpha$-author at the time of the paper's
publication instead. On the other hand, there are other practical
considerations that make the calculation of the proposed $h_{\alpha}$ very
difficult. As an alternative, we explore other ways of crediting papers to a
single author in order to test early career achievement or scientific
leadership.",credit scam
http://arxiv.org/abs/1103.2848v1,"Devising a weight assignment policy for assigning credits to multiple authors
of a manuscript is a challenging task. In this paper, we present a scheme for
assigning credits to multiple authors that we call a polynomial weight
assignment scheme. We compare our scheme with other schemes proposed in the
literature.",credit scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",auction scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",auction scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",auction scam
http://arxiv.org/abs/1204.0535v1,"Display advertisements on the web are sold via ad exchanges that use real
time auction. We describe the challenges of designing a suitable auction, and
present a simple auction called the Optional Second Price (OSP) auction that is
currently used in Doubleclick Ad Exchange.",auction scam
http://arxiv.org/abs/1110.2733v1,"We study auctions with severe bounds on the communication allowed: each
bidder may only transmit t bits of information to the auctioneer. We consider
both welfare- and profit-maximizing auctions under this communication
restriction. For both measures, we determine the optimal auction and show that
the loss incurred relative to unconstrained auctions is mild. We prove
non-surprising properties of these kinds of auctions, e.g., that in optimal
mechanisms bidders simply report the interval in which their valuation lies in,
as well as some surprising properties, e.g., that asymmetric auctions are
better than symmetric ones and that multi-round auctions reduce the
communication complexity only by a linear factor.",auction scam
http://arxiv.org/abs/1109.1093v1,"The success of online auctions has given buyers access to greater product
diversity with potentially lower prices. It has provided sellers with access to
large numbers of potential buyers and reduced transaction costs by enabling
auctions to take place without regard to time or place. However it is difficult
to spend more time period with system and closely monitor the auction until
auction participant wins the bid or closing of the auction. Determining which
items to bid on or what may be the recommended bid and when to bid it are
difficult questions to answer for online auction participants. The multi agent
auction advisor system JADE and TRACE, which is connected with decision support
system, gives the recommended bid to buyers for online auctions. The auction
advisor system relies on intelligent agents both for the retrieval of relevant
auction data and for the processing of that data to enable meaningful
recommendations, statistical reports and market prediction report to be made to
auction participants.",auction scam
http://arxiv.org/abs/1211.2875v1,"In an electronic auction protocol, the main participants are the seller, a
set of trusted auctioneer(s) and the set of bidders. In this paper we consider
the situation where there is a seller and a set of n bidders intending to come
to an agreement on the selling price of a certain good. Full private or
bidder-resolved auction means that this agreement is reached without the help
of trusted parties or auctioneers. Therefore, only the seller and the set of
bidders are involved, the role of the auctioneers becomes obsolete in this
case. property.We propose a new technique for the design of a full private
sealed-bid auction protocol.",auction scam
http://arxiv.org/abs/1509.08496v1,"This letter considers the design of an auction mechanism to sell the object
of a seller when the buyers quantize their private value estimates regarding
the object prior to communicating them to the seller. The designed auction
mechanism maximizes the utility of the seller (i.e., the auction is optimal),
prevents buyers from communicating falsified quantized bids (i.e., the auction
is incentive-compatible), and ensures that buyers will participate in the
auction (i.e., the auction is individually-rational). The letter also
investigates the design of the optimal quantization thresholds using which
buyers quantize their private value estimates. Numerical results provide
insights regarding the influence of the quantization thresholds on the auction
mechanism.",auction scam
http://arxiv.org/abs/0807.2496v2,"Search auctions have become a dominant source of revenue generation on the
Internet. Such auctions have typically used per-click bidding and pricing. We
propose the use of hybrid auctions where an advertiser can make a
per-impression as well as a per-click bid, and the auctioneer then chooses one
of the two as the pricing mechanism. We assume that the advertiser and the
auctioneer both have separate beliefs (called priors) on the click-probability
of an advertisement. We first prove that the hybrid auction is truthful,
assuming that the advertisers are risk-neutral. We then show that this auction
is superior to the existing per-click auction in multiple ways: 1) It takes
into account the risk characteristics of the advertisers. 2) For obscure
keywords, the auctioneer is unlikely to have a very sharp prior on the
click-probabilities. In such situations, the hybrid auction can result in
significantly higher revenue. 3) An advertiser who believes that its
click-probability is much higher than the auctioneer's estimate can use
per-impression bids to correct the auctioneer's prior without incurring any
extra cost. 4) The hybrid auction can allow the advertiser and auctioneer to
implement complex dynamic programming strategies. As Internet commerce matures,
we need more sophisticated pricing models to exploit all the information held
by each of the participants. We believe that hybrid auctions could be an
important step in this direction.",auction scam
http://arxiv.org/abs/1304.7718v1,"The first-price auction is popular in practice for its simplicity and
transparency. Moreover, its potential virtues grow in complex settings where
incentive compatible auctions may generate little or no revenue. Unfortunately,
the first-price auction is poorly understood in theory because equilibrium is
not {\em a priori} a credible predictor of bidder behavior.
  We take a dynamic approach to studying first-price auctions: rather than
basing performance guarantees solely on static equilibria, we study the
repeated setting and show that robust performance guarantees may be derived
from simple axioms of bidder behavior. For example, as long as a loser raises
her bid quickly, a standard first-price auction will generate at least as much
revenue as a second-price auction. We generalize this dynamic technique to
complex pay-your-bid auction settings and show that progressively stronger
assumptions about bidder behavior imply progressively stronger guarantees about
the auction's performance.
  Along the way, we find that the auctioneer's choice of bidding language is
critical when generalizing beyond the single-item setting, and we propose a
specific construction called the {\em utility-target auction} that performs
well. The utility-target auction includes a bidder's final utility as an
additional parameter, identifying the single dimension along which she wishes
to compete. This auction is closely related to profit-target bidding in
first-price and ascending proxy package auctions and gives strong revenue
guarantees for a variety of complex auction environments. Of particular
interest, the guaranteed existence of a pure-strategy equilibrium in the
utility-target auction shows how Overture might have eliminated the cyclic
behavior in their generalized first-price sponsored search auction if bidders
could have placed more sophisticated bids.",auction scam
http://arxiv.org/abs/1411.2079v1,"We propose a uniform approach for the design and analysis of prior-free
competitive auctions and online auctions. Our philosophy is to view the
benchmark function as a variable parameter of the model and study a broad class
of functions instead of a individual target benchmark. We consider a multitude
of well-studied auction settings, and improve upon a few previous results.
  (1) Multi-unit auctions. Given a $\beta$-competitive unlimited supply
auction, the best previously known multi-unit auction is $2\beta$-competitive.
We design a $(1+\beta)$-competitive auction reducing the ratio from $4.84$ to
$3.24$. These results carry over to matroid and position auctions.
  (2) General downward-closed environments. We design a $6.5$-competitive
auction improving upon the ratio of $7.5$. Our auction is noticeably simpler
than the previous best one.
  (3) Unlimited supply online auctions. Our analysis yields an auction with a
competitive ratio of $4.12$, which significantly narrows the margin of
$[4,4.84]$ previously known for this problem.
  A particularly important tool in our analysis is a simple decomposition
lemma, which allows us to bound the competitive ratio against a sum of
benchmark functions. We use this lemma in a ""divide and conquer"" fashion by
dividing the target benchmark into the sum of simpler functions.",auction scam
http://arxiv.org/abs/1605.03826v1,"We introduce a novel characterization of all Walrasian price vectors in terms
of forbidden over- and under demanded sets for monotone gross substitute
combinatorial auctions.
  For ascending and descending auctions we suggest a universal framework for
finding the minimum or maximum Walrasian price vectors for monotone gross
substitute combinatorial auctions. An ascending (descending) auction is
guaranteed to find the minimum (maximum) Walrasian if and only if it follows
the suggested framework.",auction scam
http://arxiv.org/abs/cs/0109065v1,"In this paper, we assess the applicability of auctions based on the Vickrey
second price model for allocating wireless spectrum in developing countries. We
first provide an overview of auction models for allocating resources. We then
examine the experience of auctioning spectrum in different countries. Based on
this examination, we posit some axioms that seem to have to be satisfied when
allocating spectrum in most developing countries. In light of these axioms, we
provide a critical evaluation of using Vickrey second- price auctions to
allocate spectrum in developing countries. We suggest the use of a new auction
mechanism, the Vickrey ""share auction"" which will satisfy many of these axioms.",auction scam
http://arxiv.org/abs/1210.6450v1,"In auction theory, cryptography has been used to achieve anonymity of the
participants, security and privacy of the bids, secure computation and to
simulate mediator (auctioneer). Auction theory focuses on revenue and
Cryptography focuses on security and privacy. Involving Cryptography at base
level, to enhance revenue gives entirely new perspective and insight to Auction
theory, thereby achieving the core goals of auction theory. In this report, we
try to investigate an interesting field of study in Auction Theory using
Cryptographic primitives.",auction scam
http://arxiv.org/abs/0904.1258v2,"Auctions are markets with strict regulations governing the information
available to traders in the market and the possible actions they can take.
Since well designed auctions achieve desirable economic outcomes, they have
been widely used in solving real-world optimization problems, and in
structuring stock or futures exchanges. Auctions also provide a very valuable
testing-ground for economic theory, and they play an important role in
computer-based control systems.
  Auction mechanism design aims to manipulate the rules of an auction in order
to achieve specific goals. Economists traditionally use mathematical methods,
mainly game theory, to analyze auctions and design new auction forms. However,
due to the high complexity of auctions, the mathematical models are typically
simplified to obtain results, and this makes it difficult to apply results
derived from such models to market environments in the real world. As a result,
researchers are turning to empirical approaches.
  This report aims to survey the theoretical and empirical approaches to
designing auction mechanisms and trading strategies with more weights on
empirical ones, and build the foundation for further research in the field.",auction scam
http://arxiv.org/abs/1108.2452v2,"In many settings agents participate in multiple different auctions that are
not necessarily implemented simultaneously. Future opportunities affect
strategic considerations of the players in each auction, introducing
externalities. Motivated by this consideration, we study a setting of a market
of buyers and sellers, where each seller holds one item, bidders have
combinatorial valuations and sellers hold item auctions sequentially.
  Our results are qualitatively different from those of simultaneous auctions,
proving that simultaneity is a crucial aspect of previous work. We prove that
if sellers hold sequential first price auctions then for unit-demand bidders
(matching market) every subgame perfect equilibrium achieves at least half of
the optimal social welfare, while for submodular bidders or when second price
auctions are used, the social welfare can be arbitrarily worse than the
optimal. We also show that a first price sequential auction for buying or
selling a base of a matroid is always efficient, and implements the VCG
outcome.
  An important tool in our analysis is studying first and second price auctions
with externalities (bidders have valuations for each possible winner outcome),
which can be of independent interest. We show that a Pure Nash Equilibrium
always exists in a first price auction with externalities.",auction scam
http://arxiv.org/abs/0801.3097v1,"Resource allocation is considered for cooperative transmissions in
multiple-relay wireless networks. Two auction mechanisms, SNR auctions and
power auctions, are proposed to distributively coordinate the allocation of
power among multiple relays. In the SNR auction, a user chooses the relay with
the lowest weighted price. In the power auction, a user may choose to use
multiple relays simultaneously, depending on the network topology and the
relays' prices. Sufficient conditions for the existence (in both auctions) and
uniqueness (in the SNR auction) of the Nash equilibrium are given. The fairness
of the SNR auction and efficiency of the power auction are further discussed.
It is also proven that users can achieve the unique Nash equilibrium
distributively via best response updates in a completely asynchronous manner.",auction scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",auction scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",auction scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",auction scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",auction scam
http://arxiv.org/abs/1405.0201v1,"The need of totally secure online auction has led to the invention of many
auction protocols. But as new attacks are developed, auction protocols also
require corresponding strengthening. We analyze the auction protocol based on
the well-known mathematical public-key knapsack problem for the design of
asymmetric public-key knapsack trapdoor cryptosystem. Even though the knapsack
system is not cryptographically secure, it can be used in certain auction
situations. We describe the limitations of the protocol like detecting and
solving the tie between bidders, malicious behavior of participants and also
selection of price set by the seller and offer solutions.",auction scam
http://arxiv.org/abs/1212.5766v1,"We consider prior-free auctions for revenue and welfare maximization when
agents have a common budget. The abstract environments we consider are ones
where there is a downward-closed and symmetric feasibility constraint on the
probabilities of service of the agents. These environments include position
auctions where slots with decreasing click-through rates are auctioned to
advertisers. We generalize and characterize the envy-free benchmark from
Hartline and Yan (2011) to settings with budgets and characterize the optimal
envy-free outcomes for both welfare and revenue. We give prior-free mechanisms
that approximate these benchmarks. A building block in our mechanism is a
clinching auction for position auction environments. This auction is a
generalization of the multi-unit clinching auction of Dobzinski et al. (2008)
and a special case of the polyhedral clinching auction of Goel et al. (2012).
For welfare maximization, we show that this clinching auction is a good
approximation to the envy-free optimal welfare for position auction
environments. For profit maximization, we generalize the random sampling profit
extraction auction from Fiat et al. (2002) for digital goods to give a
10.0-approximation to the envy-free optimal revenue in symmetric,
downward-closed environments. The profit maximization question is of interest
even without budgets and our mechanism is a 7.5-approximation which improving
on the 30.4 bound of Ha and Hartline (2012).",auction scam
http://arxiv.org/abs/1905.03773v1,"Bulow and Klemperer's well-known result states that, in a single-item auction
where the $n$ bidders' values are independently and identically drawn from a
regular distribution, the Vickrey auction with one additional bidder (a
duplicate) extracts at least as much revenue as the optimal auction without the
duplicate. Hartline and Roughgarden, in their influential 2009 paper, removed
the requirement that the distributions be identical, at the cost of allowing
the Vickrey auction to recruit $n$ duplicates, one from each distribution, and
relaxing its revenue advantage to a $2$-approximation.
  In this work we restore Bulow and Klemperer's number of duplicates in
Hartline and Roughgarden's more general setting with a worse approximation
ratio. We show that recruiting a duplicate from one of the distributions
suffices for the Vickrey auction to $10$-approximate the optimal revenue. We
also show that in a $k$-items unit demand auction, recruiting $k$ duplicates
suffices for the VCG auction to $O(1)$-approximate the optimal revenue.
  As another result, we tighten the analysis for Hartline and Roughgarden's
Vickrey auction with $n$ duplicates for the case with two bidders in the
auction. We show that in this case the Vickrey auction with two duplicates
obtains at least $3/4$ of the optimal revenue. This is tight by meeting a lower
bound by Hartline and Roughgarden. En route, we obtain a transparent analysis
of their $2$-approximation for $n$~bidders, via a natural connection to Ronen's
lookahead auction.",auction scam
http://arxiv.org/abs/1907.05181v1,"Auctions are protocols to allocate goods to buyers who have preferences over
them, and collect payments in return. Economists have invested significant
effort in designing auction rules that result in allocations of the goods that
are desirable for the group as a whole. However, for settings where
participants' valuations of the items on sale are their private information,
the rules of the auction must deter buyers from misreporting their preferences,
so as to maximize their own utility, since misreported preferences hinder the
ability for the auctioneer to allocate goods to those who want them most.
Manual auction design has yielded excellent mechanisms for specific settings,
but requires significant effort when tackling new domains. We propose a deep
learning based approach to automatically design auctions in a wide variety of
domains, shifting the design work from human to machine. We assume that
participants' valuations for the items for sale are independently sampled from
an unknown but fixed distribution. Our system receives a data-set consisting of
such valuation samples, and outputs an auction rule encoding the desired
incentive structure. We focus on producing truthful and efficient auctions that
minimize the economic burden on participants. We evaluate the auctions designed
by our framework on well-studied domains, such as multi-unit and combinatorial
auctions, showing that they outperform known auction designs in terms of the
economic burden placed on participants.",auction scam
http://arxiv.org/abs/1311.2820v1,"We introduce draft auctions, which is a sequential auction format where at
each iteration players bid for the right to buy items at a fixed price. We show
that draft auctions offer an exponential improvement in social welfare at
equilibrium over sequential item auctions where predetermined items are
auctioned at each time step. Specifically, we show that for any subadditive
valuation the social welfare at equilibrium is an $O(\log^2(m))$-approximation
to the optimal social welfare, where $m$ is the number of items. We also
provide tighter approximation results for several subclasses. Our welfare
guarantees hold for Bayes-Nash equilibria and for no-regret learning outcomes,
via the smooth-mechanism framework. Of independent interest, our techniques
show that in a combinatorial auction setting, efficiency guarantees of a
mechanism via smoothness for a very restricted class of cardinality valuations,
extend with a small degradation, to subadditive valuations, the largest
complement-free class of valuations. Variants of draft auctions have been used
in practice and have been experimentally shown to outperform other auctions.
Our results provide a theoretical justification.",auction scam
http://arxiv.org/abs/1307.7433v1,"Truthful spectrum auctions have been extensively studied in recent years.
Truthfulness makes bidders bid their true valuations, simplifying greatly the
analysis of auctions. However, revealing one's true valuation causes severe
privacy disclosure to the auctioneer and other bidders. To make things worse,
previous work on secure spectrum auctions does not provide adequate security.
In this paper, based on TRUST, we propose PS-TRUST, a provably secure solution
for truthful double spectrum auctions. Besides maintaining the properties of
truthfulness and special spectrum reuse of TRUST, PS-TRUST achieves provable
security against semi-honest adversaries in the sense of cryptography.
Specifically, PS-TRUST reveals nothing about the bids to anyone in the auction,
except the auction result. To the best of our knowledge, PS-TRUST is the first
provably secure solution for spectrum auctions. Furthermore, experimental
results show that the computation and communication overhead of PS-TRUST is
modest, and its practical applications are feasible.",auction scam
http://arxiv.org/abs/0912.3310v2,"We study truthful mechanisms for hiring a team of agents in three classes of
set systems: Vertex Cover auctions, k-flow auctions, and cut auctions. For
Vertex Cover auctions, the vertices are owned by selfish and rational agents,
and the auctioneer wants to purchase a vertex cover from them. For k-flow
auctions, the edges are owned by the agents, and the auctioneer wants to
purchase k edge-disjoint s-t paths, for given s and t. In the same setting, for
cut auctions, the auctioneer wants to purchase an s-t cut. Only the agents know
their costs, and the auctioneer needs to select a feasible set and payments
based on bids made by the agents.
  We present constant-competitive truthful mechanisms for all three set
systems. That is, the maximum overpayment of the mechanism is within a constant
factor of the maximum overpayment of any truthful mechanism, for every set
system in the class. The mechanism for Vertex Cover is based on scaling each
bid by a multiplier derived from the dominant eigenvector of a certain matrix.
The mechanism for k-flows prunes the graph to be minimally (k+1)-connected, and
then applies the Vertex Cover mechanism. Similarly, the mechanism for cuts
contracts the graph until all s-t paths have length exactly 2, and then applies
the Vertex Cover mechanism.",auction scam
http://arxiv.org/abs/1201.0404v3,"A central issue in applying auction theory in practice is the problem of
dealing with budget-constrained agents. A desirable goal in practice is to
design incentive compatible, individually rational, and Pareto optimal auctions
while respecting the budget constraints. Achieving this goal is particularly
challenging in the presence of nontrivial combinatorial constraints over the
set of feasible allocations.
  Toward this goal and motivated by AdWords auctions, we present an auction for
{\em polymatroidal} environments satisfying the above properties. Our auction
employs a novel clinching technique with a clean geometric description and only
needs an oracle access to the submodular function defining the polymatroid. As
a result, this auction not only simplifies and generalizes all previous
results, it applies to several new applications including AdWords Auctions,
bandwidth markets, and video on demand. In particular, our characterization of
the AdWords auction as polymatroidal constraints might be of independent
interest. This allows us to design the first mechanism for Ad Auctions taking
into account simultaneously budgets, multiple keywords and multiple slots.
  We show that it is impossible to extend this result to generic polyhedral
constraints. This also implies an impossibility result for multi-unit auctions
with decreasing marginal utilities in the presence of budget constraints.",auction scam
http://arxiv.org/abs/1805.00256v3,"With the increasing use of auctions in online advertising, there has been a
large effort to study seller revenue maximization, following Myerson's seminal
work, both theoretically and practically. We take the point of view of the
buyer in classical auctions and ask the question of whether she has an
incentive to shade her bid even in auctions that are reputed to be truthful,
when aware of the revenue optimization mechanism.
  We show that in auctions such as the Myerson auction or a VCG with reserve
price set as the monopoly price, the buyer who is aware of this information has
indeed an incentive to shade. Intuitively, by selecting the revenue maximizing
auction, the seller introduces a dependency on the buyers' distributions in the
choice of the auction. We study in depth the case of the Myerson auction and
show that a symmetric equilibrium exists in which buyers shade non-linearly
what would be their first price bid. They then end up with an expected payoff
that is equal to what they would get in a first price auction with no reserve
price.
  We conclude that a return to simple first price auctions with no reserve
price or at least non-dynamic anonymous ones is desirable from the point of
view of both buyers, sellers and increasing transparency.",auction scam
http://arxiv.org/abs/1909.00425v1,"We introduce the assortment auction optimization problem, defined as follows.
A seller has a set of substitute products with exogenously-given prices. Each
buyer has a ranked list from which she would like to purchase at most one
product. The buyers report their lists to the seller, who then allocates
products to the buyers using a truthful mechanism, subject to constraints on
how many products can be allocated. The seller collects revenues equal to the
prices of the products allocated, and would like to design an auction to
maximize total revenue, when the buyers' lists are drawn independently from
known distributions.
  If there is a single buyer, then our problem reduces to the assortment
optimization problem, which is solved for Markov Chain choice models. We extend
this result and compute the optimal assortment auction when each buyer's list
distribution arises from its own Markov chain. Moreover, we show that the
optimal auction is structurally ``Myersonian'', in that each buyer is assigned
a virtual valuation based on her list and Markov chain, and then the mechanism
maximizes virtual surplus. Since Markov Chain choice models capture valuation
distributions, our optimal assortment auction generalizes the classical
Myerson's auction. Markov chains also capture the commonly used MNL choice
model. We show that without the Markov chain assumption, the optimal assortment
auction may be structurally non-Myersonian.
  Finally, we apply the concept of an assortment auction in online assortment
problems. We show that any personalized assortment policy is a special case of
a truthful assortment auction, and that moreover, the optimal auction provides
a tighter relaxation for online policies than the commonly-used ``deterministic
LP''. Using this fact, we improve many online assortment policies, and derive
the first approximation guarantees that strictly exceed 1-1/e.",auction scam
http://arxiv.org/abs/cs/0109088v1,"In this paper, we aim to empirically examine the value of website usage and
sellers' listing behavior in the two leading Internet auctions sites, eBay and
Yahoo!Auctions. The descriptive data analysis of the seller's equilibrium
listing behavior indicates that a seller's higher expected auction revenue from
eBay is correlated with a lager number of potential bidders measured by website
usage per listing. Our estimation results, based on the logarithm
specifications of sellers' expected auction revenues and potential bidders'
website usage, show that in a median case, (i) 1 percent increase of the unique
visitors (page views) per listed item induces 0.022 (0.007) percent increase of
a seller's expected auction revenue; and (ii) 1 percent increase of sellers'
listings induces 1.99 (4.74) percent increase of the unique visitors (page
views). Since increased expected auction revenues will induce more listings, we
can infer positive feedback effects between the number of listings and website
usage. Consequently, Yahoo!Auctions, which has substantially less listings, has
greater incentives to increase listings via these feedback effects which are
reflected in its fee schedules.",auction scam
http://arxiv.org/abs/1105.0469v2,"Information technology has revolutionized the traditional structure of
markets. The removal of geographical and time constraints has fostered the
growth of online auction markets, which now include millions of economic agents
worldwide and annual transaction volumes in the billions of dollars. Here, we
analyze bid histories of a little studied type of online auctions --- lowest
unique bid auctions. Similarly to what has been reported for foraging animals
searching for scarce food, we find that agents adopt Levy flight search
strategies in their exploration of ""bid space"". The Levy regime, which is
characterized by a power-law decaying probability distribution of step lengths,
holds over nearly three orders of magnitude. We develop a quantitative model
for lowest unique bid online auctions that reveals that agents use nearly
optimal bidding strategies. However, agents participating in these auctions do
not optimize their financial gain. Indeed, as long as there are many auction
participants, a rational profit optimizing agent would choose not to
participate in these auction markets.",auction scam
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake credit
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake credit
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake credit
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake credit
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake credit
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake credit
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake credit
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake credit
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake credit
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake credit
http://arxiv.org/abs/1408.3455v1,"Collaboration among researchers is an essential component of the modern
scientific enterprise, playing a particularly important role in
multidisciplinary research. However, we continue to wrestle with allocating
credit to the coauthors of publications with multiple authors, since the
relative contribution of each author is difficult to determine. At the same
time, the scientific community runs an informal field-dependent credit
allocation process that assigns credit in a collective fashion to each work.
Here we develop a credit allocation algorithm that captures the coauthors'
contribution to a publication as perceived by the scientific community,
reproducing the informal collective credit allocation of science. We validate
the method by identifying the authors of Nobel-winning papers that are credited
for the discovery, independent of their positions in the author list. The
method can also compare the relative impact of researchers working in the same
field, even if they did not publish together. The ability to accurately measure
the relative credit of researchers could affect many aspects of credit
allocation in science, potentially impacting hiring, funding, and promotion
decisions.",fake credit
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake credit
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake credit
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake credit
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake credit
http://arxiv.org/abs/1811.00661v2,"In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.",fake credit
http://arxiv.org/abs/1312.7740v1,"Today, with respect to the increasing growth of demand to get credit from the
customers of banks and finance and credit institutions, using an effective and
efficient method to decrease the risk of non-repayment of credit given is very
necessary. Assessment of customers' credit is one of the most important and the
most essential duties of banks and institutions, and if an error occurs in this
field, it would leads to the great losses for banks and institutions. Thus,
using the predicting computer systems has been significantly progressed in
recent decades. The data that are provided to the credit institutions' managers
help them to make a straight decision for giving the credit or not-giving it.
In this paper, we will assess the customer credit through a combined
classification using artificial neural networks, genetics algorithm and
Bayesian probabilities simultaneously, and the results obtained from three
methods mentioned above would be used to achieve an appropriate and final
result. We use the K_folds cross validation test in order to assess the method
and finally, we compare the proposed method with the methods such as
Clustering-Launched Classification (CLC), Support Vector Machine (SVM) as well
as GA+SVM where the genetics algorithm has been used to improve them.",fake credit
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake credit
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake credit
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake credit
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake credit
http://arxiv.org/abs/0812.0956v2,"EcoTRADE is a multi player network game of a virtual biodiversity credit
market. Each player controls the land use of a certain amount of parcels on a
virtual landscape. The biodiversity credits of a particular parcel depend on
neighboring parcels, which may be owned by other players. The game can be used
to study the strategies of players in experiments or classroom games and also
as a communication tool for stakeholders participating in credit markets that
include spatially interdependent credits.",fake credit
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake consumer
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake consumer
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake consumer
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake consumer
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake consumer
http://arxiv.org/abs/1910.03496v2,"The evolution of the information and communication technologies has
dramatically increased the number of people with access to the Internet, which
has changed the way the information is consumed. As a consequence of the above,
fake news have become one of the major concerns because its potential to
destabilize governments, which makes them a potential danger to modern society.
An example of this can be found in the US. electoral campaign, where the term
""fake news"" gained great notoriety due to the influence of the hoaxes in the
final result of these. In this work the feasibility of applying deep learning
techniques to discriminate fake news on the Internet using only their text is
studied. In order to accomplish that, three different neural network
architectures are proposed, one of them based on BERT, a modern language model
created by Google which achieves state-of-the-art results.",fake consumer
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake consumer
http://arxiv.org/abs/1902.07207v1,"Social networks offer a ready channel for fake and misleading news to spread
and exert influence. This paper examines the performance of different
reputation algorithms when applied to a large and statistically significant
portion of the news that are spread via Twitter. Our main result is that simple
crowdsourcing-based algorithms are able to identify a large portion of fake or
misleading news, while incurring only very low false positive rates for
mainstream websites. We believe that these algorithms can be used as the basis
of practical, large-scale systems for indicating to consumers which news sites
deserve careful scrutiny and skepticism.",fake consumer
http://arxiv.org/abs/1601.06372v1,"Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.",fake consumer
http://arxiv.org/abs/1908.02589v1,"Social technologies have made it possible to propagate disinformation and
manipulate the masses at an unprecedented scale. This is particularly alarming
from a security perspective, as humans have proven to be the weakest link when
protecting critical infrastructure in general, and the power grid in
particular. Here, we consider an attack in which an adversary attempts to
manipulate the behavior of energy consumers by sending fake discount
notifications encouraging them to shift their consumption into the peak-demand
period. We conduct surveys to assess the propensity of people to follow-through
on such notifications and forward them to their friends. This allows us to
model how the disinformation propagates through social networks. Finally, using
Greater London as a case study, we show that disinformation can indeed be used
to orchestrate an attack wherein unwitting consumers synchronize their
energy-usage patterns, resulting in blackouts on a city-scale. These findings
demonstrate that in an era when disinformation can be weaponized, system
vulnerabilities arise not only from the hardware and software of critical
infrastructure, but also from the behavior of the consumers.",fake consumer
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake consumer
http://arxiv.org/abs/1802.08066v3,"Social networks offer a ready channel for fake and misleading news to spread
and exert influence. This paper examines the performance of different
reputation algorithms when applied to a large and statistically significant
portion of the news that are spread via Twitter. Our main result is that simple
algorithms based on the identity of the users spreading the news, as well as
the words appearing in the titles and descriptions of the linked articles, are
able to identify a large portion of fake or misleading news, while incurring
only very low (<1%) false positive rates for mainstream websites. We believe
that these algorithms can be used as the basis of practical, large-scale
systems for indicating to consumers which news sites deserve careful scrutiny
and skepticism.",fake consumer
http://arxiv.org/abs/1806.04853v1,"Detecting fake users (also called Sybils) in online social networks is a
basic security research problem. State-of-the-art approaches rely on a large
amount of manually labeled users as a training set. These approaches suffer
from three key limitations: 1) it is time-consuming and costly to manually
label a large training set, 2) they cannot detect new Sybils in a timely
fashion, and 3) they are vulnerable to Sybil attacks that leverage information
of the training set. In this work, we propose SybilBlind, a structure-based
Sybil detection framework that does not rely on a manually labeled training
set. SybilBlind works under the same threat model as state-of-the-art
structure-based methods. We demonstrate the effectiveness of SybilBlind using
1) a social network with synthetic Sybils and 2) two Twitter datasets with real
Sybils. For instance, SybilBlind achieves an AUC of 0.98 on a Twitter dataset.",fake consumer
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake consumer
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake consumer
http://arxiv.org/abs/1811.00656v3,"In this work, we describe a new deep learning based method that can
effectively distinguish AI-generated fake videos (referred to as {\em DeepFake}
videos hereafter) from real videos. Our method is based on the observations
that current DeepFake algorithm can only generate images of limited
resolutions, which need to be further warped to match the original faces in the
source video. Such transforms leave distinctive artifacts in the resulting
DeepFake videos, and we show that they can be effectively captured by
convolutional neural networks (CNNs). Compared to previous methods which use a
large amount of real and DeepFake generated images to train CNN classifier, our
method does not need DeepFake generated images as negative training examples
since we target the artifacts in affine face warping as the distinctive feature
to distinguish real and fake images. The advantages of our method are two-fold:
(1) Such artifacts can be simulated directly using simple image processing
operations on a image to make it as negative example. Since training a DeepFake
model to generate negative examples is time-consuming and resource-demanding,
our method saves a plenty of time and resources in training data collection;
(2) Since such artifacts are general existed in DeepFake videos from different
sources, our method is more robust compared to others. Our method is evaluated
on two sets of DeepFake video datasets for its effectiveness in practice.",fake consumer
http://arxiv.org/abs/1902.00647v2,"While there have been various studies towards Android apps and their
development, there is limited discussion of the broader class of apps that fall
in the fake area. Fake apps and their development are distinct from official
apps and belong to the mobile underground industry. Due to the lack of
knowledge of the mobile underground industry, fake apps, their ecosystem and
nature still remain in mystery. To fill the blank, we conduct the first
systematic and comprehensive empirical study on a large-scale set of fake apps.
Over 150,000 samples related to the top 50 popular apps are collected for
extensive measurement. In this paper, we present discoveries from three
different perspectives, namely fake sample characteristics, quantitative study
on fake samples and fake authors' developing trend. Moreover, valuable domain
knowledge, like fake apps' naming tendency and fake developers' evasive
strategies, is then presented and confirmed with case studies, demonstrating a
clear vision of fake apps and their ecosystem.",fake consumer
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake consumer
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake consumer
http://arxiv.org/abs/1202.2369v1,"Daily deals sites such as Groupon offer deeply discounted goods and services
to tens of millions of customers through geographically targeted daily e-mail
marketing campaigns. In our prior work we observed that a negative side effect
for merchants using Groupons is that, on average, their Yelp ratings decline
significantly. However, this previous work was essentially observational,
rather than explanatory. In this work, we rigorously consider and evaluate
various hypotheses about underlying consumer and merchant behavior in order to
understand this phenomenon, which we dub the Groupon effect. We use statistical
analysis and mathematical modeling, leveraging a dataset we collected spanning
tens of thousands of daily deals and over 7 million Yelp reviews. In
particular, we investigate hypotheses such as whether Groupon subscribers are
more critical than their peers, or whether some fraction of Groupon merchants
provide significantly worse service to customers using Groupons. We suggest an
additional novel hypothesis: reviews from Groupon subscribers are lower on
average because such reviews correspond to real, unbiased customers, while the
body of reviews on Yelp contain some fraction of reviews from biased or even
potentially fake sources. Although we focus on a specific question, our work
provides broad insights into both consumer and merchant behavior within the
daily deals marketplace.",fake consumer
http://arxiv.org/abs/1904.07569v1,"One main challenge in social media is to identify trustworthy information. If
we cannot recognize information as trustworthy, that information may become
useless or be lost. Opposite, we could consume wrong or fake information with
major consequences. How does a user handle the information provided before
consuming it? Are the comments on a post, the author or votes essential for
taking such a decision? Are these attributes considered together and which
attribute is more important? To answer these questions, we developed a trust
model to support knowledge sharing of user content in social media. This trust
model is based on the dimensions of stability, quality, and credibility. Each
dimension contains metrics (user role, user IQ, votes, etc.) that are important
to the user based on data analysis. We present in this paper, an evaluation of
the proposed trust model using conjoint analysis (CA) as an evaluation method.
The results obtained from 348 responses, validate the trust model. A trust
degree translator interprets the content as very trusted, trusted, untrusted,
and very untrusted based on the calculated value of trust. Furthermore, the
results show different importance for each dimension: stability 24%,
credibility 35% and quality 41%.",fake consumer
http://arxiv.org/abs/1803.01661v1,"Online portals include an increasing amount of user feedback in form of
ratings and reviews. Recent research highlighted the importance of this
feedback and confirmed that positive feedback improves product sales figures
and thus its success. However, online portals' operators act as central
authorities throughout the overall review process. In the worst case, operators
can exclude users from submitting reviews, modify existing reviews, and
introduce fake reviews by fictional consumers. This paper presents ReviewChain,
a decentralized review approach. Our approach avoids central authorities by
using blockchain technologies, decentralized apps and storage. Thereby, we
enable users to submit and retrieve untampered reviews. We highlight the
implementation challenges encountered when realizing our approach on the public
Ethereum blockchain. For each implementation challange, we discuss possible
design alternatives and their trade-offs regarding costs, security, and
trustworthiness. Finally, we analyze which design decision should be chosen to
support specific trade-offs and present resulting combinations of decentralized
blockchain technologies, also with conventional centralized technologies.",fake consumer
http://arxiv.org/abs/1906.11462v2,"With the recent advances in Reinforcement Learning (RL), there have been
tremendous interests in employing RL for recommender systems. However, directly
training and evaluating a new RL-based recommendation algorithm needs to
collect users' real-time feedback in the real system, which is time and efforts
consuming and could negatively impact on users' experiences. Thus, it calls for
a user simulator that can mimic real users' behaviors where we can pre-train
and evaluate new recommendation algorithms. Simulating users' behaviors in a
dynamic system faces immense challenges -- (i) the underlining item
distribution is complex, and (ii) historical logs for each user are limited. In
this paper, we develop a user simulator base on Generative Adversarial Network
(GAN). To be specific, the generator captures the underlining distribution of
users' historical logs and generates realistic logs that can be considered as
augmentations of real logs; while the discriminator not only distinguishes real
and fake logs but also predicts users' behaviors. The experimental results
based on real-world e-commerce data demonstrate the effectiveness of the
proposed simulator.",fake consumer
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake consumer
http://arxiv.org/abs/1904.00712v2,"The concept of `fake news' has been referenced and thrown around in news
reports so much in recent years that it has become a news topic in its own
right. At its core, it poses a chilling question -- what do we do if our
worldview is fundamentally wrong? Even if internally consistent, what if it
does not match the real world? Are our beliefs justified, or could we become
indoctrinated from living in a `bubble'? If the latter is true, how could we
even test the limits of said bubble from within its confines? We propose a new
method to augment the process of identifying fake news, by speeding up and
automating the more cumbersome and time-consuming tasks involved. Our
application, NewsCompare takes any list of target websites as input
(news-related in our use case, but otherwise not restricted), visits them in
parallel and retrieves any text content found within. Web pages are
subsequently compared to each other, and similarities are tentatively pointed
out. These results can be manually verified in order to determine which
websites tend to draw inspiration from one another. The data gathered on every
intermediate step can be queried and analyzed separately, and most notably we
already use the set of hyperlinks to and from the various websites we encounter
to paint a sort of `map' of that particular slice of the web. This map can then
be cross-referenced and further strengthen the conclusion that a particular
grouping of sites with strong links to each other, and posting similar content,
are likely to share the same allegiance. We run our application on the Romanian
news websites and we draw several interesting observations.",fake consumer
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake consumer
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake consumer
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake consumer
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake consumer
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake consumer
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake consumer
http://arxiv.org/abs/1007.2327v2,"BitTorrent is the most popular P2P content delivery application where
individual users share various type of content with tens of thousands of other
users. The growing popularity of BitTorrent is primarily due to the
availability of valuable content without any cost for the consumers. However,
apart from required resources, publishing (sharing) valuable (and often
copyrighted) content has serious legal implications for user who publish the
material (or publishers). This raises a question that whether (at least major)
content publishers behave in an altruistic fashion or have other incentives
such as financial. In this study, we identify the content publishers of more
than 55k torrents in 2 major BitTorrent portals and examine their behavior. We
demonstrate that a small fraction of publishers are responsible for 66% of
published content and 75% of the downloads. Our investigations reveal that
these major publishers respond to two different profiles. On one hand,
antipiracy agencies and malicious publishers publish a large amount of fake
files to protect copyrighted content and spread malware respectively. On the
other hand, content publishing in BitTorrent is largely driven by companies
with financial incentive. Therefore, if these companies lose their interest or
are unable to publish content, BitTorrent traffic/portals may disappear or at
least their associated traffic will significantly reduce.",fake consumer
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake consumer
http://arxiv.org/abs/1811.00661v2,"In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.",fake consumer
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake consumer
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",fake consumer detection
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",fake consumer detection
http://arxiv.org/abs/1903.12452v1,"The impact of online reviews on businesses has grown significantly during
last years, being crucial to determine business success in a wide array of
sectors, ranging from restaurants, hotels to e-commerce. Unfortunately, some
users use unethical means to improve their online reputation by writing fake
reviews of their businesses or competitors. Previous research has addressed
fake review detection in a number of domains, such as product or business
reviews in restaurants and hotels. However, in spite of its economical
interest, the domain of consumer electronics businesses has not yet been
thoroughly studied. This article proposes a feature framework for detecting
fake reviews that has been evaluated in the consumer electronics domain. The
contributions are fourfold: (i) Construction of a dataset for classifying fake
reviews in the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake review
detection; (iii) development of a fake review classification method based on
the proposed framework and (iv) evaluation and analysis of the results for each
of the cities under study. We have reached an 82% F-Score on the classification
task and the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.",fake consumer detection
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",fake consumer detection
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",fake consumer detection
http://arxiv.org/abs/1910.03496v2,"The evolution of the information and communication technologies has
dramatically increased the number of people with access to the Internet, which
has changed the way the information is consumed. As a consequence of the above,
fake news have become one of the major concerns because its potential to
destabilize governments, which makes them a potential danger to modern society.
An example of this can be found in the US. electoral campaign, where the term
""fake news"" gained great notoriety due to the influence of the hoaxes in the
final result of these. In this work the feasibility of applying deep learning
techniques to discriminate fake news on the Internet using only their text is
studied. In order to accomplish that, three different neural network
architectures are proposed, one of them based on BERT, a modern language model
created by Google which achieves state-of-the-art results.",fake consumer detection
http://arxiv.org/abs/1809.08754v3,"Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.",fake consumer detection
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",fake consumer detection
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",fake consumer detection
http://arxiv.org/abs/1806.04853v1,"Detecting fake users (also called Sybils) in online social networks is a
basic security research problem. State-of-the-art approaches rely on a large
amount of manually labeled users as a training set. These approaches suffer
from three key limitations: 1) it is time-consuming and costly to manually
label a large training set, 2) they cannot detect new Sybils in a timely
fashion, and 3) they are vulnerable to Sybil attacks that leverage information
of the training set. In this work, we propose SybilBlind, a structure-based
Sybil detection framework that does not rely on a manually labeled training
set. SybilBlind works under the same threat model as state-of-the-art
structure-based methods. We demonstrate the effectiveness of SybilBlind using
1) a social network with synthetic Sybils and 2) two Twitter datasets with real
Sybils. For instance, SybilBlind achieves an AUC of 0.98 on a Twitter dataset.",fake consumer detection
http://arxiv.org/abs/1609.02727v1,"Online reviews have increasingly become a very important resource for
consumers when making purchases. Though it is becoming more and more difficult
for people to make well-informed buying decisions without being deceived by
fake reviews. Prior works on the opinion spam problem mostly considered
classifying fake reviews using behavioral user patterns. They focused on
prolific users who write more than a couple of reviews, discarding one-time
reviewers. The number of singleton reviewers however is expected to be high for
many review websites. While behavioral patterns are effective when dealing with
elite users, for one-time reviewers, the review text needs to be exploited. In
this paper we tackle the problem of detecting fake reviews written by the same
person using multiple names, posting each review under a different name. We
propose two methods to detect similar reviews and show the results generally
outperform the vectorial similarity measures used in prior works. The first
method extends the semantic similarity between words to the reviews level. The
second method is based on topic modeling and exploits the similarity of the
reviews topic distributions using two models: bag-of-words and
bag-of-opinion-phrases. The experiments were conducted on reviews from three
different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset
(800 reviews).",fake consumer detection
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",fake consumer detection
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",fake consumer detection
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",fake consumer detection
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",fake consumer detection
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",fake consumer detection
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",fake consumer detection
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",fake consumer detection
http://arxiv.org/abs/1608.00684v1,"The publication of fake reviews by parties with vested interests has become a
severe problem for consumers who use online product reviews in their decision
making. To counter this problem a number of methods for detecting these fake
reviews, termed opinion spam, have been proposed. However, to date, many of
these methods focus on analysis of review text, making them unsuitable for many
review systems where accom-panying text is optional, or not possible. Moreover,
these approaches are often computationally expensive, requiring extensive
resources to handle text analysis over the scale of data typically involved.
  In this paper, we consider opinion spammers manipulation of average ratings
for products, focusing on dif-ferences between spammer ratings and the majority
opinion of honest reviewers. We propose a lightweight, effective method for
detecting opinion spammers based on these differences. This method uses
binomial regression to identify reviewers having an anomalous proportion of
ratings that deviate from the majority opinion. Experiments on real-world and
synthetic data show that our approach is able to successfully iden-tify opinion
spammers. Comparison with the current state-of-the-art approach, also based
only on ratings, shows that our method is able to achieve similar detection
accuracy while removing the need for assump-tions regarding probabilities of
spam and non-spam reviews and reducing the heavy computation required for
learning.",fake consumer detection
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",fake consumer detection
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",fake consumer detection
http://arxiv.org/abs/1611.06625v1,"Online reviews play a crucial role in helping consumers evaluate and compare
products and services. However, review hosting sites are often targeted by
opinion spamming. In recent years, many such sites have put a great deal of
effort in building effective review filtering systems to detect fake reviews
and to block malicious accounts. Thus, fraudsters or spammers now turn to
compromise, purchase or even raise reputable accounts to write fake reviews.
Based on the analysis of a real-life dataset from a review hosting site
(dianping.com), we discovered that reviewers' posting rates are bimodal and the
transitions between different states can be utilized to differentiate spammers
from genuine reviewers. Inspired by these findings, we propose a two-mode
Labeled Hidden Markov Model to detect spammers. Experimental results show that
our model significantly outperforms supervised learning using linguistic and
behavioral features in identifying spammers. Furthermore, we found that when a
product has a burst of reviews, many spammers are likely to be actively
involved in writing reviews to the product as well as to many other products.
We then propose a novel co-bursting network for detecting spammer groups. The
co-bursting network enables us to produce more accurate spammer groups than the
current state-of-the-art reviewer-product (co-reviewing) network.",fake consumer detection
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",fake consumer detection
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",fake consumer detection
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",fake consumer detection
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",fake consumer detection
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",fake consumer detection
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",fake consumer detection
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",fake consumer detection
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",fake consumer detection
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",fake consumer detection
http://arxiv.org/abs/1811.00656v3,"In this work, we describe a new deep learning based method that can
effectively distinguish AI-generated fake videos (referred to as {\em DeepFake}
videos hereafter) from real videos. Our method is based on the observations
that current DeepFake algorithm can only generate images of limited
resolutions, which need to be further warped to match the original faces in the
source video. Such transforms leave distinctive artifacts in the resulting
DeepFake videos, and we show that they can be effectively captured by
convolutional neural networks (CNNs). Compared to previous methods which use a
large amount of real and DeepFake generated images to train CNN classifier, our
method does not need DeepFake generated images as negative training examples
since we target the artifacts in affine face warping as the distinctive feature
to distinguish real and fake images. The advantages of our method are two-fold:
(1) Such artifacts can be simulated directly using simple image processing
operations on a image to make it as negative example. Since training a DeepFake
model to generate negative examples is time-consuming and resource-demanding,
our method saves a plenty of time and resources in training data collection;
(2) Since such artifacts are general existed in DeepFake videos from different
sources, our method is more robust compared to others. Our method is evaluated
on two sets of DeepFake video datasets for its effectiveness in practice.",fake consumer detection
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",fake consumer detection
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",fake consumer detection
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",fake consumer detection
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",fake consumer detection
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",fake consumer detection
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",fake consumer detection
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",fake consumer detection
http://arxiv.org/abs/1908.09805v1,"Automatic detection of fake news --- texts that are deceitful and misleading
--- is a long outstanding and largely unsolved problem. Worse yet, recent
developments in language modeling allow for the automatic generation of such
texts. One approach that has recently gained attention detects these fake news
using stylometry-based provenance, i.e. tracing a text's writing style back to
its producing source and determining whether the source is malicious. This was
shown to be highly effective under the assumption that legitimate text is
produced by humans, and fake text is produced by a language model.
  In this work, we identify a fundamental problem with provenance-based
approaches against attackers that auto-generate fake news: fake and legitimate
texts can originate from nearly identical sources. First, a legitimate text
might be auto-generated in a similar process to that of fake text, and second,
attackers can automatically corrupt articles originating from legitimate human
sources. We demonstrate these issues by simulating attacks in such settings,
and find that the provenance approach fails to defend against them. Our
findings highlight the importance of assessing the veracity of the text rather
than solely relying on its style or source. We also open up a discussion on the
types of benchmarks that should be used to evaluate neural fake news detectors.",fake consumer detection
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",Identity theft
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",Identity theft
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",Identity theft
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",Identity theft
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",Identity theft
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",Identity theft
http://arxiv.org/abs/1303.3764v3,"Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.",Identity theft
http://arxiv.org/abs/1304.6499v1,"Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.",Identity theft
http://arxiv.org/abs/1503.00454v1,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",Identity theft
http://arxiv.org/abs/0812.4181v1,"Web Services are web-based applications made available for web users or
remote Web-based programs. In order to promote interoperability, they publish
their interfaces in the so-called WSDL file and allow remote call over the
network. Although Web Services can be used in different ways, the industry
standard is the Service Oriented Architecture Web Services that doesn't rely on
the implementation details. In this architecture, communication is performed
through XML-based messages called SOAP messages. However, those messages are
prone to attacks that can lead to code injection, unauthorized accesses,
identity theft, etc. This type of attacks, called XML Rewriting Attacks, are
all based on unauthorized, yet possible, modifications of SOAP messages. We
present in this paper an explanation of this kind of attack, review the
existing solutions, and show their limitations. We also propose some ideas to
secure SOAP messages, as well as implementation ideas.",Identity theft
http://arxiv.org/abs/1511.03459v1,"Phishing is an online identity theft that aims to steal sensitive information
such as username, password and online banking details from its victims.
Phishing education needs to be considered as a means to combat this threat.
This paper reports on a design and development of a mobile game prototype as an
educational tool helping computer users to protect themselves against phishing
attacks. The elements of a game design framework for avoiding phishing attacks
were used to address the game design issues. Game design principles served as
guidelines for structuring and presenting information. Our mobile game design
aimed to enhance the users' avoidance behaviour through motivation to protect
themselves against phishing threats. A think-aloud study was conducted, along
with a pre- and post-test, to assess the game design framework though the
developed mobile game prototype. The study results showed a significant
improvement of participants' phishing avoidance behaviour in their post-test
assessment. Furthermore, the study findings suggest that participants' threat
perception, safeguard effectiveness, self-efficacy, perceived severity and
perceived susceptibility elements positively impact threat avoidance behaviour,
whereas safeguard cost had a negative impact on it.",Identity theft
http://arxiv.org/abs/1511.07093v1,"Phishing is an online identity theft, which aims to steal sensitive
information such as username, password and online banking details from victims.
To prevent this, phishing education needs to be considered. Game based
education is becoming more and more popular. This paper introduces a mobile
game prototype for the android platform based on a story, which simplifies and
exaggerates real life. The elements of a game design framework for avoiding
phishing attacks were used to address the game design issues and game design
principles were used as a set of guidelines for structuring and presenting
information. The overall mobile game design was aimed to enhance the user's
avoidance behaviour through motivation to protect themselves against phishing
threats. The prototype mobile game design was presented on MIT App Inventor
Emulator.",Identity theft
http://arxiv.org/abs/1407.7146v3,"The use of TLS proxies to intercept encrypted traffic is controversial since
the same mechanism can be used for both benevolent purposes, such as protecting
against malware, and for malicious purposes, such as identity theft or
warrantless government surveillance. To understand the prevalence and uses of
these proxies, we build a TLS proxy measurement tool and deploy it via Google
AdWords campaigns. We generate 15.2 million certificate tests across two
large-scale measurement studies. We find that 1 in 250 TLS connections are
TLS-proxied. The majority of these proxies appear to be benevolent, however we
identify over 3,600 cases where eight malware products are using this
technology nefariously. We also find numerous instances of negligent,
duplicitous, and suspicious behavior, some of which degrade security for users
without their knowledge. Distinguishing these types of practices is challenging
in practice, indicating a need for transparency and user awareness.",Identity theft
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",Identity theft
http://arxiv.org/abs/1602.03929v1,"This research aims to design an educational mobile game for home computer
users to prevent from phishing attacks. Phishing is an online identity theft
which aims to steal sensitive information such as username, password and online
banking details from victims. To prevent this, phishing education needs to be
considered. Mobile games could facilitate to embed learning in a natural
environment. The paper introduces a mobile game design based on a story which
is simplifying and exaggerating real life. We use a theoretical model derived
from Technology Threat Avoidance Theory (TTAT) to address the game design
issues and game design principles were used as a set of guidelines for
structuring and presenting information. The overall mobile game design was
aimed to enhance avoidance behaviour through motivation of home computer users
to protect against phishing threats. The prototype game design is presented on
Google App Inventor Emulator. We believe by training home computer users to
protect against phishing attacks, would be an aid to enable the cyberspace as a
secure environment.",Identity theft
http://arxiv.org/abs/1206.2597v1,"The beauty of Information Technology (IT) is with its multifunction nature;
it is a support system, a networking system, a storage system, as well as an
information facilitator. Aided with their broad line of services, an IT system
aims to support or even drive organizations towards desired paths. Trends of IT
and information security awareness (ISA) in society today, particularly within
the business environment is quite interesting phenomenon. The overviews of the
role of IT in the modern world as well as the perception towards ISA are
initially introduced. A series of scope are outlined, and also further
examination on matter of IT and ISA in the business environment-emphasis on
revolution of business with ISA, security threats such as identity thefts,
hacking and web harassment, and the different mode of protections that are
applied in different business environments. Unfortunately, the advancement of
IT is not followed by the awareness of its security issues properly, especially
in the context of the business settings and functions. This research and review
is expected to influence the awareness of information security issues in
business processes.",Identity theft
http://arxiv.org/abs/1510.04921v4,"This paper reports the results of a survey of 1,976 individuals regarding
their opinions on TLS inspection, a controversial technique that can be used
for both benevolent and malicious purposes. Responses indicate that
participants hold nuanced opinions on security and privacy trade-offs, with
most recognizing legitimate uses for the practice, but also concerned about
threats from hackers or government surveillance. There is strong support for
notification and consent when a system is intercepting their encrypted traffic,
although this support varies depending on the situation. A significant concern
about malicious uses of TLS inspection is identity theft, and many would react
negatively and some would change their behavior if they discovered inspection
occurring without their knowledge. We also find that there are a small but
significant number of participants who are jaded by the current state of
affairs and have no expectation of privacy.",Identity theft
http://arxiv.org/abs/1605.05847v1,"Personal data is collected and stored more than ever by the governments and
companies in the digital age. Even though the data is only released after
anonymization, deanonymization is possible by joining different datasets. This
puts the privacy of individuals in jeopardy. Furthermore, data leaks can unveil
personal identifiers of individuals when security is breached. Processing the
leaked dataset can provide even more information than what is visible to naked
eye. In this work, we report the results of our analyses on the recent ""Turkish
citizen database leak"", which revealed the national identifier numbers of close
to fifty million voters, along with personal information such as date of birth,
birth place, and full address. We show that with automated processing of the
data, one can uniquely identify (i) mother's maiden name of individuals and
(ii) landline numbers, for a significant portion of people. This is a serious
privacy and security threat because (i) identity theft risk is now higher, and
(ii) scammers are able to access more information about individuals. The only
and utmost goal of this work is to point out to the security risks and suggest
stricter measures to related companies and agencies to protect the security and
privacy of individuals.",Identity theft
http://arxiv.org/abs/1610.08369v1,"The explosion in the e-commerce industry which has been necessitated by the
growth and advance expansion of Information technology and its related
facilities in recent years have been met with adverse security issues
consequently affecting the industry and the entire online activities. This
paper exams the prevailing security threats e-commerce is facing which is
predominantly known as cyber-crime and how computer related technology and
facilities such as digital forensics tools can be adopted extensively to ensure
security in online related business activities. This paper investigated the
risk, damage and the cost cyber-crime poses to individuals and organizations
when they become victims. As it is obvious transacting business online as well
as all related online activities has become inherent in our everyday life. The
paper also comprehensively elucidate on some of the cyber-crime activities that
are posing serious threat to the security of E-commerce. Amazon and eBay were
used as the case of study in relation to respondents who patronizes these
renowned e-commerce sites for various transactions.
  Keywords: E-commerce Security,Cyber-Crime,digital forensics, Network
forensics, Network security, Online transactions, Identity theft, hacking.",Identity theft
http://arxiv.org/abs/1610.09511v1,"Phishing is an online identity theft that aims to steal sensitive information
such as username, passwords and online banking details from its victims.
Phishing education needs to be considered as a means to combat this threat.
This book focuses on a design and development of a mobile game prototype as an
educational tool helping computer users to protect themselves against phishing
attacks. The elements of a game design framework for avoiding phishing attacks
were used to address the game design issues. The mobile game design aimed to
enhance the user's avoidance behaviour through motivation to protect themselves
against phishing threats. A think-aloud study was conducted, along with a pre-
and post-test, to assess the game design framework through the developed mobile
game prototype. The study results showed a significant improvement of
participants' phishing avoidance behaviour in their post-test assessment.
Furthermore, the study findings suggest that participants' threat perception,
safeguard effectiveness, self-efficacy, perceived severity and perceived
susceptibility elements positively impact threat avoidance behaviour, whereas
safeguard cost had a negative impact on it.",Identity theft
http://arxiv.org/abs/1705.09936v1,"As applications of biometric verification proliferate, users become more
vulnerable to privacy infringement. Biometric data is very privacy sensitive as
it may contain information as gender, ethnicity and health conditions which
should not be shared with third parties during the verification process.
Moreover, biometric data that has fallen into the wrong hands often leads to
identity theft. Secure biometric verification schemes try to overcome such
privacy threats. Unfortunately, existing secure solutions either introduce a
heavy computational or communication overhead or have to accept a high loss in
accuracy; both of which make them impractical in real-world settings. This
paper presents a novel approach to secure biometric verification aiming at a
practical trade-off between efficiency and accuracy, while guaranteeing full
security against honest-but-curious adversaries. The system performs
verification in the encrypted domain using elliptic curve based homomorphic
ElGamal encryption for high efficiency. Classification is based on a
log-likelihood ratio classifier which has proven to be very accurate. No
private information is leaked during the verification process using a two-party
secure protocol. Initial tests show highly accurate results that have been
computed within milliseconds range.",Identity theft
http://arxiv.org/abs/1802.04159v1,"The Hypertext Transfer Protocol Secure (HTTPS) communications protocol is
used to secure traffic between a web browser and server. This technology can
significantly reduce the risk of interception and manipulation of web
information for nefarious purposes such as identity theft. Deployment of HTTPS
has reached about 50% of all webs sites. Little is known about HTTPS
implantation for hospital websites. To investigate the prevalence of HTTPS
implementation, we analyzed the websites of the 210 public hospitals in the
state of Illinois, USA. HTTPS was implemented to industry standards for 54% of
all hospital websites in Illinois. Geographical analysis showed an urban vs.
rural digital divide with 60% of urban hospitals and 40% of rural hospitals
implementing HTTPS.",Identity theft
http://arxiv.org/abs/1811.00925v1,"Botnets (networks of compromised computers) are often used for malicious
activities such as spam, click fraud, identity theft, phishing, and distributed
denial of service (DDoS) attacks. Most of previous researches have introduced
fully or partially signature-based botnet detection approaches. In this paper,
we propose a fully anomaly-based approach that requires no a priori knowledge
of bot signatures, botnet C&C protocols, and C&C server addresses. We start
from inherent characteristics of botnets. Bots connect to the C&C channel and
execute the received commands. Bots belonging to the same botnet receive the
same commands that causes them having similar netflows characteristics and
performing same attacks. Our method clusters bots with similar netflows and
attacks in different time windows and perform correlation to identify bot
infected hosts. We have developed a prototype system and evaluated it with
real-world traces including normal traffic and several real-world botnet
traces. The results show that our approach has high detection accuracy and low
false positive.",Identity theft
http://arxiv.org/abs/1208.2321v1,"Global energy crises are increasing every moment. Every one has the attention
towards more and more energy production and also trying to save it. Electricity
can be produced through many ways which is then synchronized on a main grid for
usage. The main issue for which we have written this survey paper is losses in
electrical system. Weather these losses are technical or non-technical.
Technical losses can be calculated easily, as we discussed in section of
mathematical modeling that how to calculate technical losses. Where as
nontechnical losses can be evaluated if technical losses are known. Theft in
electricity produce non-technical losses. To reduce or control theft one can
save his economic resources. Smart meter can be the best option to minimize
electricity theft, because of its high security, best efficiency, and excellent
resistance towards many of theft ideas in electromechanical meters. So in this
paper we have mostly concentrated on theft issues.",Identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft  detection
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft  detection
http://arxiv.org/abs/1902.03296v1,"Energy theft constitutes an issue of great importance for electricity
operators. The attempt to detect and reduce non-technical losses is a
challenging task due to insufficient inspection methods. With the evolution of
advanced metering infrastructure (AMI) in smart grids, a more complicated
status quo in energy theft has emerged and many new technologies are being
adopted to solve the problem. In order to identify illegal residential
consumers, a computational method of analyzing and identifying electricity
consumption patterns of consumers based on data mining techniques has been
presented. Combining principal component analysis (PCA) with mean shift
algorithm for different power theft scenarios, we can now cope with the power
theft detection problem sufficiently. The overall research has shown
encouraging results in residential consumers power theft detection that will
help utilities to improve the reliability, security and operation of power
network.",Identity theft  detection
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft  detection
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft  detection
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",Identity theft  detection
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft  detection
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",Identity theft  detection
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",Identity theft  detection
http://arxiv.org/abs/1811.00925v1,"Botnets (networks of compromised computers) are often used for malicious
activities such as spam, click fraud, identity theft, phishing, and distributed
denial of service (DDoS) attacks. Most of previous researches have introduced
fully or partially signature-based botnet detection approaches. In this paper,
we propose a fully anomaly-based approach that requires no a priori knowledge
of bot signatures, botnet C&C protocols, and C&C server addresses. We start
from inherent characteristics of botnets. Bots connect to the C&C channel and
execute the received commands. Bots belonging to the same botnet receive the
same commands that causes them having similar netflows characteristics and
performing same attacks. Our method clusters bots with similar netflows and
attacks in different time windows and perform correlation to identify bot
infected hosts. We have developed a prototype system and evaluated it with
real-world traces including normal traffic and several real-world botnet
traces. The results show that our approach has high detection accuracy and low
false positive.",Identity theft  detection
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft  detection
http://arxiv.org/abs/1805.09591v1,"Electricity theft detection issue has drawn lots of attention during last
decades. Timely identification of the electricity theft in the power system is
crucial for the safety and availability of the system. Although sustainable
efforts have been made, the detection task remains challenging and falls short
of accuracy and efficiency, especially with the increase of the data size.
Recently, convolutional neural network-based methods have achieved better
performance in comparison with traditional methods, which employ handcrafted
features and shallow-architecture classifiers. In this paper, we present a
novel approach for automatic detection by using a multi-scale dense connected
convolution neural network (multi-scale DenseNet) in order to capture the
long-term and short-term periodic features within the sequential data. We
compare the proposed approaches with the classical algorithms, and the
experimental results demonstrate that the multiscale DenseNet approach can
significantly improve the accuracy of the detection. Moreover, our method is
scalable, enabling larger data processing while no handcrafted feature
engineering is needed.",Identity theft  detection
http://arxiv.org/abs/1708.05907v1,"Non-technical losses (NTL) in electric power grids arise through electricity
theft, broken electric meters or billing errors. They can harm the power
supplier as well as the whole economy of a country through losses of up to 40%
of the total power distribution. For NTL detection, researchers use artificial
intelligence to analyse data. This work is about improving the extraction of
more meaningful features from a data set. With these features, the prediction
quality will increase.",Identity theft  detection
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft  detection
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft  detection
http://arxiv.org/abs/1811.09024v1,"Phishing attacks are prevalent and humans are central to this online identity
theft attack, which aims to steal victims' sensitive and personal information
such as username, password, and online banking details. There are many
anti-phishing tools developed to thwart against phishing attacks. Since humans
are the weakest link in phishing, it is important to educate them to detect and
avoid phishing attacks. One can argue self-efficacy is one of the most
important determinants of individual's motivation in phishing threat avoidance
behavior, which has co-relation with knowledge. The proposed research endeavors
on the user's self-efficacy in order to enhance the individual's phishing
threat avoidance behavior through their motivation. Using social cognitive
theory, we explored that various knowledge attributes such as observational
(vicarious) knowledge, heuristic knowledge and structural knowledge contributes
immensely towards the individual's self-efficacy to enhance phishing threat
prevention behavior. A theoretical framework is then developed depicting the
mechanism that links knowledge attributes, self-efficacy, threat avoidance
motivation that leads to users' threat avoidance behavior. Finally, a gaming
prototype is designed incooperating the knowledge elements identified in this
research that aimed to enhance individual's self-efficacy in phishing threat
avoidance behavior.",Identity theft  detection
http://arxiv.org/abs/1703.03378v1,"The widespread use of smartphones gives rise to new security and privacy
concerns. Smartphone thefts account for the largest percentage of thefts in
recent crime statistics. Using a victim's smartphone, the attacker can launch
impersonation attacks, which threaten the security of the victim and other
users in the network. Our threat model includes the attacker taking over the
phone after the user has logged on with his password or pin. Our goal is to
design a mechanism for smartphones to better authenticate the current user,
continuously and implicitly, and raise alerts when necessary. In this paper, we
propose a multi-sensors-based system to achieve continuous and implicit
authentication for smartphone users. The system continuously learns the owner's
behavior patterns and environment characteristics, and then authenticates the
current user without interrupting user-smartphone interactions. Our method can
adaptively update a user's model considering the temporal change of user's
patterns. Experimental results show that our method is efficient, requiring
less than 10 seconds to train the model and 20 seconds to detect the abnormal
user, while achieving high accuracy (more than 90%). Also the combination of
more sensors provide better accuracy. Furthermore, our method enables adjusting
the security level by changing the sampling rate.",Identity theft  detection
http://arxiv.org/abs/1705.00242v1,"As item trading becomes more popular, users can change their game items or
money into real money more easily. At the same time, hackers turn their eyes on
stealing other users game items or money because it is much easier to earn
money than traditional gold-farming by running game bots. Game companies
provide various security measures to block account- theft attempts, but many
security measures on the user-side are disregarded by users because of lack of
usability. In this study, we propose a server-side account theft detection
system base on action sequence analysis to protect game users from malicious
hackers. We tested this system in the real Massively Multiplayer Online Role
Playing Game (MMORPG). By analyzing users full game play log, our system can
find the particular action sequences of hackers with high accuracy. Also, we
can trace where the victim accounts stolen money goes.",Identity theft  detection
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft  detection
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft  detection
http://arxiv.org/abs/1210.7678v1,"Today, Plagiarism has become a menace. Every journal editor or conference
organizers has to deal with this problem. Simply Copying or rephrasing of text
without giving due credit to the original author has become more common. This
is considered to be an Intellectual Property Theft. We are developing a
Plagiarism Detection Tool which would deal with this problem. In this paper we
discuss the common tools available to detect plagiarism and their short comings
and the advantages of our tool over these tools.",Identity theft  detection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft  detection
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft  detection
http://arxiv.org/abs/1904.04895v1,"A method for detecting electronic data theft from computer networks is
described, capable of recognizing patterns of remote exfiltration occurring
over days to weeks. Normal traffic flow data, in the form of a host's ingress
and egress bytes over time, is used to train an ensemble of one-class learners.
The detection ensemble is modular, with individual classifiers trained on
different traffic features thought to characterize malicious data transfers. We
select features that model the egress to ingress byte balance over time,
periodicity, short time-scale irregularity, and density of the traffic. The
features are most efficiently modeled in the frequency domain, which has the
added benefit that variable duration flows are transformed to a fixed-size
feature vector, and by sampling the frequency space appropriately,
long-duration flows can be tested. When trained on days- or weeks-worth of
traffic from individual hosts, our ensemble achieves a low false positive rate
(<2%) on a range of different system types. Simulated exfiltration samples with
a variety of different timing and data characteristics were generated and used
to test ensemble performance on different kinds of systems: when trained on a
client workstation's external traffic, the ensemble was generally successful at
detecting exfiltration that is not simultaneously ingress-heavy,
connection-sparse, and of short duration---a combination that is not optimal
for attackers seeking to transfer large amounts of data. Remote exfiltration is
more difficult to detect from egress-heavy systems, like web servers, with
normal traffic exhibiting timing characteristics similar to a wide range of
exfiltration types.",Identity theft  detection
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft  detection
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft  detection
http://arxiv.org/abs/1208.3205v1,"The main stretch in the paper is buffer overflow anomaly occurring in major
source codes, designed in various programming language. It describes the
various as to how to improve your code and increase its strength to withstand
security theft occurring at vulnerable areas in the code. The main language
used is JAVA, regarded as one of the most object oriented language still create
lot of error like stack overflow, illegal/inappropriate method overriding. I
used tools confined to JAVA to test as how weak points in the code can be
rectified before compiled. The byte code theft is difficult to be conquered, so
it's a better to get rid of it in the plain java code itself. The tools used in
the research are PMD(Programming mistake detector), it helps to detect line of
code that make pop out error in near future like defect in hashcode(memory
maps) overriding due to which the java code will not function correctly.
Another tool is FindBUGS which provide the tester of the code to analyze the
weak points in the code like infinite loop, unsynchronized wait, deadlock
situation, null referring and dereferencing. Another tool which provides the
base to above tools is JaCoCo code coverage analysis used to detect unreachable
part and unused conditions of the code which improves the space complexity and
helps in easy clarification of errors.
  Through this paper, we design an algorithm to prevent the loss of data. The
main audience is the white box tester who might leave out essential line of
code like, index variables, infinite loop, and inappropriate hashcode in the
major source program. This algorithm serves to reduce the damage in case of
buffer overflow",Identity theft  detection
http://arxiv.org/abs/1607.00872v2,"Electricity theft is a major problem around the world in both developed and
developing countries and may range up to 40% of the total electricity
distributed. More generally, electricity theft belongs to non-technical losses
(NTL), which are losses that occur during the distribution of electricity in
power grids. In this paper, we build features from the neighborhood of
customers. We first split the area in which the customers are located into
grids of different sizes. For each grid cell we then compute the proportion of
inspected customers and the proportion of NTL found among the inspected
customers. We then analyze the distributions of features generated and show why
they are useful to predict NTL. In addition, we compute features from the
consumption time series of customers. We also use master data features of
customers, such as their customer class and voltage of their connection. We
compute these features for a Big Data base of 31M meter readings, 700K
customers and 400K inspection results. We then use these features to train four
machine learning algorithms that are particularly suitable for Big Data sets
because of their parallelizable structure: logistic regression, k-nearest
neighbors, linear support vector machine and random forest. Using the
neighborhood features instead of only analyzing the time series has resulted in
appreciable results for Big Data sets for varying NTL proportions of 1%-90%.
This work can therefore be deployed to a wide range of different regions around
the world.",Identity theft  detection
http://arxiv.org/abs/1712.01397v1,"As an initial assessment, over 480,000 labeled virtual images of normal
highway driving were readily generated in Grand Theft Auto V's virtual
environment. Using these images, a CNN was trained to detect following distance
to cars/objects ahead, lane markings, and driving angle (angular heading
relative to lane centerline): all variables necessary for basic autonomous
driving. Encouraging results were obtained when tested on over 50,000 labeled
virtual images from substantially different GTA-V driving environments. This
initial assessment begins to define both the range and scope of the labeled
images needed for training as well as the range and scope of labeled images
needed for testing the definition of boundaries and limitations of trained
networks. It is the efficacy and flexibility of a ""GTA-V""-like virtual
environment that is expected to provide an efficient well-defined foundation
for the training and testing of Convolutional Neural Networks for safe driving.
Additionally, described is the Princeton Virtual Environment (PVE) for the
training, testing and enhancement of safe driving AI, which is being developed
using the video-game engine Unity. PVE is being developed to recreate rare but
critical corner cases that can be used in re-training and enhancing machine
learning models and understanding the limitations of current self driving
models. The Florida Tesla crash is being used as an initial reference.",Identity theft  detection
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",Identity theft  detection
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",Identity theft  detection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",Identity theft  detection
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",Identity theft  detection
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",Identity theft  detection
http://arxiv.org/abs/1303.3764v3,"Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.",Identity theft  detection
http://arxiv.org/abs/1304.6499v1,"Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.",Identity theft  detection
http://arxiv.org/abs/1503.00454v1,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",Identity theft  detection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",Identity theft monitor
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",Identity theft monitor
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",Identity theft monitor
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",Identity theft monitor
http://arxiv.org/abs/1209.5982v1,"As smartphones become more pervasive, they are increasingly targeted by
malware. At the same time, each new generation of smartphone features
increasingly powerful onboard sensor suites. A new strain of sensor malware has
been developing that leverages these sensors to steal information from the
physical environment (e.g., researchers have recently demonstrated how malware
can listen for spoken credit card numbers through the microphone, or feel
keystroke vibrations using the accelerometer). Yet the possibilities of what
malware can see through a camera have been understudied. This paper introduces
a novel visual malware called PlaceRaider, which allows remote attackers to
engage in remote reconnaissance and what we call virtual theft. Through
completely opportunistic use of the camera on the phone and other sensors,
PlaceRaider constructs rich, three dimensional models of indoor environments.
Remote burglars can thus download the physical space, study the environment
carefully, and steal virtual objects from the environment (such as financial
documents, information on computer monitors, and personally identifiable
information). Through two human subject studies we demonstrate the
effectiveness of using mobile devices as powerful surveillance and virtual
theft platforms, and we suggest several possible defenses against visual
malware.",Identity theft monitor
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",Identity theft monitor
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",Identity theft monitor
http://arxiv.org/abs/1307.3147v2,"The wide spread of mobiles as handheld devices leads to various innovative
applications that makes use of their ever increasing presence in our daily
life. One such application is location tracking and monitoring. This paper
proposes a prototype model for location tracking using Geographical Positioning
System (GPS) and Global System for Mobile Communication (GSM) technology. The
system displays the object moving path on the monitor and the same information
can also be communicated to the user cell phone, on demand of the user by
asking the specific information via SMS. This system is very useful for car
theft situations, for adolescent drivers being watched and monitored by
parents. The result shows that the object is being tracked with a minimal
tracking error.",Identity theft monitor
http://arxiv.org/abs/1904.11882v1,"In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.",Identity theft monitor
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",Identity theft monitor
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",Identity theft monitor
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",Identity theft monitor
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",Identity theft monitor
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",Identity theft monitor
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",Identity theft monitor
http://arxiv.org/abs/1410.0519v1,"By increase of culture and knowledge of the people, request for visiting
museums has increased and made the management of these places more complex.
Valuable things in a museum or ancient place must be maintained well and also
it need to managing visitors. To maintain things we should prevent them from
theft, as well as environmental factors such as temperature, humidity, PH,
chemical factors and mechanical events should be monitored. And if the
conditions are damaging, appropriate alerts or reports to managers and experts
should be announced. Visitors should also be monitored, as well as visitors
need to be guided and getting information in the environment. By utilizing RFID
technology and short-distance network tools, technical solutions for more
efficient management and more effective retention in museums can be
implemented.",Identity theft monitor
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",Identity theft monitor
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",Identity theft monitor
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",Identity theft monitor
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",Identity theft monitor
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",Identity theft monitor
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",Identity theft monitor
http://arxiv.org/abs/1708.04278v1,"Data leakage and theft from databases is a dangerous threat to organizations.
Data Security and Data Privacy protection systems (DSDP) monitor data access
and usage to identify leakage or suspicious activities that should be
investigated. Because of the high velocity nature of database systems, such
systems audit only a portion of the vast number of transactions that take
place. Anomalies are investigated by a Security Officer (SO) in order to choose
the proper response. In this paper we investigate the effect of sampling
methods based on the risk the transaction poses and propose a new method for
""combined sampling"" for capturing a more varied sample.",Identity theft monitor
http://arxiv.org/abs/1908.10229v1,"Digital healthcare systems are very popular lately, as they provide a variety
of helpful means to monitor people's health state as well as to protect people
against an unexpected health situation. These systems contain a huge amount of
personal information in a form of electronic health records that are not
allowed to be disclosed to unauthorized users. Hence, health data and
information need to be protected against attacks and thefts. In this paper, we
propose a secure distributed architecture for healthcare data storage and
analysis. It uses a novel security model to rigorously control permissions of
accessing sensitive data in the system, as well as to protect the transmitted
data between distributed system servers and nodes. The model also satisfies the
NIST security requirements. Thorough experimental results show that the model
is very promising.",Identity theft monitor
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",Identity theft monitor
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",Identity theft monitor
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",Identity theft monitor
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",Identity theft monitor
http://arxiv.org/abs/1303.3764v3,"Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.",Identity theft monitor
http://arxiv.org/abs/1304.6499v1,"Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.",Identity theft monitor
http://arxiv.org/abs/1503.00454v1,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",Identity theft monitor
http://arxiv.org/abs/0812.4181v1,"Web Services are web-based applications made available for web users or
remote Web-based programs. In order to promote interoperability, they publish
their interfaces in the so-called WSDL file and allow remote call over the
network. Although Web Services can be used in different ways, the industry
standard is the Service Oriented Architecture Web Services that doesn't rely on
the implementation details. In this architecture, communication is performed
through XML-based messages called SOAP messages. However, those messages are
prone to attacks that can lead to code injection, unauthorized accesses,
identity theft, etc. This type of attacks, called XML Rewriting Attacks, are
all based on unauthorized, yet possible, modifications of SOAP messages. We
present in this paper an explanation of this kind of attack, review the
existing solutions, and show their limitations. We also propose some ideas to
secure SOAP messages, as well as implementation ideas.",Identity theft monitor
http://arxiv.org/abs/1511.03459v1,"Phishing is an online identity theft that aims to steal sensitive information
such as username, password and online banking details from its victims.
Phishing education needs to be considered as a means to combat this threat.
This paper reports on a design and development of a mobile game prototype as an
educational tool helping computer users to protect themselves against phishing
attacks. The elements of a game design framework for avoiding phishing attacks
were used to address the game design issues. Game design principles served as
guidelines for structuring and presenting information. Our mobile game design
aimed to enhance the users' avoidance behaviour through motivation to protect
themselves against phishing threats. A think-aloud study was conducted, along
with a pre- and post-test, to assess the game design framework though the
developed mobile game prototype. The study results showed a significant
improvement of participants' phishing avoidance behaviour in their post-test
assessment. Furthermore, the study findings suggest that participants' threat
perception, safeguard effectiveness, self-efficacy, perceived severity and
perceived susceptibility elements positively impact threat avoidance behaviour,
whereas safeguard cost had a negative impact on it.",Identity theft monitor
http://arxiv.org/abs/1511.07093v1,"Phishing is an online identity theft, which aims to steal sensitive
information such as username, password and online banking details from victims.
To prevent this, phishing education needs to be considered. Game based
education is becoming more and more popular. This paper introduces a mobile
game prototype for the android platform based on a story, which simplifies and
exaggerates real life. The elements of a game design framework for avoiding
phishing attacks were used to address the game design issues and game design
principles were used as a set of guidelines for structuring and presenting
information. The overall mobile game design was aimed to enhance the user's
avoidance behaviour through motivation to protect themselves against phishing
threats. The prototype mobile game design was presented on MIT App Inventor
Emulator.",Identity theft monitor
http://arxiv.org/abs/1407.7146v3,"The use of TLS proxies to intercept encrypted traffic is controversial since
the same mechanism can be used for both benevolent purposes, such as protecting
against malware, and for malicious purposes, such as identity theft or
warrantless government surveillance. To understand the prevalence and uses of
these proxies, we build a TLS proxy measurement tool and deploy it via Google
AdWords campaigns. We generate 15.2 million certificate tests across two
large-scale measurement studies. We find that 1 in 250 TLS connections are
TLS-proxied. The majority of these proxies appear to be benevolent, however we
identify over 3,600 cases where eight malware products are using this
technology nefariously. We also find numerous instances of negligent,
duplicitous, and suspicious behavior, some of which degrade security for users
without their knowledge. Distinguishing these types of practices is challenging
in practice, indicating a need for transparency and user awareness.",Identity theft monitor
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",Identity theft monitor
http://arxiv.org/abs/1602.03929v1,"This research aims to design an educational mobile game for home computer
users to prevent from phishing attacks. Phishing is an online identity theft
which aims to steal sensitive information such as username, password and online
banking details from victims. To prevent this, phishing education needs to be
considered. Mobile games could facilitate to embed learning in a natural
environment. The paper introduces a mobile game design based on a story which
is simplifying and exaggerating real life. We use a theoretical model derived
from Technology Threat Avoidance Theory (TTAT) to address the game design
issues and game design principles were used as a set of guidelines for
structuring and presenting information. The overall mobile game design was
aimed to enhance avoidance behaviour through motivation of home computer users
to protect against phishing threats. The prototype game design is presented on
Google App Inventor Emulator. We believe by training home computer users to
protect against phishing attacks, would be an aid to enable the cyberspace as a
secure environment.",Identity theft monitor
http://arxiv.org/abs/1811.02293v1,"3GPP Release 15, the first 5G standard, includes protection of user identity
privacy against IMSI catchers. These protection mechanisms are based on public
key encryption. Despite this protection, IMSI catching is still possible in LTE
networks which opens the possibility of a downgrade attack on user identity
privacy, where a fake LTE base station obtains the identity of a 5G user
equipment. We propose (i) to use an existing pseudonym-based solution to
protect user identity privacy of 5G user equipment against IMSI catchers in LTE
and (ii) to include a mechanism for updating LTE pseudonyms in the public key
encryption based 5G identity privacy procedure. The latter helps to recover
from a loss of synchronization of LTE pseudonyms. Using this mechanism,
pseudonyms in the user equipment and home network are automatically
synchronized when the user equipment connects to 5G. Our mechanisms utilize
existing LTE and 3GPP Release 15 messages and require modifications only in the
user equipment and home network in order to provide identity privacy.
Additionally, lawful interception requires minor patching in the serving
network.",identity protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",identity protection
http://arxiv.org/abs/1806.05943v1,"Anonymous Identity-Based Encryption can protect privacy of the receiver.
However, there are some situations that we need to recover the identity of the
receiver, for example a dispute occurs or the privacy mechanism is abused. In
this paper, we propose a new concept, referred to as Anonymous Identity-Based
Encryption with Identity Recovery(AIBEIR), which is an anonymous IBE with
identity recovery property. There is a party called the Identity Recovery
Manager(IRM) who has a secret key to recover the identity from the ciphertext
in our scheme. We construct it with an anonymous IBE and a special IBE which we
call it testable IBE. In order to ensure the semantic security in the case
where the identity recovery manager is an adversary, we define a stronger
semantic security model in which the adversary is given the secret key of the
identity recovery manager. To our knowledge, we propose the first AIBEIR scheme
and prove the security in our defined model.",identity protection
http://arxiv.org/abs/1710.03317v1,"Research use of sensitive information -- personally identifiable information
(PII), protected health information (PHI), commercial or proprietary data, and
the like -- is increasing as researchers' skill with ""big data"" matures. Duke
University's Protected Network is an environment with technical controls in
place that provide research groups with essential pieces of security measures
needed for studies using sensitive information. The environment uses
virtualization and authorization groups extensively to isolate data, provide
elasticity of resources, and flexibly meet a range of computational
requirements within tightly controlled network boundaries. Since its beginning
in 2011, the environment has supported about 200 research projects and groups
and has served as a foundation for specialized and protected IT infrastructures
in the social sciences, population studies, and medical research. This article
lays out key features of the development of the Protected Network and outlines
the IT infrastructure design and organizational features that Duke has used in
establishing this resource for researchers. It consists of four sections: 1.
Context, 2. Infrastructure, 3. Authentication and identity management, and 4.
The infrastructure as a ""platform.""",identity protection
http://arxiv.org/abs/1208.3192v1,"One of the most important issues in peer-to-peer networks is anonymity. The
major anonymity for peer-to-peer users concerned with the users' identities and
actions which can be revealed by any other members. There are many approaches
proposed to provide anonymous peer-to-peer communications. An intruder can get
information about the content of the data, the sender's and receiver's
identities. Anonymous approaches are designed with the following three goals:
to protect the identity of provider, to protect the identity of requester and
to protect the contents of transferred data between them. This article presents
a new peer-to-peer approach to achieve anonymity between a requester and a
provider in peer-to-peer networks with trusted servers called suppernode so
that the provider will not be able to identify the requester and no other peers
can identify the two communicating parties with certainty. This article shows
that the proposed algorithm improved reliability and has more security. This
algorithm, based on onion routing and randomization, protects transferring data
against traffic analysis attack. The ultimate goal of this anonymous
communications algorithm is to allow a requester to communicate with a provider
in such a manner that nobody can determine the requester's identity and the
content of transferred data.",identity protection
http://arxiv.org/abs/1701.00436v1,"Privacy has become a serious concern for modern Information Societies. The
sensitive nature of much of the data that are daily exchanged or released to
untrusted parties requires that responsible organizations undertake appropriate
privacy protection measures. Nowadays, much of these data are texts (e.g.,
emails, messages posted in social media, healthcare outcomes, etc.) that,
because of their unstructured and semantic nature, constitute a challenge for
automatic data protection methods. In fact, textual documents are usually
protected manually, in a process known as document redaction or sanitization.
To do so, human experts identify sensitive terms (i.e., terms that may reveal
identities and/or confidential information) and protect them accordingly (e.g.,
via removal or, preferably, generalization). To relieve experts from this
burdensome task, in a previous work we introduced the theoretical basis of
C-sanitization, an inherently semantic privacy model that provides the basis to
the development of automatic document redaction/sanitization algorithms and
offers clear and a priori privacy guarantees on data protection; even though
its potential benefits C-sanitization still presents some limitations when
applied to practice (mainly regarding flexibility, efficiency and accuracy). In
this paper, we propose a new more flexible model, named (C, g(C))-sanitization,
which enables an intuitive configuration of the trade-off between the desired
level of protection (i.e., controlled information disclosure) and the
preservation of the utility of the protected data (i.e., amount of semantics to
be preserved). Moreover, we also present a set of technical solutions and
algorithms that provide an efficient and scalable implementation of the model
and improve its practical accuracy, as we also illustrate through empirical
experiments.",identity protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",identity protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",identity protection
http://arxiv.org/abs/1312.7511v1,"In identity management system, frequently used biometric recognition system
needs awareness towards issue of protecting biometric template as far as more
reliable solution is apprehensive. In sight of this biometric template
protection algorithm should gratify the basic requirements viz. security,
discriminability and cancelability. As no single template protection method is
capable of satisfying these requirements, a novel scheme for face template
generation and protection is proposed. The novel scheme is proposed to provide
security and accuracy in new user enrolment and authentication process. This
novel scheme takes advantage of both the hybrid approach and the binary
discriminant analysis algorithm. This algorithm is designed on the basis of
random projection, binary discriminant analysis and fuzzy commitment scheme.
Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other
datasets are used for evaluation. The proposed novel scheme enhances the
discriminability and recognition accuracy in terms of matching score of the
face images for each stage and provides high security against potential attacks
namely brute force and smart attacks. In this paper, we discuss results viz.
averages matching score, computation time and security for hybrid approach and
novel approach.",identity protection
http://arxiv.org/abs/1401.0092v1,"In identity management system, commonly used biometric recognition system
needs attention towards issue of biometric template protection as far as more
reliable solution is concerned. In view of this biometric template protection
algorithm should satisfy security, discriminability and cancelability. As no
single template protection method is capable of satisfying the basic
requirements, a novel technique for face template generation and protection is
proposed. The novel approach is proposed to provide security and accuracy in
new user enrollment as well as authentication process. This novel technique
takes advantage of both the hybrid approach and the binary discriminant
analysis algorithm. This algorithm is designed on the basis of random
projection, binary discriminant analysis and fuzzy commitment scheme. Three
publicly available benchmark face databases are used for evaluation. The
proposed novel technique enhances the discriminability and recognition accuracy
by 80% in terms of matching score of the face images and provides high
security.",identity protection
http://arxiv.org/abs/cs/0701144v1,"Trusted Computing is a security base technology that will perhaps be
ubiquitous in a few years in personal computers and mobile devices alike.
Despite its neutrality with respect to applications, it has raised some privacy
concerns. We show that trusted computing can be applied for service access
control in a manner protecting users' privacy. We construct a ticket system --
a concept which is at the heart of Identity Management -- relying solely on the
capabilities of the trusted platform module and the standards specified by the
Trusted Computing Group. Two examples show how it can be used for pseudonymous
and protected service access.",identity protection
http://arxiv.org/abs/1506.00950v1,"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange system has been
introduced as a simple, very low cost and efficient classical physical
alternative to quantum key distribution systems. The ideal system uses only a
few electronic components - identical resistor pairs, switches and
interconnecting wires - to guarantee perfectly protected data transmission. We
show that a generalized KLJN system can provide unconditional security even if
it is used with significantly less limitations. The more universal conditions
ease practical realizations considerably and support more robust protection
against attacks. Our theoretical results are confirmed by numerical
simulations.",identity protection
http://arxiv.org/abs/1603.00182v1,"A marketplace is defined where the private data of suppliers (e.g.,
prosumers) are protected, so that neither their identity nor their level of
stock is made known to end customers, while they can sell their products at a
reduced price. A broker acts as an intermediary, which takes care of providing
the items missing to meet the customers' demand and allows end customers to
take advantages of reduced prices through the subscription of option contracts.
Formulas are provided for the option price under three different probability
models for the availability of items. Option pricing allows the broker to
partially transfer its risk on end customers.",identity protection
http://arxiv.org/abs/1907.12221v1,"We increasingly live in a world where there is a balance between the rights
to privacy and the requirements for consent, and the rights of society to
protect itself. Within this world, there is an ever-increasing requirement to
protect the identities involved within financial transactions, but this makes
things increasingly difficult for law enforcement agencies, especially in terms
of financial fraud and money laundering. This paper reviews the
state-of-the-art in terms of the methods of privacy that are being used within
cryptocurrency transactions, and in the challenges that law enforcement face.",identity protection
http://arxiv.org/abs/1506.06996v1,"Using communication services is a common part of everyday life in a personal
or business context. Communication services include Internet services like
voice services, chat service, and web 2.0 technologies (wikis, blogs, etc), but
other usage areas like home energy management and eMobility are will be
increasingly tackled. Such communication services typically authenticate
participants. For this identities of some kind are used to identify the
communication peer to the user of a service or to the service itself. Calling
line identification used in the Session Initiation Protocol (SIP) used for
Voice over IP (VoIP) is just one example. Authentication and identification of
eCar users for accounting during charging of the eCar is another example. Also,
further mechanisms rely on identities, e.g., whitelists defining allowed
communication peers. Trusted identities prevent identity spoofing, hence are a
basic building block for the protection of communication. However, providing
trusted identities in a practical way is still a difficult problem and too
often application specific identities are used, making identity handling a
hassle. Nowadays, many countries introduced electronic identity cards, e.g.,
the German ""Elektronischer Personalausweis"" (ePA). As many German citizens will
possess an ePA soon, it can be used as security token to provide trusted
identities. Especially new usage areas (like eMobility) should from the start
be based on the ubiquitous availability of trusted identities. This paper
describes how identity cards can be integrated within three domains: home
energy management, vehicle-2-grid communication, and SIP-based voice over IP
telephony. In all three domains, identity cards are used to reliably identify
users and authenticate participants. As an example for an electronic identity
card, this paper focuses on the German ePA.",identity protection
http://arxiv.org/abs/1207.0135v1,"In this work, we focus on protection against identity disclosure in the
publication of sparse multidimensional data. Existing multidimensional
anonymization techniquesa) protect the privacy of users either by altering the
set of quasi-identifiers of the original data (e.g., by generalization or
suppression) or by adding noise (e.g., using differential privacy) and/or (b)
assume a clear distinction between sensitive and non-sensitive information and
sever the possible linkage. In many real world applications the above
techniques are not applicable. For instance, consider web search query logs.
Suppressing or generalizing anonymization methods would remove the most
valuable information in the dataset: the original query terms. Additionally,
web search query logs contain millions of query terms which cannot be
categorized as sensitive or non-sensitive since a term may be sensitive for a
user and non-sensitive for another. Motivated by this observation, we propose
an anonymization technique termed disassociation that preserves the original
terms but hides the fact that two or more different terms appear in the same
record. We protect the users' privacy by disassociating record terms that
participate in identifying combinations. This way the adversary cannot
associate with high probability a record with a rare combination of terms. To
the best of our knowledge, our proposal is the first to employ such a technique
to provide protection against identity disclosure. We propose an anonymization
algorithm based on our approach and evaluate its performance on real and
synthetic datasets, comparing it against other state-of-the-art methods based
on generalization and differential privacy.",identity protection
http://arxiv.org/abs/1907.03710v1,"Data exfiltration attacks have led to huge data breaches. Recently, the
Equifax attack affected 147M users and a third-party library - Apache Struts -
was alleged to be responsible for it. These attacks often exploit the fact that
sensitive data are stored unencrypted in process memory and can be accessed by
any function executing within the same process, including untrusted third-party
library functions. This paper presents StackVault, a kernel-based system to
prevent sensitive stack-based data from being accessed in an unauthorized
manner by intra-process functions. Stack-based data includes data on stack as
well as data pointed to by pointer variables on stack. StackVault consists of
three components: (1) a set of programming APIs to allow users to specify which
data needs to be protected, (2) a kernel module which uses unforgeable function
identities to reliably carry out the sensitive data protection, and (3) an LLVM
compiler extension that enables transparent placement of stack protection
operations. The StackVault system automatically enforces stack protection
through spatial and temporal access monitoring and control over both sensitive
stack data and untrusted functions. We implemented StackVault and evaluated it
using a number of popular real-world applications, including gRPC. The results
show that StackVault is effective and efficient, incurring only up to 2.4%
runtime overhead.",identity protection
http://arxiv.org/abs/1806.04410v1,"A legally valid identification document allows impartial arbitration of the
identification of individuals. It protects individuals from a violation of
their dignity, justice, liberty and equality. It protects the nation from a
destruction of its republic, democratic, sovereign status. In order to test the
ability of an identification document to establish impartial identification of
individuals, it must be evaluated for its ability to establish identity,
undertake identification and build confidence to impartial, reliable and valid
identification. The processes of issuing, using and validating identification
documents alter the ability of the document to establish identity, undertake
identification and build confidence to impartial and valid identification.
These processes alter the ability of the document to serve as proof of
identity, proof of address, proof of being a resident, or even the proof of
existence of a person. We examine the ability of the UID number to serve as an
identification document with the ability to impartially arbitrate the
identification of individuals and serve as proof of identity, address, and
demonstrate existence of a person. We evaluate the implications of the
continued use UID system on our ability to undertake legally valid
identification ensure integrity of the identity and address databases across
the world.",identity protection
http://arxiv.org/abs/1406.7377v1,"Today, Online Social Networks such as Facebook, LinkedIn and Twitter are the
most popular platforms on the Internet, on which millions of users register to
share personal information with their friends. A large amount of data, social
links and statistics about users are collected by Online Social Networks
services and they create big digital mines of various statistical data. Leakage
of personal information is a significant concern for social network users.
Besides information propagation, some new attacks on Online Social Networks
such as Identity Clone attack (ICA) have been identified. ICA attempts to
create a fake online identity of a victim to fool their friends into believing
the authenticity of the fake identity to establish social links in order to
reap the private information of the victims friends which is not shared in
their public profiles. There are some identity validation services that perform
users identity validation, but they are passive services and they only protect
users who are informed on privacy concerns and online identity issues. This
paper starts with an explanation of two types of profile cloning attacks are
explained and a new approach for detecting clone identities is proposed by
defining profile similarity and strength of relationship measures. According to
similar attributes and strength of relationship among users which are computed
in detection steps, it will be decided which profile is clone and which one is
genuine by a predetermined threshold. Finally, the experimental results are
presented to demonstrate the effectiveness of the proposed approach.",identity protection
http://arxiv.org/abs/1508.02035v1,"Currently the Dempster-Shafer based algorithm and Uniform Random Probability
based algorithm are the preferred method of resolving security games, in which
defenders are able to identify attackers and only strategy remained ambiguous.
However this model is inefficient in situations where resources are limited and
both the identity of the attackers and their strategies are ambiguous. The
intent of this study is to find a more effective algorithm to guide the
defenders in choosing which outside agents with which to cooperate given both
ambiguities. We designed an experiment where defenders were compelled to engage
with outside agents in order to maximize protection of their targets. We
introduced two important notions: the behavior of each agent in target
protection and the tolerance threshold in the target protection process. From
these, we proposed an algorithm that was applied by each defender to determine
the best potential assistant(s) with which to cooperate. Our results showed
that our proposed algorithm is safer than the Dempster-Shafer based algorithm.",identity protection
http://arxiv.org/abs/1602.03929v1,"This research aims to design an educational mobile game for home computer
users to prevent from phishing attacks. Phishing is an online identity theft
which aims to steal sensitive information such as username, password and online
banking details from victims. To prevent this, phishing education needs to be
considered. Mobile games could facilitate to embed learning in a natural
environment. The paper introduces a mobile game design based on a story which
is simplifying and exaggerating real life. We use a theoretical model derived
from Technology Threat Avoidance Theory (TTAT) to address the game design
issues and game design principles were used as a set of guidelines for
structuring and presenting information. The overall mobile game design was
aimed to enhance avoidance behaviour through motivation of home computer users
to protect against phishing threats. The prototype game design is presented on
Google App Inventor Emulator. We believe by training home computer users to
protect against phishing attacks, would be an aid to enable the cyberspace as a
secure environment.",identity protection
http://arxiv.org/abs/1809.05369v1,"Data protection regulations generally afford individuals certain rights over
their personal data, including the rights to access, rectify, and delete the
data held on them. Exercising such rights naturally requires those with data
management obligations (service providers) to be able to match an individual
with their data. However, many mobile apps collect personal data, without
requiring user registration or collecting details of a user's identity (email
address, names, phone number, and so forth). As a result, a user's ability to
exercise their rights will be hindered without means for an individual to link
themselves with this 'nameless' data. Current approaches often involve those
seeking to exercise their legal rights having to give the app's provider more
personal information, or even to register for a service; both of which seem
contrary to the spirit of data protection law. This paper explores these
concerns, and indicates simple means for facilitating data subject rights
through both application and mobile platform (OS) design.",identity protection
http://arxiv.org/abs/1405.0351v2,"Current low-latency anonymity systems use complex overlay networks to conceal
a user's IP address, introducing significant latency and network efficiency
penalties compared to normal Internet usage. Rather than obfuscating network
identity through higher level protocols, we propose a more direct solution: a
routing protocol that allows communication without exposing network identity,
providing a strong foundation for Internet privacy, while allowing identity to
be defined in those higher level protocols where it adds value.
  Given current research initiatives advocating ""clean slate"" Internet designs,
an opportunity exists to design an internetwork layer routing protocol that
decouples identity from network location and thereby simplifies the anonymity
problem. Recently, Hsiao et al. proposed such a protocol (LAP), but it does not
protect the user against a local eavesdropper or an untrusted ISP, which will
not be acceptable for many users. Thus, we propose Dovetail, a next-generation
Internet routing protocol that provides anonymity against an active attacker
located at any single point within the network, including the user's ISP. A
major design challenge is to provide this protection without including an
application-layer proxy in data transmission. We address this challenge in path
construction by using a matchmaker node (an end host) to overlap two path
segments at a dovetail node (a router). The dovetail then trims away part of
the path so that data transmission bypasses the matchmaker. Additional design
features include the choice of many different paths through the network and the
joining of path segments without requiring a trusted third party. We develop a
systematic mechanism to measure the topological anonymity of our designs, and
we demonstrate the privacy and efficiency of our proposal by simulation, using
a model of the complete Internet at the AS-level.",identity protection
http://arxiv.org/abs/1902.03683v1,"With the development of artificial intelligence and self-driving, vehicular
ad-hoc network (VANET) has become an irreplaceable part of the Intelligent
Transportation Systems (ITSs). However, the traditional network of the ground
cannot meet the requirements of transmission, processing, and storage among
vehicles. Under this circumstance, integrating space and air nodes into the
whole network can provide comprehensive traffic information and reduce the
transmission delay. The high mobility and low latency in the Space-Air-Ground
Integrated Network (SAGIN) put forward higher requirements for security issues
such as identity authentication, privacy protection, and data security. This
paper simplifies the Blockchain and proposes an identity authentication and
privacy protection scheme based on the Hashchain in the SAGIN. The scheme
focuses on the characteristics of the wireless signal to identify and
authenticate the nodes. The verification and backup of the records on the block
are implemented with the distributed streaming platform, Kafka algorithm,
instead of the consensus. Furthermore, this paper analyzes the security of this
scheme. Afterward, the experimental results reveal the delay brought by the
scheme using the simulation of SUMO, OMNeT++, and Veins.",identity protection
http://arxiv.org/abs/1707.05518v1,"Several years of academic and industrial research efforts have converged to a
common understanding on fundamental security building blocks for the upcoming
Vehicular Communication (VC) systems. There is a growing consensus towards
deploying a special-purpose identity and credential management infrastructure,
i.e., a Vehicular Public-Key Infrastructure (VPKI), enabling pseudonymous
authentication, with standardization efforts towards that direction. In spite
of the progress made by standardization bodies (IEEE 1609.2 and ETSI) and
harmonization efforts (Car2Car Communication Consortium (C2C-CC)), significant
questions remain unanswered towards deploying a VPKI. Deep understanding of the
VPKI, a central building block of secure and privacy-preserving VC systems, is
still lacking. This paper contributes to the closing of this gap. We present
SECMACE, a VPKI system, which is compatible with the IEEE 1609.2 and ETSI
standards specifications. We provide a detailed description of our
state-of-the-art VPKI that improves upon existing proposals in terms of
security and privacy protection, and efficiency. SECMACE facilitates
multi-domain operations in the VC systems and enhances user privacy, notably
preventing linking pseudonyms based on timing information and offering
increased protection even against honest-but-curious VPKI entities. We propose
multiple policies for the vehicle-VPKI interactions, based on which and two
large-scale mobility trace datasets, we evaluate the full-blown implementation
of SECMACE. With very little attention on the VPKI performance thus far, our
results reveal that modest computing resources can support a large area of
vehicles with very low delays and the most promising policy in terms of privacy
protection can be supported with moderate overhead.",identity protection
http://arxiv.org/abs/1908.02641v1,"As AI systems develop in complexity it is becoming increasingly hard to
ensure non-discrimination on the basis of protected attributes such as gender,
age, and race. Many recent methods have been developed for dealing with this
issue as long as the protected attribute is explicitly available for the
algorithm. We address the setting where this is not the case (with either no
explicit protected attribute, or a large set of them). Instead, we assume the
existence of a fair domain expert capable of generating an extension to the
labeled dataset - a small set of example pairs, each having a different value
on a subset of protected variables, but judged to warrant a similar model
response. We define a performance metric - paired consistency. Paired
consistency measures how close the output (assigned by a classifier or a
regressor) is on these carefully selected pairs of examples for which fairness
dictates identical decisions. In some cases consistency can be embedded within
the loss function during optimization and serve as a fairness regularizer, and
in others it is a tool for fair model selection. We demonstrate our method
using the well studied Income Census dataset.",identity protection
http://arxiv.org/abs/1706.07748v1,"Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.",monitoring identity theft
http://arxiv.org/abs/1809.01774v1,"Modern smart grids rely on advanced metering infrastructure (AMI) networks
for monitoring and billing purposes. However, such an approach suffers from
electricity theft cyberattacks. Different from the existing research that
utilizes shallow, static, and customer-specific-based electricity theft
detectors, this paper proposes a generalized deep recurrent neural network
(RNN)-based electricity theft detector that can effectively thwart these
cyberattacks. The proposed model exploits the time series nature of the
customers' electricity consumption to implement a gated recurrent unit
(GRU)-RNN, hence, improving the detection performance. In addition, the
proposed RNN-based detector adopts a random search analysis in its learning
stage to appropriately fine-tune its hyper-parameters. Extensive test studies
are carried out to investigate the detector's performance using publicly
available real data of 107,200 energy consumption days from 200 customers.
Simulation results demonstrate the superior performance of the proposed
detector compared with state-of-the-art electricity theft detectors.",monitoring identity theft
http://arxiv.org/abs/1512.00351v3,"Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.",monitoring identity theft
http://arxiv.org/abs/1801.06825v1,"In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.",monitoring identity theft
http://arxiv.org/abs/1209.5982v1,"As smartphones become more pervasive, they are increasingly targeted by
malware. At the same time, each new generation of smartphone features
increasingly powerful onboard sensor suites. A new strain of sensor malware has
been developing that leverages these sensors to steal information from the
physical environment (e.g., researchers have recently demonstrated how malware
can listen for spoken credit card numbers through the microphone, or feel
keystroke vibrations using the accelerometer). Yet the possibilities of what
malware can see through a camera have been understudied. This paper introduces
a novel visual malware called PlaceRaider, which allows remote attackers to
engage in remote reconnaissance and what we call virtual theft. Through
completely opportunistic use of the camera on the phone and other sensors,
PlaceRaider constructs rich, three dimensional models of indoor environments.
Remote burglars can thus download the physical space, study the environment
carefully, and steal virtual objects from the environment (such as financial
documents, information on computer monitors, and personally identifiable
information). Through two human subject studies we demonstrate the
effectiveness of using mobile devices as powerful surveillance and virtual
theft platforms, and we suggest several possible defenses against visual
malware.",monitoring identity theft
http://arxiv.org/abs/1908.05945v3,"Digital identity is a multidimensional, multidisciplinary, and a complex
concept. As a result, it is difficult to apprehend. Many contributions have
proposed definitions and representations of digital identity. However, lots of
them are either very generic and difficult to implement or do not take into
account privacy issues. Seeing how important privacy master is, it becomes a
necessity to rethink digital identity in order to take into account privacy
issues. So, this paper aims at proposing an attribute-based digital identity
vision for privacy preservation purposes. The proposed model takes into account
identity theft, security, and privacy.",monitoring identity theft
http://arxiv.org/abs/1801.00129v1,"Data security, which is concerned with the prevention of unauthorized access
to computers, databases, and websites, helps protect digital privacy and ensure
data integrity. It is extremely difficult, however, to make security
watertight, and security breaches are not uncommon. The consequences of stolen
credentials go well beyond the leakage of other types of information because
they can further compromise other systems. This paper criticizes the practice
of using clear-text identity attributes, such as Social Security or driver's
license numbers -- which are in principle not even secret -- as acceptable
authentication tokens or assertions of ownership, and proposes a simple
protocol that straightforwardly applies public-key cryptography to make
identity claims verifiable, even when they are issued remotely via the
Internet. This protocol has the potential of elevating the business practices
of credit providers, rental agencies, and other service companies that have
hitherto exposed consumers to the risk of identity theft, to where identity
theft becomes virtually impossible.",monitoring identity theft
http://arxiv.org/abs/1307.3147v2,"The wide spread of mobiles as handheld devices leads to various innovative
applications that makes use of their ever increasing presence in our daily
life. One such application is location tracking and monitoring. This paper
proposes a prototype model for location tracking using Geographical Positioning
System (GPS) and Global System for Mobile Communication (GSM) technology. The
system displays the object moving path on the monitor and the same information
can also be communicated to the user cell phone, on demand of the user by
asking the specific information via SMS. This system is very useful for car
theft situations, for adolescent drivers being watched and monitored by
parents. The result shows that the object is being tracked with a minimal
tracking error.",monitoring identity theft
http://arxiv.org/abs/1904.11882v1,"In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.",monitoring identity theft
http://arxiv.org/abs/1009.5729v2,"In today's world password compromise by some adversaries is common for
different purpose. In ICC 2008 Lei et al. proposed a new user authentication
system based on the virtual password system. In virtual password system they
have used linear randomized function to be secure against identity theft
attacks, phishing attacks, keylogging attack and shoulder surfing system. In
ICC 2010 Li's given a security attack on the Lei's work. This paper gives
modification on Lei's work to prevent the Li's attack with reducing the server
overhead. This paper also discussed the problems with current password recovery
system and gives the better approach.",monitoring identity theft
http://arxiv.org/abs/1111.3530v1,"In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.",monitoring identity theft
http://arxiv.org/abs/1711.09260v2,"In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.",monitoring identity theft
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",monitoring identity theft
http://arxiv.org/abs/1909.08929v1,"As automobiles become intelligent, automobile theft methods are evolving
intelligently. Therefore automobile theft detection has become a major research
challenge. Data-mining, biometrics, and additional authentication methods have
been proposed to address automobile theft, in previous studies. Among these
methods, data-mining can be used to analyze driving characteristics and
identify a driver comprehensively. However, it requires a labeled driving
dataset to achieve high accuracy. It is impractical to use the actual
automobile theft detection system because real theft driving data cannot be
collected in advance. Hence, we propose a method to detect an automobile theft
attempt using only owner driving data. We cluster the key features of the owner
driving data using the k-means algorithm. After reconstructing the driving data
into one of these clusters, theft is detected using an error from the original
driving data. To validate the proposed models, we tested our actual driving
data and obtained 99% accuracy from the best model. This result demonstrates
that our proposed method can detect vehicle theft by using only the car owner's
driving data.",monitoring identity theft
http://arxiv.org/abs/1701.01505v2,"The classification of crime into discrete categories entails a massive loss
of information. Crimes emerge out of a complex mix of behaviors and situations,
yet most of these details cannot be captured by singular crime type labels.
This information loss impacts our ability to not only understand the causes of
crime, but also how to develop optimal crime prevention strategies. We apply
machine learning methods to short narrative text descriptions accompanying
crime records with the goal of discovering ecologically more meaningful latent
crime classes. We term these latent classes ""crime topics"" in reference to
text-based topic modeling methods that produce them. We use topic distributions
to measure clustering among formally recognized crime types. Crime topics
replicate broad distinctions between violent and property crime, but also
reveal nuances linked to target characteristics, situational conditions and the
tools and methods of attack. Formal crime types are not discrete in topic
space. Rather, crime types are distributed across a range of crime topics.
Similarly, individual crime topics are distributed across a range of formal
crime types. Key ecological groups include identity theft, shoplifting,
burglary and theft, car crimes and vandalism, criminal threats and confidence
crimes, and violent crimes. Though not a replacement for formal legal crime
classifications, crime topics provide a unique window into the heterogeneous
causal processes underlying crime.",monitoring identity theft
http://arxiv.org/abs/1410.0519v1,"By increase of culture and knowledge of the people, request for visiting
museums has increased and made the management of these places more complex.
Valuable things in a museum or ancient place must be maintained well and also
it need to managing visitors. To maintain things we should prevent them from
theft, as well as environmental factors such as temperature, humidity, PH,
chemical factors and mechanical events should be monitored. And if the
conditions are damaging, appropriate alerts or reports to managers and experts
should be announced. Visitors should also be monitored, as well as visitors
need to be guided and getting information in the environment. By utilizing RFID
technology and short-distance network tools, technical solutions for more
efficient management and more effective retention in museums can be
implemented.",monitoring identity theft
http://arxiv.org/abs/1103.3378v1,"Security is important for many sensor network applications. Wireless Sensor
Networks (WSN) are often deployed in hostile environments as static or mobile,
where an adversary can physically capture some of the nodes. once a node is
captured, adversary collects all the credentials like keys and identity etc.
the attacker can re-program it and replicate the node in order to eavesdrop the
transmitted messages or compromise the functionality of the network. Identity
theft leads to two types attack: clone and sybil. In particularly a harmful
attack against sensor networks where one or more node(s) illegitimately claims
an identity as replicas is known as the node replication attack. The
replication attack can be exceedingly injurious to many important functions of
the sensor network such as routing, resource allocation, misbehavior detection,
etc. This paper analyzes the threat posed by the replication attack and several
novel techniques to detect and defend against the replication attack, and
analyzes their effectiveness in both static and mobile WSN.",monitoring identity theft
http://arxiv.org/abs/1906.05754v1,"Since the first theft of the Mt.Gox exchange service in 2011, Bitcoin has
seen major thefts in subsequent years. For most thefts, the perpetrators remain
uncaught and unknown. Although every transaction is recorded and transparent in
the blockchain, thieves can hide behind pseudonymity and use transaction
obscuring techniques to disguise their transaction trail. First, this paper
investigates methods for transaction tracking with tainting analysis
techniques. Second, we propose new methods applied to a specific theft case.
Last, we propose a metrics-based evaluation framework to compare these
strategies with the goal of improving transaction tracking accuracy.",monitoring identity theft
http://arxiv.org/abs/0908.0979v1,"Privacy and security are often intertwined. For example, identity theft is
rampant because we have become accustomed to authentication by identification.
To obtain some service, we provide enough information about our identity for an
unscrupulous person to steal it (for example, we give our credit card number to
Amazon.com). One of the consequences is that many people avoid e-commerce
entirely due to privacy and security concerns. The solution is to perform
authentication without identification. In fact, all on-line actions should be
as anonymous as possible, for this is the only way to guarantee security for
the overall system. A credential system is a system in which users can obtain
credentials from organizations and demonstrate possession of these credentials.
Such a system is anonymous when transactions carried out by the same user
cannot be linked. An anonymous credential system is of significant practical
relevance because it is the best means of providing privacy for users.",monitoring identity theft
http://arxiv.org/abs/1411.7591v3,"Egocentric cameras are being worn by an increasing number of users, among
them many security forces worldwide. GoPro cameras already penetrated the mass
market, reporting substantial increase in sales every year. As head-worn
cameras do not capture the photographer, it may seem that the anonymity of the
photographer is preserved even when the video is publicly distributed.
  We show that camera motion, as can be computed from the egocentric video,
provides unique identity information. The photographer can be reliably
recognized from a few seconds of video captured when walking. The proposed
method achieves more than 90% recognition accuracy in cases where the random
success rate is only 3%.
  Applications can include theft prevention by locking the camera when not worn
by its rightful owner. Searching video sharing services (e.g. YouTube) for
egocentric videos shot by a specific photographer may also become possible. An
important message in this paper is that photographers should be aware that
sharing egocentric video will compromise their anonymity, even when their face
is not visible.",monitoring identity theft
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",monitoring identity theft
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",monitoring identity theft
http://arxiv.org/abs/1708.04278v1,"Data leakage and theft from databases is a dangerous threat to organizations.
Data Security and Data Privacy protection systems (DSDP) monitor data access
and usage to identify leakage or suspicious activities that should be
investigated. Because of the high velocity nature of database systems, such
systems audit only a portion of the vast number of transactions that take
place. Anomalies are investigated by a Security Officer (SO) in order to choose
the proper response. In this paper we investigate the effect of sampling
methods based on the risk the transaction poses and propose a new method for
""combined sampling"" for capturing a more varied sample.",monitoring identity theft
http://arxiv.org/abs/1908.10229v1,"Digital healthcare systems are very popular lately, as they provide a variety
of helpful means to monitor people's health state as well as to protect people
against an unexpected health situation. These systems contain a huge amount of
personal information in a form of electronic health records that are not
allowed to be disclosed to unauthorized users. Hence, health data and
information need to be protected against attacks and thefts. In this paper, we
propose a secure distributed architecture for healthcare data storage and
analysis. It uses a novel security model to rigorously control permissions of
accessing sensitive data in the system, as well as to protect the transmitted
data between distributed system servers and nodes. The model also satisfies the
NIST security requirements. Thorough experimental results show that the model
is very promising.",monitoring identity theft
http://arxiv.org/abs/1704.05223v1,"Although many anti-theft technologies are implemented, auto-theft is still
increasing. Also, security vulnerabilities of cars can be used for auto-theft
by neutralizing anti-theft system. This keyless auto-theft attack will be
increased as cars adopt computerized electronic devices more. To detect
auto-theft efficiently, we propose the driver verification method that analyzes
driving patterns using measurements from the sensor in the vehicle. In our
model, we add mechanical features of automotive parts that are excluded in
previous works, but can be differentiated by drivers' driving behaviors. We
design the model that uses significant features through feature selection to
reduce the time cost of feature processing and improve the detection
performance. Further, we enrich the feature set by deriving statistical
features such as mean, median, and standard deviation. This minimizes the
effect of fluctuation of feature values per driver and finally generates the
reliable model. We also analyze the effect of the size of sliding window on
performance to detect the time point when the detection becomes reliable and to
inform owners the theft event as soon as possible. We apply our model with real
driving and show the contribution of our work to the literature of driver
identification.",monitoring identity theft
http://arxiv.org/abs/1705.07121v1,"Advancements in healthcare industry with new technology and population growth
has given rise to security threat to our most personal data. The healthcare
data management system consists of records in different formats such as text,
numeric, pictures and videos leading to data which is big and unstructured.
Also, hospitals have several branches at different locations throughout a
country and overseas. In view of these requirements a cloud based healthcare
management system can be an effective solution for efficient health care data
management. One of the major concerns of a cloud based healthcare system is the
security aspect. It includes theft to identity, tax fraudulence, insurance
frauds, medical frauds and defamation of high profile patients. Hence, a secure
data access and retrieval is needed in order to provide security of critical
medical records in health care management system. Biometric authentication
mechanism is suitable in this scenario since it overcomes the limitations of
token theft and forgetting passwords in conventional token id-password
mechanism used for providing security. It also has high accuracy rate for
secure data access and retrieval. In this paper we propose BAMHealthCloud which
is a cloud based system for management of healthcare data, it ensures security
of data through biometric authentication. It has been developed after
performing a detailed case study on healthcare sector in a developing country.
Training of the signature samples for authentication purpose has been performed
in parallel on hadoop MapReduce framework using Resilient Backpropagation
neural network. From rigorous experiments it can be concluded that it achieves
a speedup of 9x, Equal error rate (EER) of 0.12, sensitivity of 0.98 and
specificity of 0.95 as compared to other approaches existing in literature.",monitoring identity theft
http://arxiv.org/abs/1109.1074v1,"In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.",monitoring identity theft
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",monitoring identity theft
http://arxiv.org/abs/1303.3764v3,"Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.",monitoring identity theft
http://arxiv.org/abs/1304.6499v1,"Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.",monitoring identity theft
http://arxiv.org/abs/1503.00454v1,"Implicit authentication consists of a server authenticating a user based on
the user's usage profile, instead of/in addition to relying on something the
user explicitly knows (passwords, private keys, etc.). While implicit
authentication makes identity theft by third parties more difficult, it
requires the server to learn and store the user's usage profile. Recently, the
first privacy-preserving implicit authentication system was presented, in which
the server does not learn the user's profile. It uses an ad hoc two-party
computation protocol to compare the user's fresh sampled features against an
encrypted stored user's profile. The protocol requires storing the usage
profile and comparing against it using two different cryptosystems, one of them
order-preserving; furthermore, features must be numerical. We present here a
simpler protocol based on set intersection that has the advantages of: i)
requiring only one cryptosystem; ii) not leaking the relative order of fresh
feature samples; iii) being able to deal with any type of features (numerical
or non-numerical).
  Keywords: Privacy-preserving implicit authentication, privacy-preserving set
intersection, implicit authentication, active authentication, transparent
authentication, risk mitigation, data brokers.",monitoring identity theft
http://arxiv.org/abs/0812.4181v1,"Web Services are web-based applications made available for web users or
remote Web-based programs. In order to promote interoperability, they publish
their interfaces in the so-called WSDL file and allow remote call over the
network. Although Web Services can be used in different ways, the industry
standard is the Service Oriented Architecture Web Services that doesn't rely on
the implementation details. In this architecture, communication is performed
through XML-based messages called SOAP messages. However, those messages are
prone to attacks that can lead to code injection, unauthorized accesses,
identity theft, etc. This type of attacks, called XML Rewriting Attacks, are
all based on unauthorized, yet possible, modifications of SOAP messages. We
present in this paper an explanation of this kind of attack, review the
existing solutions, and show their limitations. We also propose some ideas to
secure SOAP messages, as well as implementation ideas.",monitoring identity theft
http://arxiv.org/abs/1511.03459v1,"Phishing is an online identity theft that aims to steal sensitive information
such as username, password and online banking details from its victims.
Phishing education needs to be considered as a means to combat this threat.
This paper reports on a design and development of a mobile game prototype as an
educational tool helping computer users to protect themselves against phishing
attacks. The elements of a game design framework for avoiding phishing attacks
were used to address the game design issues. Game design principles served as
guidelines for structuring and presenting information. Our mobile game design
aimed to enhance the users' avoidance behaviour through motivation to protect
themselves against phishing threats. A think-aloud study was conducted, along
with a pre- and post-test, to assess the game design framework though the
developed mobile game prototype. The study results showed a significant
improvement of participants' phishing avoidance behaviour in their post-test
assessment. Furthermore, the study findings suggest that participants' threat
perception, safeguard effectiveness, self-efficacy, perceived severity and
perceived susceptibility elements positively impact threat avoidance behaviour,
whereas safeguard cost had a negative impact on it.",monitoring identity theft
http://arxiv.org/abs/1511.07093v1,"Phishing is an online identity theft, which aims to steal sensitive
information such as username, password and online banking details from victims.
To prevent this, phishing education needs to be considered. Game based
education is becoming more and more popular. This paper introduces a mobile
game prototype for the android platform based on a story, which simplifies and
exaggerates real life. The elements of a game design framework for avoiding
phishing attacks were used to address the game design issues and game design
principles were used as a set of guidelines for structuring and presenting
information. The overall mobile game design was aimed to enhance the user's
avoidance behaviour through motivation to protect themselves against phishing
threats. The prototype mobile game design was presented on MIT App Inventor
Emulator.",monitoring identity theft
http://arxiv.org/abs/1407.7146v3,"The use of TLS proxies to intercept encrypted traffic is controversial since
the same mechanism can be used for both benevolent purposes, such as protecting
against malware, and for malicious purposes, such as identity theft or
warrantless government surveillance. To understand the prevalence and uses of
these proxies, we build a TLS proxy measurement tool and deploy it via Google
AdWords campaigns. We generate 15.2 million certificate tests across two
large-scale measurement studies. We find that 1 in 250 TLS connections are
TLS-proxied. The majority of these proxies appear to be benevolent, however we
identify over 3,600 cases where eight malware products are using this
technology nefariously. We also find numerous instances of negligent,
duplicitous, and suspicious behavior, some of which degrade security for users
without their knowledge. Distinguishing these types of practices is challenging
in practice, indicating a need for transparency and user awareness.",monitoring identity theft
http://arxiv.org/abs/1410.8747v1,"Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.",monitoring identity theft
http://arxiv.org/abs/1602.03929v1,"This research aims to design an educational mobile game for home computer
users to prevent from phishing attacks. Phishing is an online identity theft
which aims to steal sensitive information such as username, password and online
banking details from victims. To prevent this, phishing education needs to be
considered. Mobile games could facilitate to embed learning in a natural
environment. The paper introduces a mobile game design based on a story which
is simplifying and exaggerating real life. We use a theoretical model derived
from Technology Threat Avoidance Theory (TTAT) to address the game design
issues and game design principles were used as a set of guidelines for
structuring and presenting information. The overall mobile game design was
aimed to enhance avoidance behaviour through motivation of home computer users
to protect against phishing threats. The prototype game design is presented on
Google App Inventor Emulator. We believe by training home computer users to
protect against phishing attacks, would be an aid to enable the cyberspace as a
secure environment.",monitoring identity theft
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing detection
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing detection
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing detection
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing detection
http://arxiv.org/abs/1104.0582v1,"Bag-of-words model is implemented and tried on 10-class visual concept
detection problem. The experimental results show that ""DURF+ERT+SVM""
outperforms ""SIFT+ERT+SVM"" both in detection performance and computation
efficiency. Besides, combining DURF and SIFT results in even better detection
performance. Real-time object detection using SIFT and RANSAC is also tried on
simple objects, e.g. drink can, and good result is achieved.",pishing detection
http://arxiv.org/abs/1311.1446v1,"Mobile ad-hoc networks are temporary wireless networks. Network resources are
abnormally consumed by intruders. Anomaly and signature based techniques are
used for intrusion detection. Classification techniques are used in anomaly
based techniques. Intrusion detection techniques are used for the network
attack detection process. Two types of intrusion detection systems are
available. They are anomaly detection and signature based detection model. The
anomaly detection model uses the historical transactions with attack labels.
The signature database is used in the signature based IDS schemes.
  The mobile ad-hoc networks are infrastructure less environment. The intrusion
detection applications are placed in a set of nodes under the mobile ad-hoc
network environment. The nodes are grouped into clusters. The leader nodes are
assigned for the clusters. The leader node is assigned for the intrusion
detection process. Leader nodes are used to initiate the intrusion detection
process. Resource sharing and lifetime management factors are considered in the
leader election process. The system optimizes the leader election and intrusion
detection process.
  The system is designed to handle leader election and intrusion detection
process. The clustering scheme is optimized with coverage and traffic level.
Cost and resource utilization is controlled under the clusters. Node mobility
is managed by the system.",pishing detection
http://arxiv.org/abs/1612.08242v1,"We introduce YOLO9000, a state-of-the-art, real-time object detection system
that can detect over 9000 object categories. First we propose various
improvements to the YOLO detection method, both novel and drawn from prior
work. The improved model, YOLOv2, is state-of-the-art on standard detection
tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At
40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like
Faster RCNN with ResNet and SSD while still running significantly faster.
Finally we propose a method to jointly train on object detection and
classification. Using this method we train YOLO9000 simultaneously on the COCO
detection dataset and the ImageNet classification dataset. Our joint training
allows YOLO9000 to predict detections for object classes that don't have
labelled detection data. We validate our approach on the ImageNet detection
task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite
only having detection data for 44 of the 200 classes. On the 156 classes not in
COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;
it predicts detections for more than 9000 different object categories. And it
still runs in real-time.",pishing detection
http://arxiv.org/abs/1810.02659v1,Research Proposal in Automated Fix Detection,pishing detection
http://arxiv.org/abs/1901.06585v1,"This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.",pishing detection
http://arxiv.org/abs/1702.04377v1,"Face detection is one of the challenging tasks in computer vision. Human face
detection plays an essential role in the first stage of face processing
applications such as face recognition, face tracking, image database
management, etc. In these applications, face objects often come from an
inconsequential part of images that contain variations, namely different
illumination, poses, and occlusion. These variations can decrease face
detection rate noticeably. Most existing face detection approaches are not
accurate, as they have not been able to resolve unstructured images due to
large appearance variations and can only detect human faces under one
particular variation. Existing frameworks of face detection need enhancements
to detect human faces under the stated variations to improve detection rate and
reduce detection time. In this study, an enhanced face detection framework is
proposed to improve detection rate based on skin color and provide a validation
process. A preliminary segmentation of the input images based on skin color can
significantly reduce search space and accelerate the process of human face
detection. The primary detection is based on Haar-like features and the
Adaboost algorithm. A validation process is introduced to reject non-face
objects, which might occur during the face detection process. The validation
process is based on two-stage Extended Local Binary Patterns. The experimental
results on the CMU-MIT and Caltech 10000 datasets over a wide range of facial
variations in different colors, positions, scales, and lighting conditions
indicated a successful face detection rate.",pishing detection
http://arxiv.org/abs/cs/0501001v1,"Distributed intrustion detection systems detect attacks on computer systems
by analyzing data aggregated from distributed sources. The distributed nature
of the data sources allows patterns in the data to be seen that might not be
detectable if each of the sources were examined individually. This paper
describes the various approaches that have been developed to share and analyze
data in such systems, and discusses some issues that must be addressed before
fully decentralized distributed intrusion detection systems can be made viable.",pishing detection
http://arxiv.org/abs/1710.03958v2,"Recent approaches for high accuracy detection and tracking of object
categories in video consist of complex multistage solutions that become more
cumbersome each year. In this paper we propose a ConvNet architecture that
jointly performs detection and tracking, solving the task in a simple and
effective way. Our contributions are threefold: (i) we set up a ConvNet
architecture for simultaneous detection and tracking, using a multi-task
objective for frame-based object detection and across-frame track regression;
(ii) we introduce correlation features that represent object co-occurrences
across time to aid the ConvNet during tracking; and (iii) we link the frame
level detections based on our across-frame tracklets to produce high accuracy
detections at the video level. Our ConvNet architecture for spatiotemporal
object detection is evaluated on the large-scale ImageNet VID dataset where it
achieves state-of-the-art results. Our approach provides better single model
performance than the winning method of the last ImageNet challenge while being
conceptually much simpler. Finally, we show that by increasing the temporal
stride we can dramatically increase the tracker speed.",pishing detection
http://arxiv.org/abs/1902.01031v1,"The main essence of this paper is to investigate the performance of RetinaNet
based object detectors on pedestrian detection. Pedestrian detection is an
important research topic as it provides a baseline for general object detection
and has a great number of practical applications like autonomous car, robotics
and Security camera. Though extensive research has made huge progress in
pedestrian detection, there are still many issues and open for more research
and improvement. Recent deep learning based methods have shown state-of-the-art
performance in computer vision tasks such as image classification, object
detection, and segmentation. Wider pedestrian detection challenge aims at
finding improve solutions for pedestrian detection problem. In this paper, We
propose a pedestrian detection system based on RetinaNet. Our solution has
scored 0.4061 mAP. The code is available at
https://github.com/miltonbd/ECCV_2018_pedestrian_detection_challenege.",pishing detection
http://arxiv.org/abs/1905.05055v2,"Object detection, as of one the most fundamental and challenging problems in
computer vision, has received great attention in recent years. Its development
in the past two decades can be regarded as an epitome of computer vision
history. If we think of today's object detection as a technical aesthetics
under the power of deep learning, then turning back the clock 20 years we would
witness the wisdom of cold weapon era. This paper extensively reviews 400+
papers of object detection in the light of its technical evolution, spanning
over a quarter-century's time (from the 1990s to 2019). A number of topics have
been covered in this paper, including the milestone detectors in history,
detection datasets, metrics, fundamental building blocks of the detection
system, speed up techniques, and the recent state of the art detection methods.
This paper also reviews some important detection applications, such as
pedestrian detection, face detection, text detection, etc, and makes an in-deep
analysis of their challenges as well as technical improvements in recent years.",pishing detection
http://arxiv.org/abs/1906.00093v1,"In this paper, we present a novel model to detect lane regions and extract
lane departure events (changes and incursions) from challenging,
lower-resolution videos recorded with mobile cameras. Our algorithm used a
Mask-RCNN based lane detection model as pre-processor. Recently, deep
learning-based models provide state-of-the-art technology for object detection
combined with segmentation. Among the several deep learning architectures,
convolutional neural networks (CNNs) outperformed other machine learning
models, especially for region proposal and object detection tasks. Recent
development in object detection has been driven by the success of region
proposal methods and region-based CNNs (R-CNNs). Our algorithm utilizes lane
segmentation mask for detection and Fix-lag Kalman filter for tracking, rather
than the usual approach of detecting lane lines from single video frames. The
algorithm permits detection of driver lane departures into left or right lanes
from continuous lane detections. Preliminary results show promise for robust
detection of lane departure events. The overall sensitivity for lane departure
events on our custom test dataset is 81.81%.",pishing detection
http://arxiv.org/abs/1503.03771v1,"This paper studies efficient means for dealing with intra-category diversity
in object detection. Strategies for occlusion and orientation handling are
explored by learning an ensemble of detection models from visual and
geometrical clusters of object instances. An AdaBoost detection scheme is
employed with pixel lookup features for fast detection. The analysis provides
insight into the design of a robust vehicle detection system, showing promise
in terms of detection performance and orientation estimation accuracy.",pishing detection
http://arxiv.org/abs/1611.00301v1,"We introduce a powerful recurrent neural network based method for novelty
detection to the application of detecting radio anomalies. This approach holds
promise in significantly increasing the ability of naive anomaly detection to
detect small anomalies in highly complex complexity multi-user radio bands. We
demonstrate the efficacy of this approach on a number of common real over the
air radio communications bands of interest and quantify detection performance
in terms of probability of detection an false alarm rates across a range of
interference to band power ratios and compare to baseline methods.",pishing detection
http://arxiv.org/abs/1812.06292v2,"Deep learning is an advanced model of traditional machine learning. This has
the capability to extract optimal feature representation from raw input
samples. This has been applied towards various use cases in cyber security such
as intrusion detection, malware classification, android malware detection, spam
and phishing detection and binary analysis. This paper outlines the survey of
all the works related to deep learning based solutions for various cyber
security use cases. Keywords: Deep learning, intrusion detection, malware
detection, Android malware detection, spam & phishing detection, traffic
analysis, binary analysis.",pishing detection
http://arxiv.org/abs/1909.12483v1,"This paper presents a method to detect reflection with 3D light detection and
ranging (Lidar) and uses it to map the back side of objects. This method uses
several approaches to analyze the point cloud, including intensity peak
detection, dual return detection, plane fitting, and finding the boundaries.
These approaches can classify the point cloud and detect the reflection in it.
By mirroring the reflection points on the detected window pane and adding
classification labels on the points, we can have improve the map quality in a
Simultaneous Localization and Mapping (SLAM) framework.",pishing detection
http://arxiv.org/abs/1908.09238v1,"Monitoring gas turbine combustors health, in particular, early detecting
abnormal behaviors and incipient faults, is critical in ensuring gas turbines
operating efficiently and in preventing costly unplanned maintenance. One
popular means of detecting combustor abnormalities is through continuously
monitoring exhaust gas temperature profiles. Over the years many anomaly
detection technologies have been explored for detecting combustor faults,
however, the performance (detection rate) of anomaly detection solutions
fielded is still inadequate. Advanced technologies that can improve detection
performance are in great need. Aiming for improving anomaly detection
performance, in this paper we introduce recently-developed deep learning (DL)
in machine learning into the combustors anomaly detection application.
Specifically, we use deep learning to hierarchically learn features from the
sensor measurements of exhaust gas temperatures. And we then use the learned
features as the input to a neural network classifier for performing combustor
anomaly detection. Since such deep learned features potentially better capture
complex relations among all sensor measurements and the underlying combustor
behavior than handcrafted features do, we expect the learned features can lead
to a more accurate and robust anomaly detection. Using the data collected from
a real-world gas turbine combustion system, we demonstrated that the proposed
deep learning based anomaly detection significantly indeed improved combustor
anomaly detection performance.",pishing detection
http://arxiv.org/abs/1004.4598v3,"The goal of an Intrusion Detection is inadequate to detect errors and unusual
activity on a network or on the hosts belonging to a local network by
monitoring network activity. Algorithms for building detection models are
broadly classified into two categories, Misuse Detection and Anomaly Detection.
The proposed approach should be taken into account, as the security system
violations caused by both incompliance with the security policy and attacks on
the system resulting in the need to describe models. However, it is based on
unified mathematical formalism which is provided for subsequent merger of the
models. The above formalism in this paper presents a state machine describing
the behavior of a system subject. The set of intrusion description models is
used by the evaluation module and determines the likelihood of undesired
actions the system is capable of detecting. The number of attacks which are not
described by models determining the completeness of detection by the IDS linked
to the ability of detecting security violations.",pishing detection
http://arxiv.org/abs/1503.03920v1,"In this contribution, we develop an accurate and effective event detection
method to detect events from a Twitter stream, which uses visual and textual
information to improve the performance of the mining process. The method
monitors a Twitter stream to pick up tweets having texts and images and stores
them into a database. This is followed by applying a mining algorithm to detect
an event. The procedure starts with detecting events based on text only by
using the feature of the bag-of-words which is calculated using the term
frequency-inverse document frequency (TF-IDF) method. Then it detects the event
based on image only by using visual features including histogram of oriented
gradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color
histogram. K nearest neighbours (Knn) classification is used in the detection.
The final decision of the event detection is made based on the reliabilities of
text only detection and image only detection. The experiment result showed that
the proposed method achieved high accuracy of 0.94, comparing with 0.89 with
texts only, and 0.86 with images only.",pishing detection
http://arxiv.org/abs/1412.3159v1,"Vision-based road detection is an essential functionality for supporting
advanced driver assistance systems (ADAS) such as road following and vehicle
and pedestrian detection. The major challenges of road detection are dealing
with shadows and lighting variations and the presence of other objects in the
scene. Current road detection algorithms characterize road areas at pixel level
and group pixels accordingly. However, these algorithms fail in presence of
strong shadows and lighting variations. Therefore, we propose a road detection
algorithm based on video alignment. The key idea of the algorithm is to exploit
the similarities occurred when a vehicle follows the same trajectory more than
once. In this way, road areas are learned in a first ride and then, this road
knowledge is used to infer areas depicting drivable road surfaces in subsequent
rides. Two different experiments are conducted to validate the proposal on
different video sequences taken at different scenarios and different daytime.
The former aims to perform on-line road detection. The latter aims to perform
off-line road detection and is applied to automatically generate the
ground-truth necessary to validate road detection algorithms. Qualitative and
quantitative evaluations prove that the proposed algorithm is a valid road
detection approach.",pishing detection
http://arxiv.org/abs/1312.2861v1,"From the past decade outlier detection has been in use. Detection of outliers
is an emerging topic and is having robust applications in medical sciences and
pharmaceutical sciences. Outlier detection is used to detect anomalous
behaviour of data. Typical problems in Bioinformatics can be addressed by
outlier detection. A computationally fast method for detecting outliers is
shown, that is particularly effective in high dimensions. PrCmpOut algorithm
make use of simple properties of principal components to detect outliers in the
transformed space, leading to significant computational advantages for high
dimensional data. This procedure requires considerably less computational time
than existing methods for outlier detection. The properties of this estimator
(Outlier error rate (FN), Non-Outlier error rate(FP) and computational costs)
are analyzed and compared with those of other robust estimators described in
the literature through simulation studies. Numerical evidence based Oxazolines
and Oxazoles molecular descriptor dataset shows that the proposed method
performs well in a variety of situations of practical interest. It is thus a
valuable companion to the existing outlier detection methods.",pishing detection
http://arxiv.org/abs/1703.01290v1,"Weakly-supervised object detection (WOD) is a challenging problems in
computer vision. The key problem is to simultaneously infer the exact object
locations in the training images and train the object detectors, given only the
training images with weak image-level labels. Intuitively, by simulating the
selective attention mechanism of human visual system, saliency detection
technique can select attractive objects in scenes and thus is a potential way
to provide useful priors for WOD. However, the way to adopt saliency detection
in WOD is not trivial since the detected saliency region might be possibly
highly ambiguous in complex cases. To this end, this paper first
comprehensively analyzes the challenges in applying saliency detection to WOD.
Then, we make one of the earliest efforts to bridge saliency detection to WOD
via the self-paced curriculum learning, which can guide the learning procedure
to gradually achieve faithful knowledge of multi-class objects from easy to
hard. The experimental results demonstrate that the proposed approach can
successfully bridge saliency detection and WOD tasks and achieve the
state-of-the-art object detection results under the weak supervision.",pishing detection
http://arxiv.org/abs/1709.05188v6,"Occluded face detection is a challenging detection task due to the large
appearance variations incurred by various real-world occlusions. This paper
introduces an Adversarial Occlusion-aware Face Detector (AOFD) by
simultaneously detecting occluded faces and segmenting occluded areas.
Specifically, we employ an adversarial training strategy to generate
occlusion-like face features that are difficult for a face detector to
recognize. Occlusion mask is predicted simultaneously while detecting occluded
faces and the occluded area is utilized as an auxiliary instead of being
regarded as a hindrance. Moreover, the supervisory signals from the
segmentation branch will reversely affect the features, aiding in detecting
heavily-occluded faces accordingly. Consequently, AOFD is able to find the
faces with few exposed facial landmarks with very high confidences and keeps
high detection accuracy even for masked faces. Extensive experiments
demonstrate that AOFD not only significantly outperforms state-of-the-art
methods on the MAFA occluded face detection dataset, but also achieves
competitive detection accuracy on benchmark dataset for general face detection
such as FDDB.",pishing detection
http://arxiv.org/abs/1802.02087v2,"Detectability of discrete event systems (DESs) is a property to determine a
priori whether the current and subsequent states can be determined based on
observations. In this paper, we investigate the verification of two
detectability properties -- strong detectability and weak detectability -- for
DESs modeled by labeled Petri nets. Strong detectability requires that we can
always determine, after a finite number of observations, the current and
subsequent markings of the system, while weak detectability requires that we
can determine, after a finite number of observations, the current and
subsequent markings for some trajectories of the system. We show that for DESs
modeled by labeled Petri nets, checking strong detectability is decidable
whereas checking weak detectability is undecidable. Our results extend the
existing studies on the verification of detectability from finite-state
automata to labeled Petri nets. As a consequence, we strengthen a result on
checking current-state opacity for labeled Petri nets.",pishing detection
http://arxiv.org/abs/1802.03154v2,"Realistic image forgeries involve a combination of splicing, resampling,
cloning, region removal and other methods. While resampling detection
algorithms are effective in detecting splicing and resampling, copy-move
detection algorithms excel in detecting cloning and region removal. In this
paper, we combine these complementary approaches in a way that boosts the
overall accuracy of image manipulation detection. We use the copy-move
detection method as a pre-filtering step and pass those images that are
classified as untampered to a deep learning based resampling detection
framework. Experimental results on various datasets including the 2017 NIST
Nimble Challenge Evaluation dataset comprising nearly 10,000 pristine and
tampered images shows that there is a consistent increase of 8%-10% in
detection rates, when copy-move algorithm is combined with different resampling
detection algorithms.",pishing detection
http://arxiv.org/abs/0910.4030v1,"Investigating the interaction of electrons in a superconductor by means of a
method of solitary waves of Korteweg - de Vries, we refute the claim of absence
of ""Cooper pairs"" in a superconductor. We also indicate that the nondissipative
transfer of energy in the superconductor is possible only with the help of a
pair of electrons.",pishing monitoring
http://arxiv.org/abs/0910.4499v1,"In this paper, we propose to draw attention to the stability criterion of the
superconductor current state. We use for this purpose the rough systems
mathematical apparatus allowing us to relate the desired criterion with the
dielectric permittivity of the matter and to identify the type of all possible
phonons trajectories in its superconducting state. The state of
superconductivity in the matter can be explained only by the phonons behavior
peculiarity. And on the basis of the above-mentioned assumption, the
corresponding mathematical model is constructed.",pishing monitoring
http://arxiv.org/abs/0910.4641v1,"The Ginzburg - Landau theory is used for the superconducting structures free
energy fluctuations study. On its basis, we have defined the value of the heat
capacity jump in the macroscopic zero-dimensional sample and in the
zero-dimensional microstructures ensemble of the total volume equal to the
macroscopic sample volume. The inference is made that in the Ginzburg - Landau
methodology frameworks, it is essential to take into account the
superconducting clean sample effective dimensionality only on the last stage of
its thermodynamical characteristics calculation.",pishing monitoring
http://arxiv.org/abs/0910.5141v1,"We consider the problem important for the condensed matter physics,
superconductivity physics, and electrodynamics of continuous media - the
problem of the matter dielectric permittivity possible values spectrum
definition. Two ways of the dielectric permittivity values spectrum
identification are analyzed. The proposed technique allows the author to
complete the universal criterion of stability identified by D. A. Kirzhnits for
a system of charged particles by the criterion of stability for a
superconducting system of charged particles.",pishing monitoring
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",pishing monitoring
http://arxiv.org/abs/1802.03667v1,"Runtime monitoring is essential for the violation detection during the
underlying software system execution. In this paper, an investigation of the
monitoring activity of MAPE-K control loop is performed which aims at
exploring:(1) the architecture of the monitoring activity in terms of the
involved components and control and data flow between them; (2) the standard
interface of the monitoring component with other MAPE-K components; (3) the
adaptive monitoring and its importance to the monitoring overhead issue; and
(4) the monitoring mode and its relevance to some specific situations and
systems. This paper also presented a Java framework for the monitoring process
for self adaptive systems.",pishing monitoring
http://arxiv.org/abs/1906.00766v1,"Monitorability delineates what properties can be verified at runtime.
Although many monitorability definitions exist, few are defined explicitly in
terms of the guarantees provided by monitors, i.e., the computational entities
carrying out the verification. We view monitorability as a spectrum: the fewer
monitor guarantees that are required, the more properties become monitorable.
We present a monitorability hierarchy and provide operational and syntactic
characterisations for its levels. Existing monitorability definitions are
mapped into our hierarchy, providing a unified framework that makes the
operational assumptions and guarantees of each definition explicit. This
provides a rigorous foundation that can inform design choices and correctness
claims for runtime verification tools.",pishing monitoring
http://arxiv.org/abs/1902.05135v1,"Monitoring kernel object modification of virtual machine is widely used by
virtual-machine-introspection-based security monitors to protect virtual
machines in cloud computing, such as monitoring dentry objects to intercept
file operations, etc. However, most of the current virtual machine monitors,
such as KVM and Xen, only support page-level monitoring, because the Intel EPT
technology can only monitor page privilege. If the out-of-virtual-machine
security tools want to monitor some kernel objects, they need to intercept the
operation of the whole memory page. Since there are some other objects stored
in the monitored pages, the modification of them will also trigger the monitor.
Therefore, page-level memory monitor usually introduces overhead to related
kernel services of the target virtual machine. In this paper, we propose a
low-overhead kernel object monitoring approach to reduce the overhead caused by
page-level monitor. The core idea is to migrate the target kernel objects to a
protected memory area and then to monitor the corresponding new memory pages.
Since the new pages only contain the kernel objects to be monitored, other
kernel objects will not trigger our monitor. Therefore, our monitor will not
introduce runtime overhead to the related kernel service. The experimental
results show that our system can monitor target kernel objects effectively only
with very low overhead.",pishing monitoring
http://arxiv.org/abs/1106.1816v1,"Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.",pishing monitoring
http://arxiv.org/abs/1106.0235v1,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",pishing monitoring
http://arxiv.org/abs/1411.5213v1,"With our growing reliability on distributed networks, the security aspect of
such networks becomes of prime importance. In large scale distributed networks
it becomes cardinal to have an efficient and effective monitoring scheme. The
monitoring schemes supervise the node behaviour in the network and look out for
any discrepancy. Monitoring schemes comprise of monitoring components that work
together to help schemes in meeting various security requirement parameters for
the networks. These security parameters are breached via various attacks by
manipulation of monitoring components of particular monitoring schemes to
produce faulty results and thereby reducing efficiency of networks, reliability
and security. In this paper we have discussed these components of monitoring,
multiple monitoring schemes, their security parameters and various types of
attacks possible on these monitoring components by manipulating assumptions of
monitoring schemes.",pishing monitoring
http://arxiv.org/abs/1708.01476v1,"System monitoring is an established tool to measure the utilization and
health of HPC systems. Usually system monitoring infrastructures make no
connection to job information and do not utilize hardware performance
monitoring (HPM) data. To increase the efficient use of HPC systems automatic
and continuous performance monitoring of jobs is an essential component. It can
help to identify pathological cases, provides instant performance feedback to
the users, offers initial data to judge on the optimization potential of
applications and helps to build a statistical foundation about application
specific system usage. The LIKWID monitoring stack is a modular framework build
on top of the LIKWID tools library. It aims on enabling job specific
performance monitoring using HPM data, system metrics and application-level
data for small to medium sized commodity clusters. Moreover, it is designed to
integrate in existing monitoring infrastructures to speed up the change from
pure system monitoring to job-aware monitoring.",pishing monitoring
http://arxiv.org/abs/1902.05152v1,"We compare the succinctness of two monitoring systems for properties of
infinite traces, namely parallel and regular monitors. Although a parallel
monitor can be turned into an equivalent regular monitor, the cost of this
transformation is a double-exponential blowup in the syntactic size of the
monitors, and a triple-exponential blowup when the goal is a deterministic
monitor. We show that these bounds are tight and that they also hold for
translations between corresponding fragments of Hennessy-Milner logic with
recursion over infinite traces.",pishing monitoring
http://arxiv.org/abs/1711.03952v2,"Trust in publicly verifiable Certificate Transparency (CT) logs is reduced
through cryptography, gossip, auditing, and monitoring. The role of a monitor
is to observe each and every log entry, looking for suspicious certificates
that interest the entity running the monitor. While anyone can run a monitor,
it requires continuous operation and copies of the logs to be inspected. This
has lead to the emergence of monitoring-as-a-service: a trusted party runs the
monitor and provides registered subjects with selective certificate
notifications, e.g., ""notify me of all foo.com certificates"". We present a
CT/bis extension for verifiable light-weight monitoring that enables subjects
to verify the correctness of such notifications, reducing the trust that is
placed in these monitors. Our extension supports verifiable monitoring of
wild-card domains and piggybacks on CT's existing gossip-audit security model.",pishing monitoring
http://arxiv.org/abs/1806.06143v2,"We study selective monitors for labelled Markov chains. Monitors observe the
outputs that are generated by a Markov chain during its run, with the goal of
identifying runs as correct or faulty. A monitor is selective if it skips
observations in order to reduce monitoring overhead. We are interested in
monitors that minimize the expected number of observations. We establish an
undecidability result for selectively monitoring general Markov chains. On the
other hand, we show for non-hidden Markov chains (where any output identifies
the state the Markov chain is in) that simple optimal monitors exist and can be
computed efficiently, based on DFA language equivalence. These monitors do not
depend on the precise transition probabilities in the Markov chain. We report
on experiments where we compute these monitors for several open-source Java
projects.",pishing monitoring
http://arxiv.org/abs/1902.00435v1,"This paper establishes a comprehensive theory of runtime monitorability for
Hennessy-Milner logic with recursion, a very expressive variant of the modal
$\mu$-calculus. It investigates the monitorability of that logic with a
linear-time semantics and then compares the obtained results with ones that
were previously presented in the literature for a branching-time setting. Our
work establishes an expressiveness hierarchy of monitorable fragments of
Hennessy-Milner logic with recursion in a linear-time setting and exactly
identifies what kinds of guarantees can be given using runtime monitors for
each fragment in the hierarchy. Each fragment is shown to be complete, in the
sense that it can express all properties that can be monitored under the
corresponding guarantees. The study is carried out using a principled approach
to monitoring that connects the semantics of the logic and the operational
semantics of monitors. The proposed framework supports the automatic,
compositional synthesis of correct monitors from monitorable properties.",pishing monitoring
http://arxiv.org/abs/1301.3839v1,"Monitoring plan preconditions can allow for replanning when a precondition
fails, generally far in advance of the point in the plan where the precondition
is relevant. However, monitoring is generally costly, and some precondition
failures have a very small impact on plan quality. We formulate a model for
optimal precondition monitoring, using partially-observable Markov decisions
processes, and describe methods for solving this model efficitively, though
approximately. Specifically, we show that the single-precondition monitoring
problem is generally tractable, and the multiple-precondition monitoring
policies can be efficitively approximated using single-precondition soultions.",pishing monitoring
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",pishing monitoring
http://arxiv.org/abs/1611.10212v1,"We examine the determinization of monitors for HML with recursion. We
demonstrate that every monitor is equivalent to a deterministic one, which is
at most doubly exponential in size with respect to the original monitor. When
monitors are described as CCS-like processes, this doubly exponential bound is
optimal. When (deterministic) monitors are described as finite automata (as
their LTS), then they can be exponentially more succinct than their CCS process
form.",pishing monitoring
http://arxiv.org/abs/0711.0315v1,"Effective resource utilisation monitoring and highly granular yet adaptive
measurements are prerequisites for a more efficient Grid scheduler. We present
a suite of measurement applications able to monitor per-process resource
utilisation, and a customisable tool for emulating observed utilisation models.",pishing monitoring
http://arxiv.org/abs/1507.01020v1,"When a property needs to be checked against an unknown or very complex
system, classical exploration techniques like model-checking are not applicable
anymore. Sometimes a~monitor can be used, that checks a given property on the
underlying system at runtime. A monitor for a property $L$ is a deterministic
finite automaton $M_L$ that after each finite execution tells whether (1) every
possible extension of the execution is in $L$, or (2) every possible extension
is in the complement of $L$, or neither (1) nor (2) holds. Moreover, $L$ being
monitorable means that it is always possible that in some future the monitor
reaches (1) or (2). Classical examples for monitorable properties are safety
and cosafety properties. On the other hand, deterministic liveness properties
like ""infinitely many $a$'s"" are not monitorable. We discuss various monitor
constructions with a focus on deterministic omega-regular languages. We locate
a proper subclass of of deterministic omega-regular languages but also strictly
large than the subclass of languages which are deterministic and
codeterministic, and for this subclass there exists a canonical monitor which
also accepts the language itself.
  We also address the problem to decide monitorability in comparison with
deciding liveness. The state of the art is as follows. Given a B\""uchi
automaton, it is PSPACE-complete to decide liveness or monitorability. Given an
LTL formula, deciding liveness becomes EXPSPACE-complete, but the complexity to
decide monitorability remains open.",pishing monitoring
http://arxiv.org/abs/cs/0310025v1,"UFO is a new implementation of FORMAN, a declarative monitoring language, in
which rules are compiled into execution monitors that run on a virtual machine
supported by the Alamo monitor architecture.",pishing monitoring
http://arxiv.org/abs/0707.3490v1,"The author solves two problems: formation of object of econophysics, creation
of the general theory of financial-economic monitoring. In the first problem he
studied two fundamental tasks: a choice of conceptual model and creation of
axiomatic base. It is accepted, that the conceptual model of econophysics is a
concrete definition of entropy conceptual model. Financial and economic
monitoring is considered as monitoring of flows on entropy manifold of phase
space - on a Diffusion field.",pishing monitoring
http://arxiv.org/abs/0709.1333v2,"Shintake monitor is a nanometer-scale electron beam size monitor. It probes a
electron beam by an interference fringe pattern formed by split laser beams.
Minimum measurable beam size by this method is less than 1/10 of laser
wavelength. In ATF2, Shintake monitor will be used for the IP beam size monitor
to measure 37 nm (design) beam size. Development status of the Shintake
monitor, including fringe phase monitoring and stabilization, gamma detector
and collimators, is described. In addition, we discuss the beam size
measurement by Shintake monitor in ILC.",pishing monitoring
http://arxiv.org/abs/1502.06904v1,"In this short paper we consider the problem of monitoring physical activity
in the smart house. The authors suggested a simple device that allows medical
staff and relatives to monitor the activity for older adults living alone. This
sensor monitors the switching-on of electrical devices. The fact of switching
is seen as confirmation of physical activity. It is confirmed by SMS
notifications to observers.",pishing monitoring
http://arxiv.org/abs/1701.07484v1,"Our machines, products, utilities, and environments have long been monitored
by embedded software systems. Our professional, commercial, social and personal
lives are also subject to monitoring as they are mediated by software systems.
Data on nearly everything now exists, waiting to be collected and analysed for
all sorts of reasons. Given the rising tide of data we pose the questions: What
is monitoring? Do diverse and disparate monitoring systems have anything in
common? We attempt answer these questions by proposing an abstract conceptual
framework for studying monitoring. We argue that it captures a structure common
to many different monitoring practices, and that from it detailed formal models
can be derived, customised to applications. The framework formalises the idea
that monitoring is a process that observes the behaviour of people and objects
in a context. The entities and their behaviours are represented by abstract
data types and the observable attributes by logics. Since monitoring usually
has a specific purpose, we extend the framework with protocols for detecting
attributes or events that require interventions and, possibly, a change in
behaviour. Our theory is illustrated by a case study from criminal justice,
that of electronic tagging.",pishing monitoring
http://arxiv.org/abs/1708.07229v1,"Runtime Monitoring is a lightweight and dynamic verification technique that
involves observing the internal operations of a software system and/or its
interactions with other external entities, with the aim of determining whether
the system satisfies or violates a correctness specification. Compilation
techniques employed in Runtime Monitoring tools allow monitors to be
automatically derived from high-level correctness specifications (aka.
properties). This allows the same property to be converted into different types
of monitors, which may apply different instrumentation techniques for checking
whether the property was satisfied or not. In this paper we compare and
contrast the various types of monitoring methodologies found in the current
literature, and classify them into a spectrum of monitoring instrumentation
techniques, ranging from completely asynchronous monitoring on the one end and
completely synchronous monitoring on the other.",pishing monitoring
http://arxiv.org/abs/1905.12949v1,"Monitoring of the large-scale data processing of the ATLAS experiment
includes monitoring of production and user analysis jobs. The Experiment
Dashboard provides a common job monitoring solution, which is shared by ATLAS
and CMS experiments. This includes an accounting portal as well as real-time
monitoring. Dashboard job monitoring for ATLAS combines information from the
PanDA job processing database, Production system database and monitoring
information from jobs submitted through GANGA to Workload Management System
(WMS) or local batch systems. Usage of Dashboard-based job monitoring
applications will decrease load on the PanDA database and overcome scale
limitations in PanDA monitoring caused by the short job rotation cycle in the
PanDA database. Aggregation of the task/job metrics from different sources
provides complete view of job processing activity in ATLAS scope.",pishing monitoring
http://arxiv.org/abs/1305.7403v1,"Monitoring is an essential aspect of maintaining and developing computer
systems that increases in difficulty proportional to the size of the system.
The need for robust monitoring tools has become more evident with the advent of
cloud computing. Infrastructure as a Service (IaaS) clouds allow end users to
deploy vast numbers of virtual machines as part of dynamic and transient
architectures. Current monitoring solutions, including many of those in the
open-source domain rely on outdated concepts including manual deployment and
configuration, centralised data collection and adapt poorly to membership
churn.
  In this paper we propose the development of a cloud monitoring suite to
provide scalable and robust lookup, data collection and analysis services for
large-scale cloud systems. In lieu of centrally managed monitoring we propose a
multi-tier architecture using a layered gossip protocol to aggregate monitoring
information and facilitate lookup, information collection and the
identification of redundant capacity. This allows for a resource aware data
collection and storage architecture that operates over the system being
monitored. This in turn enables monitoring to be done in-situ without the need
for significant additional infrastructure to facilitate monitoring services. We
evaluate this approach against alternative monitoring paradigms and demonstrate
how our solution is well adapted to usage in a cloud-computing context.",pishing monitoring
http://arxiv.org/abs/1209.4485v1,"Contemporary high-performance service-oriented applications demand a
performance efficient run-time monitoring. In this paper, we analyze a
hierarchical publish-subscribe architecture for monitoring service-oriented
applications. The analyzed architecture is based on a tree topology and
publish-subscribe communication model for aggregation of distributed monitoring
data. In order to satisfy interoperability and platform independence of
service-orientation, monitoring reports are represented as XML documents. Since
XML formatting introduces a significant processing and network load, we analyze
the performance of monitoring architecture with respect to the number of
monitored nodes, the load of system machines, and the overall latency of the
monitoring system.",pishing monitoring
http://arxiv.org/abs/1607.07780v1,"The smart grid initiative has encouraged utility companies worldwide to
rollout new and smarter versions of energy meters. Before an extensive rollout,
which is both labor-intensive and incurs high capital costs, consumers need to
be incentivized to reap the long-term benefits of smart grid. Off-the-shelf
energy monitors can provide consumers with an insight of such potential
benefits. Since energy monitors are owned by the consumer, the consumer has
greater control over data which significantly reduces privacy and data
confidentiality concerns. We evaluate several existing energy monitors using an
online technical survey and online product literature. For consumers, the use
of different off-the-shelf energy monitors can help demonstrate the potential
gains of smart grid. Our survey indicates a trend towards incorporation of
state-of-the-art capabilities, like appliance level monitoring through load
disaggregation in energy monitors, which can encourage effective consumer
participation. Multiple sensor types and ratings allow some monitors to operate
in various configurations and environments.",pishing monitoring
http://arxiv.org/abs/1412.5767v1,"The increasing complexity of photonic integrated circuits requires the
possibility to monitor the state of the circuit in order to stabilize the
working point against environmental fluctuations or to perform reliable
reconfiguration procedures. Although InP technologies can naturally integrate
high-quality photodiodes, their use as tap monitors necessarily affects the
circuit response and is restricted to few units per chip. They are hence
unsuited for very large circuits, where transparent power monitors become key
components. In this paper we present the implementation of a ContactLess
Integrated Photonic Probe (CLIPP) realizing a non invasive integrated light
monitor on InP-based devices. We describe an innovative vertical scheme of the
CLIPP monitor which exploits the back side of the chip as a common electrode,
thus enabling a reduction of the device footprint and a simplification of the
electrical connectivity. We characterize the response of the CLIPP and
demonstrate its functionality as power monitor. Lastly, we provide a direct
demonstration that CLIPP monitor allows to access more accurate information on
the working point of photonic integrated circuits compared to conventional
external or integrated photodetectors.",pishing monitoring
http://arxiv.org/abs/1807.08203v2,"Runtime monitoring is commonly used to detect the violation of desired
properties in safety critical systems by observing run prefixes of the system.
Bauer et al. introduced an influential framework for monitoring Linear Temporal
Logic (LTL) properties, which is based on a three-valued semantics: the formula
is already satisfied by the given prefix, it is already violated, or it is
still undetermined, i.e., it can be satisfied and violated. However, a wide
range of formulas are not monitorable under this approach, meaning that every
prefix is undetermined. In particular, Bauer et al. report that 44% of the
formulas they consider in their experiments fall into this category.
  Recently, robust semantics for LTL were introduced to capture degrees of
violation of universal properties. Here, we define robust semantics for run
prefixes and show its potential in monitoring: every formula considered by
Bauer et al. is monitorable under our approach. Furthermore, we show that
properties expressed with the robust semantics can be monitored by
deterministic automata.",pishing monitoring
http://arxiv.org/abs/1511.06975v1,"Considering the level of competition prevailing in Business-to-Consumer (B2C)
E-Commerce domain and the huge investments required to attract new customers,
firms are now giving more focus to reduce their customer churn rate. Churn rate
is the ratio of customers who part away with the firm in a specific time
period. One of the best mechanism to retain current customers is to identify
any potential churn and respond fast to prevent it. Detecting early signs of a
potential churn, recognizing what the customer is looking for by the movement
and automating personalized win back campaigns are essential to sustain
business in this era of competition. E-Commerce firms normally possess large
volume of data pertaining to their existing customers like transaction history,
search history, periodicity of purchases, etc. Data mining techniques can be
applied to analyse customer behaviour and to predict the potential customer
attrition so that special marketing strategies can be adopted to retain them.
This paper proposes an integrated model that can predict customer churn and
also recommend personalized win back actions.",detecting e-commerce
http://arxiv.org/abs/1308.3559v1,"Man in the middle attacks are a significant threat to modern e-commerce and
online communications, even when such transactions are protected by TLS. We
intend to show that it is possible to detect man-in-the-middle attacks on SSL
and TLS by detecting timing differences between a standard SSL session and an
attack we created.",detecting e-commerce
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",detecting e-commerce
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",detecting e-commerce
http://arxiv.org/abs/1907.01284v1,"Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.",detecting e-commerce
http://arxiv.org/abs/1507.07382v1,"Classical approaches in recommender systems such as collaborative filtering
are concentrated mainly on static user preference extraction. This approach
works well as an example for music recommendations when a user behavior tends
to be stable over long period of time, however the most common situation in
e-commerce is different which requires reactive algorithms based on a
short-term user activity analysis. This paper introduces a small mathematical
framework for short-term user interest detection formulated in terms of item
properties and its application for recommender systems enhancing. The framework
is based on the fundamental concept of information theory --- Kullback-Leibler
divergence.",detecting e-commerce
http://arxiv.org/abs/1512.04122v1,"The emergence of mobile platforms with increased storage and computing
capabilities and the pervasive use of these platforms for sensitive
applications such as online banking, e-commerce and the storage of sensitive
information on these mobile devices have led to increasing danger associated
with malware targeted at these devices. Detecting such malware presents
inimitable challenges as signature-based detection techniques available today
are becoming inefficient in detecting new and unknown malware. In this
research, a machine learning approach for the detection of malware on Android
platforms is presented. The detection system monitors and extracts features
from the applications while in execution and uses them to perform in-device
detection using a trained K-Nearest Neighbour classifier. Results shows high
performance in the detection rate of the classifier with accuracy of 93.75%,
low error rate of 6.25% and low false positive rate with ability of detecting
real Android malware.",detecting e-commerce
http://arxiv.org/abs/1510.05544v2,"Given a network with attributed edges, how can we identify anomalous
behavior? Networks with edge attributes are commonplace in the real world. For
example, edges in e-commerce networks often indicate how users rated products
and services in terms of number of stars, and edges in online social and
phonecall networks contain temporal information about when friendships were
formed and when users communicated with each other -- in such cases, edge
attributes capture information about how the adjacent nodes interact with other
entities in the network. In this paper, we aim to utilize exactly this
information to discern suspicious from typical node behavior. Our work has a
number of notable contributions, including (a) formulation: while most other
graph-based anomaly detection works use structural graph connectivity or node
information, we focus on the new problem of leveraging edge information, (b)
methodology: we introduce EdgeCentric, an intuitive and scalable
compression-based approach for detecting edge-attributed graph anomalies, and
(c) practicality: we show that EdgeCentric successfully spots numerous such
anomalies in several large, edge-attributed real-world graphs, including the
Flipkart e-commerce graph with over 3 million product reviews between 1.1
million users and 545 thousand products, where it achieved 0.87 precision over
the top 100 results.",detecting e-commerce
http://arxiv.org/abs/1905.02234v2,"In e-commerce, product content, especially product images have a significant
influence on a customer's journey from product discovery to evaluation and
finally, purchase decision. Since many e-commerce retailers sell items from
other third-party marketplace sellers besides their own, the content published
by both internal and external content creators needs to be monitored and
enriched, wherever possible. Despite guidelines and warnings, product listings
that contain offensive and non-compliant images continue to enter catalogs.
Offensive and non-compliant content can include a wide range of objects, logos,
and banners conveying violent, sexually explicit, racist, or promotional
messages. Such images can severely damage the customer experience, lead to
legal issues, and erode the company brand. In this paper, we present a computer
vision driven offensive and non-compliant image detection system for extremely
large image datasets. This paper delves into the unique challenges of applying
deep learning to real-world product image data from retail world. We
demonstrate how we resolve a number of technical challenges such as lack of
training data, severe class imbalance, fine-grained class definitions etc.
using a number of practical yet unique technical strategies. Our system
combines state-of-the-art image classification and object detection techniques
with budgeted crowdsourcing to develop a solution customized for a massive,
diverse, and constantly evolving product catalog.",detecting e-commerce
http://arxiv.org/abs/1811.04374v1,"We present an empirical study of applying deep Convolutional Neural Networks
(CNN) to the task of fashion and apparel image classification to improve
meta-data enrichment of e-commerce applications. Five different CNN
architectures were analyzed using clean and pre-trained models. The models were
evaluated in three different tasks person detection, product and gender
classification, on two small and large scale datasets.",detecting e-commerce
http://arxiv.org/abs/1909.10562v1,"Alibaba has China's largest e-commerce platform. To support its diverse
businesses, Alibaba has its own large-scale data centers providing the
computing foundation for a wide variety of software applications. Among these
applications, deep learning (DL) has been playing an important role in
delivering services like image recognition, objection detection, text
recognition, recommendation, and language processing. To build more efficient
data centers that deliver higher performance for these DL applications, it is
important to understand their computational needs and use that information to
guide the design of future computing infrastructure. An effective way to
achieve this is through benchmarks that can fully represent Alibaba's DL
applications.",detecting e-commerce
http://arxiv.org/abs/1202.1761v1,"Denial of Service (DoS) is a security threat which compromises the
confidentiality of information stored in Local Area Networks (LANs) due to
unauthorized access by spoofed IP addresses. SYN Flooding is a type of DoS
which is harmful to network as the flooding of packets may delay other users
from accessing the server and in severe cases, the server may need to be shut
down, wasting valuable resources, especially in critical real-time services
such as in e-commerce and the medical field. The objective of this paper is to
review the state-of-the art of detection mechanisms for SYN flooding. The
detection schemes for SYN Flooding attacks have been classified broadly into
three categories - detection schemes based on the router data structure,
detection schemes based on statistical analysis of the packet flow and
detection schemes based on artificial intelligence. The advantages and
disadvantages for various detection schemes under each category have been
critically examined. The performance measures of the categories have also been
compared.",detecting e-commerce
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",detecting e-commerce
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",detecting e-commerce
http://arxiv.org/abs/1411.5319v2,"In this work, we propose and address a new computer vision task, which we
call fashion item detection, where the aim is to detect various fashion items a
person in the image is wearing or carrying. The types of fashion items we
consider in this work include hat, glasses, bag, pants, shoes and so on. The
detection of fashion items can be an important first step of various e-commerce
applications for fashion industry. Our method is based on state-of-the-art
object detection method pipeline which combines object proposal methods with a
Deep Convolutional Neural Network. Since the locations of fashion items are in
strong correlation with the locations of body joints positions, we incorporate
contextual information from body poses in order to improve the detection
performance. Through the experiments, we demonstrate the effectiveness of the
proposed method.",detecting e-commerce
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",detecting e-commerce
http://arxiv.org/abs/1904.12574v2,"Learning product representations that reflect complementary relationship
plays a central role in modern recommender system for e-commerce platforms. A
notable challenge is that unlike many simple relationships such as similarity,
complementariness is often detected from customer purchase activities, which
are highly sparse and noisy. Also, standard usage of representation learning
emphasizes on only one set of embedding, which is problematic for modelling the
asymmetric property of complementariness. We propose using context-aware
multi-tasking learning with dual product embedding to solve the above
challenges. We encode contextual knowledge into product representation by
multi-task learning, in order to alleviate the sparsity issue. By explicitly
modelling with user bias terms, we take care of the noise induced by
customer-specific preferences. Furthermore, we adopt the dual embedding
framework to capture the intrinsic properties of complementariness and provide
geometric interpretation motivated by the classic separating hyperplane theory.
Finally, we propose a Bayesian network structure that unifies all the
components, which also concludes several popular models as special cases. The
proposed method compares favourably to state-of-art representation learning and
recommendation algorithms for e-commerce, in downstream classification and
recommendation tasks. We also develop an implementation that scales efficiently
to a dataset with millions of items and customers.",detecting e-commerce
http://arxiv.org/abs/1506.04584v1,"Collaborative filtering recommender systems (CFRSs) are the key components of
successful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks
since its openness. However, since attack size is far smaller than that of
genuine users, conventional supervised learning based detection methods could
be too ""dull"" to handle such imbalanced classification. In this paper, we
improve detection performance from following two aspects. First, we extract
well-designed features from user profiles based on the statistical properties
of the diverse attack models, making hard classification task becomes easier to
perform. Then, refer to the general idea of re-scale Boosting (RBoosting) and
AdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost
(RAdaBoost) as our detection method based on extracted features. RAdaBoost is
comparable to the optimal Boosting-type algorithm and can effectively improve
the performance in some hard scenarios. Finally, a series of experiments on the
MovieLens-100K data set are conducted to demonstrate the outperformance of
RAdaBoost comparing with some classical techniques such as SVM, kNN and
AdaBoost.",detecting e-commerce
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",detecting e-commerce
http://arxiv.org/abs/1002.3333v1,"In this paper, we describe an effective framework for adapting electronic
commerce or e-commerce services in developing countries like Bangladesh. The
internet has opened up a new horizon for commerce, namely electronic commerce
(e-commerce). It entails the use of the internet in the marketing,
identification, payment and delivery of goods and services. At present internet
facilities are available in Bangladesh. Slowly, but steadily these facilities
are holding a strong position in every aspects of our life. E-commerce is one
of those sectors which need more attention if we want to be a part of global
business. Bangladesh is far-far away to adapt the main stream of e-commerce
application. Though government is shouting to take the challenges of
e-commerce, but they do not take the right step, that is why e-commerce dose
not make any real contribution in our socio-economic life. Here we propose a
model which may develop the e-commerce infrastructure of Bangladesh.",detecting e-commerce
http://arxiv.org/abs/1807.04923v1,"Millions of people use online e-commerce platforms to search and buy
products. Identifying attributes in a query is a critical component in
connecting users to relevant items. However, in many cases, the queries have
multiple attributes, and some of them will be in conflict with each other. For
example, the query ""maroon 5 dvds"" has two candidate attributes, the color
""maroon"" or the band ""maroon 5"", where only one of the attributes can be
present. In this paper, we address the problem of resolving conflicting
attributes in e-commerce queries. A challenge in this problem is that knowledge
bases like Wikipedia that are used to understand web queries are not focused on
the e-commerce domain. E-commerce search engines, however, have access to the
catalog which contains detailed information about the items and its attributes.
We propose a framework that constructs knowledge graphs from catalog to resolve
conflicting attributes in e-commerce queries. Our experiments on real-world
queries on e-commerce platforms demonstrate that resolving conflicting
attributes by leveraging catalog information significantly improves attribute
identification, and also gives out more relevant search results.",detecting e-commerce
http://arxiv.org/abs/1602.07662v1,"Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.",detecting e-commerce
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",detecting e-commerce
http://arxiv.org/abs/1804.03836v3,"Anomaly Detection has several important applications. In this paper, our
focus is on detecting anomalies in seller-reviewer data using tensor
decomposition. While tensor-decomposition is mostly unsupervised, we formulate
Bayesian semi-supervised tensor decomposition to take advantage of sparse
labeled data. In addition, we use Polya-Gamma data augmentation for the
semi-supervised Bayesian tensor decomposition. Finally, we show that the
P\'olya-Gamma formulation simplifies calculation of the Fisher information
matrix for partial natural gradient learning. Our experimental results show
that our semi-supervised approach outperforms state of the art unsupervised
baselines. And that the partial natural gradient learning outperforms
stochastic gradient learning and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detecting e-commerce
http://arxiv.org/abs/1905.06112v1,"Recently, Vietnamese Natural Language Processing has been researched by
experts in academic and business. However, the existing papers have been
focused only on information classification or extraction from documents.
Nowadays, with quickly development of the e-commerce websites, forums and
social networks, the products, people, organizations or wonders are targeted of
comments or reviews of the network communities. Many people often use that
reviews to make their decision on something. Whereas, there are many people or
organizations use the reviews to mislead readers. Therefore, it is so necessary
to detect those bad behaviors in reviews. In this paper, we research this
problem and propose an appropriate method for detecting Vietnamese reviews
being spam or non-spam. The accuracy of our method is up to 90%.",detecting e-commerce
http://arxiv.org/abs/1905.06246v2,"Product reviews and ratings on e-commerce websites provide customers with
detailed insights about various aspects of the product such as quality,
usefulness, etc. Since they influence customers' buying decisions, product
reviews have become a fertile ground for abuse by sellers (colluding with
reviewers) to promote their own products or to tarnish the reputation of
competitor's products. In this paper, our focus is on detecting such abusive
entities (both sellers and reviewers) by applying tensor decomposition on the
product reviews data. While tensor decomposition is mostly unsupervised, we
formulate our problem as a semi-supervised binary multi-target tensor
decomposition, to take advantage of currently known abusive entities. We
empirically show that our multi-target semi-supervised model achieves higher
precision and recall in detecting abusive entities as compared to unsupervised
techniques. Finally, we show that our proposed stochastic partial natural
gradient inference for our model empirically achieves faster convergence than
stochastic gradient and Online-EM with sufficient statistics.",detecting e-commerce
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",detecting e-commerce
http://arxiv.org/abs/1506.05752v3,"Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to ""shilling"" attacks or ""profile injection"" attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
""shilling"" attacks or ""profile injection"" attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN.",detecting e-commerce
http://arxiv.org/abs/1102.0706v1,"E-commerce is an emerging technology. Impact of this new technology is
getting clearer with time and results are tangible to the user community. In
this paper we have tried to focus some of its issues like paradigms,
infrastructure integration, and security, which is considered to be the most
important issue in E-Commerce. At first we have elaborated the paradigms of
E-Commerce (Business-to-Business and Business-to-Consumer). Then comes the
necessity of infrastructure integration with the legacy system. Security
concerns comes next. Rest of the part contains conclusion and references.",detecting e-commerce
http://arxiv.org/abs/1407.2421v1,"Rapid evolution of information technology has contributed to the evolution of
more sophisticated E- commerce system with the better transaction time and
protection. The currently used E-commerce models lack in quality properties
such as logical security because of their poor designing and to face the highly
equipped and trained intruders. This editorial proposed a security framework
for small and medium sized E-commerce, based on service oriented architecture
and gives an analysis of the eminent security attacks which can be averted. The
proposed security framework will be implemented and validated on an open source
E-commerce, and the results achieved so far are also presented.",detecting e-commerce
http://arxiv.org/abs/1505.02766v1,"Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.",detecting e-commerce
http://arxiv.org/abs/1505.03315v1,"Article about transformation of methods of remote sales in the conditions of
e-commerce. The author classifies and analyzes features of new methods of
electronic sales in the virtual environment.",detecting e-commerce
http://arxiv.org/abs/1704.08991v1,"Recommendation plays a key role in e-commerce and in the entertainment
industry. We propose to consider successive recommendations to users under the
form of graphs of recommendations. We give models for this representation.
Motivated by the growing interest for algorithmic transparency, we then propose
a first application for those graphs, that is the potential detection of
introduced recommendation bias by the service provider. This application relies
on the analysis of the topology of the extracted graph for a given user; we
propose a notion of recommendation coherence with regards to the topological
proximity of recommended items (under the measure of items' k-closest
neighbors, reminding the ""small-world"" model by Watts & Stroggatz). We finally
illustrate this approach on a model and on Youtube crawls, targeting the
prediction of ""Recommended for you"" links (i.e., biased or not by Youtube).",detecting e-commerce
http://arxiv.org/abs/1901.01183v2,"The e-commerce has started a new trend in natural language processing through
sentiment analysis of user-generated reviews. Different consumers have
different concerns about various aspects of a specific product or service.
Aspect category detection, as a subtask of aspect-based sentiment analysis,
tackles the problem of categorizing a given review sentence into a set of
pre-defined aspect categories. In recent years, deep learning approaches have
brought revolutionary advances in multiple branches of natural language
processing including sentiment analysis. In this paper, we propose a deep
neural network method based on attention mechanism to identify different aspect
categories of a given review sentence. Our model utilizes several attentions
with different topic contexts, enabling it to attend to different parts of a
review sentence based on different topics. Experimental results on two datasets
in the restaurant domain released by SemEval workshop demonstrates that our
approach outperforms existing methods on both datasets. Visualization of the
topic attention weights shows the effectiveness of our model in identifying
words related to different topics.",detecting e-commerce
http://arxiv.org/abs/1906.02325v1,"Classification of personal text messages has many useful applications in
surveillance, e-commerce, and mental health care, to name a few. Giving
applications access to personal texts can easily lead to (un)intentional
privacy violations. We propose the first privacy-preserving solution for text
classification that is provably secure. Our method, which is based on Secure
Multiparty Computation (SMC), encompasses both feature extraction from texts,
and subsequent classification with logistic regression and tree ensembles. We
prove that when using our secure text classification method, the application
does not learn anything about the text, and the author of the text does not
learn anything about the text classification model used by the application
beyond what is given by the classification result itself. We perform end-to-end
experiments with an application for detecting hate speech against women and
immigrants, demonstrating excellent runtime results without loss of accuracy.",detecting e-commerce
http://arxiv.org/abs/1412.3056v1,"User ignorance towards the use of communication services like Instant
Messengers, emails, websites, social networks etc. is becoming the biggest
advantage for phishers. It is required to create technical awareness in users
by educating them to create a phishing detection application which would
generate phishing alerts for the user so that phishing messages are not
ignored. The lack of basic security features to detect and prevent phishing has
had a profound effect on the IM clients, as they lose their faith in e-banking
and e-commerce transactions, which will have a disastrous impact on the
corporate and banking sectors and businesses which rely heavily on the
internet. Very little research contributions were available in for phishing
detection in Instant messengers. A context based, dynamic and intelligent
phishing detection methodology in IMs is proposed, to analyze and detect
phishing in Instant Messages with relevance to domain ontology (OBIE) and
utilizes the Classification based on Association (CBA) for generating phishing
rules and alerting the victims. A PDS Monitoring system algorithm is used to
identify the phishing activity during exchange of messages in IMs, with high
ratio of precision and recall. The results have shown improvement by the
increased percentage of precision and recall when compared to the existing
methods.",detecting e-commerce
http://arxiv.org/abs/1811.03739v1,"Reviews spams are prevalent in e-commerce to manipulate product ranking and
customers decisions maliciously. While spams generated based on simple spamming
strategy can be detected effectively, hardened spammers can evade regular
detectors via more advanced spamming strategies. Previous work gave more
attention to evasion against text and graph-based detectors, but evasions
against behavior-based detectors are largely ignored, leading to
vulnerabilities in spam detection systems. Since real evasion data are scarce,
we first propose EMERAL (Evasion via Maximum Entropy and Rating sAmpLing) to
generate evasive spams to certain existing detectors. EMERAL can simulate
spammers with different goals and levels of knowledge about the detectors,
targeting at different stages of the life cycle of target products. We show
that in the evasion-defense dynamic, only a few evasion types are meaningful to
the spammers, and any spammer will not be able to evade too many detection
signals at the same time. We reveal that some evasions are quite insidious and
can fail all detection signals. We then propose DETER (Defense via Evasion
generaTion using EmeRal), based on model re-training on diverse evasive samples
generated by EMERAL. Experiments confirm that DETER is more accurate in
detecting both suspicious time window and individual spamming reviews. In terms
of security, DETER is versatile enough to be vaccinated against diverse and
unexpected evasions, is agnostic about evasion strategy and can be released
without privacy concern.",detecting e-commerce
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",e-commerce fraud
http://arxiv.org/abs/1709.01213v4,"Although mobile ad frauds have been widespread, state-of-the-art approaches
in the literature have mainly focused on detecting the so-called static
placement frauds, where only a single UI state is involved and can be
identified based on static information such as the size or location of ad
views. Other types of fraud exist that involve multiple UI states and are
performed dynamically while users interact with the app. Such dynamic
interaction frauds, although now widely spread in apps, have not yet been
explored nor addressed in the literature. In this work, we investigate a wide
range of mobile ad frauds to provide a comprehensive taxonomy to the research
community. We then propose, FraudDroid, a novel hybrid approach to detect ad
frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI
state transition graphs and collects their associated runtime network traffics,
which are then leveraged to check against a set of heuristic-based rules for
identifying ad fraudulent behaviours. We show empirically that FraudDroid
detects ad frauds with a high precision (93%) and recall (92%). Experimental
results further show that FraudDroid is capable of detecting ad frauds across
the spectrum of fraud types. By analysing 12,000 ad-supported Android apps,
FraudDroid identified 335 cases of fraud associated with 20 ad networks that
are further confirmed to be true positive results and are shared with our
fellow researchers to promote advanced ad fraud detection",e-commerce fraud
http://arxiv.org/abs/1909.02398v1,"Automated fraud behaviors detection on electronic payment platforms is a
tough problem. Fraud users often exploit the vulnerability of payment platforms
and the carelessness of users to defraud money, steal passwords, do money
laundering, etc, which causes enormous losses to digital payment platforms and
users. There are many challenges for fraud detection in practice. Traditional
fraud detection methods require a large-scale manually labeled dataset, which
is hard to obtain in reality. Manually labeled data cost tremendous human
efforts. Besides, the continuous and rapid evolution of fraud users makes it
hard to find new fraud patterns based on existing detection rules. In our work,
we propose a real-world data oriented detection paradigm which can detect fraud
users and upgrade its detection ability automatically. Based on the new
paradigm, we design a novel fraud detection model, FraudJudger, to analyze
users behaviors on digital payment platforms and detect fraud users with fewer
labeled data in training. FraudJudger can learn the latent representations of
users from unlabeled data with the help of Adversarial Autoencoder (AAE).
Furthermore, FraudJudger can find new fraud patterns from unknown users by
cluster analysis. Our experiment is based on a real-world electronic payment
dataset. Comparing with other well-known fraud detection methods, FraudJudger
can achieve better detection performance with only 10% labeled data.",e-commerce fraud
http://arxiv.org/abs/1309.3944v1,"With an upsurge in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection (FAFD) has become an
emerging topic of great importance for academic, research and industries. The
failure of internal auditing system of the organization in identifying the
accounting frauds has lead to use of specialized procedures to detect financial
accounting fraud, collective known as forensic accounting. Data mining
techniques are providing great aid in financial accounting fraud detection,
since dealing with the large data volumes and complexities of financial data
are big challenges for forensic accounting. This paper presents a comprehensive
review of the literature on the application of data mining techniques for the
detection of financial accounting fraud and proposes a framework for data
mining techniques based accounting fraud detection. The systematic and
comprehensive literature review of the data mining techniques applicable to
financial accounting fraud detection may provide a foundation to future
research in this field. The findings of this review show that data mining
techniques like logistic models, neural networks, Bayesian belief network, and
decision trees have been applied most extensively to provide primary solutions
to the problems inherent in the detection and classification of fraudulent
data.",e-commerce fraud
http://arxiv.org/abs/1907.03048v1,"Download fraud is a prevalent threat in mobile App markets, where fraudsters
manipulate the number of downloads of Apps via various cheating approaches.
Purchased fake downloads can mislead recommendation and search algorithms and
further lead to bad user experience in App markets. In this paper, we
investigate download fraud problem based on a company's App Market, which is
one of the most popular Android App markets. We release a honeypot App on the
App Market and purchase fake downloads from fraudster agents to track fraud
activities in the wild. Based on our interaction with the fraudsters, we
categorize download fraud activities into three types according to their
intentions: boosting front end downloads, optimizing App search ranking, and
enhancing user acquisition&retention rate. For the download fraud aimed at
optimizing App search ranking, we select, evaluate, and validate several
features in identifying fake downloads based on billions of download data. To
get a comprehensive understanding of download fraud, we further gather stances
of App marketers, fraudster agencies, and market operators on download fraud.
The followed analysis and suggestions shed light on the ways to mitigate
download fraud in App markets and other social platforms. To the best of our
knowledge, this is the first work that investigates the download fraud problem
in mobile App markets.",e-commerce fraud
http://arxiv.org/abs/1601.01228v1,"Financial fraud detection is an important problem with a number of design
aspects to consider. Issues such as algorithm selection and performance
analysis will affect the perceived ability of proposed solutions, so for
auditors and re-searchers to be able to sufficiently detect financial fraud it
is necessary that these issues be thoroughly explored. In this paper we will
revisit the key performance metrics used for financial fraud detection with a
focus on credit card fraud, critiquing the prevailing ideas and offering our
own understandings. There are many different performance metrics that have been
employed in prior financial fraud detection research. We will analyse several
of the popular metrics and compare their effectiveness at measuring the ability
of detection mechanisms. We further investigated the performance of a range of
computational intelligence techniques when applied to this problem domain, and
explored the efficacy of several binary classification methods.",e-commerce fraud
http://arxiv.org/abs/1503.05172v1,"This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",e-commerce fraud
http://arxiv.org/abs/1407.2423v1,"Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.",e-commerce fraud
http://arxiv.org/abs/1505.03398v1,"E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.",e-commerce fraud
http://arxiv.org/abs/0803.4058v3,"Typical arguments against scientific misconduct generally fail to support
current policies on research fraud: they may not prove wrong what is usually
considered research misconduct and they tend to make wrong things that are not
normally seen as scientific fraud, in particular honest errors. I also point
out that sanctions are not consistent with the reasons why scientific fraud is
supposed to be wrong either. Moreover honestly seeking truth should not be
contrived as a moral rule -- it is instead a necessary condition for work to
qualify as scientific.
  Keywords: cheating; ethics; fabrication; falsification; integrity;
plagiarism; research fraud; scientific misconduct.",e-commerce fraud
http://arxiv.org/abs/1510.07167v1,"Financial statement fraud detection is an important problem with a number of
design aspects to consider. Issues such as (i) problem representation, (ii)
feature selection, and (iii) choice of performance metrics all influence the
perceived performance of detection algorithms. Efficient implementation of
financial fraud detection methods relies on a clear understanding of these
issues. In this paper we present an analysis of the three key experimental
issues associated with financial statement fraud detection, critiquing the
prevailing ideas and providing new understandings.",e-commerce fraud
http://arxiv.org/abs/1808.07288v1,"Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.",e-commerce fraud
http://arxiv.org/abs/0910.2048v1,"This brief paper outlines how spreadsheets were used as one of the vehicles
for John Rusnak's fraud and the revenue control lessons this case gives us.",e-commerce fraud
http://arxiv.org/abs/1611.06439v1,"Credit card plays a very important rule in today's economy. It becomes an
unavoidable part of household, business and global activities. Although using
credit cards provides enormous benefits when used carefully and
responsibly,significant credit and financial damages may be caused by
fraudulent activities. Many techniques have been proposed to confront the
growth in credit card fraud. However, all of these techniques have the same
goal of avoiding the credit card fraud; each one has its own drawbacks,
advantages and characteristics. In this paper, after investigating difficulties
of credit card fraud detection, we seek to review the state of the art in
credit card fraud detection techniques, data sets and evaluation criteria.The
advantages and disadvantages of fraud detection methods are enumerated and
compared.Furthermore, a classification of mentioned techniques into two main
fraud detection approaches, namely, misuses (supervised) and anomaly detection
(unsupervised) is presented. Again, a classification of techniques is proposed
based on capability to process the numerical and categorical data sets.
Different data sets used in literature are then described and grouped into real
and synthesized data and the effective and common attributes are extracted for
further usage.Moreover, evaluation employed criterions in literature are
collected and discussed.Consequently, open issues for credit card fraud
detection are explained as guidelines for new researchers.",e-commerce fraud
http://arxiv.org/abs/1304.6501v1,"Occupational fraud affects many companies worldwide causing them economic
loss and liability issues towards their customers and other involved entities.
Detecting internal fraud in a company requires significant effort and,
unfortunately cannot be entirely prevented. The internal auditors have to
process a huge amount of data produced by diverse systems, which are in most
cases in textual form, with little automated support. In this paper, we exploit
the advantages of information visualization and present a system that aims to
detect occupational fraud in systems which involve a pair of entities (e.g., an
employee and a client) and periodic activity. The main visualization is based
on a spiral system on which the events are drawn appropriately according to
their time-stamp. Suspicious events are considered those which appear along the
same radius or on close radii of the spiral. Before producing the
visualization, the system ranks both involved entities according to the
specifications of the internal auditor and generates a video file of the
activity such that events with strong evidence of fraud appear first in the
video. The system is also equipped with several different visualizations and
mechanisms in order to meet the requirements of an internal fraud detection
system.",e-commerce fraud
http://arxiv.org/abs/1405.5704v1,"Contactless technologies such as RFID, NFC, and sensor networks are
vulnerable to mafia and distance frauds. Both frauds aim at passing an
authentication protocol by cheating on the actual distance between the prover
and the verifier. To cope these security issues, distance-bounding protocols
have been designed. However, none of the current proposals simultaneously
resists to these two frauds without requiring additional memory and
computation. The situation is even worse considering that just a few
distance-bounding protocols are able to deal with the inherent background noise
on the communication channels. This article introduces a noise-resilient
distance-bounding protocol that resists to both mafia and distance frauds. The
security of the protocol is analyzed with respect to these two frauds in both
scenarios, namely noisy and noiseless channels. Analytical expressions for the
adversary's success probabilities are provided, and are illustrated by
experimental results. The analysis, performed in an already existing framework
for fairness reasons, demonstrates the undeniable advantage of the introduced
lightweight design over the previous proposals.",e-commerce fraud
http://arxiv.org/abs/1804.07481v1,"Credit card fraud detection is a very challenging problem because of the
specific nature of transaction data and the labeling process. The transaction
data is peculiar because they are obtained in a streaming fashion, they are
strongly imbalanced and prone to non-stationarity. The labeling is the outcome
of an active learning process, as every day human investigators contact only a
small number of cardholders (associated to the riskiest transactions) and
obtain the class (fraud or genuine) of the related transactions. An adequate
selection of the set of cardholders is therefore crucial for an efficient fraud
detection process. In this paper, we present a number of active learning
strategies and we investigate their fraud detection accuracies. We compare
different criteria (supervised, semi-supervised and unsupervised) to query
unlabeled transactions. Finally, we highlight the existence of an
exploitation/exploration trade-off for active learning in the context of fraud
detection, which has so far been overlooked in the literature.",e-commerce fraud
http://arxiv.org/abs/1811.08212v1,"The automatic detection of frauds in banking transactions has been recently
studied as a way to help the analysts finding fraudulent operations. Due to the
availability of a human feedback, this task has been studied in the framework
of active learning: the fraud predictor is allowed to sequentially call on an
oracle. This human intervention is used to label new examples and improve the
classification accuracy of the latter. Such a setting is not adapted in the
case of fraud detection with financial data in European countries. Actually, as
a human verification is mandatory to consider a fraud as really detected, it is
not necessary to focus on improving the classifier. We introduce the setting of
'Computer-assisted fraud detection' where the goal is to minimize the number of
non fraudulent operations submitted to an oracle. The existing methods are
applied to this task and we show that a simple meta-algorithm provides
competitive results in this scenario on benchmark datasets.",e-commerce fraud
http://arxiv.org/abs/1909.01486v2,"Machine learning has automated much of financial fraud detection, notifying
firms of, or even blocking, questionable transactions instantly. However, data
imbalance starves traditionally trained models of the content necessary to
detect fraud. This study examines three separate factors of credit card fraud
detection via machine learning. First, it assesses the potential for different
sampling methods, undersampling and Synthetic Minority Oversampling Technique
(SMOTE), to improve algorithm performance in data-starved environments.
Additionally, five industry-practical machine learning algorithms are evaluated
on total fraud cost savings in addition to traditional statistical metrics.
Finally, an ensemble of individual models is trained with a genetic algorithm
to attempt to generate higher cost efficiency than its components. Monte Carlo
performance distributions discerned random undersampling outperformed SMOTE in
lowering fraud costs, and that an ensemble was unable to outperform its
individual parts. Most notably,the F-1 Score, a traditional metric often used
to measure performance with imbalanced data, was uncorrelated with derived cost
efficiency. Assuming a realistic cost structure can be derived, cost-based
metrics provide an essential supplement to objective statistical evaluation.",e-commerce fraud
http://arxiv.org/abs/1002.3333v1,"In this paper, we describe an effective framework for adapting electronic
commerce or e-commerce services in developing countries like Bangladesh. The
internet has opened up a new horizon for commerce, namely electronic commerce
(e-commerce). It entails the use of the internet in the marketing,
identification, payment and delivery of goods and services. At present internet
facilities are available in Bangladesh. Slowly, but steadily these facilities
are holding a strong position in every aspects of our life. E-commerce is one
of those sectors which need more attention if we want to be a part of global
business. Bangladesh is far-far away to adapt the main stream of e-commerce
application. Though government is shouting to take the challenges of
e-commerce, but they do not take the right step, that is why e-commerce dose
not make any real contribution in our socio-economic life. Here we propose a
model which may develop the e-commerce infrastructure of Bangladesh.",e-commerce fraud
http://arxiv.org/abs/1807.04923v1,"Millions of people use online e-commerce platforms to search and buy
products. Identifying attributes in a query is a critical component in
connecting users to relevant items. However, in many cases, the queries have
multiple attributes, and some of them will be in conflict with each other. For
example, the query ""maroon 5 dvds"" has two candidate attributes, the color
""maroon"" or the band ""maroon 5"", where only one of the attributes can be
present. In this paper, we address the problem of resolving conflicting
attributes in e-commerce queries. A challenge in this problem is that knowledge
bases like Wikipedia that are used to understand web queries are not focused on
the e-commerce domain. E-commerce search engines, however, have access to the
catalog which contains detailed information about the items and its attributes.
We propose a framework that constructs knowledge graphs from catalog to resolve
conflicting attributes in e-commerce queries. Our experiments on real-world
queries on e-commerce platforms demonstrate that resolving conflicting
attributes by leveraging catalog information significantly improves attribute
identification, and also gives out more relevant search results.",e-commerce fraud
http://arxiv.org/abs/1602.07662v1,"Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.",e-commerce fraud
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",e-commerce fraud
http://arxiv.org/abs/1903.00733v2,"Advertising is a primary means for revenue generation for millions of
websites and smartphone apps (publishers). Naturally, a fraction of publishers
abuse the ad-network to systematically defraud advertisers of their money.
Defenses have matured to overcome some forms of click fraud but are inadequate
against the threat of organic click fraud attacks. Malware detection systems
including honeypots fail to stop click fraud apps; ad-network filters are
better but measurement studies have reported that a third of the clicks
supplied by ad-networks are fake; collaborations between ad-networks and app
stores that bad-lists malicious apps works better still, but fails to prevent
criminals from writing fraudulent apps which they monetise until they get
banned and start over again. This work develops novel inference techniques that
can isolate click fraud attacks using their fundamental properties. In the {\em
mimicry defence}, we leverage the observation that organic click fraud involves
the re-use of legitimate clicks. Thus we can isolate fake-clicks by detecting
patterns of click-reuse within ad-network clickstreams with historical
behaviour serving as a baseline. Second, in {\em bait-click defence}. we
leverage the vantage point of an ad-network to inject a pattern of bait clicks
into the user's device, to trigger click fraud-apps that are gated on
user-behaviour. Our experiments show that the mimicry defence detects around
81\% of fake-clicks in stealthy (low rate) attacks with a false-positive rate
of 110110 per hundred thousand clicks. Bait-click defence enables further
improvements in detection rates of 95\% and reduction in false-positive rates
of between 0 and 30 clicks per million, a substantial improvement over current
approaches.",e-commerce fraud
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online law enforcement
http://arxiv.org/abs/1612.05030v1,"Synchronous programming is a paradigm of choice for the design of
safety-critical reactive systems. Runtime enforcement is a technique to ensure
that the output of a black-box system satisfies some desired properties. This
paper deals with the problem of runtime enforcement in the context of
synchronous programs. We propose a framework where an enforcer monitors both
the inputs and the outputs of a synchronous program and (minimally) edits
erroneous inputs/outputs in order to guarantee that a given property holds. We
define enforceability conditions, develop an online enforcement algorithm, and
prove its correctness. We also report on an implementation of the algorithm on
top of the KIELER framework for the SCCharts synchronous language. Experimental
results show that enforcement has minimal execution time overhead, which
decreases proportionally with larger benchmarks.",online law enforcement
http://arxiv.org/abs/1701.01911v2,"Exemplar-based face sketch synthesis plays an important role in both digital
entertainment and law enforcement. It generally consists of two parts: neighbor
selection and reconstruction weight representation. The most time-consuming or
main computation complexity for exemplar-based face sketch synthesis methods
lies in the neighbor selection process. State-of-the-art face sketch synthesis
methods perform neighbor selection online in a data-driven manner by $K$
nearest neighbor ($K$-NN) searching. Actually, the online search increases the
time consuming for synthesis. Moreover, since these methods need to traverse
the whole training dataset for neighbor selection, the computational complexity
increases with the scale of the training database and hence these methods have
limited scalability. In this paper, we proposed a simple but effective offline
random sampling in place of online $K$-NN search to improve the synthesis
efficiency. Extensive experiments on public face sketch databases demonstrate
the superiority of the proposed method in comparison to state-of-the-art
methods, in terms of both synthesis quality and time consumption. The proposed
method could be extended to other heterogeneous face image transformation
problems such as face hallucination. We release the source codes of our
proposed methods and the evaluation metrics for future study online:
http://www.ihitworld.com/RSLCR.html.",online law enforcement
http://arxiv.org/abs/1509.06659v3,"Human trafficking is a challenging law enforcement problem, and a large
amount of such activity manifests itself on various online forums. Given the
large, heterogeneous and noisy structure of this data, building models to
predict instances of trafficking is an even more convolved a task. In this
paper we propose and entity resolution pipeline using a notion of proxy labels,
in order to extract clusters from this data with prior history of human
trafficking activity. We apply this pipeline to 5M records from backpage.com
and report on the performance of this approach, challenges in terms of
scalability, and some significant domain specific characteristics of our
resolved entities.",online law enforcement
http://arxiv.org/abs/1902.06961v1,"Cybercrime investigators face numerous challenges when policing online
crimes. Firstly, the methods and processes they use when dealing with
traditional crimes do not necessarily apply in the cyber-world. Additionally,
cyber criminals are usually technologically-aware and constantly adapting and
developing new tools that allow them to stay ahead of law enforcement
investigations. In order to provide adequate support for cybercrime
investigators, there needs to be a better understanding of the challenges they
face at both technical and socio-technical levels. In this paper, we
investigate this problem through an analysis of current practices and workflows
of investigators. We use interviews with experts from government and private
sectors who investigate cybercrimes as our main data gathering process. From an
analysis of the collected data, we identify several outstanding challenges
faced by investigators. These pertain to practical, technical, and social
issues such as systems availability, usability, and in computer-supported
collaborative work. Importantly, we use our findings to highlight research
areas where user-centric workflows and tools are desirable. We also define a
set of recommendations that can aid in providing a better foundation for future
research in the field and allow more effective combating of cybercrimes.",online law enforcement
http://arxiv.org/abs/1404.1295v1,"The study of criminal networks using traces from heterogeneous communication
media is acquiring increasing importance in nowadays society. The usage of
communication media such as phone calls and online social networks leaves
digital traces in the form of metadata that can be used for this type of
analysis. The goal of this work is twofold: first we provide a theoretical
framework for the problem of detecting and characterizing criminal
organizations in networks reconstructed from phone call records. Then, we
introduce an expert system to support law enforcement agencies in the task of
unveiling the underlying structure of criminal networks hidden in communication
data. This platform allows for statistical network analysis, community
detection and visual exploration of mobile phone network data. It allows
forensic investigators to deeply understand hierarchies within criminal
organizations, discovering members who play central role and provide connection
among sub-groups. Our work concludes illustrating the adoption of our
computational framework for a real-word criminal investigation.",online law enforcement
http://arxiv.org/abs/1603.07823v1,"Face sketch synthesis has wide applications ranging from digital
entertainments to law enforcements. Objective image quality assessment scores
and face recognition accuracy are two mainly used tools to evaluate the
synthesis performance. In this paper, we proposed a synthesized face sketch
recognition framework based on full-reference image quality assessment metrics.
Synthesized sketches generated from four state-of-the-art methods are utilized
to test the performance of the proposed recognition framework. For the image
quality assessment metrics, we employed the classical structured similarity
index metric and other three prevalent metrics: visual information fidelity,
feature similarity index metric and gradient magnitude similarity deviation.
Extensive experiments compared with baseline methods illustrate the
effectiveness of the proposed synthesized face sketch recognition framework.
Data and implementation code in this paper are available online at
www.ihitworld.com/WNN/IQA_Sketch.zip.",online law enforcement
http://arxiv.org/abs/1712.03086v1,"In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.",online law enforcement
http://arxiv.org/abs/1801.07207v1,"Security incidents such as targeted distributed denial of service (DDoS)
attacks on power grids and hacking of factory industrial control systems (ICS)
are on the increase. This paper unpacks where emerging security risks lie for
the industrial internet of things, drawing on both technical and regulatory
perspectives. Legal changes are being ushered by the European Union (EU)
Network and Information Security (NIS) Directive 2016 and the General Data
Protection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use
the case study of the emergent smart energy supply chain to frame, scope out
and consolidate the breadth of security concerns at play, and the regulatory
responses. We argue the industrial IoT brings four security concerns to the
fore, namely: appreciating the shift from offline to online infrastructure;
managing temporal dimensions of security; addressing the implementation gap for
best practice; and engaging with infrastructural complexity. Our goal is to
surface risks and foster dialogue to avoid the emergence of an Internet of
Insecure Industrial Things",online law enforcement
http://arxiv.org/abs/1810.03965v1,"We present an algorithm for realtime anomaly detection in low to medium
density crowd videos using trajectory-level behavior learning. Our formulation
combines online tracking algorithms from computer vision, non-linear pedestrian
motion models from crowd simulation, and Bayesian learning techniques to
automatically compute the trajectory-level pedestrian behaviors for each agent
in the video. These learned behaviors are used to segment the trajectories and
motions of different pedestrians or agents and detect anomalies. We demonstrate
the interactive performance on the PETS ARENA dataset as well as indoor and
outdoor crowd video benchmarks consisting of tens of human agents. We also
discuss the implications of recent public policy and law enforcement issues
relating to surveillance and our research.",online law enforcement
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",online law enforcement
http://arxiv.org/abs/1909.00912v2,"Neural networks can emulate non-linear physical systems with high accuracy,
yet they may produce physically-inconsistent results when violating fundamental
constraints. In this letter, we introduce a systematic way of enforcing
analytic constraints in neural networks via constraints in the architecture or
the loss function. Applied to the modeling of convective processes for climate
modeling, architectural constraints can enforce conservation laws to within
machine precision without degrading performance. Furthermore, enforcing
constraints can reduce the error of variables closely related to the
constraints.",online law enforcement
http://arxiv.org/abs/1801.04565v1,"Data retrieval systems such as online search engines and online social
networks must comply with the privacy policies of personal and selectively
shared data items, regulatory policies regarding data retention and censorship,
and the provider's own policies regarding data use. Enforcing these policies is
difficult and error-prone. Systematic techniques to enforce policies are either
limited to type-based policies that apply uniformly to all data of the same
type, or incur significant runtime overhead.
  This paper presents Shai, the first system that systematically enforces
data-specific policies with near-zero overhead in the common case. Shai's key
idea is to push as many policy checks as possible to an offline, ahead-of-time
analysis phase, often relying on predicted values of runtime parameters such as
the state of access control lists or connected users' attributes. Runtime
interception is used sparingly, only to verify these predictions and to make
any remaining policy checks. Our prototype implementation relies on efficient,
modern OS primitives for sandboxing and isolation. We present the design of
Shai and quantify its overheads on an experimental data indexing and search
pipeline based on the popular search engine Apache Lucene.",online law enforcement
http://arxiv.org/abs/1302.3946v1,"We consider the classical online scheduling problem P||C_{max} in which jobs
are released over list and provide a nearly optimal online algorithm. More
precisely, an online algorithm whose competitive ratio is at most (1+\epsilon)
times that of an optimal online algorithm could be achieved in polynomial time,
where m, the number of machines, is a part of the input. It substantially
improves upon the previous results by almost closing the gap between the
currently best known lower bound of 1.88 (Rudin, Ph.D thesis, 2001) and the
best known upper bound of 1.92 (Fleischer, Wahl, Journal of Scheduling, 2000).
It has been known by folklore that an online problem could be viewed as a game
between an adversary and the online player. Our approach extensively explores
such a structure and builds up a completely new framework to show that, for the
online over list scheduling problem, given any \epsilon>0, there exists a
uniform threshold K which is polynomial in m such that if the competitive ratio
of an online algorithm is \rho<=2, then there exists a list of at most K jobs
to enforce the online algorithm to achieve a competitive ratio of at least
\rho-O(\epsilon). Our approach is substantially different from that of Gunther
et al. (Gunther et al., SODA 2013), in which an approximation scheme for online
over time scheduling problems is given, where the number of machines is fixed.
Our method could also be extended to several related online over list
scheduling models.",online law enforcement
http://arxiv.org/abs/1705.04480v1,"While online services emerge in all areas of life, the voting procedure in
many democracies remains paper-based as the security of current online voting
technology is highly disputed. We address the issue of trustworthy online
voting protocols and recall therefore their security concepts with its trust
assumptions. Inspired by the Bitcoin protocol, the prospects of distributed
online voting protocols are analysed. No trusted authority is assumed to ensure
ballot secrecy. Further, the integrity of the voting is enforced by all voters
themselves and without a weakest link, the protocol becomes more robust. We
introduce a taxonomy of notions of distribution in online voting protocols that
we apply on selected online voting protocols. Accordingly, blockchain-based
protocols seem to be promising for online voting due to their similarity with
paper-based protocols.",online law enforcement
http://arxiv.org/abs/1909.09754v1,"This work proposes an approach for latent dynamics learning that exactly
enforces physical conservation laws. The method comprises two steps. First, we
compute a low-dimensional embedding of the high-dimensional dynamical-system
state using deep convolutional autoencoders. This defines a low-dimensional
nonlinear manifold on which the state is subsequently enforced to evolve.
Second, we define a latent dynamics model that associates with a constrained
optimization problem. Specifically, the objective function is defined as the
sum of squares of conservation-law violations over control volumes in a
finite-volume discretization of the problem; nonlinear equality constraints
explicitly enforce conservation over prescribed subdomains of the problem. The
resulting dynamics model-which can be considered as a projection-based
reduced-order model-ensures that the time-evolution of the latent state exactly
satisfies conservation laws over the prescribed subdomains. In contrast to
existing methods for latent dynamics learning, this is the only method that
both employs a nonlinear embedding and computes dynamics for the latent state
that guarantee the satisfaction of prescribed physical properties. Numerical
experiments on a benchmark advection problem illustrate the method's ability to
significantly reduce the dimensionality while enforcing physical conservation.",online law enforcement
http://arxiv.org/abs/1609.07602v1,"With an exponentially increasing usage of cloud services, the need for
forensic investigations of virtual space is equally in constantly increasing
demand, which includes as a very first approach, the gaining of access to it as
well as the data stored. This is an aspect that faces a number of challenges,
stemming not only from the technical difficulties and peculiarities, but
equally covers the interaction with an emerging line of businesses offering
cloud storage and services. Beyond the forensic aspects, it also covers to an
ever increasing amount the non-forensic considerations, such as the
availability of logs and archives, legal and data protection considerations
from a global perspective and the clashes in between, as well as the ever
competing interests between law enforcement to seize evidence which is
non-physical, and businesses who need to be able to continue to operate and
provide their hosted services, even if law enforcement seek to collect
evidence. The trend post-Snowden has been unequivocally towards default
encryption, and driven by market leaders such as Apple, motivated to a large
extent by the perceived demands for privacy of the consumer. The central
question to be explored in this paper is to what extent this trend towards
default encryption will have a negative impact on law enforcement
investigations and possibilities, and will at the end attempt to provide a
solution, which takes into account the needs of both law enforcement, but also
of the service providers. It is hoped that the recommendations from this paper
will be able to have an impact in the ability for law enforcement to continue
with their investigations in an efficient manner, whilst also safeguarding the
ability for business to thrive and continue to develop and offer new and
innovative solutions, which do not put law enforcement at risk.",online law enforcement
http://arxiv.org/abs/1403.6315v2,"The spread of rumors through social media and online social networks can not
only disrupt the daily lives of citizens but also result in loss of life and
property. A rumor spreads when individuals, who are unable decide the
authenticity of the information, mistake the rumor as genuine information and
pass it on to their acquaintances. We propose a solution where a set of
individuals (based on their degree) in the social network are trained and
provided resources to help them distinguish a rumor from genuine information.
By formulating an optimization problem we calculate the optimum set of
individuals, who must undergo training, and the quality of training that
minimizes the expected training cost and ensures an upper bound on the size of
the rumor outbreak. Our primary contribution is that although the optimization
problem turns out to be non convex, we show that the problem is equivalent to
solving a set of linear programs. This result also allows us to solve the
problem of minimizing the size of rumor outbreak for a given cost budget. The
optimum solution displays an interesting pattern which can be implemented as a
heuristic. These results can prove to be very useful for social planners and
law enforcement agencies for preventing dangerous rumors and misinformation
epidemics.",online law enforcement
http://arxiv.org/abs/1401.5178v1,"The economics of an internet crime has newly developed into a field of
controlling black money. This economic approach not only provides estimated
technique of analyzing internet crimes but also gives details to analyzers of
system dependability and divergence. This paper will highlight on the subject
of online crime, which has formed its industry since. It all started from
amateur hackers who cracked websites and wrote malicious software in pursuit of
fun or achieving limited objectives to professional hacking. In the past days,
electronic fraud was main objective but now it has been changed into electronic
hacking. This study focuses the issue through an economic analysis of available
web forum to deals in malware and private information. The findings of this
survey research provide considerable in-depth sight into the functions of
malware economy spinning around computer impositions and compromise. In this
regard, the survey research paper may benefit particularly computer security
officials, the law enforcement agencies, and in general prospective anyone
involved in better understanding cybercrime from the offender standpoint.",online law enforcement
http://arxiv.org/abs/1712.00846v1,"Web-based human trafficking activity has increased in recent years but it
remains sparsely dispersed among escort advertisements and difficult to
identify due to its often-latent nature. The use of intelligent systems to
detect trafficking can thus have a direct impact on investigative resource
allocation and decision-making, and, more broadly, help curb a widespread
social problem. Trafficking detection involves assigning a normalized score to
a set of escort advertisements crawled from the Web -- a higher score indicates
a greater risk of trafficking-related (involuntary) activities. In this paper,
we define and study the problem of trafficking detection and present a
trafficking detection pipeline architecture developed over three years of
research within the DARPA Memex program. Drawing on multi-institutional data,
systems, and experiences collected during this time, we also conduct post hoc
bias analyses and present a bias mitigation plan. Our findings show that, while
automatic trafficking detection is an important application of AI for social
good, it also provides cautionary lessons for deploying predictive machine
learning algorithms without appropriate de-biasing. This ultimately led to
integration of an interpretable solution into a search system that contains
over 100 million advertisements and is used by over 200 law enforcement
agencies to investigate leads.",online law enforcement
http://arxiv.org/abs/1504.01093v1,"We consider dynamic pricing schemes in online settings where selfish agents
generate online events. Previous work on online mechanisms has dealt almost
entirely with the goal of maximizing social welfare or revenue in an auction
settings. This paper deals with quite general settings and minimizing social
costs. We show that appropriately computed posted prices allow one to achieve
essentially the same performance as the best online algorithm. This holds in a
wide variety of settings. Unlike online algorithms that learn about the event,
and then make enforceable decisions, prices are posted without knowing the
future events or even the current event, and are thus inherently dominant
strategy incentive compatible.
  In particular we show that one can give efficient posted price mechanisms for
metrical task systems, some instances of the $k$-server problem, and metrical
matching problems. We give both deterministic and randomized algorithms. Such
posted price mechanisms decrease the social cost dramatically over selfish
behavior where no decision incurs a charge. One alluring application of this is
reducing the social cost of free parking exponentially.",online law enforcement
http://arxiv.org/abs/1907.12860v1,"Websites are constantly adapting the methods used, and intensity with which
they track online visitors. However, the wide-range enforcement of GDPR since
one year ago (May 2018) forced websites serving EU-based online visitors to
eliminate or at least reduce such tracking activity, given they receive proper
user consent. Therefore, it is important to record and analyze the evolution of
this tracking activity and assess the overall ""privacy health"" of the Web
ecosystem and if it is better after GDPR enforcement. This work makes a
significant step towards this direction. In this paper, we analyze the online
ecosystem of 3rd-parties embedded in top websites which amass the majority of
online tracking through 6 time snapshots taken every few months apart, in the
duration of the last 2 years. We perform this analysis in three ways: 1) by
looking into the network activity that 3rd-parties impose on each publisher
hosting them, 2) by constructing a bipartite graph of ""publisher-to-tracker"",
connecting 3rd parties with their publishers, 3) by constructing a
""tracker-to-tracker"" graph connecting 3rd-parties who are commonly found in
publishers. We record significant changes through time in number of trackers,
traffic induced in publishers (incoming vs. outgoing), embeddedness of trackers
in publishers, popularity and mixture of trackers across publishers. We also
report how such measures compare with the ranking of publishers based on Alexa.
On the last level of our analysis, we dig deeper and look into the connectivity
of trackers with each other and how this relates to potential cookie
synchronization activity.",online law enforcement
http://arxiv.org/abs/1703.10764v1,"Global optimization algorithms have shown impressive performance in
data-association based multi-object tracking, but handling online data remains
a difficult hurdle to overcome. In this paper, we present a hybrid data
association framework with a min-cost multi-commodity network flow for robust
online multi-object tracking. We build local target-specific models interleaved
with global optimization of the optimal data association over multiple video
frames. More specifically, in the min-cost multi-commodity network flow, the
target-specific similarities are online learned to enforce the local
consistency for reducing the complexity of the global data association.
Meanwhile, the global data association taking multiple video frames into
account alleviates irrecoverable errors caused by the local data association
between adjacent frames. To ensure the efficiency of online tracking, we give
an efficient near-optimal solution to the proposed min-cost multi-commodity
flow problem, and provide the empirical proof of its sub-optimality. The
comprehensive experiments on real data demonstrate the superior tracking
performance of our approach in various challenging situations.",online law enforcement
http://arxiv.org/abs/1310.0505v1,"Online social networks such as Twitter and Facebook have gained tremendous
popularity for information exchange. The availability of unprecedented amounts
of digital data has accelerated research on information diffusion in online
social networks. However, the mechanism of information spreading in online
social networks remains elusive due to the complexity of social interactions
and rapid change of online social networks. Much of prior work on information
diffusion over online social networks has based on empirical and statistical
approaches. The majority of dynamical models arising from information diffusion
over online social networks involve ordinary differential equations which only
depend on time. In a number of recent papers, the authors propose to use
partial differential equations(PDEs) to characterize temporal and spatial
patterns of information diffusion over online social networks. Built on
intuitive cyber-distances such as friendship hops in online social networks,
the reaction-diffusion equations take into account influences from various
external out-of-network sources, such as the mainstream media, and provide a
new analytic framework to study the interplay of structural and topical
influences on information diffusion over online social networks. In this
survey, we discuss a number of PDE-based models that are validated with real
datasets collected from popular online social networks such as Digg and
Twitter. Some new developments including the conservation law of information
flow in online social networks and information propagation speeds based on
traveling wave solutions are presented to solidify the foundation of the PDE
models and highlight the new opportunities and challenges for mathematicians as
well as computer scientists and researchers in online social networks.",online law enforcement
http://arxiv.org/abs/1907.12221v1,"We increasingly live in a world where there is a balance between the rights
to privacy and the requirements for consent, and the rights of society to
protect itself. Within this world, there is an ever-increasing requirement to
protect the identities involved within financial transactions, but this makes
things increasingly difficult for law enforcement agencies, especially in terms
of financial fraud and money laundering. This paper reviews the
state-of-the-art in terms of the methods of privacy that are being used within
cryptocurrency transactions, and in the challenges that law enforcement face.",online law enforcement
http://arxiv.org/abs/1709.03581v1,"This work evaluates a novel structured method for reporting crime and
registering crime scene data by comparing the method with traditional
text-based crime reports. Swedish law enforcement officers were asked to
register a residential burglary using both methods in order to measure the
effectiveness of each method. The effectiveness was quantified as both the time
it took to register the crime as well as the number of relevant unique crime
scene details that were collected. The results show the structured method to be
significantly more efficient than traditional text-based crime reports (p <
0.05). Also, the novel method registers 2.96 times more relevant and unique
crime scene details on average. In addition to the differences in efficiency
this work also discusses more qualitative pros and cons with the novel method,
as well as various data analysis methods that could be used on the registered
data. It is concluded that the novel method can benefit law enforcement
agencies in two ways. First, related to increased quality and more elaborate
crime reports. Secondly, related to significant time-savings, which according
to a rough estimate is expect to lie in the range 11-30 full-time positions per
year for Swedish law enforcement agencies, particularly for volume crime
categories. Those resources could instead be used for unburden the workload for
our law enforcement officers, and to some extent free resources that could be
used in other parts of the organization.
  Note: this pre-print manuscript is written in Swedish.",online law enforcement
http://arxiv.org/abs/1704.00784v2,"Recurrent neural network models with an attention mechanism have proven to be
extremely effective on a wide variety of sequence-to-sequence problems.
However, the fact that soft attention mechanisms perform a pass over the entire
input sequence when producing each element in the output sequence precludes
their use in online settings and results in a quadratic time complexity. Based
on the insight that the alignment between input and output sequence elements is
monotonic in many problems of interest, we propose an end-to-end differentiable
method for learning monotonic alignments which, at test time, enables computing
attention online and in linear time. We validate our approach on sentence
summarization, machine translation, and online speech recognition problems and
achieve results competitive with existing sequence-to-sequence models.",online law enforcement
http://arxiv.org/abs/1710.08217v1,"There is an extensive literature about online controlled experiments, both on
the statistical methods available to analyze experiment results as well as on
the infrastructure built by several large scale Internet companies but also on
the organizational challenges of embracing online experiments to inform product
development. At Booking.com we have been conducting evidenced based product
development using online experiments for more than ten years. Our methods and
infrastructure were designed from their inception to reflect Booking.com
culture, that is, with democratization and decentralization of experimentation
and decision making in mind.
  In this paper we explain how building a central repository of successes and
failures to allow for knowledge sharing, having a generic and extensible code
library which enforces a loose coupling between experimentation and business
logic, monitoring closely and transparently the quality and the reliability of
the data gathering pipelines to build trust in the experimentation
infrastructure, and putting in place safeguards to enable anyone to have end to
end ownership of their experiments have allowed such a large organization as
Booking.com to truly and successfully democratize experimentation.",online law enforcement
http://arxiv.org/abs/1804.03461v3,"A new era of Information Warfare has arrived. Various actors, including
state-sponsored ones, are weaponizing information on Online Social Networks to
run false information campaigns with targeted manipulation of public opinion on
specific topics. These false information campaigns can have dire consequences
to the public: mutating their opinions and actions, especially with respect to
critical world events like major elections. Evidently, the problem of false
information on the Web is a crucial one, and needs increased public awareness,
as well as immediate attention from law enforcement agencies, public
institutions, and in particular, the research community. In this paper, we make
a step in this direction by providing a typology of the Web's false information
ecosystem, comprising various types of false information, actors, and their
motives. We report a comprehensive overview of existing research on the false
information ecosystem by identifying several lines of work: 1) how the public
perceives false information; 2) understanding the propagation of false
information; 3) detecting and containing false information on the Web; and 4)
false information on the political stage. In this work, we pay particular
attention to political false information as: 1) it can have dire consequences
to the community (e.g., when election results are mutated) and 2) previous work
show that this type of false information propagates faster and further when
compared to other types of false information. Finally, for each of these lines
of work, we report several future research directions that can help us better
understand and mitigate the emerging problem of false information dissemination
on the Web.",online law enforcement
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",online shopping fraud
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",online shopping fraud
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping fraud
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping fraud
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping fraud
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",online shopping fraud
http://arxiv.org/abs/1906.06977v1,"Machine learning and data mining techniques have been used extensively in
order to detect credit card frauds. However purchase behaviour and fraudster
strategies may change over time. This phenomenon is named dataset shift or
concept drift in the domain of fraud detection. In this paper, we present a
method to quantify day-by-day the dataset shift in our face-to-face credit card
transactions dataset (card holder located in the shop) . In practice, we
classify the days against each other and measure the efficiency of the
classification. The more efficient the classification, the more different the
buying behaviour between two days, and vice versa. Therefore, we obtain a
distance matrix characterizing the dataset shift. After an agglomerative
clustering of the distance matrix, we observe that the dataset shift pattern
matches the calendar events for this time period (holidays, week-ends, etc). We
then incorporate this dataset shift knowledge in the credit card fraud
detection task as a new feature. This leads to a small improvement of the
detection.",online shopping fraud
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",online shopping fraud
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping fraud
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",online shopping fraud
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping fraud
http://arxiv.org/abs/0801.2700v1,"Labels and tags are accompanying us in almost each moment of our life and
everywhere we are going, in the form of electronic keys or money, or simply as
labels on products we are buying in shops and markets. The label diffusion,
rapidly increasing for logistic reasons in the actual global market, carries
huge amount of information but it is demanding security and anti-fraud systems.
The first crucial point, for the consumer and producer safety, is to ensure the
authenticity of the labelled products with systems against counterfeiting and
piracy. Recent anti-fraud techniques are based on a sophisticated use of
physical effects, from holograms till magnetic resonance or tunnel transitions
between atomic sublevels. In this paper we will discuss labels and anti-fraud
technologies as a new and very promising research field for applied physics.",online shopping fraud
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",online shopping fraud
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",online shopping fraud
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",online shopping fraud
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping fraud
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",online shopping fraud
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",online shopping fraud
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",online shopping fraud
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",online shopping fraud
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",online shopping fraud
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping fraud
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping fraud
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",online shopping fraud
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",online shopping fraud
http://arxiv.org/abs/1906.04272v3,"Given the magnitude of online auction transactions, it is difficult to
safeguard consumers from dishonest sellers, such as shill bidders. To date, the
application of Machine Learning Techniques (MLTs) to auction fraud has been
limited, unlike their applications for combatting other types of fraud. Shill
Bidding (SB) is a severe auction fraud, which is driven by modern-day
technologies and clever scammers. The difficulty of identifying the behavior of
sophisticated fraudsters and the unavailability of training datasets hinder the
research on SB detection. In this study, we developed a high-quality SB
dataset. To do so, first, we crawled and preprocessed a large number of
commercial auctions and bidders' history as well. We thoroughly preprocessed
both datasets to make them usable for the computation of the SB metrics.
Nevertheless, this operation requires a deep understanding of the behavior of
auctions and bidders. Second, we introduced two new SB pattern s and
implemented other existing SB patterns. Finally, we removed outliers to improve
the quality of training SB data.",online shopping fraud
http://arxiv.org/abs/1803.01798v2,"Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.",online shopping fraud
http://arxiv.org/abs/1505.07922v1,"We address the problem of cross-domain image retrieval, considering the
following practical application: given a user photo depicting a clothing image,
our goal is to retrieve the same or attribute-similar clothing items from
online shopping stores. This is a challenging problem due to the large
discrepancy between online shopping images, usually taken in ideal
lighting/pose/background conditions, and user photos captured in uncontrolled
conditions. To address this problem, we propose a Dual Attribute-aware Ranking
Network (DARN) for retrieval feature learning. More specifically, DARN consists
of two sub-networks, one for each domain, whose retrieval feature
representations are driven by semantic attribute learning. We show that this
attribute-guided learning is a key factor for retrieval accuracy improvement.
In addition, to further align with the nature of the retrieval problem, we
impose a triplet visual similarity constraint for learning to rank across the
two sub-networks. Another contribution of our work is a large-scale dataset
which makes the network learning feasible. We exploit customer review websites
to crawl a large set of online shopping images and corresponding offline user
photos with fine-grained clothing attributes, i.e., around 450,000 online
shopping images and about 90,000 exact offline counterpart images of those
online ones. All these images are collected from real-world consumer websites
reflecting the diversity of the data modality, which makes this dataset unique
and rare in the academic community. We extensively evaluate the retrieval
performance of networks in different configurations. The top-20 retrieval
accuracy is doubled when using the proposed DARN other than the current popular
solution using pre-trained CNN features only (0.570 vs. 0.268).",online shopping fraud
http://arxiv.org/abs/1809.04683v2,"Many online platforms have deployed anti-fraud systems to detect and prevent
fraudulent activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, which maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe the user suspended time instead of the
fraudulent activity time in the training data, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.",online shopping fraud
http://arxiv.org/abs/1811.02385v1,"The ability to correctly classify and retrieve apparel images has a variety
of applications important to e-commerce, online advertising and internet
search. In this work, we propose a robust framework for fine-grained apparel
classification, in-shop and cross-domain retrieval which eliminates the
requirement of rich annotations like bounding boxes and human-joints or
clothing landmarks, and training of bounding box/ key-landmark detector for the
same. Factors such as subtle appearance differences, variations in human poses,
different shooting angles, apparel deformations, and self-occlusion add to the
challenges in classification and retrieval of apparel items. Cross-domain
retrieval is even harder due to the presence of large variation between online
shopping images, usually taken in ideal lighting, pose, positive angle and
clean background as compared with street photos captured by users in
complicated conditions with poor lighting and cluttered scenes. Our framework
uses compact bilinear CNN with tensor sketch algorithm to generate embeddings
that capture local pairwise feature interactions in a translationally invariant
manner. For apparel classification, we pass the feature embeddings through a
softmax classifier, while, the in-shop and cross-domain retrieval pipelines use
a triplet-loss based optimization approach, such that squared Euclidean
distance between embeddings measures the dissimilarity between the images.
Unlike previous works that relied on bounding box, key clothing landmarks or
human joint detectors to assist the final deep classifier, proposed framework
can be trained directly on the provided category labels or generated triplets
for triplet loss optimization. Lastly, Experimental results on the DeepFashion
fine-grained categorization, and in-shop and consumer-to-shop retrieval
datasets provide a comparative analysis with previous work performed in the
domain.",online shopping fraud
http://arxiv.org/abs/1811.08502v1,"The last decade has witnessed an explosion on the computational power and a
parallel increase of the access to large sets of data (the so called Big Data
paradigm) which is enabling to develop brand new quantitative strategies
underpinning description, understanding and control of complex scenarios. One
interesting area of application concerns fraud detection from online data, and
more particularly extracting meaningful information from massive digital
fingerprints of electoral activity to detect, a posteriori, evidence of
fraudulent behavior. In this short article we discuss a few quantitative
methodologies that have emerged in recent years on this respect, which
altogether form the nascent interdisciplinary field of election forensics.",online shopping fraud
http://arxiv.org/abs/1901.04140v1,"In the current field of computer vision, automatically generating texts from
given images has been a fully worked technique. Up till now, most works of this
area focus on image content describing, namely image-captioning. However, rare
researches focus on generating product review texts, which is ubiquitous in the
online shopping malls and is crucial for online shopping selection and
evaluation. Different from content describing, review texts include more
subjective information of customers, which may bring difference to the results.
Therefore, we aimed at a new field concerning generating review text from
customers based on images together with the ratings of online shopping
products, which appear as non-image attributes. We made several adjustments to
the existing image-captioning model to fit our task, in which we should also
take non-image features into consideration. We also did experiments based on
our model and get effective primary results.",online shopping fraud
http://arxiv.org/abs/1908.04240v1,"Detecting concept drift is a well known problem that affects production
systems. However, two important issues that are frequently not addressed in the
literature are 1) the detection of drift when the labels are not immediately
available; and 2) the automatic generation of explanations to identify possible
causes for the drift. For example, a fraud detection model in online payments
could show a drift due to a hot sale item (with an increase in false positives)
or due to a true fraud attack (with an increase in false negatives) before
labels are available. In this paper we propose SAMM, an automatic model
monitoring system for data streams. SAMM detects concept drift using a time and
space efficient unsupervised streaming algorithm and it generates alarm reports
with a summary of the events and features that are important to explain it.
SAMM was evaluated in five real world fraud detection datasets, each spanning
periods up to eight months and totaling more than 22 million online
transactions. We evaluated SAMM using human feedback from domain experts, by
sending them 100 reports generated by the system. Our results show that SAMM is
able to detect anomalous events in a model life cycle that are considered
useful by the domain experts. Given these results, SAMM will be rolled out in a
next version of Feedzai's Fraud Detection solution.",online shopping fraud
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",online shopping fraud
http://arxiv.org/abs/1212.5959v1,"The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., ""expensive products do not come with more effort in terms of
purchase"") and expected (e.g., ""the more loyal a user is to an online shop, the
less effort they spend shopping"") facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.",online shopping law enforcement
http://arxiv.org/abs/1512.02372v1,"The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages",online shopping law enforcement
http://arxiv.org/abs/1301.0963v1,"The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop",online shopping law enforcement
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",online shopping law enforcement
http://arxiv.org/abs/1705.10786v1,"Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement",online shopping law enforcement
http://arxiv.org/abs/1611.03915v2,"With the prevalence of e-commence websites and the ease of online shopping,
consumers are embracing huge amounts of various options in products.
Undeniably, shopping is one of the most essential activities in our society and
studying consumer's shopping behavior is important for the industry as well as
sociology and psychology. Indisputable, one of the most popular e-commerce
categories is clothing business. There arises the needs for analysis of popular
and attractive clothing features which could further boost many emerging
applications, such as clothing recommendation and advertising. In this work, we
design a novel system that consists of three major components: 1) exploring and
organizing a large-scale clothing dataset from a online shopping website, 2)
pruning and extracting images of best-selling products in clothing item data
and user transaction history, and 3) utilizing a machine learning based
approach to discovering fine-grained clothing attributes as the representative
and discriminative characteristics of popular clothing style elements. Through
the experiments over a large-scale online clothing shopping dataset, we
demonstrate the effectiveness of our proposed system, and obtain useful
insights on clothing consumption trends and profitable clothing features.",online shopping law enforcement
http://arxiv.org/abs/1907.05853v1,"Smart gadgets are being embedded almost in every aspect of our lives. From
smart cities to smart watches, modern industries are increasingly supporting
the Internet-of-Things (IoT). SysMART aims at making supermarkets smart,
productive, and with a touch of modern lifestyle. While similar implementations
to improve the shopping experience exists, they tend mainly to replace the
shopping activity at the store with online shopping. Although online shopping
reduces time and effort, it deprives customers from enjoying the experience.
SysMART relies on cutting-edge devices and technology to simplify and reduce
the time required during grocery shopping inside the supermarket. In addition,
the system monitors and maintains perishable products in good condition
suitable for human consumption. SysMART is built using state-of-the-art
technologies that support rapid prototyping and precision data acquisition. The
selected development environment is LabVIEW with its world-class interfacing
libraries. The paper comprises a detailed system description, development
strategy, interface design, software engineering, and a thorough analysis and
evaluation.",online shopping law enforcement
http://arxiv.org/abs/1807.05381v1,"Physical retailers, who once led the way in tracking with loyalty cards and
`reverse appends', now lag behind online competitors. Yet we might be seeing
these tables turn, as many increasingly deploy technologies ranging from simple
sensors to advanced emotion detection systems, even enabling them to tailor
prices and shopping experiences on a per-customer basis. Here, we examine these
in-store tracking technologies in the retail context, and evaluate them from
both technical and regulatory standpoints. We first introduce the relevant
technologies in context, before considering privacy impacts, the current
remedies individuals might seek through technology and the law, and those
remedies' limitations. To illustrate challenging tensions in this space we
consider the feasibility of technical and legal approaches to both a) the
recent `Go' store concept from Amazon which requires fine-grained, multi-modal
tracking to function as a shop, and b) current challenges in opting in or out
of increasingly pervasive passive Wi-Fi tracking. The `Go' store presents
significant challenges with its legality in Europe significantly unclear and
unilateral, technical measures to avoid biometric tracking likely ineffective.
In the case of MAC addresses, we see a difficult-to-reconcile clash between
privacy-as-confidentiality and privacy-as-control, and suggest a technical
framework which might help balance the two. Significant challenges exist when
seeking to balance personalisation with privacy, and researchers must work
together, including across the boundaries of preferred privacy definitions, to
come up with solutions that draw on both technology and the legal frameworks to
provide effective and proportionate protection. Retailers, simultaneously, must
ensure that their tracking is not just legal, but worthy of the trust of
concerned data subjects.",online shopping law enforcement
http://arxiv.org/abs/1305.3213v1,"As the number of online shopping websites increases day by day, so are the
online advertisement strategies and promotional techniques. The number of
people who uses internet keeps on increasing daily and it has become a vast
marketplace to promote products, surely it will be a prime reason to drive any
companies growth in the future.This paper primarily focuses on the areas on
which online shopping lags product promotion and customer retention. Sellers
must concentrate on the areas in which online marketing lags product promotion
techniques; also they should introduce new strategies to increase their market
share to gain customers attention towards their products.",online shopping law enforcement
http://arxiv.org/abs/1711.04626v1,"This research aimed at investigating the impact of website features and
involvement on immediate online shopping. The research is applied in terms of
type and it is causative in terms of methodology. The statistical population
consisted of all citizens of Tabriz, who have purchased clothes online at least
once and 260 individuals were chosen randomly and the questionnaires were
collected according to this sample size. The data were collected by
questionnaire. For analysis of the data, software SPSS and for test of the
model hypotheses, SEM was used by confirmatory factor analysis. The results
showed that the website benefit-oriented features have a positive impact on
immediate online shopping and website benefit-oriented features have no
significant impact on immediate online shopping.",online shopping law enforcement
http://arxiv.org/abs/1607.08691v2,"Human trafficking is among the most challenging law enforcement problems
which demands persistent fight against from all over the globe. In this study,
we leverage readily available data from the website ""Backpage""-- used for
classified advertisement-- to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related
advertisements. Due to the lack of ground truth, we rely on two human analysts
--one human trafficking victim survivor and one from law enforcement, for
hand-labeling the small portion of the crawled data. We then present a
semi-supervised learning approach that is trained on the available labeled and
unlabeled data and evaluated on unseen data with further verification of
experts.",online shopping law enforcement
http://arxiv.org/abs/1505.07922v1,"We address the problem of cross-domain image retrieval, considering the
following practical application: given a user photo depicting a clothing image,
our goal is to retrieve the same or attribute-similar clothing items from
online shopping stores. This is a challenging problem due to the large
discrepancy between online shopping images, usually taken in ideal
lighting/pose/background conditions, and user photos captured in uncontrolled
conditions. To address this problem, we propose a Dual Attribute-aware Ranking
Network (DARN) for retrieval feature learning. More specifically, DARN consists
of two sub-networks, one for each domain, whose retrieval feature
representations are driven by semantic attribute learning. We show that this
attribute-guided learning is a key factor for retrieval accuracy improvement.
In addition, to further align with the nature of the retrieval problem, we
impose a triplet visual similarity constraint for learning to rank across the
two sub-networks. Another contribution of our work is a large-scale dataset
which makes the network learning feasible. We exploit customer review websites
to crawl a large set of online shopping images and corresponding offline user
photos with fine-grained clothing attributes, i.e., around 450,000 online
shopping images and about 90,000 exact offline counterpart images of those
online ones. All these images are collected from real-world consumer websites
reflecting the diversity of the data modality, which makes this dataset unique
and rare in the academic community. We extensively evaluate the retrieval
performance of networks in different configurations. The top-20 retrieval
accuracy is doubled when using the proposed DARN other than the current popular
solution using pre-trained CNN features only (0.570 vs. 0.268).",online shopping law enforcement
http://arxiv.org/abs/1811.02385v1,"The ability to correctly classify and retrieve apparel images has a variety
of applications important to e-commerce, online advertising and internet
search. In this work, we propose a robust framework for fine-grained apparel
classification, in-shop and cross-domain retrieval which eliminates the
requirement of rich annotations like bounding boxes and human-joints or
clothing landmarks, and training of bounding box/ key-landmark detector for the
same. Factors such as subtle appearance differences, variations in human poses,
different shooting angles, apparel deformations, and self-occlusion add to the
challenges in classification and retrieval of apparel items. Cross-domain
retrieval is even harder due to the presence of large variation between online
shopping images, usually taken in ideal lighting, pose, positive angle and
clean background as compared with street photos captured by users in
complicated conditions with poor lighting and cluttered scenes. Our framework
uses compact bilinear CNN with tensor sketch algorithm to generate embeddings
that capture local pairwise feature interactions in a translationally invariant
manner. For apparel classification, we pass the feature embeddings through a
softmax classifier, while, the in-shop and cross-domain retrieval pipelines use
a triplet-loss based optimization approach, such that squared Euclidean
distance between embeddings measures the dissimilarity between the images.
Unlike previous works that relied on bounding box, key clothing landmarks or
human joint detectors to assist the final deep classifier, proposed framework
can be trained directly on the provided category labels or generated triplets
for triplet loss optimization. Lastly, Experimental results on the DeepFashion
fine-grained categorization, and in-shop and consumer-to-shop retrieval
datasets provide a comparative analysis with previous work performed in the
domain.",online shopping law enforcement
http://arxiv.org/abs/1901.04140v1,"In the current field of computer vision, automatically generating texts from
given images has been a fully worked technique. Up till now, most works of this
area focus on image content describing, namely image-captioning. However, rare
researches focus on generating product review texts, which is ubiquitous in the
online shopping malls and is crucial for online shopping selection and
evaluation. Different from content describing, review texts include more
subjective information of customers, which may bring difference to the results.
Therefore, we aimed at a new field concerning generating review text from
customers based on images together with the ratings of online shopping
products, which appear as non-image attributes. We made several adjustments to
the existing image-captioning model to fit our task, in which we should also
take non-image features into consideration. We also did experiments based on
our model and get effective primary results.",online shopping law enforcement
http://arxiv.org/abs/1301.4916v1,"Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.",online shopping law enforcement
http://arxiv.org/abs/1812.07143v1,"Shopping is difficult for people with motor impairments. This includes online
shopping. Proprietary software can emulate mouse and keyboard via head
tracking. However, such a solution is not common for smartphones. Unlike
desktop and laptop computers, they are also much easier to carry indoors and
outdoors.To address this, we implement and open source button that is sensitive
to head movements tracked from the front camera of iPhone X. This allows
developers to integrate in eCommerce applications easily without requiring
specialized knowledge. Other applications include gaming and use in hands-free
situations such as during cooking, auto-repair. We built a sample online
shopping application that allows users to easily browse between items from
various categories and take relevant action just by head movements. We present
results of user studies on this sample application and also include sensitivity
studies based on two independent tests performed at 3 different distances to
the screen.",online shopping law enforcement
http://arxiv.org/abs/1610.00248v1,"In everyday life. Technological advancement can be found in many facets of
life, including personal computers, mobile devices, wearables, cloud services,
video gaming, web-powered messaging, social media, Internet-connected devices,
etc. This technological influence has resulted in these technologies being
employed by criminals to conduct a range of crimes -- both online and offline.
Both the number of cases requiring digital forensic analysis and the sheer
volume of information to be processed in each case has increased rapidly in
recent years. As a result, the requirement for digital forensic investigation
has ballooned, and law enforcement agencies throughout the world are scrambling
to address this demand. While more and more members of law enforcement are
being trained to perform the required investigations, the supply is not keeping
up with the demand. Current digital forensic techniques are arduously
time-consuming and require a significant amount of man power to execute. This
paper discusses a novel solution to combat the digital forensic backlog. This
solution leverages a deduplication-based paradigm to eliminate the
reacquisition, redundant storage, and reanalysis of previously processed data.",online shopping law enforcement
http://arxiv.org/abs/1612.01603v1,"In this paper, we propose a SaaS service which prevents shoplifting using
image analysis and ERP. In Japan, total damage of shoplifting reaches 450
billion yen and more than 1000 small shops gave up their businesses because of
shoplifting. Based on recent cloud technology and data analysis technology, we
propose a shoplifting prevention service with image analysis of security camera
and ERP data check for small shops. We evaluated stream analysis of security
camera movie using online machine learining framework Jubatus.",online shopping law enforcement
http://arxiv.org/abs/1703.07371v1,"Today, huge amount of data is available on the web. Now there is a need to
convert that data in knowledge which can be useful for different purposes. This
paper depicts the use of data mining process, OLAP with the combination of
multi agent system to find the knowledge from data in cloud computing. For
this, I am also trying to explain one case study of online shopping of one
Bakery Shop. May be we can increase the sale of items by using the model, which
I am trying to represent.",online shopping law enforcement
http://arxiv.org/abs/1509.07170v1,"We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.",online shopping law enforcement
http://arxiv.org/abs/1809.06044v4,"Annotating blockchains with auxiliary data is useful for many applications.
For example, e-crime investigations of illegal Tor hidden services, such as
Silk Road, often involve linking Bitcoin addresses, from which money is sent or
received, to user accounts and related online activities. We present BlockTag,
an open-source tagging system for blockchains that facilitates such tasks. We
describe BlockTag's design and present three analyses that illustrate its
capabilities in the context of privacy research and law enforcement.",online shopping law enforcement
http://arxiv.org/abs/1708.00991v1,"Online elections make a natural target for distributed denial of service
attacks. Election agencies wary of disruptions to voting may procure DDoS
protection services from a cloud provider. However, current DDoS detection and
mitigation methods come at the cost of significantly increased trust in the
cloud provider. In this paper we examine the security implications of
denial-of-service prevention in the context of the 2017 state election in
Western Australia, revealing a complex interaction between actors and
infrastructure extending far beyond its borders.
  Based on the publicly observable properties of this deployment, we outline
several attack scenarios including one that could allow a nation state to
acquire the credentials necessary to man-in-the-middle a foreign election in
the context of an unrelated domestic law enforcement or national security
operation, and we argue that a fundamental tension currently exists between
trust and availability in online elections.",online shopping law enforcement
http://arxiv.org/abs/1804.05287v2,"In recent years, both online retail and video hosting service are
exponentially growing. In this paper, we explore a new cross-domain task,
Video2Shop, targeting for matching clothes appeared in videos to the exact same
items in online shops. A novel deep neural network, called AsymNet, is proposed
to explore this problem. For the image side, well-established methods are used
to detect and extract features for clothing patches with arbitrary sizes. For
the video side, deep visual features are extracted from detected object regions
in each frame, and further fed into a Long Short-Term Memory (LSTM) framework
for sequence modeling, which captures the temporal dynamics in videos. To
conduct exact matching between videos and online shopping images, LSTM hidden
states, representing the video, and image features, which represent static
object images, are jointly modeled under the similarity network with
reconfigurable deep tree structure. Moreover, an approximate training method is
proposed to achieve the efficiency when training. Extensive experiments
conducted on a large cross-domain dataset have demonstrated the effectiveness
and efficiency of the proposed AsymNet, which outperforms the state-of-the-art
methods.",online shopping law enforcement
http://arxiv.org/abs/1610.05562v1,"An important goal of online comparison shopping services is to ""convert"" a
viewer from general product category pages (for example product groups such as
""smartphones"" or ""air-conditioners"") to detailed product pages and ultimately
to order pages. Comparison shopping websites provide a familiar web interface
as well as a chance for consumers to purchase items at competitive prices. In
return for providing access to a large market of potential consumers, the
comparison shopping service usually receives financial compensation for product
clicks and orders. This study looked at 2.5 million product listing visits at
price.com.hk to determine whether a modification in the way prices are
displayed on general category pages resulted in more ""conversions"" to product
detail pages. We found a statistically significant improvement over-all as a
result of the new price display resulting in 3.6% more product clicks over all
categories. Additional analysis showed that the effect is heterogeneous among
different categories, and in a few cases there may be some categories
negatively affected by the display modification.",online shopping law enforcement
http://arxiv.org/abs/1806.11423v1,"While shopping for fashion products, customers usually prefer to try-out
products to examine fit, material, overall look and feel. Due to lack of try
out options during online shopping, it becomes pivotal to provide customers
with as much of this information as possible to enhance their shopping
experience. Also it becomes essential to provide same experience for new
customers. Our work here focuses on providing a production ready size
recommendation system for shoes and address the challenge of providing
recommendation for users with no previous purchases on the platform. In our
work, we present a probabilistic approach based on user co-purchase data
facilitated by generating a brand-brand relationship graph. Specifically we
address two challenges that are commonly faced while implementing such
solution. 1. Sparse signals for less popular or new products in the system 2.
Extending the solution for new users. Further we compare and contrast this
approach with our previous work and show significant improvement both in
recommendation precision and coverage.",online shopping law enforcement
http://arxiv.org/abs/1402.0582v1,"We address a dynamic repair shop scheduling problem in the context of
military aircraft fleet management where the goal is to maintain a full
complement of aircraft over the long-term. A number of flights, each with a
requirement for a specific number and type of aircraft, are already scheduled
over a long horizon. We need to assign aircraft to flights and schedule repair
activities while considering the flights requirements, repair capacity, and
aircraft failures. The number of aircraft awaiting repair dynamically changes
over time due to failures and it is therefore necessary to rebuild the repair
schedule online. To solve the problem, we view the dynamic repair shop as
successive static repair scheduling sub-problems over shorter time periods. We
propose a complete approach based on the logic-based Benders decomposition to
solve the static sub-problems, and design different rescheduling policies to
schedule the dynamic repair shop. Computational experiments demonstrate that
the Benders model is able to find and prove optimal solutions on average four
times faster than a mixed integer programming model. The rescheduling approach
having both aspects of scheduling over a longer horizon and quickly adjusting
the schedule increases aircraft available in the long term by 10% compared to
the approaches having either one of the aspects alone.",online shopping law enforcement
http://arxiv.org/abs/1612.05030v1,"Synchronous programming is a paradigm of choice for the design of
safety-critical reactive systems. Runtime enforcement is a technique to ensure
that the output of a black-box system satisfies some desired properties. This
paper deals with the problem of runtime enforcement in the context of
synchronous programs. We propose a framework where an enforcer monitors both
the inputs and the outputs of a synchronous program and (minimally) edits
erroneous inputs/outputs in order to guarantee that a given property holds. We
define enforceability conditions, develop an online enforcement algorithm, and
prove its correctness. We also report on an implementation of the algorithm on
top of the KIELER framework for the SCCharts synchronous language. Experimental
results show that enforcement has minimal execution time overhead, which
decreases proportionally with larger benchmarks.",online shopping law enforcement
http://arxiv.org/abs/1512.04912v1,"Consumer spending accounts for a large fraction of the US economic activity.
Increasingly, consumer activity is moving to the web, where digital traces of
shopping and purchases provide valuable data about consumer behavior. We
analyze these data extracted from emails and combine them with demographic
information to characterize, model, and predict consumer behavior. Breaking
down purchasing by age and gender, we find that the amount of money spent on
online purchases grows sharply with age, peaking in late 30s. Men are more
frequent online purchasers and spend more money when compared to women. Linking
online shopping to income, we find that shoppers from more affluent areas
purchase more expensive items and buy them more frequently, resulting in
significantly more money spent on online purchases. We also look at dynamics of
purchasing behavior and observe daily and weekly cycles in purchasing behavior,
similarly to other online activities.
  More specifically, we observe temporal patterns in purchasing behavior
suggesting shoppers have finite budgets: the more expensive an item, the longer
the shopper waits since the last purchase to buy it. We also observe that
shoppers who email each other purchase more similar items than socially
unconnected shoppers, and this effect is particularly evident among women.
Finally, we build a model to predict when shoppers will make a purchase and how
much will spend on it. We find that temporal features improve prediction
accuracy over competitive baselines. A better understanding of consumer
behavior can help improve marketing efforts and make online shopping more
pleasant and efficient.",online shopping law enforcement
http://arxiv.org/abs/1701.01911v2,"Exemplar-based face sketch synthesis plays an important role in both digital
entertainment and law enforcement. It generally consists of two parts: neighbor
selection and reconstruction weight representation. The most time-consuming or
main computation complexity for exemplar-based face sketch synthesis methods
lies in the neighbor selection process. State-of-the-art face sketch synthesis
methods perform neighbor selection online in a data-driven manner by $K$
nearest neighbor ($K$-NN) searching. Actually, the online search increases the
time consuming for synthesis. Moreover, since these methods need to traverse
the whole training dataset for neighbor selection, the computational complexity
increases with the scale of the training database and hence these methods have
limited scalability. In this paper, we proposed a simple but effective offline
random sampling in place of online $K$-NN search to improve the synthesis
efficiency. Extensive experiments on public face sketch databases demonstrate
the superiority of the proposed method in comparison to state-of-the-art
methods, in terms of both synthesis quality and time consumption. The proposed
method could be extended to other heterogeneous face image transformation
problems such as face hallucination. We release the source codes of our
proposed methods and the evaluation metrics for future study online:
http://www.ihitworld.com/RSLCR.html.",online shopping law enforcement
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",online shopping law enforcement
http://arxiv.org/abs/1509.06659v3,"Human trafficking is a challenging law enforcement problem, and a large
amount of such activity manifests itself on various online forums. Given the
large, heterogeneous and noisy structure of this data, building models to
predict instances of trafficking is an even more convolved a task. In this
paper we propose and entity resolution pipeline using a notion of proxy labels,
in order to extract clusters from this data with prior history of human
trafficking activity. We apply this pipeline to 5M records from backpage.com
and report on the performance of this approach, challenges in terms of
scalability, and some significant domain specific characteristics of our
resolved entities.",online shopping law enforcement
http://arxiv.org/abs/0802.0105v1,"In a convenience store chain, a tail of the cumulative density function of
the expenditure of a person during a single shopping trip follows a power law
with an exponent of -2.5. The exponent is independent of the location of the
store, the shopper's age, the day of week, and the time of day.",online shopping law enforcement
http://arxiv.org/abs/1902.06961v1,"Cybercrime investigators face numerous challenges when policing online
crimes. Firstly, the methods and processes they use when dealing with
traditional crimes do not necessarily apply in the cyber-world. Additionally,
cyber criminals are usually technologically-aware and constantly adapting and
developing new tools that allow them to stay ahead of law enforcement
investigations. In order to provide adequate support for cybercrime
investigators, there needs to be a better understanding of the challenges they
face at both technical and socio-technical levels. In this paper, we
investigate this problem through an analysis of current practices and workflows
of investigators. We use interviews with experts from government and private
sectors who investigate cybercrimes as our main data gathering process. From an
analysis of the collected data, we identify several outstanding challenges
faced by investigators. These pertain to practical, technical, and social
issues such as systems availability, usability, and in computer-supported
collaborative work. Importantly, we use our findings to highlight research
areas where user-centric workflows and tools are desirable. We also define a
set of recommendations that can aid in providing a better foundation for future
research in the field and allow more effective combating of cybercrimes.",online shopping law enforcement
http://arxiv.org/abs/1408.3829v1,"The whole world is changed rapidly and using the current technologies
Internet becomes an essential need for everyone. Web is used in every field.
Most of the people use web for a common purpose like online shopping, chatting
etc. During an online shopping large number of reviews/opinions are given by
the users that reflect whether the product is good or bad. These reviews need
to be explored, analyse and organized for better decision making. Opinion
Mining is a natural language processing task that deals with finding
orientation of opinion in a piece of text with respect to a topic. In this
paper a document based opinion mining system is proposed that classify the
documents as positive, negative and neutral. Negation is also handled in the
proposed system. Experimental results using reviews of movies show the
effectiveness of the system.",online shopping law enforcement
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam
http://arxiv.org/abs/1604.03627v1,"Social botnets have become an important phenomenon on social media. There are
many ways in which social bots can disrupt or influence online discourse, such
as, spam hashtags, scam twitter users, and astroturfing. In this paper we
considered one specific social botnet in Twitter to understand how it grows
over time, how the content of tweets by the social botnet differ from regular
users in the same dataset, and lastly, how the social botnet may have
influenced the relevant discussions. Our analysis is based on a qualitative
coding for approximately 3000 tweets in Arabic and English from the Syrian
social bot that was active for 35 weeks on Twitter before it was shutdown. We
find that the growth, behavior and content of this particular botnet did not
specifically align with common conceptions of botnets. Further we identify
interesting aspects of the botnet that distinguish it from regular users.",scam
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",scam
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam
http://arxiv.org/abs/1704.02307v1,"Obfuscation techniques are a general category of software protections widely
adopted to prevent malicious tampering of the code by making applications more
difficult to understand and thus harder to modify. Obfuscation techniques are
divided in code and data obfuscation, depending on the protected asset. While
preliminary empirical studies have been conducted to determine the impact of
code obfuscation, our work aims at assessing the effectiveness and efficiency
in preventing attacks of a specific data obfuscation technique - VarMerge. We
conducted an experiment with student participants performing two attack tasks
on clear and obfuscated versions of two applications written in C. The
experiment showed a significant effect of data obfuscation on both the time
required to complete and the successful attack efficiency. An application with
VarMerge reduces by six times the number of successful attacks per unit of
time. This outcome provides a practical clue that can be used when applying
software protections based on data obfuscation.",scam
http://arxiv.org/abs/1808.06362v1,"Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.",scam
http://arxiv.org/abs/1810.05365v1,"Blockchain technology has attracted tremendous attention in both academia and
capital market. However, overwhelming speculations on thousands of available
cryptocurrencies and numerous initial coin offering (ICO) scams have also
brought notorious debates on this emerging technology. This paper traces the
development of blockchain systems to reveal the importance of decentralized
applications (dApps) and the future value of blockchain. We survey the
state-of-the-art dApps and discuss the direction of blockchain development to
fulfill the desirable characteristics of dApps. The readers will gain an
overview of dApp research and get familiar with recent developments in the
blockchain.",scam
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",scam
http://arxiv.org/abs/1901.04942v1,"Code obfuscation is a popular approach to turn program comprehension and
analysis harder, with the aim of mitigating threats related to malicious
reverse engineering and code tampering. However, programming languages that
compile to high level bytecode (e.g., Java) can be obfuscated only to a limited
extent. In fact, high level bytecode still contains high level relevant
information that an attacker might exploit.
  In order to enable more resilient obfuscations, part of these programs might
be implemented with programming languages (e.g., C) that compile to low level
machine-dependent code. In fact, machine code contains and leaks less high
level information and it enables more resilient obfuscations.
  In this paper, we present an approach to automatically translate critical
sections of high level Java bytecode to C code, so that more effective
obfuscations can be resorted to. Moreover, a developer can still work with a
single programming language, i.e., Java.",scam
http://arxiv.org/abs/1908.06895v1,"During compilation from Java source code to bytecode, some information is
irreversibly lost. In other words, compilation and decompilation of Java code
is not symmetric. Consequently, the decompilation process, which aims at
producing source code from bytecode, must establish some strategies to
reconstruct the information that has been lost. Modern Java decompilers tend to
use distinct strategies to achieve proper decompilation. In this work, we
hypothesize that the diverse ways in which bytecode can be decompiled has a
direct impact on the quality of the source code produced by decompilers.
  We study the effectiveness of eight Java decompilers with respect to three
quality indicators: syntactic correctness, syntactic distortion and semantic
equivalence modulo inputs. This study relies on a benchmark set of 14
real-world open-source software projects to be decompiled (2041 classes in
total).
  Our results show that no single modern decompiler is able to correctly handle
the variety of bytecode structures coming from real-world programs. Even the
highest ranking decompiler in this study produces syntactically correct output
for 84% of classes of our dataset and semantically equivalent code output for
78% of classes.",scam
http://arxiv.org/abs/1701.07179v3,"Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.",scam
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam monitoring
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam monitoring
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam monitoring
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam monitoring
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam monitoring
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam monitoring
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam monitoring
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam monitoring
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam monitoring
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam monitoring
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam monitoring
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam monitoring
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam monitoring
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam monitoring
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam monitoring
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam monitoring
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam monitoring
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam monitoring
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam monitoring
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam monitoring
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",scam monitoring
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam monitoring
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam monitoring
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam monitoring
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam monitoring
http://arxiv.org/abs/1604.03627v1,"Social botnets have become an important phenomenon on social media. There are
many ways in which social bots can disrupt or influence online discourse, such
as, spam hashtags, scam twitter users, and astroturfing. In this paper we
considered one specific social botnet in Twitter to understand how it grows
over time, how the content of tweets by the social botnet differ from regular
users in the same dataset, and lastly, how the social botnet may have
influenced the relevant discussions. Our analysis is based on a qualitative
coding for approximately 3000 tweets in Arabic and English from the Syrian
social bot that was active for 35 weeks on Twitter before it was shutdown. We
find that the growth, behavior and content of this particular botnet did not
specifically align with common conceptions of botnets. Further we identify
interesting aspects of the botnet that distinguish it from regular users.",scam monitoring
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam monitoring
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",scam monitoring
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam monitoring
http://arxiv.org/abs/1704.02307v1,"Obfuscation techniques are a general category of software protections widely
adopted to prevent malicious tampering of the code by making applications more
difficult to understand and thus harder to modify. Obfuscation techniques are
divided in code and data obfuscation, depending on the protected asset. While
preliminary empirical studies have been conducted to determine the impact of
code obfuscation, our work aims at assessing the effectiveness and efficiency
in preventing attacks of a specific data obfuscation technique - VarMerge. We
conducted an experiment with student participants performing two attack tasks
on clear and obfuscated versions of two applications written in C. The
experiment showed a significant effect of data obfuscation on both the time
required to complete and the successful attack efficiency. An application with
VarMerge reduces by six times the number of successful attacks per unit of
time. This outcome provides a practical clue that can be used when applying
software protections based on data obfuscation.",scam monitoring
http://arxiv.org/abs/1808.06362v1,"Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.",scam monitoring
http://arxiv.org/abs/1810.05365v1,"Blockchain technology has attracted tremendous attention in both academia and
capital market. However, overwhelming speculations on thousands of available
cryptocurrencies and numerous initial coin offering (ICO) scams have also
brought notorious debates on this emerging technology. This paper traces the
development of blockchain systems to reveal the importance of decentralized
applications (dApps) and the future value of blockchain. We survey the
state-of-the-art dApps and discuss the direction of blockchain development to
fulfill the desirable characteristics of dApps. The readers will gain an
overview of dApp research and get familiar with recent developments in the
blockchain.",scam monitoring
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",scam monitoring
http://arxiv.org/abs/1901.04942v1,"Code obfuscation is a popular approach to turn program comprehension and
analysis harder, with the aim of mitigating threats related to malicious
reverse engineering and code tampering. However, programming languages that
compile to high level bytecode (e.g., Java) can be obfuscated only to a limited
extent. In fact, high level bytecode still contains high level relevant
information that an attacker might exploit.
  In order to enable more resilient obfuscations, part of these programs might
be implemented with programming languages (e.g., C) that compile to low level
machine-dependent code. In fact, machine code contains and leaks less high
level information and it enables more resilient obfuscations.
  In this paper, we present an approach to automatically translate critical
sections of high level Java bytecode to C code, so that more effective
obfuscations can be resorted to. Moreover, a developer can still work with a
single programming language, i.e., Java.",scam monitoring
http://arxiv.org/abs/1908.06895v1,"During compilation from Java source code to bytecode, some information is
irreversibly lost. In other words, compilation and decompilation of Java code
is not symmetric. Consequently, the decompilation process, which aims at
producing source code from bytecode, must establish some strategies to
reconstruct the information that has been lost. Modern Java decompilers tend to
use distinct strategies to achieve proper decompilation. In this work, we
hypothesize that the diverse ways in which bytecode can be decompiled has a
direct impact on the quality of the source code produced by decompilers.
  We study the effectiveness of eight Java decompilers with respect to three
quality indicators: syntactic correctness, syntactic distortion and semantic
equivalence modulo inputs. This study relies on a benchmark set of 14
real-world open-source software projects to be decompiled (2041 classes in
total).
  Our results show that no single modern decompiler is able to correctly handle
the variety of bytecode structures coming from real-world programs. Even the
highest ranking decompiler in this study produces syntactically correct output
for 84% of classes of our dataset and semantically equivalent code output for
78% of classes.",scam monitoring
http://arxiv.org/abs/1802.03667v1,"Runtime monitoring is essential for the violation detection during the
underlying software system execution. In this paper, an investigation of the
monitoring activity of MAPE-K control loop is performed which aims at
exploring:(1) the architecture of the monitoring activity in terms of the
involved components and control and data flow between them; (2) the standard
interface of the monitoring component with other MAPE-K components; (3) the
adaptive monitoring and its importance to the monitoring overhead issue; and
(4) the monitoring mode and its relevance to some specific situations and
systems. This paper also presented a Java framework for the monitoring process
for self adaptive systems.",scam monitoring
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam detection
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam detection
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam detection
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam detection
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam detection
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam detection
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam detection
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam detection
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam detection
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam detection
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam detection
http://arxiv.org/abs/1808.06362v1,"Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.",scam detection
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",scam detection
http://arxiv.org/abs/1701.07179v3,"Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.",scam detection
http://arxiv.org/abs/1405.1511v1,"Existence of spam URLs over emails and Online Social Media (OSM) has become a
growing phenomenon. To counter the dissemination issues associated with long
complex URLs in emails and character limit imposed on various OSM (like
Twitter), the concept of URL shortening gained a lot of traction. URL
shorteners take as input a long URL and give a short URL with the same landing
page in return. With its immense popularity over time, it has become a prime
target for the attackers giving them an advantage to conceal malicious content.
Bitly, a leading service in this domain is being exploited heavily to carry out
phishing attacks, work from home scams, pornographic content propagation, etc.
This imposes additional performance pressure on Bitly and other URL shorteners
to be able to detect and take a timely action against the illegitimate content.
In this study, we analyzed a dataset marked as suspicious by Bitly in the month
of October 2013 to highlight some ground issues in their spam detection
mechanism. In addition, we identified some short URL based features and coupled
them with two domain specific features to classify a Bitly URL as malicious /
benign and achieved a maximum accuracy of 86.41%. To the best of our knowledge,
this is the first large scale study to highlight the issues with Bitly's spam
detection policies and proposing a suitable countermeasure.",scam detection
http://arxiv.org/abs/1501.00802v1,"Online Social Networks (OSNs) witness a rise in user activity whenever an
event takes place. Malicious entities exploit this spur in user-engagement
levels to spread malicious content that compromises system reputation and
degrades user experience. It also generates revenue from advertisements,
clicks, etc. for the malicious entities. Facebook, the world's biggest social
network, is no exception and has recently been reported to face much abuse
through scams and other type of malicious content, especially during news
making events. Recent studies have reported that spammers earn $200 million
just by posting malicious links on Facebook. In this paper, we characterize
malicious content posted on Facebook during 17 events, and discover that
existing efforts to counter malicious content by Facebook are not able to stop
all malicious content from entering the social graph. Our findings revealed
that malicious entities tend to post content through web and third party
applications while legitimate entities prefer mobile platforms to post content.
In addition, we discovered a substantial amount of malicious content generated
by Facebook pages. Through our observations, we propose an extensive feature
set based on entity profile, textual content, metadata, and URL features to
identify malicious content on Facebook in real time and at zero-hour. This
feature set was used to train multiple machine learning models and achieved an
accuracy of 86.9%. The intent is to catch malicious content that is currently
evading Facebook's detection techniques. Our machine learning model was able to
detect more than double the number of malicious posts as compared to existing
malicious content detection techniques. Finally, we built a real world solution
in the form of a REST based API and a browser plug-in to identify malicious
Facebook posts in real time.",scam detection
http://arxiv.org/abs/1908.07087v2,"Given the reach of web platforms, bad actors have considerable incentives to
manipulate and defraud users at the expense of platform integrity. This has
spurred research in numerous suspicious behavior detection tasks, including
detection of sybil accounts, false information, and payment scams/fraud. In
this paper, we draw the insight that many such initiatives can be tackled in a
common framework by posing a detection task which seeks to find groups of
entities which share too many properties with one another across multiple
attributes (sybil accounts created at the same time and location, propaganda
spreaders broadcasting articles with the same rhetoric and with similar
reshares, etc.) Our work makes four core contributions: Firstly, we posit a
novel formulation of this task as a multi-view graph mining problem, in which
distinct views reflect distinct attribute similarities across entities, and
contextual similarity and attribute importance are respected. Secondly, we
propose a novel suspiciousness metric for scoring entity groups given the
abnormality of their synchronicity across multiple views, which obeys intuitive
desiderata that existing metrics do not. Finally, we propose the SliceNDice
algorithm which enables efficient extraction of highly suspicious entity
groups, and demonstrate its practicality in production, in terms of strong
detection performance and discoveries on Snapchat's large advertiser ecosystem
(89% precision and numerous discoveries of real fraud rings), marked
outperformance of baselines (over 97% precision/recall in simulated settings)
and linear scalability.",scam detection
http://arxiv.org/abs/1804.00451v1,"Cybercriminals abuse Online Social Networks (OSNs) to lure victims into a
variety of spam. Among different spam types, a less explored area is OSN abuse
that leverages the telephony channel to defraud users. Phone numbers are
advertized via OSNs, and users are tricked into calling these numbers. To
expand the reach of such scam / spam campaigns, phone numbers are advertised
across multiple platforms like Facebook, Twitter, GooglePlus, Flickr, and
YouTube. In this paper, we present the first data-driven characterization of
cross-platform campaigns that use multiple OSN platforms to reach their victims
and use phone numbers for monetization.
  We collect 23M posts containing 1.8M unique phone numbers from Twitter,
Facebook, GooglePlus, Youtube, and Flickr over a period of six months.
Clustering these posts helps us identify 202 campaigns operating across the
globe with Indonesia, United States, India, and United Arab Emirates being the
most prominent originators. We find that even though Indonesian campaigns
generate highest volume (3.2M posts), only 1.6% of the accounts propagating
Indonesian campaigns have been suspended so far. By examining campaigns running
across multiple OSNs, we discover that Twitter detects and suspends 93% more
accounts than Facebook. Therefore, sharing intelligence about abuse-related
user accounts across OSNs can aid in spam detection. According to our dataset,
around 35K victims and 8.8M USD could have been saved if intelligence was
shared across the OSNs. By analyzing phone number based spam campaigns running
on OSNs, we highlight the unexplored variety of phone-based attacks surfacing
on OSNs.",scam detection
http://arxiv.org/abs/1901.00579v1,"As Internet streaming of live content has gained on traditional cable TV
viewership, we have also seen significant growth of free live streaming
services which illegally provide free access to copyrighted content over the
Internet. Some of these services draw millions of viewers each month. Moreover,
this viewership has continued to increase, despite the consistent coupling of
this free content with deceptive advertisements and user-hostile tracking.
  In this paper, we explore the ecosystem of free illegal live streaming
services by collecting and examining the behavior of a large corpus of illegal
sports streaming websites. We explore and quantify evidence of user tracking
via third-party HTTP requests, cookies, and fingerprinting techniques on more
than $27,303$ unique video streams provided by $467$ unique illegal live
streaming domains. We compare the behavior of illegal live streaming services
with legitimate services and find that the illegal services go to much greater
lengths to track users than most legitimate services, and use more obscure
tracking services. Similarly, we find that moderated sites that aggregate links
to illegal live streaming content fail to moderate out sites that go to
significant lengths to track users. In addition, we perform several case
studies which highlight deceptive behavior and modern techniques used by some
domains to avoid detection, monetize traffic, or otherwise exploit their
viewers.
  Overall, we find that despite recent improvements in mechanisms for detecting
malicious browser extensions, ad-blocking, and browser warnings, users of free
illegal live streaming services are still exposed to deceptive ads, malicious
browser extensions, scams, and extensive tracking. We conclude with insights
into the ecosystem and recommendations for addressing the challenges
highlighted by this study.",scam detection
http://arxiv.org/abs/1301.6899v1,"With the advent of online social media, phishers have started using social
networks like Twitter, Facebook, and Foursquare to spread phishing scams.
Twitter is an immensely popular micro-blogging network where people post short
messages of 140 characters called tweets. It has over 100 million active users
who post about 200 million tweets everyday. Phishers have started using Twitter
as a medium to spread phishing because of this vast information dissemination.
Further, it is difficult to detect phishing on Twitter unlike emails because of
the quick spread of phishing links in the network, short size of the content,
and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects
phishing on Twitter in realtime. We use Twitter specific features along with
URL features to detect whether a tweet posted with a URL is phishing or not.
Some of the Twitter specific features we use are tweet content and its
characteristics like length, hashtags, and mentions. Other Twitter features
used are the characteristics of the Twitter user posting the tweet such as age
of the account, number of tweets, and the follower-followee ratio. These
Twitter specific features coupled with URL based features prove to be a strong
mechanism to detect phishing tweets. We use machine learning classification
techniques and detect phishing tweets with an accuracy of 92.52%. We have
deployed our system for end-users by providing an easy to use Chrome browser
extension which works in realtime and classifies a tweet as phishing or safe.
We show that we are able to detect phishing tweets at zero hour with high
accuracy which is much faster than public blacklists and as well as Twitter's
own defense mechanism to detect malicious content. To the best of our
knowledge, this is the first realtime, comprehensive and usable system to
detect phishing on Twitter.",scam detection
http://arxiv.org/abs/1705.09929v2,"Online Social Networks (OSNs) play an important role for internet users to
carry out their daily activities like content sharing, news reading, posting
messages, product reviews and discussing events etc. At the same time, various
kinds of spammers are also equally attracted towards these OSNs. These cyber
criminals including sexual predators, online fraudsters, advertising
campaigners, catfishes, and social bots etc. exploit the network of trust by
various means especially by creating fake profiles to spread their content and
carry out scams. All these malicious identities are very harmful for both the
users as well as the service providers. From the OSN service provider point of
view, fake profiles affect the overall reputation of the network in addition to
the loss of bandwidth. To spot out these malicious users, huge manpower effort
and more sophisticated automated methods are needed. In this paper, various
types of OSN threat generators like compromised profiles, cloned profiles and
online bots (spam bots, social bots, like bots and influential bots) have been
classified. An attempt is made to present several categories of features that
have been used to train classifiers in order to identify a fake profile.
Different data crawling approaches along with some existing data sources for
fake profile detection have been identified. A refresher on existing cyber laws
to curb social media based cyber crimes with their limitations is also
presented.",scam detection
http://arxiv.org/abs/1901.02819v2,"We introduce Bug-Injector, a system that automatically creates benchmarks for
customized evaluation of static analysis tools. We share a benchmark generated
using Bug-Injector and illustrate its efficacy by using it to evaluate the
recall of two leading open-source static analysis tools: Clang Static Analyzer
and Infer.
  Bug-Injector works by inserting bugs based on bug templates into real-world
host programs. It runs tests on the host program to collect dynamic traces,
searches the traces for a point where the state satisfies the preconditions for
some bug template, then modifies the host program to inject a bug based on that
template. Injected bugs are used as test cases in a static analysis tool
evaluation benchmark. Every test case is accompanied by a program input that
exercises the injected bug. We have identified a broad range of requirements
and desiderata for bug benchmarks; our approach generates on-demand test
benchmarks that meet these requirements. It also allows us to create customized
benchmarks suitable for evaluating tools for a specific use case (e.g., a given
codebase and set of bug types).
  Our experimental evaluation demonstrates the suitability of our generated
benchmark for evaluating static bug-detection tools and for comparing the
performance of different tools.",scam detection
http://arxiv.org/abs/1910.00508v1,"We present an empirical and large-scale analysis of malware samples captured
from two different enterprises from 2017 to early 2018. Particularly, we
perform threat vector, social-engineering, vulnerability and time-series
analysis on our dataset. Unlike existing malware studies, our analysis is
specifically focused on the recent enterprise malware samples. First of all,
based on our analysis on the combined datasets of two enterprises, our results
confirm the general consensus that AV-only solutions are not enough for
real-time defenses in enterprise settings because on average 40% of the malware
samples, when first appeared, are not detected by most AVs on VirusTotal or not
uploaded to VT at all (i.e., never seen in the wild yet). Moreover, our
analysis also shows that enterprise users transfer documents more than
executables and other types of files. Therefore, attackers embed malicious
codes into documents to download and install the actual malicious payload
instead of sending malicious payload directly or using vulnerability exploits.
Moreover, we also found that financial matters (e.g., purchase orders and
invoices) are still the most common subject seen in Business Email Compromise
(BEC) scams that aim to trick employees. Finally, based on our analysis on the
timestamps of captured malware samples, we found that 93% of the malware
samples were delivered on weekdays. Our further analysis also showed that while
the malware samples that require user interaction such as macro-based malware
samples have been captured during the working hours of the employees, the
massive malware attacks are triggered during the off-times of the employees to
be able to silently spread over the networks.",scam detection
http://arxiv.org/abs/1910.06277v1,"Malicious websites are responsible for a majority of the cyber-attacks and
scams today. Malicious URLs are delivered to unsuspecting users via email, text
messages, pop-ups or advertisements. Clicking on or crawling such URLs can
result in compromised email accounts, launching of phishing campaigns, download
of malware, spyware and ransomware, as well as severe monetary losses. A
machine learning based ensemble classification approach is proposed to detect
malicious URLs in emails, which can be extended to other methods of delivery of
malicious URLs. The approach uses static lexical features extracted from the
URL string, with the assumption that these features are notably different for
malicious and benign URLs. The use of such static features is safer and faster
since it does not involve crawling the URLs or blacklist lookups which tend to
introduce a significant amount of latency in producing verdicts. The goal of
the classification was to achieve high sensitivity i.e. detect as many
malicious URLs as possible. URL strings tend to be very unstructured and noisy.
Hence, bagging algorithms were found to be a good fit for the task since they
average out multiple learners trained on different parts of the training data,
thus reducing variance. The classification model was tested on five different
testing sets and produced an average False Negative Rate (FNR) of 0.1%, average
accuracy of 92% and average AUC of 0.98. The model is presently being used in
the FireEye Advanced URL Detection Engine (used to detect malicious URLs in
emails), to generate fast real-time verdicts on URLs. The malicious URL
detections from the engine have gone up by 22% since the deployment of the
model into the engine workflow. The results obtained show noteworthy evidence
that a purely lexical approach can be used to detect malicious URLs.",scam detection
http://arxiv.org/abs/1209.2557v1,"A large part of modern day communications are carried out through the medium
of E-mails, especially corporate communications. More and more people are using
E-mail for personal uses too. Companies also send notifications to their
customers in E-mail. In fact, in the Multinational business scenario E-mail is
the most convenient and sought-after method of communication. Important
features of E-mail such as its speed, reliability, efficient storage options
and a large number of added facilities make it highly popular among people from
all sectors of business and society. But being largely popular has its negative
aspects too. E-mails are the preferred medium for a large number of attacks
over the internet. Some of the most popular attacks over the internet include
spams, and phishing mails. Both spammers and phishers utilize E-mail services
quite efficiently in spite of a large number of detection and prevention
techniques already in place. Very few methods are actually good in
detection/prevention of spam/phishing related mails but they have higher false
positives. These techniques are implemented at the server and in addition to
giving higher number of false positives, they add to the processing load on the
server. This paper outlines a novel approach to detect not only spam, but also
scams, phishing and advertisement related mails. In this method, we overcome
the limitations of server-side detection techniques by utilizing some
intelligence on the part of users. Keywords parsing, token separation and
knowledge bases are used in the background to detect almost all E-mail attacks.
The proposed methodology, if implemented, can help protect E-mail users from
almost all kinds of unwanted mails with enhanced efficiency, reduced number of
false positives while not increasing the load on E-mail servers.",scam detection
http://arxiv.org/abs/1406.3687v1,"Existence of spam URLs over emails and Online Social Media (OSM) has become a
massive e-crime. To counter the dissemination of long complex URLs in emails
and character limit imposed on various OSM (like Twitter), the concept of URL
shortening has gained a lot of traction. URL shorteners take as input a long
URL and output a short URL with the same landing page (as in the long URL) in
return. With their immense popularity over time, URL shorteners have become a
prime target for the attackers giving them an advantage to conceal malicious
content. Bitly, a leading service among all shortening services is being
exploited heavily to carry out phishing attacks, work-from-home scams,
pornographic content propagation, etc. This imposes additional performance
pressure on Bitly and other URL shorteners to be able to detect and take a
timely action against the illegitimate content. In this study, we analyzed a
dataset of 763,160 short URLs marked suspicious by Bitly in the month of
October 2013. Our results reveal that Bitly is not using its claimed spam
detection services very effectively. We also show how a suspicious Bitly
account goes unnoticed despite of a prolonged recurrent illegitimate activity.
Bitly displays a warning page on identification of suspicious links, but we
observed this approach to be weak in controlling the overall propagation of
spam. We also identified some short URL based features and coupled them with
two domain specific features to classify a Bitly URL as malicious or benign and
achieved an accuracy of 86.41%. The feature set identified can be generalized
to other URL shortening services as well. To the best of our knowledge, this is
the first large scale study to highlight the issues with the implementation of
Bitly's spam detection policies and proposing suitable countermeasures.",scam detection
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam detection
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam detection
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam detection
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam detection
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam detection
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam detection
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam detection
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam detection
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam detection
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam detection
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam detection
http://arxiv.org/abs/1607.06891v3,"In technical support scams, cybercriminals attempt to convince users that
their machines are infected with malware and are in need of their technical
support. In this process, the victims are asked to provide scammers with remote
access to their machines, who will then ""diagnose the problem"", before offering
their support services which typically cost hundreds of dollars. Despite their
conceptual simplicity, technical support scams are responsible for yearly
losses of tens of millions of dollars from everyday users of the web. In this
paper, we report on the first systematic study of technical support scams and
the call centers hidden behind them. We identify malvertising as a major
culprit for exposing users to technical support scams and use it to build an
automated system capable of discovering, on a weekly basis, hundreds of phone
numbers and domains operated by scammers. By allowing our system to run for
more than 8 months we collect a large corpus of technical support scams and use
it to provide insights on their prevalence, the abused infrastructure, the
illicit profits, and the current evasion attempts of scammers. Finally, by
setting up a controlled, IRB-approved, experiment where we interact with 60
different scammers, we experience first-hand their social engineering tactics,
while collecting detailed statistics of the entire process. We explain how our
findings can be used by law-enforcing agencies and propose technical and
educational countermeasures for helping users avoid being victimized by
technical support scams.",scam monitor
http://arxiv.org/abs/1905.03108v1,"Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.",scam monitor
http://arxiv.org/abs/1906.10762v1,"Today, many different types of scams can be found on the internet. Online
criminals are always finding new creative ways to trick internet users, be it
in the form of lottery scams, downloading scam apps for smartphones or fake
gambling websites. This paper presents a large-scale study on one particular
delivery method of online scam: pop-up scam on typosquatting domains.
Typosquatting describes the concept of registering domains which are very
similar to existing ones while deliberately containing common typing errors;
these domains are then used to trick online users while under the belief of
browsing the intended website. Pop-up scam uses JavaScript alert boxes to
present a message which attracts the user's attention very effectively, as they
are a blocking user interface element.
  Our study among typosquatting domains derived from the Alexa Top 1 Million
list revealed on 8255 distinct typosquatting URLs a total of 9857 pop-up
messages, out of which 8828 were malicious. The vast majority of those distinct
URLs (7176) were targeted and displayed pop-up messages to one specific HTTP
user agent only. Based on our scans, we present an in-depth analysis as well as
a detailed classification of different targeting parameters (user agent and
language) which triggered varying kinds of pop-up scams.",scam monitor
http://arxiv.org/abs/1105.3671v3,"In this paper we conduct a large scale measurement study in order to analyse
the fake content publishing phenomenon in the BitTorrent Ecosystem. Our results
reveal that fake content represents an important portion (35%) of those files
shared in BitTorrent and just a few tens of users are responsible for 90% of
this content. Furthermore, more than 99% of the analysed fake files are linked
to either malware or scam websites. This creates a serious threat for the
BitTorrent ecosystem. To address this issue, we present a new detection tool
named TorrentGuard for the early detection of fake content. Based on our
evaluation this tool may prevent the download of more than 35 millions of fake
files per year. This could help to reduce the number of computer infections and
scams suffered by BitTorrent users. TorrentGuard is already available and it
can be accessed through both a webpage or a Vuze plugin.",scam monitor
http://arxiv.org/abs/1204.2774v1,"Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.",scam monitor
http://arxiv.org/abs/1803.00646v1,"Soon after its introduction in 2009, Bitcoin has been adopted by
cyber-criminals, which rely on its pseudonymity to implement virtually
untraceable scams. One of the typical scams that operate on Bitcoin are the
so-called Ponzi schemes. These are fraudulent investments which repay users
with the funds invested by new users that join the scheme, and implode when it
is no longer possible to find new investments. Despite being illegal in many
countries, Ponzi schemes are now proliferating on Bitcoin, and they keep
alluring new victims, who are plundered of millions of dollars. We apply data
mining techniques to detect Bitcoin addresses related to Ponzi schemes. Our
starting point is a dataset of features of real-world Ponzi schemes, that we
construct by analysing, on the Bitcoin blockchain, the transactions used to
perform the scams. We use this dataset to experiment with various machine
learning algorithms, and we assess their effectiveness through standard
validation protocols and performance metrics. The best of the classifiers we
have experimented can identify most of the Ponzi schemes in the dataset, with a
low number of false positives.",scam monitor
http://arxiv.org/abs/1803.03670v1,"Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.",scam monitor
http://arxiv.org/abs/1710.05305v1,"The advance of smartphones and cellular networks boosts the need of mobile
advertising and targeted marketing. However, it also triggers the unseen
security threats. We found that the phone scams with fake calling numbers of
very short lifetime are increasingly popular and have been used to trick the
users. The harm is worldwide. On the other hand, deceptive advertising
(deceptive ads), the fake ads that tricks users to install unnecessary apps via
either alluring or daunting texts and pictures, is an emerging threat that
seriously harms the reputation of the advertiser. To counter against these two
new threats, the conventional blacklist (or whitelist) approach and the machine
learning approach with predefined features have been proven useless.
Nevertheless, due to the success of deep learning in developing the highly
intelligent program, our system can efficiently and effectively detect phone
scams and deceptive ads by taking advantage of our unified framework on deep
neural network (DNN) and convolutional neural network (CNN). The proposed
system has been deployed for operational use and the experimental results
proved the effectiveness of our proposed system. Furthermore, we keep our
research results and release experiment material on
http://DeceptiveAds.TWMAN.ORG and http://PhoneScams.TWMAN.ORG if there is any
update.",scam monitor
http://arxiv.org/abs/1709.08331v1,"Technical Support Scams (TSS), which combine online abuse with social
engineering over the phone channel, have persisted despite several law
enforcement actions. The tactics used by these scammers have evolved over time
and they have targeted an ever increasing number of technology brands. Although
recent research has provided insights into TSS, these scams have now evolved to
exploit ubiquitously used online services such as search and sponsored
advertisements served in response to search queries. We use a data-driven
approach to understand search-and-ad abuse by TSS to gain visibility into the
online infrastructure that facilitates it. By carefully formulating tech
support queries with multiple search engines, we collect data about both the
support infrastructure and the websites to which TSS victims are directed when
they search online for tech support resources. We augment this with a DNS-based
amplification technique to further enhance visibility into this abuse
infrastructure. By analyzing the collected data, we demonstrate that tech
support scammers are (1) successful in getting major as well as custom search
engines to return links to websites controlled by them, and (2) they are able
to get ad networks to serve malicious advertisements that lead to scam pages.
Our study period of 8 months uncovered over 9,000 TSS domains, of both passive
and aggressive types, with minimal overlap between sets that are reached via
organic search results and sponsored ads. Also, we found over 2,400 support
domains which aid the TSS domains in manipulating organic search results.
Moreover, we found little overlap with domains that are reached via abuse of
domain parking and URL-shortening services which was investigated previously.
Thus, investigation of search-and-ad abuse provides new insights into TSS
tactics and helps detect previously unknown abuse infrastructure that
facilitates these scams.",scam monitor
http://arxiv.org/abs/1508.04123v1,"Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\""ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\""ive Bayes and IBK at 95%
confidence level.",scam monitor
http://arxiv.org/abs/1807.02261v1,"Studies show that software developers often either misuse exception handling
features or use them inefficiently, and such a practice may lead an undergoing
software project to a fragile, insecure and non-robust application system. In
this paper, we propose a context-aware code recommendation approach that
recommends exception handling code examples from a number of popular open
source code repositories hosted at GitHub. It collects the code examples
exploiting GitHub code search API, and then analyzes, filters and ranks them
against the code under development in the IDE by leveraging not only the
structural (i.e., graph-based) and lexical features but also the heuristic
quality measures of exception handlers in the examples. Experiments with 4,400
code examples and 65 exception handling scenarios as well as comparisons with
four existing approaches show that the proposed approach is highly promising.",scam monitor
http://arxiv.org/abs/1807.02278v1,"Recently, automatic code comment generation is proposed to facilitate program
comprehension. Existing code comment generation techniques focus on describing
the functionality of the source code. However, there are other aspects such as
insights about quality or issues of the code, which are overlooked by earlier
approaches. In this paper, we describe a mining approach that recommends
insightful comments about the quality, deficiencies or scopes for further
improvement of the source code. First, we conduct an exploratory study that
motivates crowdsourced knowledge from Stack Overflow discussions as a potential
resource for source code comment recommendation. Second, based on the findings
from the exploratory study, we propose a heuristic-based technique for mining
insightful comments from Stack Overflow Q & A site for source code comment
recommendation. Experiments with 292 Stack Overflow code segments and 5,039
discussion comments show that our approach has a promising recall of 85.42%. We
also conducted a complementary user study which confirms the accuracy and
usefulness of the recommended comments.",scam monitor
http://arxiv.org/abs/1902.03110v1,"Interest surrounding cryptocurrencies, digital or virtual currencies that are
used as a medium for financial transactions, has grown tremendously in recent
years. The anonymity surrounding these currencies makes investors particularly
susceptible to fraud---such as ""pump and dump"" scams---where the goal is to
artificially inflate the perceived worth of a currency, luring victims into
investing before the fraudsters can sell their holdings. Because of the speed
and relative anonymity offered by social platforms such as Twitter and
Telegram, social media has become a preferred platform for scammers who wish to
spread false hype about the cryptocurrency they are trying to pump. In this
work we propose and evaluate a computational approach that can automatically
identify pump and dump scams as they unfold by combining information across
social media platforms. We also develop a multi-modal approach for predicting
whether a particular pump attempt will succeed or not. Finally, we analyze the
prevalence of bots in cryptocurrency related tweets, and observe a significant
increase in bot activity during the pump attempts.",scam monitor
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",scam monitor
http://arxiv.org/abs/1307.4062v2,"In this paper, we study how object-oriented classes are used across thousands
of software packages. We concentrate on ""usage diversity'"", defined as the
different statically observable combinations of methods called on the same
object. We present empirical evidence that there is a significant usage
diversity for many classes. For instance, we observe in our dataset that Java's
String is used in 2460 manners. We discuss the reasons of this observed
diversity and the consequences on software engineering knowledge and research.",scam monitor
http://arxiv.org/abs/1810.08420v1,"Interest in cryptocurrencies has skyrocketed since their introduction a
decade ago, with hundreds of billions of dollars now invested across a
landscape of thousands of different cryptocurrencies. While there is
significant diversity, there is also a significant number of scams as people
seek to exploit the current popularity. In this paper, we seek to identify the
extent of innovation in the cryptocurrency landscape using the open-source
repositories associated with each one. Among other findings, we observe that
while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of
Ethereum as a platform has enabled the deployment of cryptocurrencies with more
diverse functionalities.",scam monitor
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",scam monitor
http://arxiv.org/abs/1905.05041v1,"Ethereum is an open-source, public, blockchain-based distributed computing
platform and operating system featuring smart contract functionality. In this
paper, we proposed an Ethereum based eletronic voting (e-voting) protocol,
Ques-Chain, which can ensure the authentication can be done without hurting
confidentiality and the anonymity can be protected without problems of scams at
the same time. Furthermore, the authors considered the wider usages Ques-Chain
can be applied on, pointing out that it is able to process all kinds of
messages and can be used in all fields with similar needs.",scam monitor
http://arxiv.org/abs/1905.08036v1,"We present an exploration of a reputation system based on explicit ratings
weighted by the values of corresponding financial transactions from the
perspective of its ability to grant ""security"" to market participants by
protecting them from scam and ""equity"" in terms of having real qualities of the
participants correctly assessed. We present a simulation modeling approach
based on the selected reputation system and discuss the results of the
simulation.",scam monitor
http://arxiv.org/abs/1001.1993v1,"The malaise of electronic spam mail that solicit illicit partnership using
bogus business proposals (popularly called 419 mails) remained unabated on the
internet despite concerted efforts. In addition to these are the emergence and
prevalence of phishing scams that use social engineering tactics to obtain
online access codes such as credit card number, ATM pin numbers, bank account
details, social security number and other personal information (22). In an age
where dependence on electronic transaction is on the increase, the web security
community will have to devise more pragmatic measures to make the cyberspace
safe from these demeaning ills. Understanding the perpetrators of internet
crimes and their mode of operation is a basis for any meaningful effort towards
stemming these crimes. This paper discusses the nature of the criminals engaged
in fraudulent cyberspace activities with special emphasis on the Nigeria 419
scam mails. Based on a qualitative analysis and experiments to trace the source
of electronic spam and phishing emails received over a six months period, we
provide information about the scammers personalities, motivation, methodologies
and victims. We posited that popular email clients are deficient in the
provision of effective mechanisms that can aid users in identifying fraud mails
and protect them against phishing attacks. We demonstrate, using state of the
art techniques, how users can detect and avoid fraudulent emails and conclude
by making appropriate recommendations based on our findings.",scam monitor
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",scam monitor
http://arxiv.org/abs/1010.2802v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, Phishing etc., Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
study the characteristics of phishing and technology used by phishers. In order
to counter anti phishing technology, phishers change their mode of operation;
therefore continuous evaluation of phishing helps us to combat phishers
effectively. We have collected seven hundred thousand spam from a corporate
server for a period of 13 months from February 2008 to February 2009. From the
collected date, we identified different kinds of phishing scams and mode of
their operation. Our observation shows that phishers are dynamic and depend
more on social engineering techniques rather than software vulnerabilities. We
believe that this study would be useful to develop more efficient anti phishing
methodologies.",scam monitor
http://arxiv.org/abs/1106.4692v1,"The history of phishing traces back in important ways to the mid-1990s when
hacking software facilitated the mass targeting of people in password stealing
scams on America Online (AOL). The first of these software programs was mine,
called AOHell, and it was where the word phishing was coined. The software
provided an automated password and credit card-stealing mechanism starting in
January 1995. Though the practice of tricking users in order to steal passwords
or information possibly goes back to the earliest days of computer networking,
AOHell's phishing system was the first automated tool made publicly available
for this purpose. The program influenced the creation of many other automated
phishing systems that were made over a number of years. These tools were
available to amateurs who used them to engage in a countless number of phishing
attacks. By the later part of the decade, the activity moved from AOL to other
networks and eventually grew to involve professional criminals on the internet.
What began as a scheme by rebellious teenagers to steal passwords evolved into
one of the top computer security threats affecting people, corporations, and
governments.",scam monitor
http://arxiv.org/abs/1108.1593v1,"Spam messes up users inbox, consumes resources and spread attacks like DDoS,
MiM, phishing etc. Phishing is a byproduct of email and causes financial loss
to users and loss of reputation to financial institutions. In this paper we
examine the characteristics of phishing and technology used by Phishers. In
order to counter anti-phishing technology, phishers change their mode of
operation; therefore a continuous evaluation of phishing only helps us combat
phisher effectiveness. In our study, we collected seven hundred thousand spam
from a corporate server for a period of 13 months from February 2008 to
February 2009. From the collected data, we identified different kinds of
phishing scams and mode of operation. Our observation shows that phishers are
dynamic and depend more on social engineering techniques rather than software
vulnerabilities. We believe that this study will develop more efficient
anti-phishing methodologies. Based on our analysis, we developed an
anti-phishing methodology and implemented in our network. The results show that
this approach is highly effective to prevent phishing attacks. The proposed
approach reduced more than 80% of the false negatives and more than 95% of
phishing attacks in our network.",scam monitor
http://arxiv.org/abs/1110.0360v1,"Phishing is the combination of social engineering and technical exploits
designed to convince a victim to provide personal information, usually for the
monetary gain of the attacker. Phishing has become the most popular practice
among the criminals of the Web. Phishing attacks are becoming more frequent and
sophisticated. The impact of phishing is drastic and significant since it can
involve the risk of identity theft and financial losses. Phishing scams have
become a problem for online banking and e-commerce users. In this paper we
propose a novel approach to detect phishing attacks. We implemented a prototype
web browser which can be used as an agent and processes each arriving email for
phishing attacks. Using email data collected over a period time we demonstrate
data that our approach is able to detect more phishing attacks than existing
schemes.",scam monitor
http://arxiv.org/abs/1604.03627v1,"Social botnets have become an important phenomenon on social media. There are
many ways in which social bots can disrupt or influence online discourse, such
as, spam hashtags, scam twitter users, and astroturfing. In this paper we
considered one specific social botnet in Twitter to understand how it grows
over time, how the content of tweets by the social botnet differ from regular
users in the same dataset, and lastly, how the social botnet may have
influenced the relevant discussions. Our analysis is based on a qualitative
coding for approximately 3000 tweets in Arabic and English from the Syrian
social bot that was active for 35 weeks on Twitter before it was shutdown. We
find that the growth, behavior and content of this particular botnet did not
specifically align with common conceptions of botnets. Further we identify
interesting aspects of the botnet that distinguish it from regular users.",scam monitor
http://arxiv.org/abs/1410.4672v1,"One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.",scam monitor
http://arxiv.org/abs/1608.04090v1,"With the recent advance of micro-blogs and social networks, people can view
and post comments on the websites in a very convenient way. However, it is also
a big concern that the malicious users keep polluting the cyber environment by
scamming, spamming or repeatedly advertising. So far the most common way to
detect and report malicious comments is based on voluntary reviewing from
honest users. To encourage contribution, very often some non-monetary credits
will be given to an honest user who validly reports a malicious comment. In
this note we argue that such credit-based incentive mechanisms should fail in
most cases: if reporting a malicious comment receives diminishing revenue, then
in the long term no rational honest user will participate in comment reviewing.",scam monitor
http://arxiv.org/abs/1603.02767v1,"With more than 294 million registered domain names as of late 2015, the
domain name ecosystem has evolved to become a cornerstone for the operation of
the Internet. Domain names today serve everyone, from individuals for their
online presence to big brands for their business operations. Such ecosystem
that facilitated legitimate business and personal uses has also fostered
""creative"" cases of misuse, including phishing, spam, hit and traffic stealing,
online scams, among others. As a first step towards this misuse, the
registration of a legitimately-looking domain is often required. For that,
domain typosquatting provides a great avenue to cybercriminals to conduct their
crimes.
  In this paper, we review the landscape of domain name typosquatting,
highlighting models and advanced techniques for typosquatted domain names
generation, models for their monetization, and the existing literature on
countermeasures. We further highlight potential fruitful directions on
technical countermeasures that are lacking in the literature.",scam monitor
http://arxiv.org/abs/1704.02307v1,"Obfuscation techniques are a general category of software protections widely
adopted to prevent malicious tampering of the code by making applications more
difficult to understand and thus harder to modify. Obfuscation techniques are
divided in code and data obfuscation, depending on the protected asset. While
preliminary empirical studies have been conducted to determine the impact of
code obfuscation, our work aims at assessing the effectiveness and efficiency
in preventing attacks of a specific data obfuscation technique - VarMerge. We
conducted an experiment with student participants performing two attack tasks
on clear and obfuscated versions of two applications written in C. The
experiment showed a significant effect of data obfuscation on both the time
required to complete and the successful attack efficiency. An application with
VarMerge reduces by six times the number of successful attacks per unit of
time. This outcome provides a practical clue that can be used when applying
software protections based on data obfuscation.",scam monitor
http://arxiv.org/abs/1808.06362v1,"Software systems naturally evolve, and this evolution often brings design
problems that cause system degradation. Architectural smells are typical
symptoms of such problems, and several of these smells are related to undesired
dependencies among modules. The early detection of these smells is important
for developers, because they can plan ahead for maintenance or refactoring
efforts, thus preventing system degradation. Existing tools for identifying
architectural smells can detect the smells once they exist in the source code.
This means that their undesired dependencies are already created. In this work,
we explore a forward-looking approach that is able to infer groups of likely
module dependencies that can anticipate architectural smells in a future system
version. Our approach considers the current module structure as a network,
along with information from previous versions, and applies link prediction
techniques (from the field of social network analysis). In particular, we focus
on dependency-related smells, such as Cyclic Dependency and Hublike Dependency,
which fit well with the link prediction model. An initial evaluation with two
open-source projects shows that, under certain considerations, the predictions
of our approach are satisfactory. Furthermore, the approach can be extended to
other types of dependency-based smells or metrics.",scam monitor
http://arxiv.org/abs/1810.05365v1,"Blockchain technology has attracted tremendous attention in both academia and
capital market. However, overwhelming speculations on thousands of available
cryptocurrencies and numerous initial coin offering (ICO) scams have also
brought notorious debates on this emerging technology. This paper traces the
development of blockchain systems to reveal the importance of decentralized
applications (dApps) and the future value of blockchain. We survey the
state-of-the-art dApps and discuss the direction of blockchain development to
fulfill the desirable characteristics of dApps. The readers will gain an
overview of dApp research and get familiar with recent developments in the
blockchain.",scam monitor
http://arxiv.org/abs/1811.06624v1,"Cybercrime is a significant challenge to society, but it can be particularly
harmful to the individuals who become victims. This chapter engages in a
comprehensive and topical analysis of the cybercrimes that target individuals.
It also examines the motivation of criminals that perpetrate such attacks and
the key human factors and psychological aspects that help to make
cybercriminals successful. Key areas assessed include social engineering (e.g.,
phishing, romance scams, catfishing), online harassment (e.g., cyberbullying,
trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity
theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and
denial-of-service crimes. As a part of its contribution, the chapter introduces
a summary taxonomy of cybercrimes against individuals and a case for why they
will continue to occur if concerted interdisciplinary efforts are not pursued.",scam monitor
http://arxiv.org/abs/1901.04942v1,"Code obfuscation is a popular approach to turn program comprehension and
analysis harder, with the aim of mitigating threats related to malicious
reverse engineering and code tampering. However, programming languages that
compile to high level bytecode (e.g., Java) can be obfuscated only to a limited
extent. In fact, high level bytecode still contains high level relevant
information that an attacker might exploit.
  In order to enable more resilient obfuscations, part of these programs might
be implemented with programming languages (e.g., C) that compile to low level
machine-dependent code. In fact, machine code contains and leaks less high
level information and it enables more resilient obfuscations.
  In this paper, we present an approach to automatically translate critical
sections of high level Java bytecode to C code, so that more effective
obfuscations can be resorted to. Moreover, a developer can still work with a
single programming language, i.e., Java.",scam monitor
http://arxiv.org/abs/1908.06895v1,"During compilation from Java source code to bytecode, some information is
irreversibly lost. In other words, compilation and decompilation of Java code
is not symmetric. Consequently, the decompilation process, which aims at
producing source code from bytecode, must establish some strategies to
reconstruct the information that has been lost. Modern Java decompilers tend to
use distinct strategies to achieve proper decompilation. In this work, we
hypothesize that the diverse ways in which bytecode can be decompiled has a
direct impact on the quality of the source code produced by decompilers.
  We study the effectiveness of eight Java decompilers with respect to three
quality indicators: syntactic correctness, syntactic distortion and semantic
equivalence modulo inputs. This study relies on a benchmark set of 14
real-world open-source software projects to be decompiled (2041 classes in
total).
  Our results show that no single modern decompiler is able to correctly handle
the variety of bytecode structures coming from real-world programs. Even the
highest ranking decompiler in this study produces syntactically correct output
for 84% of classes of our dataset and semantically equivalent code output for
78% of classes.",scam monitor
http://arxiv.org/abs/1802.03667v1,"Runtime monitoring is essential for the violation detection during the
underlying software system execution. In this paper, an investigation of the
monitoring activity of MAPE-K control loop is performed which aims at
exploring:(1) the architecture of the monitoring activity in terms of the
involved components and control and data flow between them; (2) the standard
interface of the monitoring component with other MAPE-K components; (3) the
adaptive monitoring and its importance to the monitoring overhead issue; and
(4) the monitoring mode and its relevance to some specific situations and
systems. This paper also presented a Java framework for the monitoring process
for self adaptive systems.",scam monitor
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detect fake advise
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detect fake advise
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detect fake advise
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detect fake advise
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detect fake advise
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detect fake advise
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detect fake advise
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detect fake advise
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detect fake advise
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detect fake advise
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",detect fake advise
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detect fake advise
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",detect fake advise
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",detect fake advise
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",detect fake advise
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",detect fake advise
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",detect fake advise
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",detect fake advise
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",detect fake advise
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",detect fake advise
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detect fake advise
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detect fake advise
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",detect fake advise
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detect fake advise
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",detect fake advise
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",detect fake advise
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detect fake advise
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",detect fake advise
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",detect fake advise
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",detect fake advise
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",detect fake advise
http://arxiv.org/abs/1908.09805v1,"Automatic detection of fake news --- texts that are deceitful and misleading
--- is a long outstanding and largely unsolved problem. Worse yet, recent
developments in language modeling allow for the automatic generation of such
texts. One approach that has recently gained attention detects these fake news
using stylometry-based provenance, i.e. tracing a text's writing style back to
its producing source and determining whether the source is malicious. This was
shown to be highly effective under the assumption that legitimate text is
produced by humans, and fake text is produced by a language model.
  In this work, we identify a fundamental problem with provenance-based
approaches against attackers that auto-generate fake news: fake and legitimate
texts can originate from nearly identical sources. First, a legitimate text
might be auto-generated in a similar process to that of fake text, and second,
attackers can automatically corrupt articles originating from legitimate human
sources. We demonstrate these issues by simulating attacks in such settings,
and find that the provenance approach fails to defend against them. Our
findings highlight the importance of assessing the veracity of the text rather
than solely relying on its style or source. We also open up a discussion on the
types of benchmarks that should be used to evaluate neural fake news detectors.",detect fake advise
http://arxiv.org/abs/1804.03508v1,"The fake news epidemic makes it imperative to develop a diagnostic framework
that is both parsimonious and valid to guide present and future efforts in fake
news detection. This paper represents one of the very first attempts to fill a
void in the research on this topic. The LeSiE (Lexical Structure, Simplicity,
Emotion) framework we created and validated allows lay people to identify
potential fake news without the use of calculators or complex statistics by
looking out for three simple cues.",detect fake advise
http://arxiv.org/abs/1903.01728v1,"Microblog has become a popular platform for people to post, share, and seek
information due to its convenience and low cost. However, it also facilitates
the generation and propagation of fake news, which could cause detrimental
societal consequences. Detecting fake news on microblogs is important for
societal good. Emotion is a significant indicator while verifying information
on social media. Existing fake news detection studies utilize emotion mainly
through users stances or simple statistical emotional features; and exploiting
the emotion information from both news content and user comments is also
limited. In the realistic scenarios, to impress the audience and spread
extensively, the publishers typically either post a tweet with intense emotion
which could easily resonate with the crowd, or post a controversial statement
unemotionally but aim to evoke intense emotion among the users. Therefore, in
this paper, we study the novel problem of exploiting emotion information for
fake news detection. We propose a new Emotion-based Fake News Detection
framework (EFN), which can i) learn content- and comment- emotion
representations for publishers and users respectively; and ii) exploit content
and social emotions simultaneously for fake news detection. Experimental
results on real-world dataset demonstrate the effectiveness of the proposed
framework.",detect fake advise
http://arxiv.org/abs/1708.07104v1,"The proliferation of misleading information in everyday access media outlets
such as social media feeds, news blogs, and online newspapers have made it
challenging to identify trustworthy news sources, thus increasing the need for
computational tools able to provide insights into the reliability of online
content. In this paper, we focus on the automatic identification of fake
content in online news. Our contribution is twofold. First, we introduce two
novel datasets for the task of fake news detection, covering seven different
news domains. We describe the collection, annotation, and validation process in
detail and present several exploratory analysis on the identification of
linguistic differences in fake and legitimate news content. Second, we conduct
a set of learning experiments to build accurate fake news detectors. In
addition, we provide comparative analyses of the automatic and manual
identification of fake news.",detect fake advise
http://arxiv.org/abs/1806.07516v2,"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.",detect fake advise
http://arxiv.org/abs/1901.09657v1,"News plays a significant role in shaping people's beliefs and opinions. Fake
news has always been a problem, which wasn't exposed to the mass public until
the past election cycle for the 45th President of the United States. While
quite a few detection methods have been proposed to combat fake news since
2015, they focus mainly on linguistic aspects of an article without any fact
checking. In this paper, we argue that these models have the potential to
misclassify fact-tampering fake news as well as under-written real news.
Through experiments on Fakebox, a state-of-the-art fake news detector, we show
that fact tampering attacks can be effective. To address these weaknesses, we
argue that fact checking should be adopted in conjunction with linguistic
characteristics analysis, so as to truly separate fake news from real news. A
crowdsourced knowledge graph is proposed as a straw man solution to collecting
timely facts about news events.",detect fake advise
http://arxiv.org/abs/1905.04260v1,"Over the past few years, we have been witnessing the rise of misinformation
on the Web. People fall victims of fake news during their daily lives and
assist their further propagation knowingly and inadvertently. There have been
many initiatives that are trying to mitigate the damage caused by fake news,
focusing on signals from either domain flag-lists, online social networks or
artificial intelligence. In this work, we present Check-It, a system that
combines, in an intelligent way, a variety of signals into a pipeline for fake
news identification. Check-It is developed as a web browser plugin with the
objective of efficient and timely fake news detection, respecting the user's
privacy. Experimental results show that Check-It is able to outperform the
state-of-the-art methods. On a dataset, consisting of 9 millions of articles
labeled as fake and real, Check-It obtains classification accuracies that
exceed 99%.",detect fake advise
http://arxiv.org/abs/1805.08751v2,"In recent years, due to the booming development of online social networks,
fake news for various commercial and political purposes has been appearing in
large numbers and widespread in the online world. With deceptive words, online
social network users can get infected by these online fake news easily, which
has brought about tremendous effects on the offline society already. An
important goal in improving the trustworthiness of information in online social
networks is to identify the fake news timely. This paper aims at investigating
the principles, methodologies and algorithms for detecting fake news articles,
creators and subjects from online social networks and evaluating the
corresponding performance. This paper addresses the challenges introduced by
the unknown characteristics of fake news and diverse connections among news
articles, creators and subjects. This paper introduces a novel automatic fake
news credibility inference model, namely FAKEDETECTOR. Based on a set of
explicit and latent features extracted from the textual information,
FAKEDETECTOR builds a deep diffusive network model to learn the representations
of news articles, creators and subjects simultaneously. Extensive experiments
have been done on a real-world fake news dataset to compare FAKEDETECTOR with
several state-of-the-art models, and the experimental results have demonstrated
the effectiveness of the proposed model.",detect fake advise
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",detect fake advise
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",detect fake advise
http://arxiv.org/abs/1907.07759v1,"The wide spread of fake news in social networks is posing threats to social
stability, economic development and political democracy etc. Numerous studies
have explored the effective detection approaches of online fake news, while few
works study the intrinsic propagation and cognition mechanisms of fake news.
Since the development of cognitive science paves a promising way for the
prevention of fake news, we present a new research area called Cognition
Security (CogSec), which studies the potential impacts of fake news to human
cognition, ranging from misperception, untrusted knowledge acquisition,
targeted opinion/attitude formation, to biased decision making, and
investigates the effective ways for fake news debunking. CogSec is a
multidisciplinary research field that leverages knowledge from social science,
psychology, cognition science, neuroscience, AI and computer science. We first
propose related definitions to characterize CogSec and review the literature
history. We further investigate the key research challenges and techniques of
CogSec, including human-content cognition mechanism, social influence and
opinion diffusion, fake news detection and malicious bot detection. Finally, we
summarize the open issues and future research directions, such as early
detection of fake news, explainable fake news debunking, social contagion and
diffusion models of fake news, and so on.",detect fake advise
http://arxiv.org/abs/1910.03090v1,"Fake engagement is one of the significant problems in Online Social Networks
(OSNs) which is used to increase the popularity of an account in an inorganic
manner. The detection of fake engagement is crucial because it leads to loss of
money for businesses, wrong audience targeting in advertising, wrong product
predictions systems, and unhealthy social network environment. This study is
related with the detection of fake and automated accounts which leads to fake
engagement on Instagram. As far as we know, there is no publicly available
dataset for fake and automated accounts. For this purpose, two datasets have
been generated for the detection of fake and automated accounts. For the
detection of these accounts, machine learning algorithms like Naive Bayes,
logistic regression, support vector machines and neural networks are applied.
Additionally, for the detection of automated accounts, cost sensitive genetic
algorithm is applied because of the unnatural bias in the dataset. To deal with
the unevenness problem in the fake dataset, Smote-nc algorithm is implemented.
For the automated and fake account detection problem, 86% and 96% are obtained,
respectively.",detect fake advise
http://arxiv.org/abs/1904.11679v1,"The explosive growth of fake news and its erosion of democracy, justice, and
public trust has significantly increased the demand for accurate fake news
detection. Recent advancements in this area have proposed novel techniques that
aim to detect fake news by exploring how it propagates on social networks.
However, to achieve fake news early detection, one is only provided with
limited to no information on news propagation; hence, motivating the need to
develop approaches that can detect fake news by focusing mainly on news
content. In this paper, a theory-driven model is proposed for fake news
detection. The method investigates news content at various levels:
lexicon-level, syntax-level, semantic-level and discourse-level. We represent
news at each level, relying on well-established theories in social and forensic
psychology. Fake news detection is then conducted within a supervised machine
learning framework. As an interdisciplinary research, our work explores
potential fake news patterns, enhances the interpretability in fake news
feature engineering, and studies the relationships among fake news,
deception/disinformation, and clickbaits. Experiments conducted on two
real-world datasets indicate that the proposed method can outperform the
state-of-the-art and enable fake news early detection, even when there is
limited content information.",detect fake advise
http://arxiv.org/abs/1908.04472v1,"The increasing popularity of social media promotes the proliferation of fake
news. With the development of multimedia technology, fake news attempts to
utilize multimedia contents with images or videos to attract and mislead
readers for rapid dissemination, which makes visual contents an important part
of fake news. Fake-news images, images attached in fake news posts,include not
only fake images which are maliciously tampered but also real images which are
wrongly used to represent irrelevant events. Hence, how to fully exploit the
inherent characteristics of fake-news images is an important but challenging
problem for fake news detection. In the real world, fake-news images may have
significantly different characteristics from real-news images at both physical
and semantic levels, which can be clearly reflected in the frequency and pixel
domain, respectively. Therefore, we propose a novel framework Multi-domain
Visual Neural Network (MVNN) to fuse the visual information of frequency and
pixel domains for detecting fake news. Specifically, we design a CNN-based
network to automatically capture the complex patterns of fake-news images in
the frequency domain; and utilize a multi-branch CNN-RNN model to extract
visual features from different semantic levels in the pixel domain. An
attention mechanism is utilized to fuse the feature representations of
frequency and pixel domains dynamically. Extensive experiments conducted on a
real-world dataset demonstrate that MVNN outperforms existing methods with at
least 9.2% in accuracy, and can help improve the performance of multimodal fake
news detection by over 5.2%.",detect fake advise
http://arxiv.org/abs/1903.09196v1,"Consuming news from social media is becoming increasingly popular. However,
social media also enables the widespread of fake news. Because of its
detrimental effects brought by social media, fake news detection has attracted
increasing attention. However, the performance of detecting fake news only from
news content is generally limited as fake news pieces are written to mimic true
news. In the real world, news pieces spread through propagation networks on
social media. The news propagation networks usually involve multi-levels. In
this paper, we study the challenging problem of investigating and exploiting
news hierarchical propagation network on social media for fake news detection.
  In an attempt to understand the correlations between news propagation
networks and fake news, first, we build a hierarchical propagation network from
macro-level and micro-level of fake news and true news; second, we perform a
comparative analysis of the propagation network features of linguistic,
structural and temporal perspectives between fake and real news, which
demonstrates the potential of utilizing these features to detect fake news;
third, we show the effectiveness of these propagation network features for fake
news detection. We further validate the effectiveness of these features from
feature important analysis. Altogether, this work presents a data-driven view
of hierarchical propagation network and fake news and paves the way towards a
healthier online news ecosystem.",detect fake advise
http://arxiv.org/abs/1906.04210v1,"Fake news gains has gained significant momentum, strongly motivating the need
for fake news research. Many fake news detection approaches have thus been
proposed, where most of them heavily rely on news content. However,
network-based clues revealed when analyzing news propagation on social networks
is an information that has hardly been comprehensively explored or used for
fake news detection. We bridge this gap by proposing a network-based
pattern-driven fake news detection approach. We aim to study the patterns of
fake news in social networks, which refer to the news being spread, spreaders
of the news and relationships among the spreaders. Empirical evidence and
interpretations on the existence of such patterns are provided based on social
psychological theories. These patterns are then represented at various network
levels (i.e., node-level, ego-level, triad-level, community-level and the
overall network) for being further utilized to detect fake news. The proposed
approach enhances the explainability in fake news feature engineering.
Experiments conducted on real-world data demonstrate that the proposed approach
can outperform the state of the arts.",detect fake advise
http://arxiv.org/abs/1904.13355v1,"Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.",detect fake advise
http://arxiv.org/abs/1812.00315v1,"The explosive growth in fake news and its erosion to democracy, justice, and
public trust has increased the demand for fake news analysis, detection and
intervention. This survey comprehensively and systematically reviews fake news
research. The survey identifies and specifies fundamental theories across
various disciplines, e.g., psychology and social science, to facilitate and
enhance the interdisciplinary research of fake news. Current fake news research
is reviewed, summarized and evaluated. These studies focus on fake news from
four perspective: (1) the false knowledge it carries, (2) its writing style,
(3) its propagation patterns, and (4) the credibility of its creators and
spreaders. We characterize each perspective with various analyzable and
utilizable information provided by news and its spreaders, various strategies
and frameworks that are adaptable, and techniques that are applicable. By
reviewing the characteristics of fake news and open issues in fake news
studies, we highlight some potential research tasks at the end of this survey.",detect fake advise
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",detect fake advise
http://arxiv.org/abs/1712.07709v2,"Social media is becoming popular for news consumption due to its fast
dissemination, easy access, and low cost. However, it also enables the wide
propagation of fake news, i.e., news with intentionally false information.
Detecting fake news is an important task, which not only ensures users to
receive authentic information but also help maintain a trustworthy news
ecosystem. The majority of existing detection algorithms focus on finding clues
from news contents, which are generally not effective because fake news is
often intentionally written to mislead users by mimicking true news. Therefore,
we need to explore auxiliary information to improve detection. The social
context during news dissemination process on social media forms the inherent
tri-relationship, the relationship among publishers, news pieces, and users,
which has potential to improve fake news detection. For example,
partisan-biased publishers are more likely to publish fake news, and
low-credible users are more likely to share fake news. In this paper, we study
the novel problem of exploiting social context for fake news detection. We
propose a tri-relationship embedding framework TriFN, which models
publisher-news relations and user-news interactions simultaneously for fake
news classification. We conduct experiments on two real-world datasets, which
demonstrate that the proposed approach significantly outperforms other baseline
methods for fake news detection.",detect fake advise
http://arxiv.org/abs/1904.03016v1,"Combating fake news needs a variety of defense methods. Although rumor
detection and various linguistic analysis techniques are common methods to
detect false content in social media, there are other feasible mitigation
approaches that could be explored in the machine learning community. In this
paper, we present open issues and opportunities in fake news research that need
further attention. We first review different stages of the news life cycle in
social media and discuss core vulnerability issues for news feed algorithms in
propagating fake news content with three examples. We then discuss how
complexity and unclarity of the fake news problem limit the advancements in
this field. Lastly, we present research opportunities from interpretable
machine learning to mitigate fake news problems with 1) interpretable fake news
detection and 2) transparent news feed algorithms. We propose three dimensions
of interpretability consisting of algorithmic interpretability, human
interpretability, and the inclusion of supporting evidence that can benefit
fake news mitigation methods in different ways.",detect fake advise
http://arxiv.org/abs/1809.01286v3,"Social media has become a popular means for people to consume news.
Meanwhile, it also enables the wide dissemination of fake news, i.e., news with
intentionally false information, which brings significant negative effects to
the society. Thus, fake news detection is attracting increasing attention.
However, fake news detection is a non-trivial task, which requires multi-source
information such as news content, social context, and dynamic information.
First, fake news is written to fool people, which makes it difficult to detect
fake news simply based on news contents. In addition to news contents, we need
to explore social contexts such as user engagements and social behaviors. For
example, a credible user's comment that ""this is a fake news"" is a strong
signal for detecting fake news. Second, dynamic information such as how fake
news and true news propagate and how users' opinions toward news pieces are
very important for extracting useful patterns for (early) fake news detection
and intervention. Thus, comprehensive datasets which contain news content,
social context, and dynamic information could facilitate fake news propagation,
detection, and mitigation; while to the best of our knowledge, existing
datasets only contains one or two aspects. Therefore, in this paper, to
facilitate fake news related researches, we provide a fake news data repository
FakeNewsNet, which contains two comprehensive datasets that includes news
content, social context, and dynamic information. We present a comprehensive
description of datasets collection, demonstrate an exploratory analysis of this
data repository from different perspectives, and discuss the benefits of
FakeNewsNet for potential applications on fake news study on social media.",detect fake advise
http://arxiv.org/abs/1803.08810v1,"Massive content about user's social, personal and professional life stored on
Online Social Networks (OSNs) has attracted not only the attention of
researchers and social analysts but also the cyber criminals. These cyber
criminals penetrate illegally into an OSN by establishing fake profiles or by
designing bots and exploit the vulnerabilities of an OSN to carry out illegal
activities. With the growth of technology cyber crimes have been increasing
manifold. Daily reports of the security and privacy threats in the OSNs demand
not only the intelligent automated detection systems that can identify and
alleviate fake profiles in real time but also the reinforcement of the security
and privacy laws to curtail the cyber crime. In this paper, we have studied
various categories of fake profiles like compromised profiles, cloned profiles
and online bots (spam-bots, social-bots, like-bots and influential-bots) on
different OSN sites along with existing cyber laws to mitigate their threats.
In order to design fake profile detection systems, we have highlighted
different category of fake profile features which are capable to distinguish
different kinds of fake entities from real ones. Another major challenges faced
by researchers while building the fake profile detection systems is the
unavailability of data specific to fake users. The paper addresses this
challenge by providing extremely obliging data collection techniques along with
some existing data sources. Furthermore, an attempt is made to present several
machine learning techniques employed to design different fake profile detection
systems.",detect fake advise
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",detect fake advise
http://arxiv.org/abs/1903.07389v6,"On the one hand, nowadays, fake news articles are easily propagated through
various online media platforms and have become a grand threat to the
trustworthiness of information. On the other hand, our understanding of the
language of fake news is still minimal. Incorporating hierarchical
discourse-level structure of fake and real news articles is one crucial step
toward a better understanding of how these articles are structured.
Nevertheless, this has rarely been investigated in the fake news detection
domain and faces tremendous challenges. First, existing methods for capturing
discourse-level structure rely on annotated corpora which are not available for
fake news datasets. Second, how to extract out useful information from such
discovered structures is another challenge. To address these challenges, we
propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
learns and constructs a discourse-level structure for fake/real news articles
in an automated and data-driven manner. Moreover, we identify insightful
structure-related properties, which can explain the discovered structures and
boost our understating of fake news. Conducted experiments show the
effectiveness of the proposed approach. Further structural analysis suggests
that real and fake news present substantial differences in the hierarchical
discourse-level structures.",detect fake advise
http://arxiv.org/abs/1909.06122v1,"In recent years, we have witnessed the unprecedented success of generative
adversarial networks (GANs) and its variants in image synthesis. These
techniques are widely adopted in synthesizing fake faces which poses a serious
challenge to existing face recognition (FR) systems and brings potential
security threats to social networks and media as the fakes spread and fuel the
misinformation. Unfortunately, robust detectors of these AI-synthesized fake
faces are still in their infancy and are not ready to fully tackle this
emerging challenge. Currently, image forensic-based and learning-based
approaches are the two major categories of strategies in detecting fake faces.
In this work, we propose an alternative category of approaches based on
monitoring neuron behavior. The studies on neuron coverage and interactions
have successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Empirically, we have shown that the proposed FakeSpotter, based on
neuron coverage behavior, in tandem with a simple linear classifier can greatly
outperform deeply trained convolutional neural networks (CNNs) for spotting
AI-synthesized fake faces. Extensive experiments carried out on three deep
learning (DL) based FR systems, with two GAN variants for synthesizing fake
faces, and on two public high-resolution face datasets have demonstrated the
potential of the FakeSpotter serving as a simple, yet robust baseline for fake
face detection in the wild.",detect fake advise
http://arxiv.org/abs/1806.00749v1,"With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.",detect fake advise
http://arxiv.org/abs/1806.02877v2,"The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.",detect fake advise
http://arxiv.org/abs/1803.07817v1,"Fingerprint authentication is widely used in biometrics due to its simple
process, but it is vulnerable to fake fingerprints. This study proposes a
patch-based fake fingerprint detection method using a fully convolutional
neural network with a small number of parameters and an optimal threshold to
solve the above-mentioned problem. Unlike the existing methods that classify a
fingerprint as live or fake, the proposed method classifies fingerprints as
fake, live, or background, so preprocessing methods such as segmentation are
not needed. The proposed convolutional neural network (CNN) structure applies
the Fire module of SqueezeNet, and the fewer parameters used require only 2.0
MB of memory. The network that has completed training is applied to the
training data in a fully convolutional way, and the optimal threshold to
distinguish fake fingerprints is determined, which is used in the final test.
As a result of this study experiment, the proposed method showed an average
classification error of 1.35%, demonstrating a fake fingerprint detection
method using a high-performance CNN with a small number of parameters.",detect fake advise
http://arxiv.org/abs/1811.05825v1,"With the development of the E-commerce and reviews website, the comment
information is influencing people's life. More and more users share their
consumption experience and evaluate the quality of commodity by comment. When
people make a decision, they will refer these comments. The dependency of the
comments make the fake comment appear. The fake comment is that for profit and
other bad motivation, business fabricate untrue consumption experience and they
preach or slander some products. The fake comment is easy to mislead users'
opinion and decision. The accuracy of humans identifying fake comment is low.
It's meaningful to detect fake comment using natural language processing
technology for people getting true comment information. This paper uses the
sentimental analysis to detect fake comment.",detect fake advise
http://arxiv.org/abs/1904.12607v1,"App stores include an increasing amount of user feedback in form of app
ratings and reviews. Research and recently also tool vendors have proposed
analytics and data mining solutions to leverage this feedback to developers and
analysts, e.g., for supporting release decisions. Research also showed that
positive feedback improves apps' downloads and sales figures and thus their
success. As a side effect, a market for fake, incentivized app reviews emerged
with yet unclear consequences for developers, app users, and app store
operators. This paper studies fake reviews, their providers, characteristics,
and how well they can be automatically detected. We conducted disguised
questionnaires with 43 fake review providers and studied their review policies
to understand their strategies and offers. By comparing 60,000 fake reviews
with 62 million reviews from the Apple App Store we found significant
differences, e.g., between the corresponding apps, reviewers, rating
distribution, and frequency. This inspired the development of a simple
classifier to automatically detect fake reviews in app stores. On a labelled
and imbalanced dataset including one-tenth of fake reviews, as reported in
other domains, our classifier achieved a recall of 91% and an AUC/ROC value of
98%. We discuss our findings and their impact on software engineering, app
users, and app store operators.",detect fake advise
http://arxiv.org/abs/1808.02831v1,"Identifying the stance of a news article body with respect to a certain
headline is the first step to automated fake news detection. In this paper, we
introduce a 2-stage ensemble model to solve the stance detection task. By using
only hand-crafted features as input to a gradient boosting classifier, we are
able to achieve a score of 9161.5 out of 11651.25 (78.63%) on the official Fake
News Challenge (Stage 1) dataset. We identify the most useful features for
detecting fake news and discuss how sampling techniques can be used to improve
recall accuracy on a highly imbalanced dataset.",detect fake advise
http://arxiv.org/abs/1708.01967v3,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.",detect fake advise
http://arxiv.org/abs/1711.09025v2,"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.",detect fake advise
http://arxiv.org/abs/1705.00648v1,"Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.",detect fake advise
http://arxiv.org/abs/1811.00770v1,"Fake news detection is a critical yet challenging problem in Natural Language
Processing (NLP). The rapid rise of social networking platforms has not only
yielded a vast increase in information accessibility but has also accelerated
the spread of fake news. Given the massive amount of Web content, automatic
fake news detection is a practical NLP problem required by all online content
providers. This paper presents a survey on fake news detection. Our survey
introduces the challenges of automatic fake news detection. We systematically
review the datasets and NLP solutions that have been developed for this task.
We also discuss the limits of these datasets and problem formulations, our
insights, and recommended solutions.",detect fake advise
http://arxiv.org/abs/1312.5050v1,"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.",detect fake advise
http://arxiv.org/abs/1908.03957v1,"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.",detect fake advise
http://arxiv.org/abs/1901.02212v2,"We present a novel approach to detect synthetic content in portrait videos,
as a preventive solution for the emerging threat of deep fakes. In other words,
we introduce a deep fake detector. We observe that detectors blindly utilizing
deep learning are not effective in catching fake content, as generative models
produce formidably realistic results. Our key assertion follows that biological
signals hidden in portrait videos can be used as an implicit descriptor of
authenticity, because they are neither spatially nor temporally preserved in
fake content. To prove and exploit this assertion, we first exhibit several
unary and binary signal transformations for the pairwise separation problem,
achieving 99.39% accuracy. Second, we utilize those findings to formulate a
generalized classifier for fake content, by analyzing proposed signal
transformations and corresponding feature sets. Third, we generate novel signal
maps and employ a CNN to improve our traditional classifier for detecting
synthetic content. Lastly, we release an ""in the wild"" dataset of fake portrait
videos that we collected as a part of our evaluation process. We evaluate
FakeCatcher both on Face Forensics dataset and on our new Deep Fakes dataset,
performing with 96% and 91.07% accuracies respectively. In addition, our
approach produces a significantly superior detection rate against baselines,
and does not depend on the source, generator, or properties of the fake
content. We also analyze signals from various facial regions, with varying
segment durations, and under several dimensionality reduction techniques.",detect fake advise
http://arxiv.org/abs/1804.10233v1,"Social media for news consumption is becoming increasingly popular due to its
easy access, fast dissemination, and low cost. However, social media also
enable the wide propagation of ""fake news"", i.e., news with intentionally false
information. Fake news on social media poses significant negative societal
effects, and also presents unique challenges. To tackle the challenges, many
existing works exploit various features, from a network perspective, to detect
and mitigate fake news. In essence, news dissemination ecosystem involves three
dimensions on social media, i.e., a content dimension, a social dimension, and
a temporal dimension. In this chapter, we will review network properties for
studying fake news, introduce popular network types and how these networks can
be used to detect and mitigation fake news on social media.",detect fake advise
http://arxiv.org/abs/1908.09805v1,"Automatic detection of fake news --- texts that are deceitful and misleading
--- is a long outstanding and largely unsolved problem. Worse yet, recent
developments in language modeling allow for the automatic generation of such
texts. One approach that has recently gained attention detects these fake news
using stylometry-based provenance, i.e. tracing a text's writing style back to
its producing source and determining whether the source is malicious. This was
shown to be highly effective under the assumption that legitimate text is
produced by humans, and fake text is produced by a language model.
  In this work, we identify a fundamental problem with provenance-based
approaches against attackers that auto-generate fake news: fake and legitimate
texts can originate from nearly identical sources. First, a legitimate text
might be auto-generated in a similar process to that of fake text, and second,
attackers can automatically corrupt articles originating from legitimate human
sources. We demonstrate these issues by simulating attacks in such settings,
and find that the provenance approach fails to defend against them. Our
findings highlight the importance of assessing the veracity of the text rather
than solely relying on its style or source. We also open up a discussion on the
types of benchmarks that should be used to evaluate neural fake news detectors.",detect fake advise
http://arxiv.org/abs/1804.03508v1,"The fake news epidemic makes it imperative to develop a diagnostic framework
that is both parsimonious and valid to guide present and future efforts in fake
news detection. This paper represents one of the very first attempts to fill a
void in the research on this topic. The LeSiE (Lexical Structure, Simplicity,
Emotion) framework we created and validated allows lay people to identify
potential fake news without the use of calculators or complex statistics by
looking out for three simple cues.",detect fake advise
http://arxiv.org/abs/1903.01728v1,"Microblog has become a popular platform for people to post, share, and seek
information due to its convenience and low cost. However, it also facilitates
the generation and propagation of fake news, which could cause detrimental
societal consequences. Detecting fake news on microblogs is important for
societal good. Emotion is a significant indicator while verifying information
on social media. Existing fake news detection studies utilize emotion mainly
through users stances or simple statistical emotional features; and exploiting
the emotion information from both news content and user comments is also
limited. In the realistic scenarios, to impress the audience and spread
extensively, the publishers typically either post a tweet with intense emotion
which could easily resonate with the crowd, or post a controversial statement
unemotionally but aim to evoke intense emotion among the users. Therefore, in
this paper, we study the novel problem of exploiting emotion information for
fake news detection. We propose a new Emotion-based Fake News Detection
framework (EFN), which can i) learn content- and comment- emotion
representations for publishers and users respectively; and ii) exploit content
and social emotions simultaneously for fake news detection. Experimental
results on real-world dataset demonstrate the effectiveness of the proposed
framework.",detect fake advise
http://arxiv.org/abs/1708.07104v1,"The proliferation of misleading information in everyday access media outlets
such as social media feeds, news blogs, and online newspapers have made it
challenging to identify trustworthy news sources, thus increasing the need for
computational tools able to provide insights into the reliability of online
content. In this paper, we focus on the automatic identification of fake
content in online news. Our contribution is twofold. First, we introduce two
novel datasets for the task of fake news detection, covering seven different
news domains. We describe the collection, annotation, and validation process in
detail and present several exploratory analysis on the identification of
linguistic differences in fake and legitimate news content. Second, we conduct
a set of learning experiments to build accurate fake news detectors. In
addition, we provide comparative analyses of the automatic and manual
identification of fake news.",detect fake advise
http://arxiv.org/abs/1806.07516v2,"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.",detect fake advise
http://arxiv.org/abs/1901.09657v1,"News plays a significant role in shaping people's beliefs and opinions. Fake
news has always been a problem, which wasn't exposed to the mass public until
the past election cycle for the 45th President of the United States. While
quite a few detection methods have been proposed to combat fake news since
2015, they focus mainly on linguistic aspects of an article without any fact
checking. In this paper, we argue that these models have the potential to
misclassify fact-tampering fake news as well as under-written real news.
Through experiments on Fakebox, a state-of-the-art fake news detector, we show
that fact tampering attacks can be effective. To address these weaknesses, we
argue that fact checking should be adopted in conjunction with linguistic
characteristics analysis, so as to truly separate fake news from real news. A
crowdsourced knowledge graph is proposed as a straw man solution to collecting
timely facts about news events.",detect fake advise
http://arxiv.org/abs/1905.04260v1,"Over the past few years, we have been witnessing the rise of misinformation
on the Web. People fall victims of fake news during their daily lives and
assist their further propagation knowingly and inadvertently. There have been
many initiatives that are trying to mitigate the damage caused by fake news,
focusing on signals from either domain flag-lists, online social networks or
artificial intelligence. In this work, we present Check-It, a system that
combines, in an intelligent way, a variety of signals into a pipeline for fake
news identification. Check-It is developed as a web browser plugin with the
objective of efficient and timely fake news detection, respecting the user's
privacy. Experimental results show that Check-It is able to outperform the
state-of-the-art methods. On a dataset, consisting of 9 millions of articles
labeled as fake and real, Check-It obtains classification accuracies that
exceed 99%.",detect fake advise
http://arxiv.org/abs/1805.08751v2,"In recent years, due to the booming development of online social networks,
fake news for various commercial and political purposes has been appearing in
large numbers and widespread in the online world. With deceptive words, online
social network users can get infected by these online fake news easily, which
has brought about tremendous effects on the offline society already. An
important goal in improving the trustworthiness of information in online social
networks is to identify the fake news timely. This paper aims at investigating
the principles, methodologies and algorithms for detecting fake news articles,
creators and subjects from online social networks and evaluating the
corresponding performance. This paper addresses the challenges introduced by
the unknown characteristics of fake news and diverse connections among news
articles, creators and subjects. This paper introduces a novel automatic fake
news credibility inference model, namely FAKEDETECTOR. Based on a set of
explicit and latent features extracted from the textual information,
FAKEDETECTOR builds a deep diffusive network model to learn the representations
of news articles, creators and subjects simultaneously. Extensive experiments
have been done on a real-world fake news dataset to compare FAKEDETECTOR with
several state-of-the-art models, and the experimental results have demonstrated
the effectiveness of the proposed model.",detect fake advise
http://arxiv.org/abs/1811.04670v1,"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.",detect fake advise
http://arxiv.org/abs/1706.01560v1,"The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.",ai consumer fraud online
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",ai consumer fraud online
http://arxiv.org/abs/1906.04272v3,"Given the magnitude of online auction transactions, it is difficult to
safeguard consumers from dishonest sellers, such as shill bidders. To date, the
application of Machine Learning Techniques (MLTs) to auction fraud has been
limited, unlike their applications for combatting other types of fraud. Shill
Bidding (SB) is a severe auction fraud, which is driven by modern-day
technologies and clever scammers. The difficulty of identifying the behavior of
sophisticated fraudsters and the unavailability of training datasets hinder the
research on SB detection. In this study, we developed a high-quality SB
dataset. To do so, first, we crawled and preprocessed a large number of
commercial auctions and bidders' history as well. We thoroughly preprocessed
both datasets to make them usable for the computation of the SB metrics.
Nevertheless, this operation requires a deep understanding of the behavior of
auctions and bidders. Second, we introduced two new SB pattern s and
implemented other existing SB patterns. Finally, we removed outliers to improve
the quality of training SB data.",ai consumer fraud online
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",ai consumer fraud online
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",ai consumer fraud online
http://arxiv.org/abs/1109.0689v1,"Online auction, shopping, electronic billing etc. all such types of
application involves problems of fraudulent transactions. Online fraud
occurrence and its detection is one of the challenging fields for web
development and online phantom transaction. As no-secure specification of
online frauds is in research database, so the techniques to evaluate and stop
them are also in study. We are providing an approach with Hidden Markov Model
(HMM) and mobile implicit authentication to find whether the user interacting
online is a fraud or not. We propose a model based on these approaches to
counter the occurred fraud and prevent the loss of the customer. Our technique
is more parameterized than traditional approaches and so,chances of detecting
legitimate user as a fraud will reduce.",ai consumer fraud online
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",ai consumer fraud online
http://arxiv.org/abs/1908.10678v1,"Alzheimer's Disease (AD) is the most common type of dementia, comprising
60-80% of cases. There were an estimated 5.8 million Americans living with
Alzheimer's dementia in 2019, and this number will almost double every 20
years. The total lifetime cost of care for someone with dementia is estimated
to be $350,174 in 2018, 70% of which is associated with family-provided care.
Most family caregivers face emotional, financial and physical difficulties. As
a medium to relieve this burden, online communities in social media websites
such as Twitter, Reddit, and Yahoo! Answers provide potential venues for
caregivers to search relevant questions and answers, or post questions and seek
answers from other members. However, there are often a limited number of
relevant questions and responses to search from, and posted questions are
rarely answered immediately. Due to recent advancement in Artificial
Intelligence (AI), particularly Natural Language Processing (NLP), we propose
to utilize AI to automatically generate answers to AD-related consumer
questions posted by caregivers and evaluate how good AI is at answering those
questions. To the best of our knowledge, this is the first study in the
literature applying and evaluating AI models designed to automatically answer
consumer questions related to AD.",ai consumer fraud online
http://arxiv.org/abs/1906.10418v1,"The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process",ai consumer fraud online
http://arxiv.org/abs/1309.7262v1,"Fake websites have emerged as a major source of online fraud, accounting for
billions of dollars of loss by Internet users. We explore the process by which
salient design elements could increase the use of protective tools, thus
reducing the success rate of fake websites. Using the protection motivation
theory, we conceptualize a model to investigate how salient design elements of
detection tools could influence user perceptions of the tools, efficacy in
dealing with threats, and use of such tools. The research method was a
controlled lab experiment with a novel and extensive experimental design and
protocol. We found that trust in the detector is the pivotal coping mechanism
in dealing with security threats and is a major conduit for transforming
salient design elements into increased use. We also found that design elements
have profound and unexpected impacts on self-efficacy. The significant
theoretical and empirical implications of findings are discussed.",ai consumer fraud online
http://arxiv.org/abs/1002.2353v1,"Online advertising is currently the greatest source of revenue for many
Internet giants. The increased number of specialized websites and modern
profiling techniques, have all contributed to an explosion of the income of ad
brokers from online advertising. The single biggest threat to this growth, is
however, click-fraud. Trained botnets and even individuals are hired by
click-fraud specialists in order to maximize the revenue of certain users from
the ads they publish on their websites, or to launch an attack between
competing businesses.
  In this note we wish to raise the awareness of the networking research
community on potential research areas within this emerging field. As an example
strategy, we present Bluff ads; a class of ads that join forces in order to
increase the effort level for click-fraud spammers. Bluff ads are either
targeted ads, with irrelevant display text, or highly relevant display text,
with irrelevant targeting information. They act as a litmus test for the
legitimacy of the individual clicking on the ads. Together with standard
threshold-based methods, fake ads help to decrease click-fraud levels.",ai consumer fraud online
http://arxiv.org/abs/1905.13649v6,"Online reviews play a crucial role in deciding the quality before purchasing
any product. Unfortunately, spammers often take advantage of online review
forums by writing fraud reviews to promote/demote certain products. It may turn
out to be more detrimental when such spammers collude and collectively inject
spam reviews as they can take complete control of users' sentiment due to the
volume of fraud reviews they inject. Group spam detection is thus more
challenging than individual-level fraud detection due to unclear definition of
a group, variation of inter-group dynamics, scarcity of labeled group-level
spam data, etc. Here, we propose DeFrauder, an unsupervised method to detect
online fraud reviewer groups. It first detects candidate fraud groups by
leveraging the underlying product review graph and incorporating several
behavioral signals which model multi-faceted collaboration among reviewers. It
then maps reviewers into an embedding space and assigns a spam score to each
group such that groups comprising spammers with highly similar behavioral
traits achieve high spam score. While comparing with five baselines on four
real-world datasets (two of them were curated by us), DeFrauder shows superior
performance by outperforming the best baseline with 17.11% higher NDCG@50 (on
average) across datasets.",ai consumer fraud online
http://arxiv.org/abs/1404.2671v1,"We consider the {\em multi-shop ski rental} problem. This problem generalizes
the classic ski rental problem to a multi-shop setting, in which each shop has
different prices for renting and purchasing a pair of skis, and a
\emph{consumer} has to make decisions on when and where to buy. We are
interested in the {\em optimal online (competitive-ratio minimizing) mixed
strategy} from the consumer's perspective. For our problem in its basic form,
we obtain exciting closed-form solutions and a linear time algorithm for
computing them. We further demonstrate the generality of our approach by
investigating three extensions of our basic problem, namely ones that consider
costs incurred by entering a shop or switching to another shop. Our solutions
to these problems suggest that the consumer must assign positive probability in
\emph{exactly one} shop at any buying time. Our results apply to many
real-world applications, ranging from cost management in \texttt{IaaS} cloud to
scheduling in distributed computing.",ai consumer fraud online
http://arxiv.org/abs/1910.03033v1,"Two elements have been essential to AI's recent boom: (1) deep neural nets
and the theory and practice behind them; and (2) cloud computing with its
abundant labeled data and large computing resources.
  Abundant labeled data is available for key domains such as images, speech,
natural language processing, and recommendation engines. However, there are
many other domains where such data is not available, or access to it is highly
restricted for privacy reasons, as with health and financial data. Even when
abundant data is available, it is often not labeled. Doing such labeling is
labor-intensive and non-scalable.
  As a result, to the best of our knowledge, key domains still lack labeled
data or have at most toy data; or the synthetic data must have access to real
data from which it can mimic new data. This paper outlines work to generate
realistic synthetic data for an important domain: credit card transactions.
  Some challenges: there are many patterns and correlations in real purchases.
There are millions of merchants and innumerable locations. Those merchants
offer a wide variety of goods. Who shops where and when? How much do people
pay? What is a realistic fraudulent transaction?
  We use a mixture of technical approaches and domain knowledge including
mechanics of credit card processing, a broad set of consumer domains:
electronics, clothing, hair styling, etc. Connecting everything is a virtual
world. This paper outlines some of our key techniques and provides evidence
that the data generated is indeed realistic.
  Beyond the scope of this paper: (1) use of our data to develop and train
models to predict fraud; (2) coupling models and the synthetic dataset to
assess performance in designing accelerators such as GPUs and TPUs.",ai consumer fraud online
http://arxiv.org/abs/1510.07165v1,"Financial fraud is an issue with far reaching consequences in the finance
industry, government, corporate sectors, and for ordinary consumers. Increasing
dependence on new technologies such as cloud and mobile computing in recent
years has compounded the problem. Traditional methods of detection involve
extensive use of auditing, where a trained individual manually observes reports
or transactions in an attempt to discover fraudulent behaviour. This method is
not only time consuming, expensive and inaccurate, but in the age of big data
it is also impractical. Not surprisingly, financial institutions have turned to
automated processes using statistical and computational methods. This paper
presents a comprehensive investigation on financial fraud detection practices
using such data mining methods, with a particular focus on computational
intelligence-based techniques. Classification of the practices based on key
aspects such as detection algorithm used, fraud type investigated, and success
rate have been covered. Issues and challenges associated with the current
practices and potential future direction of research have also been identified.",ai consumer fraud online
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",ai consumer fraud online
http://arxiv.org/abs/1503.03208v1,"Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System & Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.",ai consumer fraud online
http://arxiv.org/abs/1805.10053v2,"Frauds severely hurt many kinds of Internet businesses. Group-based fraud
detection is a popular methodology to catch fraudsters who unavoidably exhibit
synchronized behaviors. We combine both graph-based features (e.g. cluster
density) and information-theoretical features (e.g. probability for the
similarity) of fraud groups into two intuitive metrics. Based on these metrics,
we build an extensible fraud detection framework, BadLink, to support
multimodal datasets with different data types and distributions in a scalable
way. Experiments on real production workload, as well as extensive comparison
with existing solutions demonstrate the state-of-the-art performance of
BadLink, even with sophisticated camouflage traffic.",ai consumer fraud online
http://arxiv.org/abs/1006.2689v1,"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.",ai consumer fraud online
http://arxiv.org/abs/1808.05329v1,"Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer's interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.",ai consumer fraud online
http://arxiv.org/abs/1611.02260v1,"Food fraud has been an area of great concern due to its risk to public
health, reduction of food quality or nutritional value and for its economic
consequences. For this reason, it's been object of regulation in many countries
(e.g. [1], [2]). One type of food that has been frequently object of fraud
through the addition of water or an aqueous solution is bovine meat. The
traditional methods used to detect this kind of fraud are expensive,
time-consuming and depend on physicochemical analysis that require complex
laboratory techniques, specific for each added substance. In this paper, based
on digital images of histological cuts of adulterated and not-adulterated
(normal) bovine meat, we evaluate the of digital image analysis methods to
identify the aforementioned kind of fraud, with focus on the Local Binary
Pattern (LBP) algorithm.",ai consumer fraud online
http://arxiv.org/abs/1805.09741v2,"The Automobile Insurance Fraud is one of the main challenges for insurance
companies. This form of fraud is performed either opportunistic or professional
occurring through group cooperation that leads to greater financial losses,
while most presented methods thus far are unsuited for flagging these groups.
The article has put forward a new approach for identification, representation,
and analysis of organized fraudulent groups in automobile insurance through
focusing on structural aspects of networks, and cycles in particular, that
demonstrate the occurrence of potential fraud. Suspicious groups have been
detected by applying cycle detection algorithms (using both DFS, BFS trees),
afterward, the probability of being fraudulent for suspicious components were
investigated to reveal fraudulent groups with the maximum likelihood, and their
reviews were prioritized. The actual data of Iran Insurance Company is used for
evaluating the provided approach. As a result, the detection of cycles is not
only more efficient, accurate, but also less time-consuming in comparison with
previous methods for finding such groups.",ai consumer fraud online
http://arxiv.org/abs/1904.10604v1,"Credit card has become popular mode of payment for both online and offline
purchase, which leads to increasing daily fraud transactions. An Efficient
fraud detection methodology is therefore essential to maintain the reliability
of the payment system. In this study, we perform a comparison study of credit
card fraud detection by using various supervised and unsupervised approaches.
Specifically, 6 supervised classification models, i.e., Logistic Regression
(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree
(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4
unsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),
Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative
Adversarial Networks (GAN), are explored in this study. We train all these
models on a public credit card transaction dataset from Kaggle website, which
contains 492 frauds out of 284,807 transactions. The labels of the transactions
are used for supervised learning models only. The performance of each model is
evaluated through 5-fold cross validation in terms of Area Under the Receiver
Operating Curves (AUROC). Within supervised approaches, XGB and RF obtain the
best performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for
unsupervised approaches, RBM achieves the best performance with AUROC = 0.961,
followed by GAN with AUROC = 0.954. The experimental results show that
supervised models perform slightly better than unsupervised models in this
study. Anyway, unsupervised approaches are still promising for credit card
fraud transaction detection due to the insufficient annotation and the data
imbalance issue in real-world applications.",ai consumer fraud online
http://arxiv.org/abs/1905.04576v1,"In this paper, we describe a new type of online fraud, referred to as
'eWhoring' by offenders. This crime script analysis provides an overview of the
'eWhoring' business model, drawing on more than 6,500 posts crawled from an
online underground forum. This is an unusual fraud type, in that offenders
readily share information about how it is committed in a way that is almost
prescriptive. There are economic factors at play here, as providing information
about how to make money from 'eWhoring' can increase the demand for the types
of images that enable it to happen. We find that sexualised images are
typically stolen and shared online. While some images are shared for free,
these can quickly become 'saturated', leading to the demand for (and trade in)
more exclusive 'packs'. These images are then sold to unwitting customers who
believe they have paid for a virtual sexual encounter. A variety of online
services are used for carrying out this fraud type, including email, video,
dating sites, social media, classified advertisements, and payment platforms.
This analysis reveals potential interventions that could be applied to each
stage of the crime commission process to prevent and disrupt this crime type.",ai consumer fraud online
http://arxiv.org/abs/1607.04451v4,"Emerging trends in smartphones, online maps, social media, and the resulting
geo-located data, provide opportunities to collect traces of people's
socio-economical activities in a much more granular and direct fashion,
triggering a revolution in empirical research. These vast mobile data offer new
perspectives and approaches for measurements of economic dynamics and are
broadening the research fields of social science and economics. In this paper,
we explore the potential of using mobile big data for measuring economic
activities of China. Firstly, We build indices for gauging employment and
consumer trends based on billions of geo-positioning data. Secondly, we advance
the estimation of store offline foot traffic via location search data derived
from Baidu Maps, which is then applied to predict revenues of Apple in China
and detect box-office fraud accurately. Thirdly, we construct consumption
indicators to track the trends of various industries in service sector, which
are verified by several existing indicators. To the best of our knowledge, we
are the first to measure the second largest economy by mining such
unprecedentedly large scale and fine granular spatial-temporal data. Our
research provides new approaches and insights on measuring economic activities.",ai consumer fraud online
http://arxiv.org/abs/1905.12593v2,"Online romance scams are a prevalent form of mass-marketing fraud in the
West, and yet few studies have addressed the technical or data-driven responses
to this problem. In this type of scam, fraudsters craft fake profiles and
manually interact with their victims. Because of the characteristics of this
type of fraud and of how dating sites operate, traditional detection methods
(e.g., those used in spam filtering) are ineffective. In this paper, we present
the results of a multi-pronged investigation into the archetype of online
dating profiles used in this form of fraud, including their use of
demographics, profile descriptions, and images, shedding light on both the
strategies deployed by scammers to appeal to victims and the traits of victims
themselves. Further, in response to the severe financial and psychological harm
caused by dating fraud, we develop a system to detect romance scammers on
online dating platforms. Our work presents the first system for automatically
detecting this fraud. Our aim is to provide an early detection system to stop
romance scammers as they create fraudulent profiles or before they engage with
potential victims. Previous research has indicated that the victims of romance
scams score highly on scales for idealized romantic beliefs. We combine a range
of structured, unstructured, and deep-learned features that capture these
beliefs. No prior work has fully analyzed whether these notions of romance
introduce traits that could be leveraged to build a detection system. Our
ensemble machine-learning approach is robust to the omission of profile details
and performs at high accuracy (97\%). The system enables development of
automated tools for dating site providers and individual users.",ai consumer fraud online
http://arxiv.org/abs/1806.00656v2,"In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.",ai consumer fraud online
http://arxiv.org/abs/1803.01798v2,"Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.",ai consumer fraud online
http://arxiv.org/abs/1809.04683v2,"Many online platforms have deployed anti-fraud systems to detect and prevent
fraudulent activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, which maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe the user suspended time instead of the
fraudulent activity time in the training data, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.",ai consumer fraud online
http://arxiv.org/abs/1811.08502v1,"The last decade has witnessed an explosion on the computational power and a
parallel increase of the access to large sets of data (the so called Big Data
paradigm) which is enabling to develop brand new quantitative strategies
underpinning description, understanding and control of complex scenarios. One
interesting area of application concerns fraud detection from online data, and
more particularly extracting meaningful information from massive digital
fingerprints of electoral activity to detect, a posteriori, evidence of
fraudulent behavior. In this short article we discuss a few quantitative
methodologies that have emerged in recent years on this respect, which
altogether form the nascent interdisciplinary field of election forensics.",ai consumer fraud online
http://arxiv.org/abs/0801.2700v1,"Labels and tags are accompanying us in almost each moment of our life and
everywhere we are going, in the form of electronic keys or money, or simply as
labels on products we are buying in shops and markets. The label diffusion,
rapidly increasing for logistic reasons in the actual global market, carries
huge amount of information but it is demanding security and anti-fraud systems.
The first crucial point, for the consumer and producer safety, is to ensure the
authenticity of the labelled products with systems against counterfeiting and
piracy. Recent anti-fraud techniques are based on a sophisticated use of
physical effects, from holograms till magnetic resonance or tunnel transitions
between atomic sublevels. In this paper we will discuss labels and anti-fraud
technologies as a new and very promising research field for applied physics.",ai consumer fraud online
http://arxiv.org/abs/1908.04240v1,"Detecting concept drift is a well known problem that affects production
systems. However, two important issues that are frequently not addressed in the
literature are 1) the detection of drift when the labels are not immediately
available; and 2) the automatic generation of explanations to identify possible
causes for the drift. For example, a fraud detection model in online payments
could show a drift due to a hot sale item (with an increase in false positives)
or due to a true fraud attack (with an increase in false negatives) before
labels are available. In this paper we propose SAMM, an automatic model
monitoring system for data streams. SAMM detects concept drift using a time and
space efficient unsupervised streaming algorithm and it generates alarm reports
with a summary of the events and features that are important to explain it.
SAMM was evaluated in five real world fraud detection datasets, each spanning
periods up to eight months and totaling more than 22 million online
transactions. We evaluated SAMM using human feedback from domain experts, by
sending them 100 reports generated by the system. Our results show that SAMM is
able to detect anomalous events in a model life cycle that are considered
useful by the domain experts. Given these results, SAMM will be rolled out in a
next version of Feedzai's Fraud Detection solution.",ai consumer fraud online
http://arxiv.org/abs/1705.01010v3,"Image-based modeling techniques can now generate photo-realistic 3D models
from images. But it is up to users to provide high quality images with good
coverage and view overlap, which makes the data capturing process tedious and
time consuming. We seek to automate data capturing for image-based modeling.
The core of our system is an iterative linear method to solve the multi-view
stereo (MVS) problem quickly and plan the Next-Best-View (NBV) effectively. Our
fast MVS algorithm enables online model reconstruction and quality assessment
to determine the NBVs on the fly. We test our system with a toy unmanned aerial
vehicle (UAV) in simulated, indoor and outdoor experiments. Results show that
our system improves the efficiency of data acquisition and ensures the
completeness of the final model.",ai consumer fraud online
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",ai consumer fraud online
http://arxiv.org/abs/0909.1145v1,"The development of electronic commerce is characterized with anonymity,
uncertainty, lack of control and potential opportunism. Therefore, the success
of electronic commerce significantly depends on providing security and privacy
for its consumers sensitive personal data. Consumers lack of acceptance in
electronic commerce adoption today is not merely due to the concern on security
and privacy of their personal data, but also lack of trust and reliability of
Web vendors. Consumers trust in online transactions is crucial for the
continuous growth and development of electronic commerce. Since Business to
Consumer (B2C) ecommerce requires the consumers to engage the technologies, the
consumers face a variety of security risks. This study addressed the role of
security, privacy and risk perceptions of consumers to shop online in order to
establish a consensus among them. The analyses provided descriptive frequencies
for the research variables and for each of the study s research constructs. In
addition, the analyses were completed with factor analysis and Pearson
correlation coefficients. The findings suggested that perceived privacy of
online transaction on trust is mediated by perceived security, and consumers
trust in online transaction is significantly related with the trustworthiness
of Web vendors. Also, consumers trust is negatively associated with perceived
risks in online transactions. However, there is no significant impact from
perceived security and perceived privacy to trust in online transactions.",ai consumer fraud online
http://arxiv.org/abs/1705.10443v1,"Games have always been popular testbeds for Artificial Intelligence (AI). In
the last decade, we have seen the rise of the Multiple Online Battle Arena
(MOBA) games, which are the most played games nowadays. In spite of this, there
are few works that explore MOBA as a testbed for AI Research. In this paper we
present and discuss the main features and opportunities offered by MOBA games
to Game AI Research. We describe the various challenges faced along the game
and also propose a discrete model that can be used to better understand and
explore the game. With this, we aim to encourage the use of MOBA as a novel
research platform for Game AI.",ai consumer fraud online
http://arxiv.org/abs/1706.03122v1,"MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.",ai consumer fraud online
http://arxiv.org/abs/1808.07261v2,"Accuracy is an important concern for suppliers of artificial intelligence
(AI) services, but considerations beyond accuracy, such as safety (which
includes fairness and explainability), security, and provenance, are also
critical elements to engender consumers' trust in a service. Many industries
use transparent, standardized, but often not legally required documents called
supplier's declarations of conformity (SDoCs) to describe the lineage of a
product along with the safety and performance testing it has undergone. SDoCs
may be considered multi-dimensional fact sheets that capture and quantify
various aspects of the product and its development to make it worthy of
consumers' trust. Inspired by this practice, we propose FactSheets to help
increase trust in AI services. We envision such documents to contain purpose,
performance, safety, security, and provenance information to be completed by AI
service providers for examination by consumers. We suggest a comprehensive set
of declaration items tailored to AI and provide examples for two fictitious AI
services in the appendix of the paper.",ai consumer fraud online
http://arxiv.org/abs/1811.12740v1,"Micropayment channels are the most prominent solution to the limitation on
transaction throughput in current blockchain systems. However, in practice
channels are risky because participants have to be online constantly to avoid
fraud, and inefficient because participants have to open multiple channels and
lock funds in them. To address the security issue, we propose a novel mechanism
that involves watchtowers incentivized to watch the channels and reveal a
fraud. Our protocol does not require participants to be online constantly
watching the blockchain. The protocol is secure, incentive compatible and
lightweight in communication. Furthermore, we present an adaptation of our
protocol implementable on the Lightning protocol. Towards efficiency, we
examine specific topological structures in the blockchain transaction graph and
generalize the construction of channels to enable topologies better suited to
specific real-world needs. In these cases, our construction reduces the
required amount of signatures for a transaction and the total amount of locked
funds in the system.",ai consumer fraud online
http://arxiv.org/abs/1910.04133v1,"The massive growth of the Internet of Things (IoT) as a network of
interconnected entities [18], brings up new challenges in terms of privacy and
security requirements to the traditional software engineering domain [4]. To
protect the individuals' privacy, the FTC's Fair Information Practice
Principles (FIPPs) [6] proposes to companies to give notice to the consumer
about their data practices, provide them with choices and give them means to
have control over their own data.. Using privacy policy is the most common way
for this type of notices. However, privacy policies are not generally effective
due to two main reasons: first, privacy policies are long and full of legal
jargon which are not understandable by a normal user; second, it is not
guaranteed that an IoT device behave as it is explained in its privacy policy.
In this technical report, we propose and discuss our methodologies to analyze
privacy policies. By the help of this analysis, we reduce the length of a
privacy policy and make it organized based on privacy practices to improve
understanding level for the user. We also come up with a method to find the
inconsistencies between IoT devices and their privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1903.06068v2,"In this report, we present an approach to enhance informed consent for the
processing of personal data. The approach relies on a privacy policy language
used to express, compare and analyze privacy policies. We describe a tool that
automatically reports the privacy risks associated with a given privacy policy
in order to enhance data subjects' awareness and to allow them to make more
informed choices. The risk analysis of privacy policies is illustrated with an
IoT example.",analyzing privacy policies
http://arxiv.org/abs/1809.02236v1,"In this paper, we demonstrate the effectiveness of using the theory of
contextual integrity (CI) to annotate and evaluate privacy policy statements.
We perform a case study using CI annotations to compare Facebook's privacy
policy before and after the Cambridge Analytica scandal. The updated Facebook
privacy policy provides additional details about what information is being
transferred, from whom, by whom, to whom, and under what conditions. However,
some privacy statements prescribe an incomprehensibly large number of
information flows by including many CI parameters in single statements. Other
statements result in incomplete information flows due to the use of vague terms
or omitting contextual parameters altogether. We then demonstrate that
crowdsourcing can effectively produce CI annotations of privacy policies at
scale. We test the CI annotation task on 48 excerpts of privacy policies from
17 companies with 141 crowdworkers. The resulting high precision annotations
indicate that crowdsourcing could be used to produce a large corpus of
annotated privacy policies for future research.",analyzing privacy policies
http://arxiv.org/abs/1906.12038v1,"With the arrival of the European Union's General Data Protection Regulation
(GDPR), several companies are making significant changes to their systems to
achieve compliance. The changes range from modifying privacy policies to
redesigning systems which process personal data. This work analyzes the privacy
policies of large-scaled cloud services which seek to be GDPR compliant. The
privacy policy is the main medium of information dissemination between the data
controller and the users. We show that many services that claim compliance
today do not have clear and concise privacy policies. We identify several
points in the privacy policies which potentially indicate non-compliance; we
term these GDPR vulnerabilities. We identify GDPR vulnerabilities in ten cloud
services. Based on our analysis, we propose seven best practices for crafting
GDPR privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1908.06814v1,"Privacy policies are the main way to obtain information related to personal
data collection and processing.Originally, privacy policies were presented as
textual documents. However, the unsuitability of this format for the needs of
today's society gave birth to others means of expression. In this report, we
systematically study the different means of expression of privacy policies. In
doing so, we have identified three main categories, which we call dimensions,
i.e., natural language, graphical and machine-readable privacy policies. Each
of these dimensions focus on the particular needs of the communities they come
from, i.e., law experts, organizations and privacy advocates, and academics,
respectively. We then analyze the benefits and limitations of each dimension,
and explain why solutions based on a single dimension do not cover the needs of
other communities. Finally, we propose a new approach to expressing privacy
policies which brings together the benefits of each dimension as an attempt to
overcome their limitations.",analyzing privacy policies
http://arxiv.org/abs/1805.01187v1,"A dominant regulatory model for web privacy is ""notice and choice"". In this
model, users are notified of data collection and provided with options to
control it. To examine the efficacy of this approach, this study presents the
first large-scale audit of disclosure of third-party data collection in website
privacy policies. Data flows on one million websites are analyzed and over
200,000 websites' privacy policies are audited to determine if users are
notified of the names of the companies which collect their data. Policies from
25 prominent third-party data collectors are also examined to provide deeper
insights into the totality of the policy environment. Policies are additionally
audited to determine if the choice expressed by the ""Do Not Track"" browser
setting is respected.
  Third-party data collection is wide-spread, but fewer than 15% of attributed
data flows are disclosed. The third-parties most likely to be disclosed are
those with consumer services users may be aware of, those without consumer
services are less likely to be mentioned. Policies are difficult to understand
and the average time requirement to read both a given site{\guillemotright}s
policy and the associated third-party policies exceeds 84 minutes. Only 7% of
first-party site policies mention the Do Not Track signal, and the majority of
such mentions are to specify that the signal is ignored. Among third-party
policies examined, none offer unqualified support for the Do Not Track signal.
Findings indicate that current implementations of ""notice and choice"" fail to
provide notice or respect choice.",analyzing privacy policies
http://arxiv.org/abs/1810.11153v4,"A deterministic privacy metric using non-stochastic information theory is
developed. Particularly, minimax information is used to construct a measure of
information leakage, which is inversely proportional to the measure of privacy.
Anyone can submit a query to a trusted agent with access to a non-stochastic
uncertain private dataset. Optimal deterministic privacy-preserving policies
for responding to the submitted query are computed by maximizing the measure of
privacy subject to a constraint on the worst-case quality of the response
(i.e., the worst-case difference between the response by the agent and the
output of the query computed on the private dataset). The optimal
privacy-preserving policy is proved to be a piecewise constant function in the
form of a quantization operator applied on the output of the submitted query.
The measure of privacy is also used to analyze the performance of $k$-anonymity
methodology (a popular deterministic mechanism for privacy-preserving release
of datasets using suppression and generalization techniques), proving that it
is in fact not privacy-preserving.",analyzing privacy policies
http://arxiv.org/abs/1808.05096v4,"The European Union's General Data Protection Regulation (GDPR) went into
effect on May 25, 2018. Its privacy regulations apply to any service and
company collecting or processing personal data in Europe. Many companies had to
adjust their data handling processes, consent forms, and privacy policies to
comply with the GDPR's transparency requirements. We monitored this rare event
by analyzing the GDPR's impact on popular websites in all 28 member states of
the European Union. For each country, we periodically examined its 500 most
popular websites - 6,579 in total - for the presence of and updates to their
privacy policy. While many websites already had privacy policies, we find that
in some countries up to 15.7 % of websites added new privacy policies by May
25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of
websites with existing privacy policies updated them close to the date. Most
visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 %
more than in January 2018. These notices inform users about a site's cookie use
and user tracking practices. We categorized all observed cookie consent notices
and evaluated 16 common implementations with respect to their technical
realization of cookie consent. Our analysis shows that core web security
mechanisms such as the same-origin policy pose problems for the implementation
of consent according to GDPR rules, and opting out of third-party cookies
requires the third party to cooperate. Overall, we conclude that the GDPR is
making the web more transparent, but there is still a lack of both functional
and usable mechanisms for users to consent to or deny processing of their
personal data on the Internet.",analyzing privacy policies
http://arxiv.org/abs/cs/0001011v1,"A variety of tools have been introduced recently that are designed to help
people protect their privacy on the Internet. These tools perform many
different functions in-cluding encrypting and/or anonymizing communications,
preventing the use of persistent identifiers such as cookies, automatically
fetching and analyzing web site privacy policies, and displaying
privacy-related information to users. This paper discusses the set of privacy
tools that aim specifically at facilitating notice and choice about Web site
data practices. While these tools may also have components that perform other
functions such as encryption, or they may be able to work in conjunction with
other privacy tools, the primary pur-pose of these tools is to help make users
aware of web site privacy practices and to make it easier for users to make
informed choices about when to provide data to web sites. Examples of such
tools include the Platform for Privacy Preferences (P3P) and various
infomediary services.",analyzing privacy policies
http://arxiv.org/abs/1902.00174v1,"Many reinforcement learning applications involve the use of data that is
sensitive, such as medical records of patients or financial information.
However, most current reinforcement learning methods can leak information
contained within the (possibly sensitive) data on which they are trained. To
address this problem, we present the first differentially private approach for
off-policy evaluation. We provide a theoretical analysis of the
privacy-preserving properties of our algorithm and analyze its utility (speed
of convergence). After describing some results of this theoretical analysis, we
show empirically that our method outperforms previous methods (which are
restricted to the on-policy setting).",analyzing privacy policies
http://arxiv.org/abs/1608.04671v2,"Privacy analysis is critical but also a time-consuming and tedious task. We
present a formalization which eases designing and auditing high-level privacy
properties of software architectures. It is incorporated into a larger policy
analysis and verification framework and enables the assessment of commonly
accepted data protection goals of privacy. The formalization is based on static
taint analysis and makes flow and processing of privacy-critical data explicit,
globally as well as on the level of individual data subjects. Formally, we show
equivalence to traditional label-based information flow security and prove
overall soundness of our tool with Isabelle/HOL. We demonstrate applicability
in two real-world case studies, thereby uncovering previously unknown
violations of privacy constraints in the analyzed software architectures.",analyzing privacy policies
http://arxiv.org/abs/1710.08306v1,"Mobile phones provide an excellent opportunity for building context-aware
applications. In particular, location-based services are important
context-aware services that are more and more used for enforcing security
policies, for supporting indoor room navigation, and for providing personalized
assistance. However, a major problem still remains unaddressed---the lack of
solutions that work across buildings while not using additional infrastructure
and also accounting for privacy and reliability needs. In this paper, a
privacy-preserving, multi-modal, cross-building, collaborative localization
platform is proposed based on Wi-Fi RSSI (existing infrastructure), Cellular
RSSI, sound and light levels, that enables room-level localization as main
application (though sub room level granularity is possible). The privacy is
inherently built into the solution based on onion routing, and
perturbation/randomization techniques, and exploits the idea of weighted
collaboration to increase the reliability as well as to limit the effect of
noisy devices (due to sensor noise/privacy). The proposed solution has been
analyzed in terms of privacy, accuracy, optimum parameters, and other overheads
on location data collected at multiple indoor and outdoor locations using an
Android app.",analyzing privacy policies
http://arxiv.org/abs/1802.02561v2,"Privacy policies are the primary channel through which companies inform users
about their data collection and sharing practices. These policies are often
long and difficult to comprehend. Short notices based on information extracted
from privacy policies have been shown to be useful but face a significant
scalability hurdle, given the number of policies and their evolution over time.
Companies, users, researchers, and regulators still lack usable and scalable
tools to cope with the breadth and depth of privacy policies. To address these
hurdles, we propose an automated framework for privacy policy analysis
(Polisis). It enables scalable, dynamic, and multi-dimensional queries on
natural language privacy policies. At the core of Polisis is a privacy-centric
language model, built with 130K privacy policies, and a novel hierarchy of
neural-network classifiers that accounts for both high-level aspects and
fine-grained details of privacy practices. We demonstrate Polisis' modularity
and utility with two applications supporting structured and free-form querying.
The structured querying application is the automated assignment of privacy
icons from privacy policies. With Polisis, we can achieve an accuracy of 88.4%
on this task. The second application, PriBot, is the first freeform
question-answering system for privacy policies. We show that PriBot can produce
a correct answer among its top-3 results for 82% of the test questions. Using
an MTurk user study with 700 participants, we show that at least one of
PriBot's top-3 answers is relevant to users for 89% of the test questions.",analyzing privacy policies
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",analyzing privacy policies
http://arxiv.org/abs/1512.00201v1,"For security and privacy management and enforcement purposes, various policy
languages have been presented. We give an overview on 27 security and privacy
policy languages and present a categorization framework for policy languages.
We show how the current policy languages are represented in the framework and
summarize our interpretation. We show up identified gaps and motivate for the
adoption of policy languages for the specification of privacy-utility trade-off
policies.",analyzing privacy policies
http://arxiv.org/abs/1806.00114v1,"We examine the problem of target tracking whilst simultaneously preserving
the target's privacy as epitomized by the robotic panda tracking scenario,
which O'Kane introduced at the 2008 Workshop on the Algorithmic Foundations of
Robotics in order to elegantly illustrate the utility of ignorance. The present
paper reconsiders his formulation and the tracking strategy he proposed, along
with its completeness. We explore how the capabilities of the robot and panda
affect the feasibility of tracking with a privacy stipulation, uncovering
intrinsic limits, no matter the strategy employed. This paper begins with a
one-dimensional setting and, putting the trivially infeasible problems aside,
analyzes the strategy space as a function of problem parameters. We show that
it is not possible to actively track the target as well as protect its privacy
for every nontrivial pair of tracking and privacy stipulations. Secondly,
feasibility can be sensitive, in several cases, to the information available to
the robot initially. Quite naturally in the one-dimensional model, one may
quantify sensing power by the number of perceptual (or output) classes
available to the robot. The robot's power to achieve privacy-preserving
tracking is bounded, converging asymptotically with increasing sensing power.
We analyze the entire space of possible tracking problems, characterizing every
instance as either achievable, constructively by giving a policy where one
exists (some of which depend on the initial information), or proving the
instance impossible. Finally, to relate some of the impossibility results in
one dimension to their higher-dimensional counterparts, including the planar
panda tracking problem studied by O'Kane, we establish a connection between
tracking dimensionality and the sensing power of a one-dimensional robot.",analyzing privacy policies
http://arxiv.org/abs/1809.08396v3,"The EU General Data Protection Regulation (GDPR) is one of the most demanding
and comprehensive privacy regulations of all time. A year after it went into
effect, we study its impact on the landscape of privacy policies online. We
conduct the first longitudinal, in-depth, and at-scale assessment of privacy
policies before and after the GDPR. We gauge the complete consumption cycle of
these policies, from the first user impressions until the compliance
assessment. We create a diverse corpus of two sets of 6,278 unique
English-language privacy policies from inside and outside the EU, covering
their pre-GDPR and the post-GDPR versions. The results of our tests and
analyses suggest that the GDPR has been a catalyst for a major overhaul of the
privacy policies inside and outside the EU. This overhaul of the policies,
manifesting in extensive textual changes, especially for the EU-based websites,
comes at mixed benefits to the users. While the privacy policies have become
considerably longer, our user study with 470 participants on Amazon MTurk
indicates a significant improvement in the visual representation of privacy
policies from the users' perspective for the EU websites. We further develop a
new workflow for the automated assessment of requirements in privacy policies.
Using this workflow, we show that privacy policies cover more data practices
and are more consistent with seven compliance requirements post the GDPR. We
also assess how transparent the organizations are with their privacy practices
by performing specificity analysis. In this analysis, we find evidence for
positive changes triggered by the GDPR, with the specificity level improving on
average. Still, we find the landscape of privacy policies to be in a
transitional phase; many policies still do not meet several key GDPR
requirements or their improved coverage comes with reduced specificity.",analyzing privacy policies
http://arxiv.org/abs/1704.01218v1,"More data is currently being collected and shared by software applications
than ever before. In many cases, the user is asked if either all or none of
their data can be shared. We hypothesize that in some cases, users would like
to share data in more complex ways. In order to implement the sharing of data
using more complicated privacy preferences, complex data sharing policies must
be used. These complex sharing policies require more space to store than a
simple ""all or nothing"" approach to data sharing. In this paper, we present a
new probabilistic data structure, called the Min Mask Sketch, to efficiently
store these complex data sharing policies. We describe an implementation for
the Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility
of using a probabilistic data structure for storing complex data sharing
policies.",analyzing privacy policies
http://arxiv.org/abs/1910.03622v1,"Mobile applications (apps) have become deeply personal, constantly demanding
access to privacy-sensitive information in exchange for more personalized user
experiences. Such privacy-invading practices have generated major
multidimensional and unconventional privacy concerns among app users. To
address these concerns, the research on mobile app privacy has experienced
rapid growth over the past decade. In general, this line of research is aimed
at systematically exposing the privacy practices of apps and proposing
solutions to protect the privacy of mobile app users. In this survey paper, we
conduct a systematic mapping study of 54 Software Engineering (SE) primary
studies on mobile app privacy. Our objectives are to a) explore trends in SE
app privacy research, b) categorize existing evidence, and c) identify
potential directions for future research. Our results show that existing
literature can be divided into four main categories: privacy policy,
requirements, user perspective, and leak detection. Furthermore, our survey
reveals an imbalance between these categories; majority of existing research
focuses on proposing tools for detecting privacy leaks, with less studies
targeting privacy requirements and policy and even less on user perspective.
Finally, our survey exposes several gaps in existing research and suggests
areas for improvement.",analyzing privacy policies
http://arxiv.org/abs/1710.06494v2,"In this paper we propose a formal framework for studying privacy in
information systems. The proposal follows a two-axes schema where the first
axis considers privacy as a taxonomy of rights and the second axis involves the
ways an information system stores and manipulates information. We develop a
correspondence between the above schema and an associated model of computation.
In particular, we propose the \Pcalc, a calculus based on the $\pi$-calculus
with groups extended with constructs for reasoning about private data. The
privacy requirements of an information system are captured via a privacy policy
language. The correspondence between the privacy model and the \Pcalc semantics
is established using a type system for the calculus and a satisfiability
definition between types and privacy policies. We deploy a type preservation
theorem to show that a system respects a policy and it is safe if the typing of
the system satisfies the policy. We illustrate our methodology via analysis of
two use cases: a privacy-aware scheme for electronic traffic pricing and a
privacy-preserving technique for speed-limit enforcement.",analyzing privacy policies
http://arxiv.org/abs/1905.00111v1,"Smart meters enable improvements in electricity distribution system
efficiency at some cost in customer privacy. Users with home batteries can
mitigate this privacy loss by applying charging policies that mask their
underlying energy use. A battery charging policy is proposed and shown to
provide universal privacy guarantees subject to a constraint on energy cost.
The guarantee bounds our strategy's maximal information leakage from the user
to the utility provider under general stochastic models of user energy
consumption. The policy construction adapts coding strategies for
non-probabilistic permuting channels to this privacy problem.",analyzing privacy policies
http://arxiv.org/abs/1307.6980v1,"Privacy-aware processing of personal data on the web of services requires
managing a number of issues arising both from the technical and the legal
domain. Several approaches have been proposed to matching privacy requirements
(on the clients side) and privacy guarantees (on the service provider side).
Still, the assurance of effective data protection (when possible) relies on
substantial human effort and exposes organizations to significant
(non-)compliance risks. In this paper we put forward the idea that a privacy
certification scheme producing and managing machine-readable artifacts in the
form of privacy certificates can play an important role towards the solution of
this problem. Digital privacy certificates represent the reasons why a privacy
property holds for a service and describe the privacy measures supporting it.
Also, privacy certificates can be used to automatically select services whose
certificates match the client policies (privacy requirements).
  Our proposal relies on an evolution of the conceptual model developed in the
Assert4Soa project and on a certificate format specifically tailored to
represent privacy properties. To validate our approach, we present a worked-out
instance showing how privacy property Retention-based unlinkability can be
certified for a banking financial service.",analyzing privacy policies
http://arxiv.org/abs/1705.06805v1,"The increasing popularity of specialized Internet-connected devices and
appliances, dubbed the Internet-of-Things (IoT), promises both new conveniences
and new privacy concerns. Unlike traditional web browsers, many IoT devices
have always-on sensors that constantly monitor fine-grained details of users'
physical environments and influence the devices' network communications.
Passive network observers, such as Internet service providers, could
potentially analyze IoT network traffic to infer sensitive details about users.
Here, we examine four IoT smart home devices (a Sense sleep monitor, a Nest Cam
Indoor security camera, a WeMo switch, and an Amazon Echo) and find that their
network traffic rates can reveal potentially sensitive user interactions even
when the traffic is encrypted. These results indicate that a technological
solution is needed to protect IoT device owner privacy, and that IoT-specific
concerns must be considered in the ongoing policy debate around ISP data
collection and usage.",analyzing privacy policies
http://arxiv.org/abs/1001.4459v1,"The Privacy Coach is an application running on a mobile phone that supports
customers in making privacy decisions when confronted with RFID tags. The
approach we take to increase customer privacy is a radical departure from the
mainstream research efforts that focus on implementing privacy enhancing
technologies on the RFID tags themselves. Instead the Privacy Coach functions
as a mediator between customer privacy preferences and corporate privacy
policies, trying to find a match between the two, and informing the user of the
outcome. In this paper we report on the architecture of the Privacy Coach, and
show how it enables users to make informed privacy decisions in a user-friendly
manner. We also spend considerable time to discuss lessons learnt and to
describe future plans to further improve on the Privacy Coach concept.",analyzing privacy policies
http://arxiv.org/abs/1808.06219v2,"Website privacy policies represent the single most important source of
information for users to gauge how their personal data are collected, used and
shared by companies. However, privacy policies are often vague and people
struggle to understand the content. Their opaqueness poses a significant
challenge to both users and policy regulators. In this paper, we seek to
identify vague content in privacy policies. We construct the first corpus of
human-annotated vague words and sentences and present empirical studies on
automatic vagueness detection. In particular, we investigate context-aware and
context-agnostic models for predicting vague words, and explore
auxiliary-classifier generative adversarial networks for characterizing
sentence vagueness. Our experimental results demonstrate the effectiveness of
proposed approaches. Finally, we provide suggestions for resolving vagueness
and improving the usability of privacy policies.",analyzing privacy policies
http://arxiv.org/abs/1710.03890v1,"End user privacy is a critical concern for all organizations that collect,
process and store user data as a part of their business. Privacy concerned
users, regulatory bodies and privacy experts continuously demand organizations
provide users with privacy protection. Current research lacks an understanding
of organizational characteristics that affect an organization's motivation
towards user privacy. This has resulted in a ""one solution fits all"" approach,
which is incapable of providing sustainable solutions for organizational issues
related to user privacy. In this work, we have empirically investigated 40
diverse organizations on their motivations and approaches towards user privacy.
Resources such as newspaper articles, privacy policies and internal privacy
reports that display information about organizational motivations and
approaches towards user privacy were used in the study. We could observe
organizations to have two primary motivations to provide end users with privacy
as voluntary driven inherent motivation, and risk driven compliance motivation.
Building up on these findings we developed a taxonomy of organizational privacy
approaches and further explored the taxonomy through limited exclusive
interviews. With his work, we encourage authorities and scholars to understand
organizational characteristics that define an organization's approach towards
privacy, in order to effectively communicate regulations that enforce and
encourage organizations to consider privacy within their business practices.",analyzing privacy policies
http://arxiv.org/abs/1710.05363v1,"With the advent of numerous online content providers, utilities and
applications, each with their own specific version of privacy policies and its
associated overhead, it is becoming increasingly difficult for concerned users
to manage and track the confidential information that they share with the
providers. Users consent to providers to gather and share their Personally
Identifiable Information (PII). We have developed a novel framework to
automatically track details about how a users' PII data is stored, used and
shared by the provider. We have integrated our Data Privacy ontology with the
properties of blockchain, to develop an automated access control and audit
mechanism that enforces users' data privacy policies when sharing their data
across third parties. We have also validated this framework by implementing a
working system LinkShare. In this paper, we describe our framework on detail
along with the LinkShare system. Our approach can be adopted by Big Data users
to automatically apply their privacy policy on data operations and track the
flow of that data across various stakeholders.",analyzing privacy policies
http://arxiv.org/abs/1404.3722v3,"The problem of designing error optimal differentially private algorithms is
well studied. Recent work applying differential privacy to real world settings
have used variants of differential privacy that appropriately modify the notion
of neighboring databases. The problem of designing error optimal algorithms for
such variants of differential privacy is open. In this paper, we show a novel
transformational equivalence result that can turn the problem of query
answering under differential privacy with a modified notion of neighbors to one
of query answering under standard differential privacy, for a large class of
neighbor definitions.
  We utilize the Blowfish privacy framework that generalizes differential
privacy. Blowfish uses a {\em policy graph} to instantiate different notions of
neighboring databases. We show that the error incurred when answering a
workload $\mathbf{W}$ on a database $\mathbf{x}$ under a Blowfish policy graph
$G$ is identical to the error required to answer a transformed workload
$f_G(\mathbf{W})$ on database $g_G(\mathbf{x})$ under standard differential
privacy, where $f_G$ and $g_G$ are linear transformations based on $G$. Using
this result, we develop error efficient algorithms for releasing histograms and
multidimensional range queries under different Blowfish policies. We believe
the tools we develop will be useful for finding mechanisms to answer many other
classes of queries with low error under other policy graphs.",analyzing privacy policies
http://arxiv.org/abs/1312.3913v5,"Privacy definitions provide ways for trading-off the privacy of individuals
in a statistical database for the utility of downstream analysis of the data.
In this paper, we present Blowfish, a class of privacy definitions inspired by
the Pufferfish framework, that provides a rich interface for this trade-off. In
particular, we allow data publishers to extend differential privacy using a
policy, which specifies (a) secrets, or information that must be kept secret,
and (b) constraints that may be known about the data. While the secret
specification allows increased utility by lessening protection for certain
individual properties, the constraint specification provides added protection
against an adversary who knows correlations in the data (arising from
constraints). We formalize policies and present novel algorithms that can
handle general specifications of sensitive information and certain count
constraints. We show that there are reasonable policies under which our privacy
mechanisms for k-means clustering, histograms and range queries introduce
significantly lesser noise than their differentially private counterparts. We
quantify the privacy-utility trade-offs for various policies analytically and
empirically on real datasets.",analyzing privacy policies
http://arxiv.org/abs/1309.6204v2,"In an undirected social graph, a friendship link involves two users and the
friendship is visible in both the users' friend lists. Such a dual visibility
of the friendship may raise privacy threats. This is because both users can
separately control the visibility of a friendship link to other users and their
privacy policies for the link may not be consistent. Even if one of them
conceals the link from a third user, the third user may find such a friendship
link from another user's friend list. In addition, as most users allow their
friends to see their friend lists in most social network systems, an adversary
can exploit the inconsistent policies to launch privacy attacks to identify and
infer many of a targeted user's friends. In this paper, we propose, analyze and
evaluate such an attack which is called Friendship Identification and Inference
(FII) attack. In a FII attack scenario, we assume that an adversary can only
see his friend list and the friend lists of his friends who do not hide the
friend lists from him. Then, a FII attack contains two attack steps: 1) friend
identification and 2) friend inference. In the friend identification step, the
adversary tries to identify a target's friends based on his friend list and
those of his friends. In the friend inference step, the adversary attempts to
infer the target's friends by using the proposed random walk with restart
approach. We present experimental results using three real social network
datasets and show that FII attacks are generally efficient and effective when
adversaries and targets are friends or 2-distant neighbors. We also
comprehensively analyze the attack results in order to find what values of
parameters and network features could promote FII attacks. Currently, most
popular social network systems with an undirected friendship graph, such as
Facebook, LinkedIn and Foursquare, are susceptible to FII attacks.",analyzing privacy policies
http://arxiv.org/abs/1404.1951v2,"This article investigates privacy risks to those visiting health- related web
pages. The population of pages analyzed is derived from the 50 top search
results for 1,986 common diseases. This yielded a total population of 80,124
unique pages which were analyzed for the presence of third-party HTTP requests.
91% of pages were found to make requests to third parties. Investigation of
URIs revealed that 70% of HTTP Referer strings contained information exposing
specific conditions, treatments, and diseases. This presents a risk to users in
the form of personal identification and blind discrimination. An examination of
extant government and corporate policies reveals that users are insufficiently
protected from such risks.",analyzing privacy policies
http://arxiv.org/abs/1611.10097v1,"Privacy has been frequently identified as a main concern for system
developers while dealing with/managing personal information. Despite this, most
existing work on privacy requirements deals with them as a special case of
security requirements. Therefore, key aspects of privacy are, usually,
overlooked. In this context, wrong design decisions might be made due to
insufficient understanding of privacy concerns. In this paper, we address this
problem with a systematic literature review whose main purpose is to identify
the main concepts/relations for capturing privacy requirements. In addition,
the identified concepts/relations are further analyzed to propose a novel
privacy ontology to be used by software engineers when dealing with privacy
requirements.",analyzing privacy policies
http://arxiv.org/abs/1312.6393v1,"The enforcement of sensitive policies in untrusted environments is still an
open challenge for policy-based systems. On the one hand, taking any
appropriate security decision requires access to these policies. On the other
hand, if such access is allowed in an untrusted environment then confidential
information might be leaked by the policies. The key challenge is how to
enforce sensitive policies and protect content in untrusted environments. In
the context of untrusted environments, we mainly distinguish between outsourced
and distributed environments. The most attractive paradigms concerning
outsourced and distributed environments are cloud computing and opportunistic
networks, respectively.
  In this dissertation, we present the design, technical and implementation
details of our proposed policy-based access control mechanisms for untrusted
environments. First of all, we provide full confidentiality of access policies
in outsourced environments, where service providers do not learn private
information about policies. We support expressive policies and take into
account contextual information. The system entities do not share any encryption
keys. For complex user management, we offer the full-fledged Role-Based Access
Control (RBAC) policies.
  In opportunistic networks, we protect content by specifying expressive
policies. In our proposed approach, brokers match subscriptions against
policies associated with content without compromising privacy of subscribers.
As a result, unauthorised brokers neither gain access to content nor learn
policies and authorised nodes gain access only if they satisfy policies
specified by publishers. Our proposed system provides scalable key management
in which loosely-coupled publishers and subscribers communicate without any
prior contact. Finally, we have developed a prototype of the system that runs
on real smartphones and analysed its performance.",analyzing privacy policies
http://arxiv.org/abs/1011.0527v1,"In Ciphertext Policy Attribute based Encryption scheme, the encryptor can fix
the policy, who can decrypt the encrypted message. The policy can be formed
with the help of attributes. In CP-ABE, access policy is sent along with the
ciphertext. We propose a method in which the access policy need not be sent
along with the ciphertext, by which we are able to preserve the privacy of the
encryptor. The proposed construction is provably secure under Decision Bilinear
Diffe-Hellman assumption.",analyzing privacy policies
http://arxiv.org/abs/1805.10393v1,"Website privacy policies are too long to read and difficult to understand.
The over-sophisticated language makes privacy notices to be less effective than
they should be. People become even less willing to share their personal
information when they perceive the privacy policy as vague. This paper focuses
on decoding vagueness from a natural language processing perspective. While
thoroughly identifying the vague terms and their linguistic scope remains an
elusive challenge, in this work we seek to learn vector representations of
words in privacy policies using deep neural networks. The vector
representations are fed to an interactive visualization tool (LSTMVis) to test
on their ability to discover syntactically and semantically related vague
terms. The approach holds promise for modeling and understanding language
vagueness.",analyzing privacy policies
http://arxiv.org/abs/1806.03235v1,"Increasingly accelerating technology advancement affects and disrupts almost
all aspects of human society and civilization at large as we know it. Actually,
this has been true since the technology started at the dawn of human society,
yet the mere speed and magnitude of modern technology development brings about
the situation where societies and economies have to adapt to the changing
technological landscape as much as technologies have to integrate into the
social fabric. The way to to achieve such integration in a changing and
unpredictable world is to support the close interaction between the world of
societal expectations and the world of technology. Policy Scan and Technology
Strategy design methodology presented in this paper was developed precisely for
the purpose of addressing specific types of 'ill-defined' problems in terms of
observing, analyzing and integrating technology developments and availabilities
with policy requirements, social governance and societal expectations. The
methodology consists of conceptual tools and methods for developing concrete
actions and products for guiding technology adoption for social change. It is
geared towards increasingly complex and uncertain situations where existing
analysis and problem solving methods often fail due to many non-linearities
inherent in the social and technology worlds and, especially, at their area of
their inter intersection. The development of the methodology followed the
grounded theory construction process which requires a close relation to a
specific context of an application domain, determined by actual interaction
between the worlds of societal problems and technology. The chosen application
domain of this research is the intersection of smart mobility problematics and
opportunities, the rising autonomous driving technology, data privacy,
provenance and security challenges, policies and legislation.",analyzing privacy policies
http://arxiv.org/abs/1603.02010v1,"We present the first differentially private algorithms for reinforcement
learning, which apply to the task of evaluating a fixed policy. We establish
two approaches for achieving differential privacy, provide a theoretical
analysis of the privacy and utility of the two algorithms, and show promising
results on simple empirical examples.",analyzing privacy policies
http://arxiv.org/abs/1611.08936v2,"Differential privacy is a formal mathematical {stand-ard} for quantifying the
degree of that individual privacy in a statistical database is preserved. To
guarantee differential privacy, a typical method is adding random noise to the
original data for data release. In this paper, we investigate the conditions of
differential privacy considering the general random noise adding mechanism, and
then apply the obtained results for privacy analysis of the privacy-preserving
consensus algorithm. Specifically, we obtain a necessary and sufficient
condition of $\epsilon$-differential privacy, and the sufficient conditions of
$(\epsilon, \delta)$-differential privacy. We apply them to analyze various
random noises. For the special cases with known results, our theory matches
with the literature; for other cases that are unknown, our approach provides a
simple and effective tool for differential privacy analysis. Applying the
obtained theory, on privacy-preserving consensus algorithms, it is proved that
the average consensus and $\epsilon$-differential privacy cannot be guaranteed
simultaneously by any privacy-preserving consensus algorithm.",analyzing privacy policies
http://arxiv.org/abs/1902.08968v1,"This position paper considers the privacy and security implications of
EUI-64-based IPv6 addresses. By encoding MAC addresses, EUI-64 addresses
violate layers by exposing hardware identifiers in IPv6 addresses. The
hypothetical threat of EUI-64 addresses is well-known, and the adoption of
privacy extensions in operating systems (OSes) suggests this vulnerability has
been mitigated. Instead, our work seeks to quantify the empirical existence of
EUI-64 IPv6 addresses in today's Internet. By analyzing: i) traceroutes; ii)
DNS records; and iii) mobile phone behaviors, we find surprisingly significant
use of EUI-64. We characterize the origins and behaviors of these EUI-64 IPv6
addresses, and advocate for changes in provider IPv6 addressing policies.",analyzing privacy policies
http://arxiv.org/abs/1807.07468v1,"A text mining approach is proposed based on latent Dirichlet allocation (LDA)
to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints.
The proposed approach aims to extract latent topics in the CFPB complaint
narratives, and explores their associated trends over time. The time trends
will then be used to evaluate the effectiveness of the CFPB regulations and
expectations on financial institutions in creating a consumer oriented culture
that treats consumers fairly and prioritizes consumer protection in their
decision making processes. The proposed approach can be easily operationalized
as a decision support system to automate detection of emerging topics in
consumer complaints. Hence, the technology-human partnership between the
proposed approach and the CFPB team could certainly improve consumer
protections from unfair, deceptive or abusive practices in the financial
markets by providing more efficient and effective investigations of consumer
complaint narratives.",consumer protection
http://arxiv.org/abs/1705.06809v1,"The growing market for smart home IoT devices promises new conveniences for
consumers while presenting novel challenges for preserving privacy within the
home. Specifically, Internet service providers or neighborhood WiFi
eavesdroppers can measure Internet traffic rates from smart home devices and
infer consumers' private in-home behaviors. Here we propose four strategies
that device manufacturers and third parties can take to protect consumers from
side-channel traffic rate privacy threats: 1) blocking traffic, 2) concealing
DNS, 3) tunneling traffic, and 4) shaping and injecting traffic. We hope that
these strategies, and the implementation nuances we discuss, will provide a
foundation for the future development of privacy-sensitive smart homes.",consumer protection
http://arxiv.org/abs/1907.11717v1,"The benefits of the ubiquitous caching in ICN are profound, such features
make ICN promising for content distribution, but it also introduces a challenge
to content protection against the unauthorized access. The protection of a
content against unauthorized access requires consumer authentication and
involves the conventional end-to-end encryption. However, in
information-centric networking (ICN), such end-to-end encryption makes the
content caching ineffective since encrypted contents stored in a cache are
useless for any consumers except those who know the encryption key. For
effective caching of encrypted contents in ICN, we propose a secure
distribution of protected content (SDPC) scheme, which ensures that only
authenticated consumers can access the content. SDPC is lightweight and allows
consumers to verify the originality of the published content by using a
symmetric key encryption. SDPC also provides protection against privacy
leakage. The security of SDPC was proved with the BAN logic and Scyther tool
verification, and simulation results show that SDPC can reduce the content
download delay.",consumer protection
http://arxiv.org/abs/1703.00518v1,"Consumer protection agencies are charged with safeguarding the public from
hazardous products, but the thousands of products under their jurisdiction make
it challenging to identify and respond to consumer complaints quickly. From the
consumer's perspective, online reviews can provide evidence of product defects,
but manually sifting through hundreds of reviews is not always feasible. In
this paper, we propose a system to mine Amazon.com reviews to identify products
that may pose safety or health hazards. Since labeled data for this task are
scarce, our approach combines positive unlabeled learning with domain
adaptation to train a classifier from consumer complaints submitted to the U.S.
Consumer Product Safety Commission. On a validation set of manually annotated
Amazon product reviews, we find that our approach results in an absolute F1
score improvement of 8% over the best competing baseline. Furthermore, we apply
the classifier to Amazon reviews of known recalled products; the classifier
identifies reviews reporting safety hazards prior to the recall date for 45% of
the products. This suggests that the system may be able to provide an early
warning system to alert consumers to hazardous products before an official
recall is announced.",consumer protection
http://arxiv.org/abs/1405.3342v1,"In the event that a bacteriological or chemical toxin is intro- duced to a
water distribution network, a large population of consumers may become exposed
to the contaminant. A contamination event may be poorly predictable dynamic
process due to the interactions of consumers and utility managers during an
event. Consumers that become aware of a threat may select protective actions
that change their water demands from typical demand patterns, and new hydraulic
conditions can arise that differ from conditions that are predicted when
demands are considered as exogenous inputs. Consequently, the movement of the
contaminant plume in the pipe network may shift from its expected trajectory. A
sociotechnical model is developed here to integrate agent-based models of
consumers with an engineering water distribution system model and capture the
dynamics between consumer behaviors and the water distribution system for
predicting contaminant transport and public exposure. Consumers are simulated
as agents with behaviors defined for water use activities, mobility,
word-of-mouth communication, and demand reduction, based on a set of rules
representing an agents autonomy and reaction to health impacts, the
environment, and the actions of other agents. As consumers decrease their water
use, the demand exerted on the water distribution system is updated; as the
flow directions and volumes shift in response, the location of the contaminant
plume is updated and the amount of contaminant consumed by each agent is
calculated. The framework is tested through simulating realistic contamination
scenarios for a virtual city and water distribution system.",consumer protection
http://arxiv.org/abs/1808.03289v1,"The secure distribution of protected content requires consumer authentication
and involves the conventional method of end-to-end encryption. However, in
information-centric networking (ICN) the end-to-end encryption makes the
content caching ineffective since encrypted content stored in a cache is
useless for any consumer except those who know the encryption key. For
effective caching of encrypted content in ICN, we propose a novel scheme,
called the Secure Distribution of Protected Content (SDPC). SDPC ensures that
only authenticated consumers can access the content. The SDPC is a lightweight
authentication and key distribution protocol; it allows consumer nodes to
verify the originality of the published article by using a symmetric key
encryption. The security of the SDPC was proved with BAN logic and Scyther tool
verification.",consumer protection
http://arxiv.org/abs/1711.07220v1,"The AN.ON-Next project aims to integrate privacy-enhancing technologies into
the internet's infrastructure and establish them in the consumer mass market.
  The technologies in focus include a basis protection at internet service
provider level, an improved overlay network-based protection and a concept for
privacy protection in the emerging 5G mobile network. A crucial success factor
will be the viable adjustment and development of standards, business models and
pricing strategies for those new technologies.",consumer protection
http://arxiv.org/abs/1806.08274v1,"Pre-configured cycle (p-Cycle) method has been studied in literature
extensively for optical network protection. A large p-cycle has high capacity
efficiency and can protect a large number of nodes against the single link
failure scenarios. All the links protected by such a p-cycle lose protection
when the p-cycle is consumed to restore traffic after a failure. As the
probability of multiple link failure is high for a large network, it also means
that with higher probability, on the second failure, protection may not be
there for the failed link. Thus, if the number of links protected by a p-cycle
is large, it makes the network unprotected with high probability on the advent
of the second failure. In this paper, we study the impact zone due to a first
link failure in the various configurations of the p-cycles. The study gives
insight into how to choose the p-cycle configuration to reduce the impact zone
while using minimum spare capacity. We propose few methods and compare them to
show how the impact zone analysis can be used to improve the fault tolerance in
an optical network.",consumer protection
http://arxiv.org/abs/cs/9908012v1,"E-business, information serving, and ubiquitous computing will create heavy
request traffic from strangers or even incognitos. Such requests must be
managed automatically. Two ways of doing this are well known: giving every
incognito consumer the same treatment, and rendering service in return for
money. However, different behavior will be often wanted, e.g., for a university
library with different access policies for undergraduates, graduate students,
faculty, alumni, citizens of the same state, and everyone else.
  For a data or process server contacted by client machines on behalf of users
not previously known, we show how to provide reliable automatic access
administration conforming to service agreements. Implementations scale well
from very small collections of consumers and producers to immense client/server
networks. Servers can deliver information, effect state changes, and control
external equipment.
  Consumer privacy is easily addressed by the same protocol. We support
consumer privacy, but allow servers to deny their resources to incognitos. A
protocol variant even protects against statistical attacks by consortia of
service organizations.
  One e-commerce application would put the consumer's tokens on a smart card
whose readers are in vending kiosks. In e-business we can simplify supply chain
administration. Our method can also be used in sensitive networks without
introducing new security loopholes.",consumer protection
http://arxiv.org/abs/1908.10201v1,"Service-oriented architecture (SOA) system has been widely utilized at many
present business areas. However, SOA system is loosely coupled with multiple
services and lacks the relevant security protection mechanisms, thus it can
easily be attacked by unauthorized access and information theft. The existed
access control mechanism can only prevent unauthorized users from accessing the
system, but they can not prevent those authorized users (insiders) from
attacking the system. To address this problem, we propose a behavior-aware
service access control mechanism using security policy monitoring for SOA
system. In our mechanism, a monitor program can supervise consumer's behaviors
in run time. By means of trustful behavior model (TBM), if finding the
consumer's behavior is of misusing, the monitor will deny its request. If
finding the consumer's behavior is of malicious, the monitor will early
terminate the consumer's access authorizations in this session or add the
consumer into the Blacklist, whereby the consumer will not access the system
from then on. In order to evaluate the feasibility of proposed mechanism, we
implement a prototype system. The final results illustrate that our mechanism
can effectively monitor consumer's behaviors and make effective responses when
malicious behaviors really occur in run time. Moreover, as increasing the
rule's number in TBM continuously, our mechanism can still work well.",consumer protection
http://arxiv.org/abs/1805.02722v1,"Data encryption is the primary method of protecting the privacy of consumer
device Internet communications from network observers. The ability to
automatically detect unencrypted data in network traffic is therefore an
essential tool for auditing Internet-connected devices. Existing methods
identify network packets containing cleartext but cannot differentiate packets
containing encrypted data from packets containing compressed unencrypted data,
which can be easily recovered by reversing the compression algorithm. This
makes it difficult for consumer protection advocates to identify devices that
risk user privacy by sending sensitive data in a compressed unencrypted format.
Here, we present the first technique to automatically distinguish encrypted
from compressed unencrypted network transmissions on a per-packet basis. We
apply three machine learning models and achieve a maximum 66.9% accuracy with a
convolutional neural network trained on raw packet data. This result is a
baseline for this previously unstudied machine learning problem, which we hope
will motivate further attention and accuracy improvements. To facilitate
continuing research on this topic, we have made our training and test datasets
available to the public.",consumer protection
http://arxiv.org/abs/1902.08712v1,"Resource allocation is the process of optimizing the rare resources. In the
area of security, how to allocate limited resources to protect a massive number
of targets is especially challenging. This paper addresses this resource
allocation issue by constructing a game theoretic model. A defender and an
attacker are players and the interaction is formulated as a trade-off between
protecting targets and consuming resources. The action cost which is a
necessary role of consuming resource, is considered in the proposed model.
Additionally, a bounded rational behavior model (Quantal Response, QR), which
simulates a human attacker of the adversarial nature, is introduced to improve
the proposed model. To validate the proposed model, we compare the different
utility functions and resource allocation strategies. The comparison results
suggest that the proposed resource allocation strategy performs better than
others in the perspective of utility and resource effectiveness.",consumer protection
http://arxiv.org/abs/1612.05120v4,"The roll-out of smart meters in electricity networks introduces risks for
consumer privacy due to increased measurement frequency and granularity.
Through various Non-Intrusive Load Monitoring techniques, consumer behavior may
be inferred from their metering data. In this paper, we propose an energy
management method that reduces energy cost and protects privacy through the
minimization of information leakage. The method is based on a Model Predictive
Controller that utilizes energy storage and local generation, and that predicts
the effects of its actions on the statistics of the actual energy consumption
of a consumer and that seen by the grid. Computationally, the method requires
solving a Mixed-Integer Quadratic Program of manageable size whenever new meter
readings are available. We simulate the controller on generated residential
load profiles with different privacy costs in a two-tier time-of-use energy
pricing environment. Results show that information leakage is effectively
reduced at the expense of increased energy cost. The results also show that
with the proposed controller the consumer load profile seen by the grid
resembles a mixture between that obtained with Non-Intrusive Load Leveling and
Lazy Stepping.",consumer protection
http://arxiv.org/abs/1708.02629v1,"In the post-genomic era, large-scale personal DNA sequences are produced and
collected for genetic medical diagnoses and new drug discovery, which, however,
simultaneously poses serious challenges to the protection of personal genomic
privacy. Existing genomic privacy-protection methods are either time-consuming
or with low accuracy. To tackle these problems, this paper proposes a sequence
similarity-based obfuscation method, namely IterMegaBLAST, for fast and
reliable protection of personal genomic privacy. Specifically, given a randomly
selected sequence from a dataset of DNA sequences, we first use MegaBLAST to
find its most similar sequence from the dataset. These two aligned sequences
form a cluster, for which an obfuscated sequence was generated via a DNA
generalization lattice scheme. These procedures are iteratively performed until
all of the sequences in the dataset are clustered and their obfuscated
sequences are generated. Experimental results on two benchmark datasets
demonstrate that under the same degree of anonymity, IterMegaBLAST
significantly outperforms existing state-of-the-art approaches in terms of both
utility accuracy and time complexity.",consumer protection
http://arxiv.org/abs/1310.1551v1,"Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.",consumer protection
http://arxiv.org/abs/1709.09614v1,"Power grids are undergoing major changes due to rapid growth in renewable
energy resources and improvements in battery technology. While these changes
enhance sustainability and efficiency, they also create significant management
challenges as the complexity of power systems increases. To tackle these
challenges, decentralized Internet-of-Things (IoT) solutions are emerging,
which arrange local communities into transactive microgrids. Within a
transactive microgrid, ""prosumers"" (i.e., consumers with energy generation and
storage capabilities) can trade energy with each other, thereby smoothing the
load on the main grid using local supply. It is hard, however, to provide
security, safety, and privacy in a decentralized and transactive energy system.
On the one hand, prosumers' personal information must be protected from their
trade partners and the system operator. On the other hand, the system must be
protected from careless or malicious trading, which could destabilize the
entire grid. This paper describes Privacy-preserving Energy Transactions
(PETra), which is a secure and safe solution for transactive microgrids that
enables consumers to trade energy without sacrificing their privacy. PETra
builds on distributed ledgers, such as blockchains, and provides anonymity for
communication, bidding, and trading.",consumer protection
http://arxiv.org/abs/1803.10099v1,"Ad targeting is getting more powerful with introduction of new tools, such as
Custom Audiences, behavioral targeting, and Audience Insights. Although this is
beneficial for businesses as it enables people to receive more relevant
advertising, the power of the tools has downsides. In this paper, we focus on
three downsides: privacy violations, microtargeting (i.e., the ability to reach
a specific individual or individuals without their explicit knowledge that they
are the only ones an ad reaches) and ease of reaching marginalized groups.
Using Facebook's ad system as a case study, we demonstrate the feasibility of
such downsides. We then discuss Facebook's response to our responsible
disclosures of the findings and call for additional policy, science, and
engineering work to protect consumers in the rapidly evolving ecosystem of ad
targeting.",consumer protection
http://arxiv.org/abs/1908.02589v1,"Social technologies have made it possible to propagate disinformation and
manipulate the masses at an unprecedented scale. This is particularly alarming
from a security perspective, as humans have proven to be the weakest link when
protecting critical infrastructure in general, and the power grid in
particular. Here, we consider an attack in which an adversary attempts to
manipulate the behavior of energy consumers by sending fake discount
notifications encouraging them to shift their consumption into the peak-demand
period. We conduct surveys to assess the propensity of people to follow-through
on such notifications and forward them to their friends. This allows us to
model how the disinformation propagates through social networks. Finally, using
Greater London as a case study, we show that disinformation can indeed be used
to orchestrate an attack wherein unwitting consumers synchronize their
energy-usage patterns, resulting in blackouts on a city-scale. These findings
demonstrate that in an era when disinformation can be weaponized, system
vulnerabilities arise not only from the hardware and software of critical
infrastructure, but also from the behavior of the consumers.",consumer protection
http://arxiv.org/abs/1807.11052v3,"Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.",consumer protection
http://arxiv.org/abs/1601.06372v1,"Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.",consumer protection
http://arxiv.org/abs/1312.3665v2,"Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed ""top-to-bottom"" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.",consumer protection
http://arxiv.org/abs/1811.11039v1,"Limiting online data collection to the minimum required for specific purposes
is mandated by modern privacy legislation such as the General Data Protection
Regulation (GDPR) and the California Consumer Protection Act. This is
particularly true in online services where broad collection of personal
information represents an obvious concern for privacy. We challenge the view
that broad personal data collection is required to provide personalised
services. By first developing formal models of privacy and utility, we show how
users can obtain personalised content, while retaining an ability to plausibly
deny their interests in topics they regard as sensitive using a system of
proxy, group identities we call 3PS. Through extensive experiment on a
prototype implementation, using openly accessible data sources, we show that
3PS provides personalised content to individual users over 98% of the time in
our tests, while protecting plausible deniability effectively in the face of
worst-case threats from a variety of attack types.",consumer protection
http://arxiv.org/abs/1901.03603v1,"Billions of users rely on the security of the Android platform to protect
phones, tablets, and many different types of consumer electronics. While
Android's permission model is well studied, the enforcement of the protection
policy has received relatively little attention. Much of this enforcement is
spread across system services, taking the form of hard-coded checks within
their implementations. In this paper, we propose Authorization Check Miner
(ACMiner), a framework for evaluating the correctness of Android's access
control enforcement through consistency analysis of authorization checks.
ACMiner combines program and text analysis techniques to generate a rich set of
authorization checks, mines the corresponding protection policy for each
service entry point, and uses association rule mining at a service granularity
to identify inconsistencies that may correspond to vulnerabilities. We used
ACMiner to study the AOSP version of Android 7.1.1 to identify 28
vulnerabilities relating to missing authorization checks. In doing so, we
demonstrate ACMiner's ability to help domain experts process thousands of
authorization checks scattered across millions of lines of code.",consumer protection
http://arxiv.org/abs/0712.2587v1,"The code that combines channel estimation and error protection has received
general attention recently, and has been considered a promising methodology to
compensate multi-path fading effect. It has been shown by simulations that such
code design can considerably improve the system performance over the
conventional design with separate channel estimation and error protection
modules under the same code rate. Nevertheless, the major obstacle that
prevents from the practice of the codes is that the existing codes are mostly
searched by computers, and hence exhibit no good structure for efficient
decoding. Hence, the time-consuming exhaustive search becomes the only decoding
choice, and the decoding complexity increases dramatically with the codeword
length. In this paper, by optimizing the signal-tonoise ratio, we found a
systematic construction for the codes for combined channel estimation and error
protection, and confirmed its equivalence in performance to the
computer-searched codes by simulations. Moreover, the structural codes that we
construct by rules can now be maximum-likelihoodly decodable in terms of a
newly derived recursive metric for use of the priority-first search decoding
algorithm. Thus,the decoding complexity reduces significantly when compared
with that of the exhaustive decoder. The extension code design for fast-fading
channels is also presented. Simulations conclude that our constructed extension
code is robust in performance even if the coherent period is shorter than the
codeword length.",consumer protection
http://arxiv.org/abs/cs/0611102v1,"We present a method to secure the complete path between a server and the
local human user at a network node. This is useful for scenarios like internet
banking, electronic signatures, or online voting. Protection of input
authenticity and output integrity and authenticity is accomplished by a
combination of traditional and novel technologies, e.g., SSL, ActiveX, and
DirectX. Our approach does not require administrative privileges to deploy and
is hence suitable for consumer applications. Results are based on the
implementation of a proof-of-concept application for the Windows platform.",consumer protection
http://arxiv.org/abs/1004.4732v2,"In this paper, we calculate energy required to copy one bit of useful
information in the presence of thermal noise. For this purpose, we consider a
quantum system capable of storing one bit of classical information, which is
initially in a mixed state corresponding to temperature T. We calculate how
many of these systems must be used to store useful information and control bits
protecting the content against transmission errors. Finally, we analyze how
adding these extra bits changes the total energy consumed during the copying.",consumer protection
http://arxiv.org/abs/1201.0949v1,"Phishing (password + fishing) is a form of cyber crime based on social
engineering and site spoofing techniques. The name of 'phishing' is a conscious
misspelling of the word 'fishing' and involves stealing confidential data from
a user's computer and subsequently using the data to steal the user's money. In
this paper, we study, discuss and propose the phishing attack stages and types,
technologies for detection of phishing web pages, and conclude our paper with
some important recommendations for preventing phishing for both consumer and
company.",consumer protection
http://arxiv.org/abs/1607.06377v1,"Advanced Metering Infrastructure (AMI) have rapidly become a topic of
international interest as governments have sponsored their deployment for the
purposes of utility service reliability and efficiency, e.g., water and
electricity conservation. Two problems plague such deployments. First is the
protection of consumer privacy. Second is the problem of huge amounts of data
from such deployments. A new architecture is proposed to address these problems
through the use of Aggregators, which incorporate temporary data buffering and
the modularization of utility grid analysis. These Aggregators are used to
deliver anonymized summary data to the central utility while preserving billing
and automated connection services.",consumer protection
http://arxiv.org/abs/1902.03857v1,"In this work we explore the implementation of the reputation system for a
generic marketplace, describe details of the algorithm and parameters driving
its operation, justify an approach to simulation modeling, and explore how
various kinds of reputation systems with different parameters impact the
economic security of the marketplace. Our emphasis here is on the protection of
consumers by means of an ability to distinguish between cheating participants
and honest participants, as well as the ability to minimize losses of honest
participants due to scam.",consumer protection
http://arxiv.org/abs/1201.4376v2,"Participatory Sensing is an emerging computing paradigm that enables the
distributed collection of data by self-selected participants. It allows the
increasing number of mobile phone users to share local knowledge acquired by
their sensor-equipped devices, e.g., to monitor temperature, pollution level or
consumer pricing information. While research initiatives and prototypes
proliferate, their real-world impact is often bounded to comprehensive user
participation. If users have no incentive, or feel that their privacy might be
endangered, it is likely that they will not participate. In this article, we
focus on privacy protection in Participatory Sensing and introduce a suitable
privacy-enhanced infrastructure. First, we provide a set of definitions of
privacy requirements for both data producers (i.e., users providing sensed
information) and consumers (i.e., applications accessing the data). Then, we
propose an efficient solution designed for mobile phone users, which incurs
very low overhead. Finally, we discuss a number of open problems and possible
research directions.",consumer protection
http://arxiv.org/abs/1903.04794v2,"Blockchain is a disruptive technology that has been characterised to be the
next big thing and has already gained a broad recognition by experts in diverse
fields. In this paper, we consider possible use cases and applications of the
blockchain for the consumer electronics (CE) industry and its interplay with
the Internet of things. Instead of discussing how the blockchain can
revolutionise the supply chain, we focus on how it could be employed for
enhancing the security of networked CE devices. This work is motivated by the
large number of recent attacks that use easily hackable devices as a weaponry.
Towards this direction, privacy and data protection aspects of blockchain
solutions are also presented and are linked to regulatory framework provisions.
Information on existing blockchain solutions is also provided.",consumer protection
http://arxiv.org/abs/1103.0759v1,"In hardware virtualization a hypervisor provides multiple Virtual Machines
(VMs) on a single physical system, each executing a separate operating system
instance. The hypervisor schedules execution of these VMs much as the scheduler
in an operating system does, balancing factors such as fairness and I/O
performance. As in an operating system, the scheduler may be vulnerable to
malicious behavior on the part of users seeking to deny service to others or
maximize their own resource usage.
  Recently, publically available cloud computing services such as Amazon EC2
have used virtualization to provide customers with virtual machines running on
the provider's hardware, typically charging by wall clock time rather than
resources consumed. Under this business model, manipulation of the scheduler
may allow theft of service at the expense of other customers, rather than
merely reallocating resources within the same administrative domain.
  We describe a flaw in the Xen scheduler allowing virtual machines to consume
almost all CPU time, in preference to other users, and demonstrate kernel-based
and user-space versions of the attack. We show results demonstrating the
vulnerability in the lab, consuming as much as 98% of CPU time regardless of
fair share, as well as on Amazon EC2, where Xen modifications protect other
users but still allow theft of service. In case of EC2, following the
responsible disclosure model, we have reported this vulnerability to Amazon;
they have since implemented a fix that we have tested and verified (See
Appendix B). We provide a novel analysis of the necessary conditions for such
attacks, and describe scheduler modifications to eliminate the vulnerability.
  We present experimental results demonstrating the effectiveness of these
defenses while imposing negligible overhead.",consumer protection
http://arxiv.org/abs/1802.01166v4,"The next-generation energy network, the so-called smart grid (SG), promises a
tremendous increase in efficiency, safety and flexibility of managing the
electricity grid as compared to the legacy energy network. This is needed today
more than ever, as the global energy consumption is growing at an unprecedented
rate, and renewable energy sources have to be seamlessly integrated into the
grid to assure a sustainable human development. Smart meters (SMs) are among
the crucial enablers of the SG concept; they supply accurate high-frequency
information about users' household energy consumption to a utility provider,
which is essential for time of use pricing, rapid fault detection, energy theft
prevention, while also providing consumers with more flexibility and control
over their consumption. However, highly accurate and granular SM data also
poses a threat to consumer privacy as non-intrusive load monitoring techniques
enable a malicious attacker to infer many details of a user's private life.
This article focuses on privacy-enhancing energy management techniques that
provide accurate energy consumption information to the grid operator, without
sacrificing consumer privacy. In particular, we focus on techniques that shape
and modify the actual user energy consumption by means of physical resources,
such as rechargeable batteries, renewable energy sources or demand shaping. A
rigorous mathematical analysis of privacy is presented under various physical
constraints on the available physical resources. Finally, open questions and
challenges that need to be addressed to pave the way to the effective
protection of users' privacy in future SGs are presented.",consumer protection
http://arxiv.org/abs/1308.2921v1,"Participatory sensing is emerging as an innovative computing paradigm that
targets the ubiquity of always-connected mobile phones and their sensing
capabilities. In this context, a multitude of pioneering applications
increasingly carry out pervasive collection and dissemination of information
and environmental data, such as, traffic conditions, pollution, temperature,
etc. Participants collect and report measurements from their mobile devices and
entrust them to the cloud to be made available to applications and users.
Naturally, due to the personal information associated to the reports (e.g.,
location, movements, etc.), a number of privacy concerns need to be taken into
account prior to a large-scale deployment of these applications. Motivated by
the need for privacy protection in Participatory Sensing, this work presents
PEPSI: a Privacy-Enhanced Participatory Sensing Infrastructure. We explore
realistic architectural assumptions and a minimal set of formal requirements
aiming at protecting privacy of both data producers and consumers. We propose
two instantiations that attain privacy guarantees with provable security at
very low additional computational cost and almost no extra communication
overhead.",consumer protection
http://arxiv.org/abs/1710.00381v1,"This work introduces CHIRP - an algorithm for communication between
ultra-portable heterogeneous IoT devices with a type of round-robin protection
mechanism. This algorithm is presented both in its basic form as well as in a
secured form in order to secure and maintain trust boundaries and communication
within specific groups of heterogeneous devices. The specific target
application scenarios includes resource constrained environments where a
co-located swarm of devices (adversarial in mission or objective) is also
present. CHIRP, and its secured version (S-CHIRP), enables complete
peer-to-peer communication of a $n$-agent network of devices in as few as n
rounds. In addition to the n-round cycle length, the proposed communication
mechanism has the following major properties: nodes communication is entirely
decentralized, communication is resilient to the loss of nodes, and finally
communication is resilient to the (re)-entry of nodes. Theoretical models show
that even the secure implementation of this mechanism is capable of scaling to
IoT swarms in the million device range with memory constraints in the < 10 MB
range",consumer protection
http://arxiv.org/abs/1006.2718v1,"RESTful services on the Web expose information through retrievable resource
representations that represent self-describing descriptions of resources, and
through the way how these resources are interlinked through the hyperlinks that
can be found in those representations. This basic design of RESTful services
means that for extracting the most useful information from a service, it is
necessary to understand a service's representations, which means both the
semantics in terms of describing a resource, and also its semantics in terms of
describing its linkage with other resources. Based on the Resource Linking
Language (ReLL), this paper describes a framework for how RESTful services can
be described, and how these descriptions can then be used to harvest
information from these services. Building on this framework, a layered model of
RESTful service semantics allows to represent a service's information in
RDF/OWL. Because REST is based on the linkage between resources, the same model
can be used for aggregating and interlinking multiple services for extracting
RDF data from sets of RESTful services.",terms of service
http://arxiv.org/abs/1303.5926v1,"Service discovery is one of the key problems that has been widely researched
in the area of Service Oriented Architecture (SOA) based systems. Service
category learning is a technique for efficiently facilitating service
discovery. Most approaches for service category learning are based on suitable
similarity distance measures using thresholds. Threshold selection is
essentially difficult and often leads to unsatisfactory accuracy. In this
paper, we have proposed a self-organizing based clustering algorithm called
Semantic Taxonomical Clustering (STC) for taxonomically organizing services
with self-organizing information and knowledge. We have tested the STC
algorithm on both randomly generated data and the standard OWL-S TC dataset. We
have observed promising results both in terms of classification accuracy and
runtime performance compared to existing approaches.",terms of service
http://arxiv.org/abs/1605.02432v1,"In the service landscape, the issues of service selection, negotiation of
Service Level Agreements (SLA), and SLA-compliance monitoring have typically
been used in separate and disparate ways, which affect the quality of the
services that consumers obtain from their providers. In this work, we propose a
broker-based framework to deal with these concerns in an integrated manner for
Software as a Service (SaaS) provisioning. The SaaS Broker selects a suitable
SaaS provider on behalf of the service consumer by using a utility-driven
selection algorithm that ranks the QoS offerings of potential SaaS providers.
Then, it negotiates the SLA terms with that provider based on the quality
requirements of the service consumer. The monitoring infrastructure observes
SLA-compliance during service delivery by using measurements obtained from
third-party monitoring services. We also define a utility-based bargaining
decision model that allows the service consumer to express her sensitivity for
each of the negotiated quality attributes and to evaluate the SaaS provider
offer in each round of negotiation. A use-case with few quality attributes and
their respective utility functions illustrates the approach.",terms of service
http://arxiv.org/abs/1111.5733v1,"The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.",terms of service
http://arxiv.org/abs/1604.07642v1,"Service-Oriented Computing is a paradigm that uses services as building
blocks for building distributed applications. The primary motivation for
orchestrating services in the cloud used to be distributed business processes,
which drove the standardization of the Business Process Execution Language
(BPEL) and its central notion that a service is a business process. In recent
years, there has been a transition towards other motivations for orchestrating
services in the cloud, {\em e.g.}, XaaS, RMAD. Although it is theoretically
possible to make all of those services into WSDL/SOAP services, it would be too
complicated and costly for industry adoption. Therefore, the central notion
that a service is a business process is too restrictive. Instead, we view a
service as a technology neutral, loosely coupled, location transparent
procedure. With these ideas in mind, we introduce a new approach to services
orchestration: Ozy, a general orchestration container. We define this new
approach in terms of existing technology, and we show that the Ozy container
relaxes many traditional constraints and allows for simpler, more feature-rich
applications.",terms of service
http://arxiv.org/abs/1903.04709v1,"An edge computing environment features multiple edge servers and multiple
service clients. In this environment, mobile service providers can offload
client-side computation tasks from service clients' devices onto edge servers
to reduce service latency and power consumption experienced by the clients. A
critical issue that has yet to be properly addressed is how to allocate edge
computing resources to achieve two optimization objectives: 1) minimize the
service cost measured by the service latency and the power consumption
experienced by service clients; and 2) maximize the service capacity measured
by the number of service clients that can offload their computation tasks in
the long term. This paper formulates this long-term problem as a stochastic
optimization problem and solves it with an online algorithm based on Lyapunov
optimization. This NP-hard problem is decomposed into three sub-problems, which
are then solved with a suite of techniques. The experimental results show that
our approach significantly outperforms two baseline approaches.",terms of service
http://arxiv.org/abs/1710.01476v2,"An increasing number of technology enterprises are adopting cloud-native
architectures to offer their web-based products, by moving away from
privately-owned data-centers and relying exclusively on cloud service
providers. As a result, cloud vendors have lately increased, along with the
estimated annual revenue they share. However, in the process of selecting a
provider's cloud service over the competition, we observe a lack of universal
common ground in terms of terminology, functionality of services and billing
models. This is an important gap especially under the new reality of the
industry where each cloud provider has moved towards his own service taxonomy,
while the number of specialized services has grown exponentially. This work
discusses cloud services offered by four dominant, in terms of their current
market share, cloud vendors. We provide a taxonomy of their services and
sub-services that designates major service families namely computing, storage,
databases, analytics, data pipelines, machine learning, and networking. The aim
of such clustering is to indicate similarities, common design approaches and
functional differences of the offered services. The outcomes are essential both
for individual researchers, and bigger enterprises in their attempt to identify
the set of cloud services that will utterly meet their needs without
compromises. While we acknowledge the fact that this is a dynamic industry,
where new services arise constantly, and old ones experience important updates,
this study paints a solid image of the current offerings and gives prominence
to the directions that cloud service providers are following.",terms of service
http://arxiv.org/abs/1111.5493v2,"Collaboration models and tools aim at improving the efficiency and
effectiveness of human interactions. Although social relations among
collaborators have been identified as having a strong influence on
collaboration, they are still insufficiently taken into account in current
collaboration models and tools. In this paper, the concept of service protocols
is proposed as a model for human interactions supporting social requirements,
i.e., sets of constraints on the relations among interacting humans. Service
protocols have been proposed as an answer to the need for models for human
interactions in which not only the potential sequences of activities are
specified-as in process models-but also the constraints on the relations among
collaborators. Service protocols are based on two main ideas: first, service
protocols are rooted in the service-oriented architecture (SOA): each service
protocol contains a service-oriented summary which provides a representation of
the activities of an associated process model in SOA terms. Second, a
class-based graph-referred to as a service network schema-restricts the set of
potential service elements that may participate in the service protocol by
defining constraints on nodes and constraints on arcs, i.e., social
requirements. Another major contribution to the modelling of human interactions
is a unified approach organized around the concept of service, understood in a
broad sense with services being not only Web services, but also provided by
humans.",terms of service
http://arxiv.org/abs/1504.02052v1,"Exchange of services and resources in, or over, networks is attracting
nowadays renewed interest. However, despite the broad applicability and the
extensive study of such models, e.g., in the context of P2P networks, many
fundamental questions regarding their properties and efficiency remain
unanswered. We consider such a service exchange model and analyze the users'
interactions under three different approaches. First, we study a centrally
designed service allocation policy that yields the fair total service each user
should receive based on the service it others to the others. Accordingly, we
consider a competitive market where each user determines selfishly its
allocation policy so as to maximize the service it receives in return, and a
coalitional game model where users are allowed to coordinate their policies. We
prove that there is a unique equilibrium exchange allocation for both game
theoretic formulations, which also coincides with the central fair service
allocation. Furthermore, we characterize its properties in terms of the
coalitions that emerge and the equilibrium allocations, and analyze its
dependency on the underlying network graph. That servicing policy is the
natural reference point to the various mechanisms that are currently proposed
to incentivize user participation and improve the efficiency of such networked
service (or, resource) exchange markets.",terms of service
http://arxiv.org/abs/cs/0212051v1,"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.",terms of service
http://arxiv.org/abs/1305.6011v2,"Information system evolved as the evolution of information technology. The
current state of information technology, placed the internet as a main
resources of computing. Cloud technology as the backbone of internet has been
utilized as a powerful computing resources. Therefore, cloud introduced new
term of service oriented technology, popular with ""as a service"" kind of name.
In this paper, the service oriented paradigm will be used to address future
trend of information system. Thus, this paper try to introduce the term
""information system as a service"", holistic view of infrastructure as a
service, platform as a service, software as a service, and data as a service.",terms of service
http://arxiv.org/abs/1106.1523v1,"Interactive query expansion can assist users during their query formulation
process. We conducted a user study with over 4,000 unique visitors and four
different design approaches for a search term suggestion service. As a basis
for our evaluation we have implemented services which use three different
vocabularies: (1) user search terms, (2) terms from a terminology service and
(3) thesaurus terms. Additionally, we have created a new combined service which
utilizes thesaurus term and terms from a domain-specific search term
re-commender. Our results show that the thesaurus-based method clearly is used
more often compared to the other single-method implementations. We interpret
this as a strong indicator that term suggestion mechanisms should be
domain-specific to be close to the user terminology. Our novel combined
approach which interconnects a thesaurus service with additional statistical
relations out-performed all other implementations. All our observations show
that domain-specific vocabulary can support the user in finding alternative
concepts and formulating queries.",terms of service
http://arxiv.org/abs/1907.13293v1,"Blockchain is an innovative distributed ledger technology which has attracted
a wide range of interests for building the next generation of applications to
address lack-of-trust issues in business. Blockchain as a service (BaaS) is a
promising solution to improve the productivity of blockchain application
development. However, existing BaaS deployment solutions are mostly
vendor-locked: they are either bound to a cloud provider or a blockchain
platform. In addition to deployment, design and implementation of
blockchain-based applications is a hard task requiring deep expertise.
Therefore, this paper presents a unified blockchain as a service platform
(uBaaS) to support both design and deployment of blockchain-based applications.
The services in uBaaS include deployment as a service, design pattern as a
service and auxiliary services. In uBaaS, deployment as a service is platform
agnostic, which can avoid lock-in to specific cloud platforms, while design
pattern as a service applies design patterns for data management and smart
contract design to address the scalability and security issues of blockchain.
The proposed solutions are evaluated using a real-world quality tracing use
case in terms of feasibility and scalability.",terms of service
http://arxiv.org/abs/1501.04298v1,"As the number of Web services with the same or similar functions increases
steadily on the Internet, nowadays more and more service consumers pay great
attention to the non-functional properties of Web services, also known as
quality of service (QoS), when finding and selecting appropriate Web services.
For most of the QoS-aware Web service recommendation systems, the list of
recommended Web services is generally obtained based on a rating-oriented
prediction approach, aiming at predicting the potential ratings that an active
user may assign to the unrated services as accurately as possible. However, in
some application scenarios, high accuracy of rating prediction may not
necessarily lead to a satisfactory recommendation result. In this paper, we
propose a ranking-oriented hybrid approach by combining the item-based
collaborative filtering and latent factor models to address the problem of Web
services ranking. In particular, the similarity between two Web services is
measured in terms of the correlation coefficient between their rankings instead
of between the traditional QoS ratings. Besides, we also improve the measure
NDCG (Normalized Discounted Cumulative Gain) for evaluating the accuracy of the
top K recommendations returned in ranked order. Comprehensive experiments on
the QoS data set composed of real-world Web services are conducted to test our
approach, and the experimental results demonstrate that our approach
outperforms other competing approaches.",terms of service
http://arxiv.org/abs/1611.05380v2,"The emerging marketplace for online free services in which service providers
earn revenue from using consumer data in direct and indirect ways has lead to
significant privacy concerns. This leads to the following question: can the
online marketplace sustain multiple service providers (SPs) that offer
privacy-differentiated free services? This paper studies the problem of market
segmentation for the free online services market by augmenting the classical
Hotelling model for market segmentation analysis to include the fact that for
the free services market, a consumer values service not in monetized terms but
by its quality of service (QoS) and that the differentiator of services is not
product price but the privacy risk advertised by a SP. Building upon the
Hotelling model, this paper presents a parametrized model for SP profit and
consumer valuation of service for both the two- and multi-SP problems to show
that: (i) when consumers place a high value on privacy, it leads to a lower use
of private data by SPs (i.e., their advertised privacy risk reduces), and thus,
SPs compete on the QoS; (ii) SPs that are capable of differentiating on
services that do not directly target consumers gain larger market share; and
(iii) a higher valuation of privacy by consumers forces SPs with smaller
untargeted revenue to offer lower privacy risk to attract more consumers. The
work also illustrates the market segmentation problem for more than two SPs and
highlights the instability of such markets.",terms of service
http://arxiv.org/abs/1608.08799v2,"Efficient service composition in real time while providing necessary Quality
of Service (QoS) guarantees has been a challenging research problem with ever
growing complexity. Several heuristic based approaches with diverse proposals
for taming the scale and complexity of web service composition, have been
proposed in literature. In this paper, we present a new approach for efficient
service composition based on abstraction refinement. Instead of considering
individual services during composition, we propose several abstractions to form
service groups and the composition is done on these abstract services.
Abstraction reduces the search space significantly and thereby can be done
reasonably fast. While this can expedite solution construction to a great
extent, this also entails a possibility that it may fail to generate any
solution satisfying the QoS constraints, though the individual services
construct a valid solution. Hence, we propose to refine an abstraction to
generate the composite solution with desired QoS values. A QoS satisfying
solution, if one exists, can be constructed with multiple iterations of
abstraction refinement. While in the worst case, this approach may end up
exploring the complete composition graph constructed on individual services, on
an average, the solution can be achieved on the abstract graph. The abstraction
refinement techniques give a significant speed-up compared to the traditional
composition techniques. Experimental results on real benchmarks show the
efficiency of our proposed mechanism in terms of time and the number of
services considered for composition.",terms of service
http://arxiv.org/abs/1308.5397v1,"Traffic shaping is a mechanism used by Internet Service Providers (ISPs) to
limit subscribers' traffic based on their service contracts. This paper
investigates the current implementation of traffic shaping based on the token
bucket filter (TBF), discusses its advantages and disadvantages, and proposes a
cooperative TBF that can improve subscribers' quality of service (QoS)/quality
of experience (QoE) without compromising business aspects of the service
contract model by proportionally allocating excess bandwidth from inactive
subscribers to active ones based on the long-term bandwidths per their service
contracts.",terms of service
http://arxiv.org/abs/1409.7233v1,"In this paper we propose I/O state transition diagrams for service
description In contrast to other techniques like for example Statecharts we
allow to model non atomic services by sequences of transitions This is
especially important in a distributed system where concurrent service
invocation cannot be prohibited We give a mathematical model of object
behaviour based on concurrent and sequential messages Then we give a precise
semantics of the service descriptions in terms of the mathematical model.",terms of service
http://arxiv.org/abs/1708.01412v1,"Background: Cloud Computing is increasingly booming in industry with many
competing providers and services. Accordingly, evaluation of commercial Cloud
services is necessary. However, the existing evaluation studies are relatively
chaotic. There exists tremendous confusion and gap between practices and theory
about Cloud services evaluation. Aim: To facilitate relieving the
aforementioned chaos, this work aims to synthesize the existing evaluation
implementations to outline the state-of-the-practice and also identify research
opportunities in Cloud services evaluation. Method: Based on a conceptual
evaluation model comprising six steps, the Systematic Literature Review (SLR)
method was employed to collect relevant evidence to investigate the Cloud
services evaluation step by step. Results: This SLR identified 82 relevant
evaluation studies. The overall data collected from these studies essentially
represent the current practical landscape of implementing Cloud services
evaluation, and in turn can be reused to facilitate future evaluation work.
Conclusions: Evaluation of commercial Cloud services has become a world-wide
research topic. Some of the findings of this SLR identify several research gaps
in the area of Cloud services evaluation (e.g., the Elasticity and Security
evaluation of commercial Cloud services could be a long-term challenge), while
some other findings suggest the trend of applying commercial Cloud services
(e.g., compared with PaaS, IaaS seems more suitable for customers and is
particularly important in industry). This SLR study itself also confirms some
previous experiences and reveals new Evidence-Based Software Engineering (EBSE)
lessons.",terms of service
http://arxiv.org/abs/1904.05864v1,"Although network functions virtualization and software-defined networking
offer many dynamic features such as flexibility, scalability, and
programmability for easy provisioning of services at a lesser cost and time
through service function chaining, it introduces new challenges in terms of
reliability, availability, and latency of services. Particularly,
softwarization of network and service functions (e.g., virtualization, anything
as a service, dynamic virtual chaining, and routing) impose high possibility of
network failures due to software issues than hardware. In this letter, we
propose a novel solution called eRESERV to enhance the reliability of service
chains in 5G while meeting the service level agreements.",terms of service
http://arxiv.org/abs/1905.09771v1,"Network slicing is increasingly used to partition network infrastructure
between different mobile services. Precise service-wise mobile traffic
forecasting becomes essential in this context, as mobile operators seek to
pre-allocate resources to each slice in advance, to meet the distinct
requirements of individual services. This paper attacks the problem of
multi-service mobile traffic forecasting using a sequence-to-sequence (S2S)
learning paradigm and convolutional long short-term memories (ConvLSTMs). The
proposed architecture is designed so as to effectively extract complex
spatiotemporal features of mobile network traffic and predict with high
accuracy the future demands for individual services at city scale. We conduct
experiments on a mobile traffic dataset collected in a large European
metropolis, demonstrating that the proposed S2S-ConvLSTM can forecast the
mobile traffic volume produced by tens of different services in advance of up
to one hour, by just using measurements taken during the past hour. In
particular, our solution achieves mean absolute errors (MAE) at antenna level
that are below 13KBps, outperforming other deep learning approaches by up to
31.2%.",terms of service
http://arxiv.org/abs/1205.5960v1,"The semantic e-government is a new application field accompanying the
development of semantic web where the ontologies have become a fertile field of
investigation. This is due firstly to both the complexity and the size of
e-government systems and secondly to the importance of the issues. However,
permitting easy and personalized access to e-government services has become, at
this juncture, an arduous and not spontaneous process. Indeed, the provided
e-gov services to the user represent a critical contact point between
administrations and users. The encountered problems in the e-gov services
retrieving process are: the absence of an integrated one-stop government, the
difficulty of localizing the services' sources, the lack of mastery of search
terms and the deficiency of multilingualism of the online services. In order to
solve these problems, to facilitate access to e-gov services and to satisfy the
needs of potential users, we propose an original approach to this issue. This
approach incorporates a semantic layer as a crucial element in the retrieving
process. It consists in implementing a personalized search system that
integrates ontology of the e-gov domain in this process.",terms of service
http://arxiv.org/abs/1003.5440v1,"The wide-band code division multiple access (WCDMA) based 3G and beyond
cellular mobile wireless networks are expected to provide a diverse range of
multimedia services to mobile users with guaranteed quality of service (QoS).
To serve diverse quality of service requirements of these networks it
necessitates new radio resource management strategies for effective utilization
of network resources with coding schemes. Call admission control (CAC) is a
significant component in wireless networks to guarantee quality of service
requirements and also to enhance the network resilience. In this paper capacity
enhancement for WCDMA network with convolutional coding scheme is discussed and
compared with block code and without coding scheme to achieve a better balance
between resource utilization and quality of service provisioning. The model of
this network is valid for the real-time (RT) and non-real-time (NRT) services
having different data rate. Simulation results demonstrate the effectiveness of
the network using convolutional code in terms of capacity enhancement and QoS
of the voice and video services.",terms of service
http://arxiv.org/abs/1512.07685v1,"We propose the use of structured natural language (English) in specifying
service choreographies, focusing on the what rather than the how of the
required coordination of participant services in realising a business
application scenario. The declarative approach we propose uses the OMG standard
Semantics of Business Vocabulary and Rules (SBVR) as a modelling language. The
service choreography approach has been proposed for describing the global
orderings of the invocations on interfaces of participant services. We
therefore extend SBVR with a notion of time which can capture the coordination
of the participant services, in terms of the observable message exchanges
between them. The extension is done using existing modelling constructs in
SBVR, and hence respects the standard specification. The idea is that users -
domain specialists rather than implementation specialists - can verify the
requested service composition by directly reading the structured English used
by SBVR. At the same time, the SBVR model can be represented in formal logic so
it can be parsed and executed by a machine.",terms of service
http://arxiv.org/abs/1306.4063v1,"Quality of Service (QoS) has gained more importance with the increase in
usage and adoption of web services. In recent years, various tools and
techniques developed for measurement and evaluation of QoS of web services.
There are commercial as well as open-source tools available today which are
being used for monitoring and testing QoS for web services. These tools
facilitate in QoS measurement and analysis and are helpful in evaluation of
service performance in real-time network. In this paper, we describe three
popular open-source tools and compare them in terms of features, usability,
performance, and software requirements. Results of the comparison will help in
adoption and usage of these tools, and also promote development and usage of
open-source web service testing tools.",terms of service
http://arxiv.org/abs/1502.02840v1,"In this paper we present a theoretical analysis of graph-based service
composition in terms of its dependency with service discovery. Driven by this
analysis we define a composition framework by means of integration with
fine-grained I/O service discovery that enables the generation of a graph-based
composition which contains the set of services that are semantically relevant
for an input-output request. The proposed framework also includes an optimal
composition search algorithm to extract the best composition from the graph
minimising the length and the number of services, and different graph
optimisations to improve the scalability of the system. A practical
implementation used for the empirical analysis is also provided. This analysis
proves the scalability and flexibility of our proposal and provides insights on
how integrated composition systems can be designed in order to achieve good
performance in real scenarios for the Web.",terms of service
http://arxiv.org/abs/1612.05416v2,"The ""Smart City"" (SC) concept revolves around the idea of embodying
cutting-edge ICT solutions in the very fabric of future cities, in order to
offer new and better services to citizens while lowering the city management
costs, both in monetary, social, and environmental terms. In this framework,
communication technologies are perceived as subservient to the SC services,
providing the means to collect and process the data needed to make the services
function. In this paper, we propose a new vision in which technology and SC
services are designed to take advantage of each other in a symbiotic manner.
According to this new paradigm, which we call ""SymbioCity"", SC services can
indeed be exploited to improve the performance of the same communication
systems that provide them with data. Suggestive examples of this symbiotic
ecosystem are discussed in the paper. The dissertation is then substantiated in
a proof-of-concept case study, where we show how the traffic monitoring service
provided by the London Smart City initiative can be used to predict the density
of users in a certain zone and optimize the cellular service in that area.",terms of service
http://arxiv.org/abs/1707.01064v2,"High transmission rate and secure communication have been identified as the
key targets that need to be effectively addressed by fifth generation (5G)
wireless systems. In this context, the concept of physical-layer security
becomes attractive, as it can establish perfect security using only the
characteristics of wireless medium. Nonetheless, to further increase the
spectral efficiency, an emerging concept, termed physical-layer service
integration (PHY-SI), has been recognized as an effective means. Its basic idea
is to combine multiple coexisting services, i.e., multicast/broadcast service
and confidential service, into one integral service for one-time transmission
at the transmitter side. This article first provides a tutorial on typical
PHY-SI models. Furthermore, we propose some state-of-the-art solutions to
improve the overall performance of PHY-SI in certain important communication
scenarios. In particular, we highlight the extension of several concepts
borrowed from conventional single-service communications, such as artificial
noise (AN), eigenmode transmission etc., to the scenario of PHY-SI. These
techniques are shown to be effective in the design of reliable and robust
PHY-SI schemes. Finally, several potential research directions are identified
for future work.",terms of service
http://arxiv.org/abs/1710.06190v1,"In their $1996$ paper Anantharam and Verd\'u showed that feedback does not
increase the capacity of a queue when the service time is exponentially
distributed. Whether this conclusion holds for general service times has
remained an open question which this paper addresses.
  Two main results are established for both the discrete-time and the
continuous-time models. First, a sufficient condition on the service
distribution for feedback to increase capacity under FIFO service policy.
Underlying this condition is a notion of weak feedback wherein instead of the
queue departure times the transmitter is informed about the instants when
packets start to be served. Second, a condition in terms of output entropy rate
under which feedback does not increase capacity. This condition is general in
that it depends on the output entropy rate of the queue but explicitly depends
neither on the queue policy nor on the service time distribution. This
condition is satisfied, for instance, by queues with LCFS service policies and
bounded service times.",terms of service
http://arxiv.org/abs/1406.5354v1,"With the rapid development of high-speed railway (HSR), how to provide the
passengers with multimedia services has attracted increasing attention. A key
issue is to develop an effective scheduling algorithm for multiple services
with different quality of service (QoS) requirements. In this paper, we
investigate the downlink service scheduling problem in HSR network taking
account of end-to-end deadline constraints and successfully packet delivery
ratio requirements. Firstly, by exploiting the deterministic high-speed train
trajectory, we present a time-distance mapping in order to obtain the highly
dynamic link capacity effectively. Next, a novel service model is developed for
deadline constrained services with delivery ratio requirements, which enables
us to turn the delivery ratio requirement into a single queue stability
problem. Based on the Lyapunov drift, the optimal scheduling problem is
formulated and the corresponding scheduling service algorithm is proposed by
stochastic network optimization approach. Simulation results show that the
proposed algorithm outperforms the conventional schemes in terms of QoS
requirements.",terms of service
http://arxiv.org/abs/1105.2213v1,"As a result of the phenomenal proliferation of modern mobile Internet-enabled
devices and the widespread utilization of wireless and cellular data networks,
mobile users are increasingly requiring services tailored to their current
context. High-level context information is typically obtained from context
services that aggregate raw context information sensed by various sensors and
mobile devices. Given the massive amount of sensed data, traditional context
services are lacking the necessary resources to store and process these data,
as well as to disseminate high-level context information to a variety of
potential context consumers. In this paper, we propose a novel framework for
context information provisioning, which relies on deploying context services on
the cloud and using context brokers to mediate between context consumers and
context services using a publish/subscribe model. Moreover, we describe a
multi-attributes decision algorithm for the selection of potential context
services that can fulfill context consumers' requests for context information.
The algorithm calculates the score of each context service, per context
information type, based on the quality-of-service (QoS) and quality-of-context
information (QoC) requirements expressed by the context consumer. One of the
benefits of the approach is that context providers can scale up and down, in
terms of cloud resources they use, depending on current demand for context
information. Besides, the selection algorithm allows ranking context services
by matching their QoS and QoC offers against the QoS and QoC requirements of
the context consumer.",terms of service
http://arxiv.org/abs/1808.06818v1,"In Interactive Information Retrieval (IIR) different services such as search
term suggestion can support users in their search process. The applicability
and performance of such services is either measured with different
user-centered studies (like usability tests or laboratory experiments) or, in
the context of IR, with their contribution to measures like precision and
recall. However, each evaluation methodology has its certain disadvantages. For
example, user-centered experiments are often costly and small-scaled; IR
experiments rely on relevance assessments and measure only relevance of
documents. In this work we operationalize the usefulness model of Cole et al.
(2009) on the level of system support to measure not only the local effect of
an IR service, but the impact it has on the whole search process. We therefore
use a log-based evaluation approach which models user interactions within
sessions with positive signals and apply it for the case of a search term
suggestion service. We found that the usage of the service significantly often
implicates the occurrence of positive signals during the following session
steps.",terms of service
http://arxiv.org/abs/1105.0417v1,"We consider a generalized processing system having several queues, where the
available service rate combinations are fluctuating over time due to
reliability and availability variations. The objective is to allocate the
available resources, and corresponding service rates, in response to both
workload and service capacity considerations, in order to maintain the long
term stability of the system. The service configurations are completely
arbitrary, including negative service rates which represent forwarding and
service-induced cross traffic. We employ a trace-based trajectory asymptotic
technique, which requires minimal assumptions about the arrival dynamics of the
system.
  We prove that cone schedules, which leverage the geometry of the queueing
dynamics, maximize the system throughput for a broad class of processing
systems, even under adversarial arrival processes. We study the impact of
fluctuating service availability, where resources are available only some of
the time, and the schedule must dynamically respond to the changing available
service rates, establishing both the capacity of such systems and the class of
schedules which will stabilize the system at full capacity. The rich geometry
of the system dynamics leads to important insights for stability, performance
and scalability, and substantially generalizes previous findings.
  The processing system studied here models a broad variety of computer,
communication and service networks, including varying channel conditions and
cross-traffic in wireless networking, and call centers with fluctuating
capacity. The findings have implications for bandwidth and processor allocation
in communication networks and workforce scheduling in congested call centers.",terms of service
http://arxiv.org/abs/1301.4839v1,"Service-Oriented Computing (SOC) enables the composition of loosely coupled
service agents provided with varying Quality of Service (QoS) levels,
effectively forming a multiagent system (MAS). Selecting a (near-)optimal set
of services for a composition in terms of QoS is crucial when many functionally
equivalent services are available. As the number of distributed services,
especially in the cloud, is rising rapidly, the impact of the network on the
QoS keeps increasing. Despite this and opposed to most MAS approaches, current
service approaches depend on a centralized architecture which cannot adapt to
the network. Thus, we propose a scalable distributed architecture composed of a
flexible number of distributed control nodes. Our architecture requires no
changes to existing services and adapts from a centralized to a completely
distributed realization by adding control nodes as needed. Also, we propose an
extended QoS aggregation algorithm that allows to accurately estimate network
QoS. Finally, we evaluate the benefits and optimality of our architecture in a
distributed environment.",terms of service
http://arxiv.org/abs/1511.02960v1,"Modern latency-critical online services often rely on composing results from
a large number of server components. Hence the tail latency (e.g. the 99th
percentile of response time), rather than the average, of these components
determines the overall service performance. When hosted on a cloud environment,
the components of a service typically co-locate with short batch jobs to
increase machine utilizations, and share and contend resources such as caches
and I/O bandwidths with them. The highly dynamic nature of batch jobs in terms
of their workload types and input sizes causes continuously changing
performance interference to individual components, hence leading to their
latency variability and high tail latency. However, existing techniques either
ignore such fine-grained component latency variability when managing service
performance, or rely on executing redundant requests to reduce the tail
latency, which adversely deteriorate the service performance when load gets
heavier. In this paper, we propose PCS, a predictive and component-level
scheduling framework to reduce tail latency for large-scale, parallel online
services. It uses an analytical performance model to simultaneously predict the
component latency and the overall service performance on different nodes. Based
on the predicted performance, the scheduler identifies straggling components
and conducts near-optimal component-node allocations to adapt to the changing
performance interferences from batch jobs. We demonstrate that, using realistic
workloads, the proposed scheduler reduces the component tail latency by an
average of 67.05\% and the average overall service latency by 64.16\% compared
with the state-of-the-art techniques on reducing tail latency.",terms of service
http://arxiv.org/abs/1705.10554v2,"Software Defined Networking and Network Function Virtualization are two
paradigms that offer flexible software-based network management. Service
providers are instantiating Virtualized Network Functions - e.g., firewalls,
DPIs, gateways - to highly facilitate the deployment and reconfiguration of
network services with reduced time-to-value. They employ Service Function
Chaining technologies to dynamically reconfigure network paths traversing
physical and virtual network functions. Providing a cost-efficient virtual
function deployment over the network for a set of service chains is a key
technical challenge for service providers, and this problem has recently caught
much attention from both Industry and Academia. In this paper, we propose a
formulation of this problem as an Integer Linear Program that allows one to
find the best feasible paths and virtual function placement for a set of
services with respect to a total financial cost, while taking into account the
(total or partial) order constraints for Service Function Chains of each
service and other constraints such as end-to-end latency, anti-affinity rules
between network functions on the same physical node and resource limitations in
terms of network and processing capacities. Furthermore, we propose a heuristic
algorithm based on a linear relaxation of the problem that performs close to
optimum for large scale instances.",terms of service
http://arxiv.org/abs/1502.06735v1,"In this paper, we propose first to start by presenting a state of the art of
existing approaches about scientific workflows (including neuroscience
workflows) in order to highlight business users' needs in terms of Web Services
combination. Then we discuss about intentional process modeling for scientific
workflows especially to search for Web Services. Next we present our approach
SATIS to provide reasoning and traceability capabilities on Web Services
business combination know-how, in order to bridge the gap between workflows
providers and users.",terms of service
http://arxiv.org/abs/1206.5469v1,"Quality of Service (QoS) techniques are applied in IP networks to utilize
available network resources in the most efficient manner to minimize delays and
delay variations (jitters) in network traffic having multiple type of services.
Multimedia services may include voice, video and database. Researchers have
done considerable work on queuing disciplines to analyze and improve QoS
performance in wired and wireless IP networks. This paper highlights QoS
analysis in a wired IP network with more realistic enterprise modeling and
presents simulation results of a few statistics not presented and discussed
before. Four different applications are used i.e. FTP, Database, Voice over IP
(VoIP) and Video Conferencing (VC). Two major queuing disciplines are evaluated
i.e. 'Priority Queuing' and 'Weighted Fair Queuing' for packet identification
under Differentiated Services Code Point (DSCP). The simulation results show
that WFQ has an edge over PQ in terms of queuing delays and jitters experienced
by low priority services. For high priority traffic, dependency of 'Traffic
Drop', 'Buffer Usage' and 'Packet Delay Variation' on selected buffer sizes is
simulated and discussed to evaluate QoS deeper. In the end, it is also analyzed
how network's database service with applied Quality of Service may be affected
in terms of throughput (average rate of data received) for internal network
users when the server is also accessed by external user(s) through Virtual
Private Network (VPN).",terms of service
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",unfair terms
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",unfair terms
http://arxiv.org/abs/1705.08804v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1706.09838v2,"We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.",unfair terms
http://arxiv.org/abs/1002.4833v1,"The number of users using wireless Local Area Network is increasing
exponentially and their behavior is changing day after day. Nowadays, users of
wireless LAN are using huge amount of bandwidth because of the explosive growth
of some services and applications such as video sharing. This situation imposes
massive pressure on the wireless LAN performance especially in term of fairness
among wireless stations. The limited resources are not distributed fairly in
saturated conditions. The most important resource is the access point buffer
space. This importance is a result of access point being the bottleneck between
two different types of networks. These two types are wired network with
relatively huge bandwidth and wireless network with much smaller bandwidth.
Also the unfairness problem is keep getting worse because of the greedy nature
Transmission Control Protocol (TCP). In this paper, we conduct a comprehensive
study on wireless LAN dynamics and proposed a new mathematical model that
describes the performance and effects of its behavior. We validate the proposed
model by using the simulation technique. The proposed model was able to produce
very good approximation in most of the cases. It also gave us a great insight
into the effective variables in the wireless LAN behavior and what are the
dimensions of the unfairness problem.",unfair terms
http://arxiv.org/abs/1607.07021v1,"We consider single-hop topologies with saturated transmitting nodes, using
IEEE~802.11 DCF for medium access. However, unlike the conventional WiFi, we
study systems where one or more of the protocol parameters are different from
the standard, and/or where the propagation delays among the nodes are not
negligible compared to the duration of a backoff slot. We observe that for
several classes of protocol parameters, and for large propagation delays, such
systems exhibit a certain performance anomaly known as short term unfairness,
which may lead to severe performance degradation. The standard fixed point
analysis technique (and its simple extensions) do not predict the system
behavior well in such cases; a mean field model based asymptotic approach also
is not adequate to predict the performance for networks of practical sizes in
such cases. We provide a detailed stochastic model that accurately captures the
system evolution. Since an exact analysis of this model is computationally
intractable, we develop a novel approximate, but accurate, analysis that uses a
parsimonious state representation for computational tractability. Apart from
providing insights into the system behavior, the analytical method is also able
to quantify the extent of short term unfairness in the system, and can
therefore be used for tuning the protocol parameters to achieve desired
throughput and fairness objectives.",unfair terms
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",unfair terms
http://arxiv.org/abs/0806.1093v1,"We present the station-based unfair access problem among the uplink and the
downlink stations in the IEEE 802.11e infrastructure Basic Service Set (BSS)
when the default settings of the Enhanced Distributed Channel Access (EDCA)
parameters are used. We discuss how the transport layer protocol
characteristics alleviate the unfairness problem. We design a simple,
practical, and standard-compliant framework to be employed at the Access Point
(AP) for fair and efficient access provisioning. A dynamic measurement-based
EDCA parameter adaptation block lies in the core of this framework. The
proposed framework is unique in the sense that it considers the characteristic
differences of Transmission Control Protocol (TCP) and User Datagram Protocol
(UDP) flows and the coexistence of stations with varying bandwidth or
Quality-of-Service (QoS) requirements. Via simulations, we show that our
solution provides short- and long-term fair access for all stations in the
uplink and downlink employing TCP and UDP flows with non-uniform packet rates
in a wired-wireless heterogeneous network. In the meantime, the QoS
requirements of coexisting real-time flows are also maintained.",unfair terms
http://arxiv.org/abs/1510.01125v1,"-Performance of Vehicular Adhoc Networks (VANETs) in high node density
situation has long been a major field of studies. Particular attention has been
paid to the frequent exchange of Cooperative Awareness Messages (CAMs) on which
many road safety applications rely. In the present paper, se focus on the
European Telecommunications Standard Institute (ETSI) Decentralized Congestion
Control (DCC) mechanism, particularly on the evaluation of its facility layers
component when applied in the context of dense networks. For this purpose, a
set of simulations has been conducted over several scenarios, considering rural
highway and urban mobility in order to investigate unfairness and oscillation
issues, and analyze the triggering factors. The experimental results show that
the latest technical specification of the ETSI DCC presents a significant
enhancement in terms of fairness. In contrast, the stability criterion leaves
room for improvement as channel load measurement presents (i) considerable
fluctuations when only the facility layer control is applied and (i.i) severe
state oscillation when different DCC control methods are combined.",unfair terms
http://arxiv.org/abs/1509.03815v1,"In this paper, we revisit two fundamental results of the self-stabilizing
literature about silent BFS spanning tree constructions: the Dolev et al
algorithm and the Huang and Chen's algorithm. More precisely, we propose in the
composite atomicity model three straightforward adaptations inspired from those
algorithms. We then present a deep study of these three algorithms. Our results
are related to both correctness (convergence and closure, assuming a
distributed unfair daemon) and complexity (analysis of the stabilization time
in terms of rounds and steps).",unfair terms
http://arxiv.org/abs/0806.1089v1,"When the stations in an IEEE 802.11 infrastructure Basic Service Set (BSS)
employ Transmission Control Protocol (TCP) in the transport layer, this
exacerbates per-flow unfair access which is a direct result of uplink/downlink
bandwidth asymmetry in the BSS. We propose a novel and simple analytical model
to approximately calculate the per-flow TCP congestion window limit that
provides fair and efficient TCP access in a heterogeneous wired-wireless
scenario. The proposed analysis is unique in that it considers the effects of
varying number of uplink and downlink TCP flows, differing Round Trip Times
(RTTs) among TCP connections, and the use of delayed TCP Acknowledgment (ACK)
mechanism. Motivated by the findings of this analysis, we design a link layer
access control block to be employed only at the Access Point (AP) in order to
resolve the unfair access problem. The novel and simple idea of the proposed
link layer access control block is employing a congestion control and filtering
algorithm on TCP ACK packets of uplink flows, thereby prioritizing the access
of TCP data packets of downlink flows at the AP. Via simulations, we show that
short- and long-term fair access can be provisioned with the introduction of
the proposed link layer access control block to the protocol stack of the AP
while improving channel utilization and access delay.",unfair terms
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",unfair terms
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",unfair terms
http://arxiv.org/abs/1903.01209v2,"Most existing notions of algorithmic fairness are one-shot: they ensure some
form of allocative equality at the time of decision making, but do not account
for the adverse impact of the algorithmic decisions today on the long-term
welfare and prosperity of certain segments of the population. We take a broader
perspective on algorithmic fairness. We propose an effort-based measure of
fairness and present a data-driven framework for characterizing the long-term
impact of algorithmic policies on reshaping the underlying population.
Motivated by the psychological literature on \emph{social learning} and the
economic literature on equality of opportunity, we propose a micro-scale model
of how individuals may respond to decision-making algorithms. We employ
existing measures of segregation from sociology and economics to quantify the
resulting macro-scale population-level change. Importantly, we observe that
different models may shift the group-conditional distribution of qualifications
in different directions. Our findings raise a number of important questions
regarding the formalization of fairness for decision-making models.",unfair terms
http://arxiv.org/abs/1511.06035v7,"Applications running in modern multithreaded environments are sometimes
\emph{over-threaded}. The excess threads do not improve performance, and in
fact may act to degrade performance via \emph{scalability collapse}. Often,
such software also has highly contended locks. We opportunistically leverage
the existence of such locks by modifying the lock admission policy so as to
intentionally limit the number of threads circulating over the lock in a given
period. Specifically, if there are more threads circulating than are necessary
to keep the lock saturated, our approach will selectively cull and passivate
some of those threads. We borrow the concept of \emph{swapping} from the field
of memory management and intentionally impose \emph{concurrency restriction}
(CR) if a lock is oversubscribed. In the worst case CR does no harm, but it
often yields performance benefits. The resultant admission order is unfair over
the short term but we explicitly provide long-term fairness by periodically
shifting threads between the set of passivated threads and those actively
circulating. Our approach is palliative, but often effective.",unfair terms
http://arxiv.org/abs/1905.11260v3,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called r-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and learning algorithm used as a black-box. We provide a
fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves O(log(T)) r-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1907.10516v1,"We study an interesting variant of the stochastic multi-armed bandit problem,
called the Fair-SMAB problem, where each arm is required to be pulled for at
least a given fraction of the total available rounds. We investigate the
interplay between learning and fairness in terms of a pre-specified vector
denoting the fractions of guaranteed pulls. We define a fairness-aware regret,
called $r$-Regret, that takes into account the above fairness constraints and
naturally extends the conventional notion of regret. Our primary contribution
is characterizing a class of Fair-SMAB algorithms by two parameters: the
unfairness tolerance and the learning algorithm used as a black-box. We provide
a fairness guarantee for this class that holds uniformly over time irrespective
of the choice of the learning algorithm. In particular, when the learning
algorithm is UCB1, we show that our algorithm achieves $O(\ln T)$ $r$-Regret.
Finally, we evaluate the cost of fairness in terms of the conventional notion
of regret.",unfair terms
http://arxiv.org/abs/1403.4357v1,"High speed railways (HSRs) have been deployed widely all over the world in
recent years. Different from traditional cellular communication, its high
mobility makes it essential to implement power allocation along the time. In
the HSR case, the transmission rate depends greatly on the distance between the
base station (BS) and the train. As a result, the train receives a time varying
data rate service when passing by a BS. It is clear that the most efficient
power allocation will spend all the power when the train is nearest from the
BS, which will cause great unfairness along the time. On the other hand, the
channel inversion allocation achieves the best fairness in terms of constant
rate transmission. However, its power efficiency is much lower. Therefore, the
power efficiency and the fairness along time are two incompatible objects. For
the HSR cellular system considered in this paper, a trade-off between the two
is achieved by proposing a temporal proportional fair power allocation scheme.
Besides, near optimal closed form solution and one algorithm finding the
$\epsilon$-optimal allocation are presented.",unfair terms
http://arxiv.org/abs/1805.12572v3,"We compare and contrast fourteen measures that have been proposed for the
purpose of quantifying partisan gerrymandering. We consider measures that,
rather than examining the shapes of districts, utilize only the partisan vote
distribution among districts. The measures considered are two versions of
partisan bias; the efficiency gap and several of its variants; the mean-median
difference and the equal vote weight standard; the declination and one variant;
and the lopsided-means test. Our primary means of evaluating these measures is
a suite of hypothetical elections we classify from the start as fair or unfair.
We conclude that the declination is the most successful measure in terms of
avoiding false positives and false negatives on the elections considered. We
include in an appendix the most extreme outliers for each measure among
historical congressional and state legislative elections.",unfair terms
http://arxiv.org/abs/1806.09936v1,"Black box systems for automated decision making, often based on machine
learning over (big) data, map a user's features into a class or a score without
exposing the reasons why. This is problematic not only for lack of
transparency, but also for possible biases hidden in the algorithms, due to
human prejudices and collection artifacts hidden in the training data, which
may lead to unfair or wrong decisions. We introduce the local-to-global
framework for black box explanation, a novel approach with promising early
results, which paves the road for a wide spectrum of future developments along
three dimensions: (i) the language for expressing explanations in terms of
highly expressive logic-based rules, with a statistical and causal
interpretation; (ii) the inference of local explanations aimed at revealing the
logic of the decision adopted for a specific instance by querying and auditing
the black box in the vicinity of the target instance; (iii), the bottom-up
generalization of the many local explanations into simple global ones, with
algorithms that optimize the quality and comprehensibility of explanations.",unfair terms
http://arxiv.org/abs/1905.12535v1,"Despite the potential of online sharing economy platforms such as Uber, Lyft,
or Foodora to democratize the labor market, these services are often accused of
fostering unfair working conditions and low wages. These problems have been
recognized by researchers and regulators but the size and complexity of these
socio-technical systems, combined with the lack of transparency about
algorithmic practices, makes it difficult to understand system dynamics and
large-scale behavior. This paper combines approaches from complex systems and
algorithmic fairness to investigate the effect of algorithm design decisions on
wage inequality in ride-hailing markets. We first present a computational model
that includes conditions about locations of drivers and passengers, traffic,
the layout of the city, and the algorithm that matches requests with drivers.
We calibrate the model with parameters derived from empirical data. Our
simulations show that small changes in the system parameters can cause large
deviations in the income distributions of drivers, leading to a highly
unpredictable system which often distributes vastly different incomes to
identically performing drivers. As suggested by recent studies about feedback
loops in algorithmic systems, these initial income differences can result in
enforced and long-term wage gaps.",unfair terms
http://arxiv.org/abs/1907.07944v1,"We present results on the last topic we collaborate with our late friend,
Professor Ajoy Kumar Datta (1958-2019).
  In this work, we shed new light on a self-stabilizing wave algorithm proposed
by Colette Johnen in 1997. This algorithm constructs a BFS spanning tree in any
connected rooted network. Nowadays, it is still the best existing
self-stabilizing BFS spanning tree construction in terms of memory requirement,
{\em i.e.}, it only requires $\Theta(1)$ bits per edge. However, it has been
proven assuming a weakly fair daemon. Moreover, its stabilization time was
unknown.
  Here, we study the slightly modified version of this algorithm, still keeping
the same memory requirement. We prove the self-stabilization of this variant
under the distributed unfair daemon and show a stabilization time in $O(D.n^2)$
rounds, where $D$ is the network diameter and $n$ the number of processes.",unfair terms
http://arxiv.org/abs/0803.1530v2,"Since cheating is obviously wrong, arguments against it (it provides an
unfair advantage, it hinders learning) need only be mentioned in passing. But
the argument of unfair advantage absurdly takes education to be essentially a
race of all against all; moreover, it ignores that many cases of unfair
(dis)advantages are widely accepted. That cheating can hamper learning does not
mean that punishing cheating will necessarily favor learning, so that this
argument does not obviously justify sanctioning cheaters.
  -- Keywords: academic dishonesty, academic integrity, academic misconduct,
education, ethics, homework, plagiarism",unfair terms
http://arxiv.org/abs/1805.01788v1,"Rankings of people and items are at the heart of selection-making,
match-making, and recommender systems, ranging from employment sites to sharing
economy platforms. As ranking positions influence the amount of attention the
ranked subjects receive, biases in rankings can lead to unfair distribution of
opportunities and resources, such as jobs or income.
  This paper proposes new measures and mechanisms to quantify and mitigate
unfairness from a bias inherent to all rankings, namely, the position bias,
which leads to disproportionately less attention being paid to low-ranked
subjects. Our approach differs from recent fair ranking approaches in two
important ways. First, existing works measure unfairness at the level of
subject groups while our measures capture unfairness at the level of individual
subjects, and as such subsume group unfairness. Second, as no single ranking
can achieve individual attention fairness, we propose a novel mechanism that
achieves amortized fairness, where attention accumulated across a series of
rankings is proportional to accumulated relevance.
  We formulate the challenge of achieving amortized individual fairness subject
to constraints on ranking quality as an online optimization problem and show
that it can be solved as an integer linear program. Our experimental evaluation
reveals that unfair attention distribution in rankings can be substantial, and
demonstrates that our method can improve individual fairness while retaining
high ranking quality.",unfair terms
http://arxiv.org/abs/1910.05591v1,"One often finds in the literature connections between measures of fairness
and measures of feature importance employed to interpret trained classifiers.
However, there seems to be no study that compares fairness measures and feature
importance measures. In this paper we propose ways to evaluate and compare such
measures. We focus in particular on SHAP, a game-theoretic measure of feature
importance; we present results for a number of unfairness-prone datasets.",unfair terms
http://arxiv.org/abs/1801.00594v3,"In this paper, we discuss the effects on throughput and fairness of dynamic
channel bonding (DCB) in spatially distributed high-density wireless local area
networks (WLANs). First, we present an analytical framework based on
continuous-time Markov networks (CTMNs) for depicting the behavior of different
DCB policies in spatially distributed scenarios, where nodes are not required
to be within the carrier sense range of each other. Then, we assess the
performance of DCB in high-density IEEE 802.11ac/ax WLANs by means of
simulations. We show that there may be critical interrelations among nodes in
the spatial domain - even if they are located outside the carrier sense range
of each other - in a chain reaction manner. Results also reveal that, while
always selecting the widest available channel normally maximizes the individual
long-term throughput, it often generates unfair situations where other WLANs
starve. Moreover, we show that there are scenarios where DCB with stochastic
channel width selection improves the latter approach both in terms of
individual throughput and fairness. It follows that there is not a unique
optimal DCB policy for every case. Instead, smarter bandwidth adaptation is
required in the challenging scenarios of next-generation WLANs.",unfair terms
http://arxiv.org/abs/1004.1042v1,"Multi-access networks may exhibit severe unfairness in throughput. Recent
studies show that this unfairness is due to local differences in the
neighborhood structure: Nodes with less neighbors receive better access. We
study the unfairness in saturated linear networks, and adapt the multi-access
CSMA protocol to remove the unfairness completely, by choosing the activation
rates of nodes appropriately as a function of the number of neighbors. We then
investigate the consequences of this choice of activation rates on the
network-average saturated throughput, and we show that these rates perform well
in a non-saturated setting.",unfair terms
http://arxiv.org/abs/1306.4999v1,"In electronic marketplaces, after each transaction buyers will rate the
products provided by the sellers. To decide the most trustworthy sellers to
transact with, buyers rely on trust models to leverage these ratings to
evaluate the reputation of sellers. Although the high effectiveness of
different trust models for handling unfair ratings have been claimed by their
designers, recently it is argued that these models are vulnerable to more
intelligent attacks, and there is an urgent demand that the robustness of the
existing trust models has to be evaluated in a more comprehensive way. In this
work, we classify the existing trust models into two broad categories and
propose an extendable e-marketplace testbed to evaluate their robustness
against different unfair rating attacks comprehensively. On top of highlighting
the robustness of the existing trust models for handling unfair ratings is far
from what they were claimed to be, we further propose and validate a novel
combination mechanism for the existing trust models, Discount-then-Filter, to
notably enhance their robustness against the investigated attacks.",unfair terms
http://arxiv.org/abs/1905.10921v1,"Noisy channels are a valuable resource from a cryptographic point of view.
They can be used for exchanging secret-keys as well as realizing other
cryptographic primitives such as commitment and oblivious transfer. To be
really useful, noisy channels have to be consider in the scenario where a
cheating party has some degree of control over the channel characteristics.
Damg\r{a}rd et al. (EUROCRYPT 1999) proposed a more realistic model where such
level of control is permitted to an adversary, the so called unfair noisy
channels, and proved that they can be used to obtain commitment and oblivious
transfer protocols. Given that noisy channels are a precious resource for
cryptographic purposes, one important question is determining the optimal rate
in which they can be used. The commitment capacity has already been determined
for the cases of discrete memoryless channels and Gaussian channels. In this
work we address the problem of determining the commitment capacity of unfair
noisy channels. We compute a single-letter characterization of the commitment
capacity of unfair noisy channels. In the case where an adversary has no
control over the channel (the fair case) our capacity reduces to the well-known
capacity of a discrete memoryless binary symmetric channel.",unfair terms
http://arxiv.org/abs/1105.3228v1,"Financial economic models often assume that investors know (or agree on) the
fundamental value of the shares of the firm, easing the passage from the
individual to the collective dimension of the financial system generated by the
Share Exchange over time. Our model relaxes that heroic assumption of one
unique ""true value"" and deals with the formation of share market prices through
the dynamic formation of individual and social opinions (or beliefs) based upon
a fundamental signal of economic performance and position of the firm, the
forecast revision by heterogeneous individual investors, and their social mood
or sentiment about the ongoing state of the market pricing process. Market
clearing price formation is then featured by individual and group dynamics that
make its collective dimension irreducible to its individual level. This dynamic
holistic approach can be applied to better understand the market exuberance
generated by the Share Exchange over time.",individual pricing
http://arxiv.org/abs/1809.03110v1,"Cloud spot markets rent VMs for a variable price that is typically much lower
than the price of on-demand VMs, which makes them attractive for a wide range
of large-scale applications. However, applications that run on spot VMs suffer
from cost uncertainty, since spot prices fluctuate, in part, based on supply,
demand, or both. The difficulty in predicting spot prices affects users and
applications: the former cannot effectively plan their IT expenditures, while
the latter cannot infer the availability and performance of spot VMs, which are
a function of their variable price. To address the problem, we use properties
of cloud infrastructure and workloads to show that prices become more stable
and predictable as they are aggregated together. We leverage this observation
to define an aggregate index price for spot VMs that serves as a reference for
what users should expect to pay. We show that, even when the spot prices for
individual VMs are volatile, the index price remains stable and predictable. We
then introduce cloud index tracking: a migration policy that tracks the index
price to ensure applications running on spot VMs incur a predictable cost by
migrating to a new spot VM if the current VM's price significantly deviates
from the index price.",individual pricing
http://arxiv.org/abs/1702.07032v2,"We show that the Revenue-Optimal Deterministic Mechanism Design problem for a
single additive buyer is #P-hard, even when the distributions have support size
2 for each item and, more importantly, even when the optimal solution is
guaranteed to be of a very simple kind: the seller picks a price for each
individual item and a price for the grand bundle of all the items; the buyer
can purchase either the grand bundle at its given price or any subset of items
at their total individual prices. The following problems are also #P-hard, as
immediate corollaries of the proof:
  1. determining if individual item pricing is optimal for a given instance,
  2. determining if grand bundle pricing is optimal, and
  3. computing the optimal (deterministic) revenue.
  On the positive side, we show that when the distributions are i.i.d. with
support size 2, the optimal revenue obtainable by any mechanism, even a
randomized one, can be achieved by a simple solution of the above kind
(individual item pricing with a discounted price for the grand bundle) and
furthermore, it can be computed in polynomial time. The problem can be solved
in polynomial time too when the number of items is constant.",individual pricing
http://arxiv.org/abs/1808.04039v1,"Mobile data demand is increasing tremendously in wireless social networks,
and thus an efficient pricing scheme for social-enabled services is urgently
needed. Though static pricing is dominant in the actual data market, price
intuitively ought to be dynamically changed to yield greater revenue. The
critical question is how to design the optimal dynamic pricing scheme, with
prospects for maximizing the expected long-term revenue. In this paper, we
study the sequential dynamic pricing scheme of a monopoly mobile network
operator in the social data market. In the market, the operator, i.e., the
seller, individually offers each mobile user, i.e., the buyer, a certain price
in multiple time periods dynamically and repeatedly. The proposed scheme
exploits the network effects in the mobile users' behaviors that boost the
social data demand. Furthermore, due to limited radio resource, the impact of
wireless network congestion is taken into account in the pricing scheme.
Thereafter, we propose a modified sequential pricing policy in order to ensure
social fairness among mobile users in terms of their individual utilities. We
analytically demonstrate that the proposed sequential dynamic pricing scheme
can help the operator gain greater revenue and mobile users achieve higher
total utilities than those of the baseline static pricing scheme. To gain more
insights, we further study a simultaneous dynamic pricing scheme in which the
operator determines the pricing strategy at the beginning of each time period.
Mobile users decide on their individual data demand in each time period
simultaneously, considering the network effects in the social domain and the
congestion effects in the network domain. We construct the social graph using
Erd\H{o}s-R\'enyi (ER) model and the real dataset based social network for
performance evaluation.",individual pricing
http://arxiv.org/abs/cs/0106028v1,"We describe a model of a communication network that allows us to price
complex network services as financial derivative contracts based on the spot
price of the capacity in individual routers. We prove a theorem of a Girsanov
transform that is useful for pricing linear derivatives on underlying assets,
which can be used to price many complex network services, and it is used to
price an option that gives access to one of several virtual channels between
two network nodes, during a specified future time interval. We give the
continuous time hedging strategy, for which the option price is independent of
the service providers attitude towards risk. The option price contains the
density function of a sum of lognormal variables, which has to be evaluated
numerically.",individual pricing
http://arxiv.org/abs/0906.4838v1,"This paper presents a model based on multilayer feedforward neural network to
forecast crude oil spot price direction in the short-term, up to three days
ahead. A great deal of attention was paid on finding the optimal ANN model
structure. In addition, several methods of data pre-processing were tested. Our
approach is to create a benchmark based on lagged value of pre-processed spot
price, then add pre-processed futures prices for 1, 2, 3,and four months to
maturity, one by one and also altogether. The results on the benchmark suggest
that a dynamic model of 13 lags is the optimal to forecast spot price direction
for the short-term. Further, the forecast accuracy of the direction of the
market was 78%, 66%, and 53% for one, two, and three days in future
conclusively. For all the experiments, that include futures data as an input,
the results show that on the short-term, futures prices do hold new information
on the spot price direction. The results obtained will generate comprehensive
understanding of the crude oil dynamic which help investors and individuals for
risk managements.",individual pricing
http://arxiv.org/abs/1701.08711v5,"In Chinese societies, superstition is of paramount importance, and vehicle
license plates with desirable numbers can fetch very high prices in auctions.
Unlike other valuable items, license plates are not allocated an estimated
price before auction. I propose that the task of predicting plate prices can be
viewed as a natural language processing (NLP) task, as the value depends on the
meaning of each individual character on the plate and its semantics. I
construct a deep recurrent neural network (RNN) to predict the prices of
vehicle license plates in Hong Kong, based on the characters on a plate. I
demonstrate the importance of having a deep network and of retraining.
Evaluated on 13 years of historical auction prices, the deep RNN's predictions
can explain over 80 percent of price variations, outperforming previous models
by a significant margin. I also demonstrate how the model can be extended to
become a search engine for plates and to provide estimates of the expected
price distribution.",individual pricing
http://arxiv.org/abs/1608.08744v1,"Crowdsourced wireless community networks can effectively alleviate the
limited coverage issue of Wi-Fi access points (APs), by encouraging individuals
(users) to share their private residential Wi-Fi APs with others. In this
paper, we provide a comprehensive economic analysis for such a crowdsourced
network, with the particular focus on the users' behavior analysis and the
community network operator's pricing design. Specifically, we formulate the
interactions between the network operator and users as a two-layer Stackelberg
model, where the operator determining the pricing scheme in Layer I, and then
users determining their Wi-Fi sharing schemes in Layer II. First, we analyze
the user behavior in Layer II via a two-stage membership selection and network
access game, for both small-scale networks and large-scale networks. Then, we
design a partial price differentiation scheme for the operator in Layer I,
which generalizes both the complete price differentiation scheme and the single
pricing scheme (i.e., no price differentiation). We show that the proposed
partial pricing scheme can achieve a good tradeoff between the revenue and the
implementation complexity. Numerical results demonstrate that when using the
partial pricing scheme with only two prices, we can increase the operator's
revenue up to 124.44% comparing with the single pricing scheme, and can achieve
an average of 80% of the maximum operator revenue under the complete price
differentiation scheme.",individual pricing
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",individual pricing
http://arxiv.org/abs/1811.07166v3,"In the isolated auction of a single item, second price often dominates first
price in properties of theoretical interest. But, single items are rarely sold
in true isolation, so considering the broader context is critical when adopting
a pricing strategy. In this paper, we study a model centrally relevant to
Internet advertising and show that when items (ad impressions) are individually
auctioned within the context of a larger system that is managing budgets,
theory offers surprising endorsement for using a first price auction to sell
each individual item. In particular, first price auctions offer theoretical
guarantees of equilibrium uniqueness, monotonicity, and other desirable
properties, as well as efficient computability as the solution to the
well-studied Eisenberg-Gale convex program. We also use simulations to
demonstrate that a bidder's incentive to deviate vanishes in thick markets.",individual pricing
http://arxiv.org/abs/1611.00123v1,"The concept of device-to-device (D2D) communications underlaying cellular
networks opens up potential benefits for improving system performance but also
brings new challenges such as interference management. In this paper, we
propose a pricing framework for interference management from the D2D users to
the cellular system, where the base station (BS) protects itself (or its
serving cellular users) by pricing the crosstier interference caused from the
D2D users. A Stackelberg game is formulated to model the interactions between
the BS and D2D users. Specifically, the BS sets prices to a maximize its
revenue (or any desired utility) subject to an interference temperature
constraint. For given prices, the D2D users competitively adapt their power
allocation strategies for individual utility maximization. We first analyze the
competition among the D2D users by noncooperative game theory and an iterative
based distributed power allocation algorithm is proposed. Then, depending on
how much network information the BS knows, we develop two optimal algorithms,
one for uniform pricing with limited network information and the other for
differentiated pricing with global network information. The uniform pricing
algorithm can be implemented by a fully distributed manner and requires minimum
information exchange between the BS and D2D users, and the differentiated
pricing algorithm is partially distributed and requires no iteration between
the BS and D2D users. Then a suboptimal differentiated pricing scheme is
proposed to reduce complexity and it can be implemented in a fully distributed
fashion. Extensive simulations are conducted to verify the proposed framework
and algorithms.",individual pricing
http://arxiv.org/abs/1208.4589v1,"A case study of the Singapore road network provides empirical evidence that
road pricing can significantly affect commuter trip timing behaviors. In this
paper, we propose a model of trip timing decisions that reasonably matches the
observed commuters' behaviors. Our model explicitly captures the difference in
individuals' sensitivity to price, travel time and early or late arrival at
destination. New pricing schemes are suggested to better spread peak travel and
reduce traffic congestion. Simulation results based on the proposed model are
provided in comparison with the real data for the Singapore case study.",individual pricing
http://arxiv.org/abs/0911.1502v1,"In this paper, we analyze a round-based pricing scheme that encourages
favorable behavior from users of real-time P2P applications like P2PTV. In the
design of pricing schemes, we consider price to be a function of usage and
capacity of download/upload streams, and quality of content served. Users are
consumers and servers at the same time in such networks, and often exhibit
behavior that is unfavorable towards maximization of social benefits.
Traditionally, network designers have overcome this difficulty by building-in
traffic latencies. However, using simulations, we show that appropriate pricing
schemes and usage terms can enable designers to limit required traffic
latencies, and be able to earn nearly 30% extra revenue from providing P2PTV
services. The service provider adjusts the prices of individual programs
incrementally within rounds, while making relatively large-scale adjustments at
the end of each round. Through simulations, we show that it is most beneficial
for the service provider to carry out 5 such rounds of price adjustments for
maximizing his average profit and minimizing the associated standard deviation
at the same time.",individual pricing
http://arxiv.org/abs/1909.00344v1,"Stock prices are driven by various factors. In particular, many individual
investors who have relatively little financial knowledge rely heavily on the
information from news stories when making investment decisions in the stock
market. However, these stories may not reflect future stock prices because of
the subjectivity in the news; stock prices may instead affect the news
contents. This study aims to discover whether it is news or stock prices that
have a greater impact on the other. To achieve this, we analyze the
relationship between news sentiment and stock prices based on time series
analysis using five different classification models. Our experimental results
show that stock prices have a bigger impact on the news contents than news does
on stock prices.",individual pricing
http://arxiv.org/abs/0905.3191v1,"We consider the Item Pricing problem for revenue maximization in the limited
supply setting, where a single seller with $n$ items caters to $m$ buyers with
unknown subadditive valuation functions who arrive in a sequence. The seller
sets the prices on individual items. Each buyer buys a subset of yet unsold
items that maximizes her utility. Our goal is to design pricing strategies that
guarantee an expected revenue that is within a small factor $\alpha$ of the
maximum possible social welfare -- an upper bound on the maximum revenue that
can be generated. Most earlier work has focused on the unlimited supply
setting, where selling items to some buyer does not affect their availability
to the future buyers. Balcan et. al. (EC 2008) studied the limited supply
setting, giving a randomized strategy that assigns a single price to all items
(uniform strategy), and never changes it (static strategy), that gives an
$2^{O(\sqrt{\log n \log \log n})}$-approximation, and moreover, no static
uniform pricing strategy can give better than $2^{\Omega(\log^{1/4} n)}$-
approximation. We improve this lower bound to $2^{\Omega(sqrt{\log n})}$.
  We consider dynamic uniform strategies, which can change the price upon the
arrival of each buyer but the price on all unsold items is the same at all
times, and static non-uniform strategies, which can assign different prices to
different items but can never change it after setting it initially. We design
such pricing strategies that give a poly-logarithmic approximation to maximum
revenue. Thus in the limited supply setting, our results highlight a strong
separation between the power of dynamic and non-uniform pricing versus static
uniform pricing. To our knowledge, this is the first non-trivial analysis of
dynamic and non-uniform pricing schemes for revenue maximization.",individual pricing
http://arxiv.org/abs/1710.01567v3,"The mining process in blockchain requires solving a proof-of-work puzzle,
which is resource expensive to implement in mobile devices due to the high
computing power and energy needed. In this paper, we, for the first time,
consider edge computing as an enabler for mobile blockchain. In particular, we
study edge computing resource management and pricing to support mobile
blockchain applications in which the mining process of miners can be offloaded
to an edge computing service provider. We formulate a two-stage Stackelberg
game to jointly maximize the profit of the edge computing service provider and
the individual utilities of the miners. In the first stage, the service
provider sets the price of edge computing nodes. In the second stage, the
miners decide on the service demand to purchase based on the observed prices.
We apply the backward induction to analyze the sub-game perfect equilibrium in
each stage for both uniform and discriminatory pricing schemes. For the uniform
pricing where the same price is applied to all miners, the existence and
uniqueness of Stackelberg equilibrium are validated by identifying the best
response strategies of the miners. For the discriminatory pricing where the
different prices are applied to different miners, the Stackelberg equilibrium
is proved to exist and be unique by capitalizing on the Variational Inequality
theory. Further, the real experimental results are employed to justify our
proposed model.",individual pricing
http://arxiv.org/abs/1711.01049v1,"As the core issue of blockchain, the mining requires solving a proof-of-work
puzzle, which is resource expensive to implement in mobile devices due to high
computing power needed. Thus, the development of blockchain in mobile
applications is restricted. In this paper, we consider the edge computing as
the network enabler for mobile blockchain. In particular, we study optimal
pricing-based edge computing resource management to support mobile blockchain
applications where the mining process can be offloaded to an Edge computing
Service Provider (ESP). We adopt a two-stage Stackelberg game to jointly
maximize the profit of the ESP and the individual utilities of different
miners. In Stage I, the ESP sets the price of edge computing services. In Stage
II, the miners decide on the service demand to purchase based on the observed
prices. We apply the backward induction to analyze the sub-game perfect
equilibrium in each stage for uniform and discriminatory pricing schemes.
Further, the existence and uniqueness of Stackelberg game are validated for
both pricing schemes. At last, the performance evaluation shows that the ESP
intends to set the maximum possible value as the optimal price for profit
maximization under uniform pricing. In addition, the discriminatory pricing
helps the ESP encourage higher total service demand from miners and achieve
greater profit correspondingly.",individual pricing
http://arxiv.org/abs/1309.7119v3,"The prediction of a stock market direction may serve as an early
recommendation system for short-term investors and as an early financial
distress warning system for long-term shareholders. Many stock prediction
studies focus on using macroeconomic indicators, such as CPI and GDP, to train
the prediction model. However, daily data of the macroeconomic indicators are
almost impossible to obtain. Thus, those methods are difficult to be employed
in practice. In this paper, we propose a method that directly uses prices data
to predict market index direction and stock price direction. An extensive
empirical study of the proposed method is presented on the Korean Composite
Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual
constituents included in the indices. The experimental results show notably
high hit ratios in predicting the movements of the individual constituents in
the KOSPI and HIS.",individual pricing
http://arxiv.org/abs/1801.04015v3,"Ridesharing platforms match drivers and riders to trips, using dynamic prices
to balance supply and demand. A challenge is to set prices that are
appropriately smooth in space and time, so that drivers will choose to accept
their dispatched trips, rather than drive to another area or wait for higher
prices or a better trip. We work in a complete information, discrete time,
multi-period, multi-location model, and introduce the Spatio-Temporal Pricing
(STP) mechanism. The mechanism is incentive-aligned, in that it is a
subgame-perfect equilibrium for drivers to accept their dispatches. The
mechanism is also welfare-optimal, envy-free, individually rational, budget
balanced and core-selecting from any history onward. The proof of incentive
alignment makes use of the $M^\natural$ concavity of min-cost flow objectives.
We also give an impossibility result, that there can be no dominant-strategy
mechanism with the same economic properties. An empirical analysis conducted in
simulation suggests that the STP mechanism can achieve significantly higher
social welfare than a myopic pricing mechanism.",individual pricing
http://arxiv.org/abs/1811.09721v1,"Serverless computing has recently experienced significant adoption by several
applications, especially Internet of Things (IoT) applications. In serverless
computing, rather than deploying and managing dedicated virtual machines, users
are able to deploy individual functions, and pay only for the time that their
code is actually executing. However, since serverless platforms are relatively
new, they have a completely different pricing model that depends on the memory,
duration, and the number of executions of a sequence/workflow of functions. In
this paper we present an algorithm that optimizes the price of serverless
applications in AWS Lambda. We first describe the factors affecting price of
serverless applications which include: (1) fusing a sequence of functions, (2)
splitting functions across edge and cloud resources, and (3) allocating the
memory for each function. We then present an efficient algorithm to explore
different function fusion-placement solutions and find the solution that
optimizes the application's price while keeping the latency under a certain
threshold. Our results on image processing workflows show that the algorithm
can find solutions optimizing the price by more than 35%-57% with only 5%-15%
increase in latency. We also show that our algorithm can find non-trivial
memory configurations that reduce both latency and price.",individual pricing
http://arxiv.org/abs/1208.4167v1,"As the complexity of enterprise systems increases, the need for monitoring
and analyzing such systems also grows. A number of companies have built
sophisticated monitoring tools that go far beyond simple resource utilization
reports. For example, based on instrumentation and specialized APIs, it is now
possible to monitor single method invocations and trace individual transactions
across geographically distributed systems. This high-level of detail enables
more precise forms of analysis and prediction but comes at the price of high
data rates (i.e., big data). To maximize the benefit of data monitoring, the
data has to be stored for an extended period of time for ulterior analysis.
This new wave of big data analytics imposes new challenges especially for the
application performance monitoring systems. The monitoring data has to be
stored in a system that can sustain the high data rates and at the same time
enable an up-to-date view of the underlying infrastructure. With the advent of
modern key-value stores, a variety of data storage systems have emerged that
are built with a focus on scalability and high data rates as predominant in
this monitoring use case. In this work, we present our experience and a
comprehensive performance evaluation of six modern (open-source) data stores in
the context of application performance monitoring as part of CA Technologies
initiative. We evaluated these systems with data and workloads that can be
found in application performance monitoring, as well as, on-line advertisement,
power monitoring, and many other use cases. We present our insights not only as
performance results but also as lessons learned and our experience relating to
the setup and configuration complexity of these data stores in an industry
setting.",monitoring individual pricing
http://arxiv.org/abs/1008.0147v1,"We consider a multi-user network where a network manager and selfish users
interact. The network manager monitors the behavior of users and intervenes in
the interaction among users if necessary, while users make decisions
independently to optimize their individual objectives. In this paper, we
develop a framework of intervention mechanism design, which is aimed to
optimize the objective of the manager, or the network performance, taking the
incentives of selfish users into account. Our framework is general enough to
cover a wide range of application scenarios, and it has advantages over
existing approaches such as Stackelberg strategies and pricing. To design an
intervention mechanism and to predict the resulting operating point, we
formulate a new class of games called intervention games and a new solution
concept called intervention equilibrium. We provide analytic results about
intervention equilibrium and optimal intervention mechanisms in the case of a
benevolent manager with perfect monitoring. We illustrate these results with a
random access model. Our illustrative example suggests that intervention
requires less knowledge about users than pricing.",monitoring individual pricing
http://arxiv.org/abs/1610.01684v1,"Indirect reciprocity based on reputation is a leading mechanism driving human
cooperation, where monitoring of behaviour and sharing reputation-related
information are crucial. Because collecting information is costly, a tragedy of
the commons can arise, with some individuals free-riding on information
supplied by others. This can be overcome by organising monitors that aggregate
information, supported by fees from their information users. We analyse a
co-evolutionary model of individuals playing a social dilemma game and monitors
watching them; monitors provide information and players vote for a more
beneficial monitor. We find that (1) monitors that simply rate defection badly
cannot stabilise cooperation---they have to overlook defection against
ill-reputed players; (2) such overlooking monitors can stabilise cooperation if
players vote for monitors rather than to change their own strategy; (3) STERN
monitors, who rate cooperation with ill-reputed players badly, stabilise
cooperation more easily than MILD monitors, who do not do so; (4) a STERN
monitor wins if it competes with a MILD monitor; and (5) STERN monitors require
a high level of surveillance and achieve only lower levels of cooperation,
whereas MILD monitors achieve higher levels of cooperation with loose and thus
lower cost monitoring.",monitoring individual pricing
http://arxiv.org/abs/1206.6487v1,"We present a new anytime algorithm that achieves near-optimal regret for any
instance of finite stochastic partial monitoring. In particular, the new
algorithm achieves the minimax regret, within logarithmic factors, for both
""easy"" and ""hard"" problems. For easy problems, it additionally achieves
logarithmic individual regret. Most importantly, the algorithm is adaptive in
the sense that if the opponent strategy is in an ""easy region"" of the strategy
space then the regret grows as if the problem was easy. As an implication, we
show that under some reasonable additional assumptions, the algorithm enjoys an
O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.
(2011).",monitoring individual pricing
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",monitoring individual pricing
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",monitoring individual pricing
http://arxiv.org/abs/1105.3228v1,"Financial economic models often assume that investors know (or agree on) the
fundamental value of the shares of the firm, easing the passage from the
individual to the collective dimension of the financial system generated by the
Share Exchange over time. Our model relaxes that heroic assumption of one
unique ""true value"" and deals with the formation of share market prices through
the dynamic formation of individual and social opinions (or beliefs) based upon
a fundamental signal of economic performance and position of the firm, the
forecast revision by heterogeneous individual investors, and their social mood
or sentiment about the ongoing state of the market pricing process. Market
clearing price formation is then featured by individual and group dynamics that
make its collective dimension irreducible to its individual level. This dynamic
holistic approach can be applied to better understand the market exuberance
generated by the Share Exchange over time.",monitoring individual pricing
http://arxiv.org/abs/1106.0235v1,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",monitoring individual pricing
http://arxiv.org/abs/1809.03110v1,"Cloud spot markets rent VMs for a variable price that is typically much lower
than the price of on-demand VMs, which makes them attractive for a wide range
of large-scale applications. However, applications that run on spot VMs suffer
from cost uncertainty, since spot prices fluctuate, in part, based on supply,
demand, or both. The difficulty in predicting spot prices affects users and
applications: the former cannot effectively plan their IT expenditures, while
the latter cannot infer the availability and performance of spot VMs, which are
a function of their variable price. To address the problem, we use properties
of cloud infrastructure and workloads to show that prices become more stable
and predictable as they are aggregated together. We leverage this observation
to define an aggregate index price for spot VMs that serves as a reference for
what users should expect to pay. We show that, even when the spot prices for
individual VMs are volatile, the index price remains stable and predictable. We
then introduce cloud index tracking: a migration policy that tracks the index
price to ensure applications running on spot VMs incur a predictable cost by
migrating to a new spot VM if the current VM's price significantly deviates
from the index price.",monitoring individual pricing
http://arxiv.org/abs/1810.01851v1,"In Advanced Metering Infrastructure (AMI) networks, smart meters should send
fine-grained power consumption readings to electric utilities to perform
real-time monitoring and energy management. However, these readings can leak
sensitive information about consumers' activities. Various privacy-preserving
schemes for collecting fine-grained readings have been proposed for AMI
networks. These schemes aggregate individual readings and send an aggregated
reading to the utility, but they extensively use asymmetric-key cryptography
which involves large computation/communication overhead. Furthermore, they do
not address End-to-End (E2E) data integrity, authenticity, and computing
electricity bills based on dynamic prices. In this paper, we propose EPIC, an
efficient and privacy-preserving data collection scheme with E2E data integrity
verification for AMI networks. Using efficient cryptographic operations, each
meter should send a masked reading to the utility such that all the masks are
canceled after aggregating all meters' masked readings, and thus the utility
can only obtain an aggregated reading to preserve consumers' privacy. The
utility can verify the aggregated reading integrity without accessing the
individual readings to preserve privacy. It can also identify the attackers and
compute electricity bills efficiently by using the fine-grained readings
without violating privacy. Furthermore, EPIC can resist collusion attacks in
which the utility colludes with a relay node to extract the meters' readings. A
formal proof, probabilistic analysis are used to evaluate the security of EPIC,
and ns-3 is used to implement EPIC and evaluate the network performance. In
addition, we compare EPIC to existing data collection schemes in terms of
overhead and security/privacy features.",monitoring individual pricing
http://arxiv.org/abs/1702.07032v2,"We show that the Revenue-Optimal Deterministic Mechanism Design problem for a
single additive buyer is #P-hard, even when the distributions have support size
2 for each item and, more importantly, even when the optimal solution is
guaranteed to be of a very simple kind: the seller picks a price for each
individual item and a price for the grand bundle of all the items; the buyer
can purchase either the grand bundle at its given price or any subset of items
at their total individual prices. The following problems are also #P-hard, as
immediate corollaries of the proof:
  1. determining if individual item pricing is optimal for a given instance,
  2. determining if grand bundle pricing is optimal, and
  3. computing the optimal (deterministic) revenue.
  On the positive side, we show that when the distributions are i.i.d. with
support size 2, the optimal revenue obtainable by any mechanism, even a
randomized one, can be achieved by a simple solution of the above kind
(individual item pricing with a discounted price for the grand bundle) and
furthermore, it can be computed in polynomial time. The problem can be solved
in polynomial time too when the number of items is constant.",monitoring individual pricing
http://arxiv.org/abs/1808.04039v1,"Mobile data demand is increasing tremendously in wireless social networks,
and thus an efficient pricing scheme for social-enabled services is urgently
needed. Though static pricing is dominant in the actual data market, price
intuitively ought to be dynamically changed to yield greater revenue. The
critical question is how to design the optimal dynamic pricing scheme, with
prospects for maximizing the expected long-term revenue. In this paper, we
study the sequential dynamic pricing scheme of a monopoly mobile network
operator in the social data market. In the market, the operator, i.e., the
seller, individually offers each mobile user, i.e., the buyer, a certain price
in multiple time periods dynamically and repeatedly. The proposed scheme
exploits the network effects in the mobile users' behaviors that boost the
social data demand. Furthermore, due to limited radio resource, the impact of
wireless network congestion is taken into account in the pricing scheme.
Thereafter, we propose a modified sequential pricing policy in order to ensure
social fairness among mobile users in terms of their individual utilities. We
analytically demonstrate that the proposed sequential dynamic pricing scheme
can help the operator gain greater revenue and mobile users achieve higher
total utilities than those of the baseline static pricing scheme. To gain more
insights, we further study a simultaneous dynamic pricing scheme in which the
operator determines the pricing strategy at the beginning of each time period.
Mobile users decide on their individual data demand in each time period
simultaneously, considering the network effects in the social domain and the
congestion effects in the network domain. We construct the social graph using
Erd\H{o}s-R\'enyi (ER) model and the real dataset based social network for
performance evaluation.",monitoring individual pricing
http://arxiv.org/abs/1606.08410v1,"The electricity grid is crucial to our lives. House- holds and institutions
count on it. In recent years, the sources of energy have become less and less
available and they are driving the price of electricity higher and higher. It
has been estimated that 40% of power is spent in residential and institutional
buildings. Most of this power is absorbed by space cooling and heating. In
modern buildings, the HVAC (heating, ventilation, and air conditioning) system
is centralised and operated by a department usually called the central plant.
The central plant produces chilled water and steam that is then consumed by the
building AHUs (Air Handling Units) to maintain the buildings at a comfortable
temperature. However, the heating and cooling model does not take into account
human occupancy. The AHU within the building distributes air according to the
design parameters of the building ignoring the occupancy. As a matter of fact,
there is a potential for optimization lowering consumption to utilize energy
efficiently and also to be able to adapt to the changing cost of energy in a
micro-grid environment. This system makes it possible to reduce the consumption
when needed minimizing impact on the consumer. In this study, we will show,
through a set of studies conducted at the University of Houston, that there is
a potential for energy conservation and efficiency in both the buildings and
the central plant. We also present a strategy that can be undertaken to meet
this goal. This strategy, airflow monitoring and control, is tested in a
software simulation and the results are presented. This system enables the user
to control and monitor the temperature in the individual rooms according the
locals needs.",monitoring individual pricing
http://arxiv.org/abs/cs/0106028v1,"We describe a model of a communication network that allows us to price
complex network services as financial derivative contracts based on the spot
price of the capacity in individual routers. We prove a theorem of a Girsanov
transform that is useful for pricing linear derivatives on underlying assets,
which can be used to price many complex network services, and it is used to
price an option that gives access to one of several virtual channels between
two network nodes, during a specified future time interval. We give the
continuous time hedging strategy, for which the option price is independent of
the service providers attitude towards risk. The option price contains the
density function of a sum of lognormal variables, which has to be evaluated
numerically.",monitoring individual pricing
http://arxiv.org/abs/0906.4838v1,"This paper presents a model based on multilayer feedforward neural network to
forecast crude oil spot price direction in the short-term, up to three days
ahead. A great deal of attention was paid on finding the optimal ANN model
structure. In addition, several methods of data pre-processing were tested. Our
approach is to create a benchmark based on lagged value of pre-processed spot
price, then add pre-processed futures prices for 1, 2, 3,and four months to
maturity, one by one and also altogether. The results on the benchmark suggest
that a dynamic model of 13 lags is the optimal to forecast spot price direction
for the short-term. Further, the forecast accuracy of the direction of the
market was 78%, 66%, and 53% for one, two, and three days in future
conclusively. For all the experiments, that include futures data as an input,
the results show that on the short-term, futures prices do hold new information
on the spot price direction. The results obtained will generate comprehensive
understanding of the crude oil dynamic which help investors and individuals for
risk managements.",monitoring individual pricing
http://arxiv.org/abs/1701.08711v5,"In Chinese societies, superstition is of paramount importance, and vehicle
license plates with desirable numbers can fetch very high prices in auctions.
Unlike other valuable items, license plates are not allocated an estimated
price before auction. I propose that the task of predicting plate prices can be
viewed as a natural language processing (NLP) task, as the value depends on the
meaning of each individual character on the plate and its semantics. I
construct a deep recurrent neural network (RNN) to predict the prices of
vehicle license plates in Hong Kong, based on the characters on a plate. I
demonstrate the importance of having a deep network and of retraining.
Evaluated on 13 years of historical auction prices, the deep RNN's predictions
can explain over 80 percent of price variations, outperforming previous models
by a significant margin. I also demonstrate how the model can be extended to
become a search engine for plates and to provide estimates of the expected
price distribution.",monitoring individual pricing
http://arxiv.org/abs/1608.08744v1,"Crowdsourced wireless community networks can effectively alleviate the
limited coverage issue of Wi-Fi access points (APs), by encouraging individuals
(users) to share their private residential Wi-Fi APs with others. In this
paper, we provide a comprehensive economic analysis for such a crowdsourced
network, with the particular focus on the users' behavior analysis and the
community network operator's pricing design. Specifically, we formulate the
interactions between the network operator and users as a two-layer Stackelberg
model, where the operator determining the pricing scheme in Layer I, and then
users determining their Wi-Fi sharing schemes in Layer II. First, we analyze
the user behavior in Layer II via a two-stage membership selection and network
access game, for both small-scale networks and large-scale networks. Then, we
design a partial price differentiation scheme for the operator in Layer I,
which generalizes both the complete price differentiation scheme and the single
pricing scheme (i.e., no price differentiation). We show that the proposed
partial pricing scheme can achieve a good tradeoff between the revenue and the
implementation complexity. Numerical results demonstrate that when using the
partial pricing scheme with only two prices, we can increase the operator's
revenue up to 124.44% comparing with the single pricing scheme, and can achieve
an average of 80% of the maximum operator revenue under the complete price
differentiation scheme.",monitoring individual pricing
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",monitoring individual pricing
http://arxiv.org/abs/1811.07166v3,"In the isolated auction of a single item, second price often dominates first
price in properties of theoretical interest. But, single items are rarely sold
in true isolation, so considering the broader context is critical when adopting
a pricing strategy. In this paper, we study a model centrally relevant to
Internet advertising and show that when items (ad impressions) are individually
auctioned within the context of a larger system that is managing budgets,
theory offers surprising endorsement for using a first price auction to sell
each individual item. In particular, first price auctions offer theoretical
guarantees of equilibrium uniqueness, monotonicity, and other desirable
properties, as well as efficient computability as the solution to the
well-studied Eisenberg-Gale convex program. We also use simulations to
demonstrate that a bidder's incentive to deviate vanishes in thick markets.",monitoring individual pricing
http://arxiv.org/abs/1611.00123v1,"The concept of device-to-device (D2D) communications underlaying cellular
networks opens up potential benefits for improving system performance but also
brings new challenges such as interference management. In this paper, we
propose a pricing framework for interference management from the D2D users to
the cellular system, where the base station (BS) protects itself (or its
serving cellular users) by pricing the crosstier interference caused from the
D2D users. A Stackelberg game is formulated to model the interactions between
the BS and D2D users. Specifically, the BS sets prices to a maximize its
revenue (or any desired utility) subject to an interference temperature
constraint. For given prices, the D2D users competitively adapt their power
allocation strategies for individual utility maximization. We first analyze the
competition among the D2D users by noncooperative game theory and an iterative
based distributed power allocation algorithm is proposed. Then, depending on
how much network information the BS knows, we develop two optimal algorithms,
one for uniform pricing with limited network information and the other for
differentiated pricing with global network information. The uniform pricing
algorithm can be implemented by a fully distributed manner and requires minimum
information exchange between the BS and D2D users, and the differentiated
pricing algorithm is partially distributed and requires no iteration between
the BS and D2D users. Then a suboptimal differentiated pricing scheme is
proposed to reduce complexity and it can be implemented in a fully distributed
fashion. Extensive simulations are conducted to verify the proposed framework
and algorithms.",monitoring individual pricing
http://arxiv.org/abs/1210.6365v1,"This paper establishes a connection between the notion of observation (or
monitoring) structure in game theory and the one of communication channels in
Shannon theory. One of the objectives is to know under which conditions an
arbitrary monitoring structure can be transformed into a more pertinent
monitoring structure. To this end, a mediator is added to the game. The
objective of the mediator is to choose a signalling scheme that allows the
players to have perfect, almost perfect or public monitoring and all of this,
at a minimum cost in terms of signalling. Graph coloring, source coding, and
channel coding are exploited to deal with these issues. A wireless power
control game is used to illustrate these notions but the applicability of the
provided results and, more importantly, the framework of transforming
monitoring structures go much beyond this example.",monitoring individual pricing
http://arxiv.org/abs/1909.13426v1,"Negotiation is a complex activity involving strategic reasoning, persuasion,
and psychology. An average person is often far from an expert in negotiation.
Our goal is to assist humans to become better negotiators through a
machine-in-the-loop approach that combines machine's advantage at data-driven
decision-making and human's language generation ability. We consider a
bargaining scenario where a seller and a buyer negotiate the price of an item
for sale through a text-based dialog. Our negotiation coach monitors messages
between them and recommends tactics in real time to the seller to get a better
deal (e.g., ""reject the proposal and propose a price"", ""talk about your
personal experience with the product""). The best strategy and tactics largely
depend on the context (e.g., the current price, the buyer's attitude).
Therefore, we first identify a set of negotiation tactics, then learn to
predict the best strategy and tactics in a given dialog context from a set of
human-human bargaining dialogs. Evaluation on human-human dialogs shows that
our coach increases the profits of the seller by almost 60%.",monitoring personal pricing
http://arxiv.org/abs/1804.03178v1,"In practical crowdsourcing systems such as Amazon Mechanical Turk, posted
pricing is widely used due to its simplicity, where a task requester publishes
a pricing rule a priori, on which workers decide whether to accept and perform
the task or not, and are often paid according to the quality of their effort.
One of the key ingredients of a good posted pricing lies in how to recruit more
high-quality workers with less budget, for which the following two schemes are
considered: (i) personalized pricing by profiling users in terms of their
quality and cost, and (ii) additional bonus payment offered for more qualified
task completion. Despite their potential benefits in crowdsourced pricing, it
has been under-explored how much gain each or both of personalization and bonus
payment actually provides to the requester. In this paper, we study four
possible combinations of posted pricing made by pricing with/without
personalization and bonus. We aim at analytically quantifying when and how much
such two ideas contribute to the requester's utility. To this end, we first
derive the optimal personalized and common pricing schemes and analyze their
computational tractability. Next, we quantify the gap in the utility between
with and without bonus payment in both pricing schemes. We analytically prove
that the impact of bonus is negligible significantly marginal in personalized
pricing, whereas crucial in common pricing. Finally, we study the notion of
Price of Agnosticity that quantifies the utility gap between personalized and
common pricing policies. This implies that a complex personalized pricing with
privacy concerns can be replaced by a simple common pricing with bonus. We
validate our analytical findings through extensive simulations and real
experiments done in Amazon Mechanical Turk, and provide additional implications
that are useful in designing a pricing policy.",monitoring personal pricing
http://arxiv.org/abs/1907.11768v2,"Range uncertainties in proton therapy hamper treatment precision. Prompt
gamma-rays were suggested 16 years ago for real-time range verification, and
have already shown promising results in clinical studies with collimated
cameras. Simultaneously, alternative imaging concepts without collimation are
investigated to reduce the footprint and price of current prototypes. In this
manuscript, a compact range verification method is presented. It monitors
prompt gamma-rays with a single scintillation detector positioned coaxially to
the beam and behind the patient. Thanks to the solid angle effect, proton range
deviations can be derived from changes in the number of gamma-rays detected per
proton, provided that the number of incident protons is well known. A
theoretical background is formulated and the requirements for a future
proof-of-principle experiment are identified. The potential benefits and
disadvantages of the method are discussed, and the prospects and potential
obstacles for its use during patient treatments are assessed. The final
milestone is to monitor proton range differences in clinical cases with a
statistical precision of 1 mm, a material cost of 25000 USD and a weight below
10 kg. This technique could facilitate the widespread application of in vivo
range verification in proton therapy and eventually the improvement of
treatment quality.",monitoring personal pricing
http://arxiv.org/abs/1705.02982v2,"A personal data market is a platform including three participants: data
owners (individuals), data buyers and market maker. Data owners who provide
personal data are compensated according to their privacy loss. Data buyers can
submit a query and pay for the result according to their desired accuracy.
Market maker coordinates between data owner and buyer. This framework has been
previously studied based on differential privacy. However, the previous study
assumes data owners can accept any level of privacy loss and data buyers can
conduct the transaction without regard to the financial budget. In this paper,
we propose a practical personal data trading framework that is able to strike a
balance between money and privacy. In order to gain insights on user
preferences, we first conducted an online survey on human attitude to- ward
privacy and interest in personal data trading. Second, we identify the 5 key
principles of personal data market, which is important for designing a
reasonable trading frame- work and pricing mechanism. Third, we propose a
reason- able trading framework for personal data which provides an overview of
how the data is traded. Fourth, we propose a balanced pricing mechanism which
computes the query price for data buyers and compensation for data owners
(whose data are utilized) as a function of their privacy loss. The main goal is
to ensure a fair trading for both parties. Finally, we will conduct an
experiment to evaluate the output of our proposed pricing mechanism in
comparison with other previously proposed mechanism.",monitoring personal pricing
http://arxiv.org/abs/1709.04767v1,"In this paper we examine information privacy from a value-centered angle. We
review and compare different personal data valuation approaches. We consider
the value of personal data now and in the near future, and we find that the
enduring part of personal data will soon become common knowledge and that their
price and value will drop substantially. Therefore the sector that is based on
personal data extraction will need to focus on new ways of monetization and on
the of the dynamic part of personal information.",monitoring personal pricing
http://arxiv.org/abs/1604.04157v1,"In \cite{EK10} the use of VCG in matching markets is motivated by saying that
in order to compute market clearing prices in a matching market, the auctioneer
needs to know the true valuations of the bidders. Hence VCG and corresponding
personalized prices are proposed as an incentive compatible mechanism. The same
line of argument pops up in several lecture sheets and other documents related
to courses based on Easley and Kleinberg's book, seeming to suggest that
computing market clearing prices and corresponding assignments were \emph{not}
incentive compatible. Main purpose of our note is to observe that, in contrast,
assignments based on buyer optimal market clearing prices are indeed incentive
compatible.",monitoring personal pricing
http://arxiv.org/abs/1905.01526v2,"We study the problem of computing personalized reserve prices in eager second
price auctions without having any assumption on valuation distributions. Here,
the input is a dataset that contains the submitted bids of $n$ buyers in a set
of auctions and the goal is to return personalized reserve prices $\textbf r$
that maximize the revenue earned on these auctions by running eager second
price auctions with reserve $\textbf r$. We present a novel LP formulation to
this problem and a rounding procedure which achieves a
$(1+2(\sqrt{2}-1)e^{\sqrt{2}-2})^{-1} \approx 0.684$-approximation. This
improves over the $\frac{1}{2}$-approximation Algorithm due to Roughgarden and
Wang. We show that our analysis is tight for this rounding procedure. We also
bound the integrality gap of the LP, which bounds the performance of any
algorithm based on this LP.",monitoring personal pricing
http://arxiv.org/abs/cs/0109080v1,"Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.",monitoring personal pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",monitoring personal pricing
http://arxiv.org/abs/1702.02046v1,"Breathing signal monitoring can provide important clues for human's physical
health problems. Comparing to existing techniques that require wearable devices
and special equipment, a more desirable approach is to provide contact-free and
long-term breathing rate monitoring by exploiting wireless signals. In this
paper, we propose TensorBeat, a system to employ channel state information
(CSI) phase difference data to intelligently estimate breathing rates for
multiple persons with commodity WiFi devices. The main idea is to leverage the
tensor decomposition technique to handle the CSI phase difference data. The
proposed TensorBeat scheme first obtains CSI phase difference data between
pairs of antennas at the WiFi receiver to create CSI tensor data. Then
Canonical Polyadic (CP) decomposition is applied to obtain the desired
breathing signals. A stable signal matching algorithm is developed to find the
decomposed signal pairs, and a peak detection method is applied to estimate the
breathing rates for multiple persons. Our experimental study shows that
TensorBeat can achieve high accuracy under different environments for
multi-person breathing rate monitoring.",monitoring personal pricing
http://arxiv.org/abs/1507.02750v2,"Partial monitoring is a generic framework for sequential decision-making with
incomplete feedback. It encompasses a wide class of problems such as dueling
bandits, learning with expect advice, dynamic pricing, dark pools, and label
efficient prediction. We study the utility-based dueling bandit problem as an
instance of partial monitoring problem and prove that it fits the time-regret
partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We
survey some partial monitoring algorithms and see how they could be used to
solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,
Partial Monitoring, Partial Feedback, Multiarmed Bandits",monitoring personal pricing
http://arxiv.org/abs/1011.3852v1,"This paper describes a mobile health monitoring system called iCare for the
elderly. We use wireless body sensors and smart phones to monitor the wellbeing
of the elderly. It can offer remote monitoring for the elderly anytime anywhere
and provide tailored services for each person based on their personal health
condition. When detecting an emergency, the smart phone will automatically
alert pre-assigned people who could be the old people's family and friends, and
call the ambulance of the emergency centre. It also acts as the personal health
information system and the medical guidance which offers one communication
platform and the medical knowledge database so that the family and friends of
the served people can cooperate with doctors to take care of him/her. The
system also features some unique functions that cater to the living demands of
the elderly, including regular reminder, quick alarm, medical guidance, etc.
iCare is not only a real-time health monitoring system for the elderly, but
also a living assistant which can make their lives more convenient and
comfortable.",monitoring personal pricing
http://arxiv.org/abs/physics/0611130v1,"The importance of the power law has been well realized in econophysics over
the last decade. For instance, the distribution of the rate of stock price
variation and of personal assets show the power law. While these results reveal
the striking scale invariance of financial markets, the behaviour of price in
real economy is less known in spite of its extreme importance. As an example of
markets in real economy, here we take up the price of precious stones which
increases with size while the amount of their production rapidly decreases with
size. We show for the first time that the price of natural precious stones
(quartz crystal ball, gemstones such as diamond, emerald, and sapphire) as a
function of weight obeys the power law. This indicates that the price is
determined by the same evaluation measure for different sizes. Our results
demonstrate that not only the distribution of an economical observable but also
the price itself obeys the power law. We anticipate our findings to be a
starting point for the quantitative study of scale invariance in real economy.
While the Black--Sholes model provided the framework for optimal pricing in
financial markets, our method of analysis prvides a new framework that
characterizes the market in real economy.",monitoring personal pricing
http://arxiv.org/abs/1407.0566v2,"In the context of a myriad of mobile apps which collect personally
identifiable information (PII) and a prospective market place of personal data,
we investigate a user-centric monetary valuation of mobile PII. During a 6-week
long user study in a living lab deployment with 60 participants, we collected
their daily valuations of 4 categories of mobile PII (communication, e.g.
phonecalls made/received, applications, e.g. time spent on different apps,
location and media, photos taken) at three levels of complexity (individual
data points, aggregated statistics and processed, i.e. meaningful
interpretations of the data). In order to obtain honest valuations, we employ a
reverse second price auction mechanism. Our findings show that the most
sensitive and valued category of personal information is location. We report
statistically significant associations between actual mobile usage, personal
dispositions, and bidding behavior. Finally, we outline key implications for
the design of mobile services and future markets of personal data.",monitoring personal pricing
http://arxiv.org/abs/1906.05457v2,"As personal data have been the new oil of the digital era, there is a growing
trend perceiving personal data as a commodity. Although some people are willing
to trade their personal data for money, they might still expect limited
individual privacy loss, and the maximum tolerable privacy loss varies with
each individual. In this paper, we propose a framework that enables individuals
to trade their location data streams under personalized privacy loss, which can
be bounded in w successive time points. However, the introduction of such
personalized bounds of individual privacy loss over streaming data raises
several technical challenges in the aspects of budget allocation, utility
estimation of personalized differentially private mechanism, and arbitrage-free
pricing. To deal with those challenges, we modularize three key modules in our
framework and propose arbitrage-free trading mechanisms by combining instances
of the modules. Finally, our experiments verify the effectiveness of the
proposed mechanisms.",monitoring personal pricing
http://arxiv.org/abs/1701.07484v1,"Our machines, products, utilities, and environments have long been monitored
by embedded software systems. Our professional, commercial, social and personal
lives are also subject to monitoring as they are mediated by software systems.
Data on nearly everything now exists, waiting to be collected and analysed for
all sorts of reasons. Given the rising tide of data we pose the questions: What
is monitoring? Do diverse and disparate monitoring systems have anything in
common? We attempt answer these questions by proposing an abstract conceptual
framework for studying monitoring. We argue that it captures a structure common
to many different monitoring practices, and that from it detailed formal models
can be derived, customised to applications. The framework formalises the idea
that monitoring is a process that observes the behaviour of people and objects
in a context. The entities and their behaviours are represented by abstract
data types and the observable attributes by logics. Since monitoring usually
has a specific purpose, we extend the framework with protocols for detecting
attributes or events that require interventions and, possibly, a change in
behaviour. Our theory is illustrated by a case study from criminal justice,
that of electronic tagging.",monitoring personal pricing
http://arxiv.org/abs/1605.03035v1,"For improving e-health services, we propose a context-aware framework to
monitor the activities of daily living of dependent persons. We define a
strategy for generating long-term realistic scenarios and a framework
containing an adaptive monitoring algorithm based on three approaches for
optimizing resource usage. The used approaches provide a deep knowledge about
the person's context by considering: the person's profile, the activities and
the relationships between activities. We evaluate the performances of our
framework and show its adaptability and significant reduction in network,
energy and processing usage over a traditional monitoring implementation.",monitoring personal pricing
http://arxiv.org/abs/1811.10073v1,"Objective: Asthma is a chronic pulmonary disease with multiple triggers
manifesting as symptoms with various intensities. This paper evaluates the
suitability of long-term monitoring of pediatric asthma using diverse data to
qualify and quantify triggers that contribute to the asthma symptoms and
control to enable a personalized management plan.
  Materials and Methods: Asthma condition, environment, and adherence to the
prescribed care plan were continuously tracked for 97 pediatric patients using
kHealth-Asthma technology for one or three months.
  Result: At the cohort level, among 21% of the patients deployed in spring,
63% and 19% indicated pollen and Particulate Matter (PM2.5), respectively, as
the major asthma contributors. Of the 18% of the patients deployed in fall, 29%
and 21% found pollen and PM2.5 respectively, to be the contributors. For the
28% of the patients deployed in winter, PM2.5 was identified as the major
contributor for 80% of them. One patient across each season has been chosen to
explain the determination of personalized triggers by observing correlations
between triggers and asthma symptoms gathered from anecdotal evidence.
  Discussion and Conclusion: Both public and personal health signals including
compliance to prescribed care plan have been captured through continuous
monitoring using the kHealth-Asthma technology which generated insights on
causes of asthma symptoms across different seasons. Collectively, they can form
the underlying basis for personalized management plan and intervention.
  KEYWORDS: Personalized Digital Health, Medical Internet of Things, Pediatric
Asthma Management, Patient Generated Health Data, Personalized Triggers,
Telehealth,",monitoring personal pricing
http://arxiv.org/abs/1910.01770v1,"Because stress is subjective and is expressed differently from one person to
another, generic stress prediction models (i.e., models that predict the stress
of any person) perform crudely. Only person-specific ones (i.e., models that
predict the stress of a preordained person) yield reliable predictions, but
they are not adaptable and costly to deploy in real-world environments. For
illustration, in an office environment, a stress monitoring system that uses
person-specific models would require collecting new data and training a new
model for every employee. Moreover, once deployed, the models would deteriorate
and need expensive periodic upgrades because stress is dynamic and depends on
unforeseeable factors. We propose a simple, yet practical and cost effective
calibration technique that derives an accurate and personalized stress
prediction model from physiological samples collected from a large population.
We validate our approach on two stress datasets. The results show that our
technique performs much better than a generic model. For instance, a generic
model achieved only a 42.5% accuracy. However, with only 100 calibration
samples, we raised its accuracy to 95.2% We also propose a blueprint for a
stress monitoring system based on our strategy, and we debate its merits and
limitation. Finally, we made public our source code and the relevant datasets
to allow other researchers to replicate our findings.",monitoring personal pricing
http://arxiv.org/abs/1906.11657v1,"What fraction of the single item $n$ buyers setting's expected optimal
revenue MyeRev can the second price auction with reserves achieve? In the
special case where the buyers' valuation distributions are all drawn i.i.d. and
the distributions satisfy the regularity condition, the second price auction
with an anonymous reserve (ASP) is the optimal auction itself. As the setting
gets more complex, there are established upper bounds on the fraction of MyeRev
that ASP can achieve. On the contrary, no such upper bounds are known for the
fraction of MyeRev achievable by the second price auction with eager
personalized reserves (ESP). In particular, no separation was earlier known
between ESP's revenue and MyeRev even in the most general setting of
non-identical product distributions that don't satisfy the regularity
condition. In this paper we establish the first separation results for ESP: we
show that even in the case of distributions drawn i.i.d., but not necessarily
satisfying the regularity condition, the ESP cannot achieve more than a $0.778$
fraction of MyeRev in general. Combined with Correa et al.'s result (EC 2017)
that ESP can achieve at least a $0.745$ fraction of MyeRev, this nearly bridges
the gap between upper and lower bounds on ESP's approximation factor.",monitoring personal pricing
http://arxiv.org/abs/physics/0604179v1,"Involving effects of media, opinion leader and other agents on the opinion of
individuals of market society, a trader based model is developed and utilized
to simulate price via supply and demand. Pronounced effects are considered with
several weights and some personal differences between traders are taken into
account. Resulting time series and probabilty distribution function involving a
power law for price come out similar to the real ones.",monitoring personal pricing
http://arxiv.org/abs/1712.06236v2,"A smartphone user's personal hotspot (pH) allows him to share cellular
connection to another (e.g., a traveler) in the vicinity, but such sharing
consumes the limited data quota in his two-part tariff plan and may lead to
overage charge. This paper studies how to motivate such pH-enabled data-plan
sharing between local users and travelers in the ever-growing roaming markets,
and proposes pricing incentive for a data-plan buyer to reward surrounding pH
sellers (if any). The pricing scheme practically takes into account the
information uncertainty at the traveler side, including the random mobility and
the sharing cost distribution of selfish local users who potentially share pHs.
Though the pricing optimization problem is non-convex, we show that there
always exists a unique optimal price to tradeoff between the successful sharing
opportunity and the sharing price. We further generalize the optimal pricing to
the case of heterogeneous selling pHs who have diverse data usage behaviors in
the sharing cost distributions, and we show such diversity may or may not
benefit the traveler. Lacking selfish pHs' information, the traveler's expected
cost is higher than that under the complete information, but the gap diminishes
as the pHs' spatial density increases. Finally, we analyze the challenging
scenario that multiple travelers overlap for demanding data-plan sharing, by
resorting to a near-optimal pricing scheme. We show that a traveler suffers as
the travelers' spatial density increases.",monitoring personal pricing
http://arxiv.org/abs/1801.02484v1,"Data minimisation is a privacy enhancing principle, stating that personal
data collected should be no more than necessary for the specific purpose
consented by the user. Checking that a program satisfies the data minimisation
principle is not easy, even for the simple case when considering deterministic
programs-as-functions. In this paper we prove (im)possibility results
concerning runtime monitoring of (non-)minimality for deterministic programs
both when the program has one input source (monolithic) and for the more
general case when inputs come from independent sources (distributed case). We
propose monitoring mechanisms where a monitor observes the inputs and the
outputs of a program, to detect violation of data minimisation policies. We
show that monitorability of (non) minimality is decidable only for specific
cases, and detection of satisfaction of different notions of minimality in
undecidable in general. That said, we show that under certain conditions
monitorability is decidable and we provide an algorithm and a bound to check
such properties in a pre-deployment controlled environment, also being able to
compute a minimiser for the given program. Finally, we provide a
proof-of-concept implementation for both offline and online monitoring and
apply that to some case studies.",monitoring personal pricing
http://arxiv.org/abs/1602.07720v1,"We study the question of setting and testing reserve prices in single item
auctions when the bidders are not identical. At a high level, there are two
generalizations of the standard second price auction: in the lazy version we
first determine the winner, and then apply reserve prices; in the eager version
we first discard the bidders not meeting their reserves, and then determine the
winner among the rest. We show that the two versions have dramatically
different properties: lazy reserves are easy to optimize, and A/B test in
production, whereas eager reserves always lead to higher welfare, but their
optimization is NP-complete, and naive A/B testing will lead to incorrect
conclusions. Despite their different characteristics, we show that the overall
revenue for the two scenarios is always within a factor of 2 of each other,
even in the presence of correlated bids. Moreover, we prove that the eager
auction dominates the lazy auction on revenue whenever the bidders are
independent or symmetric. We complement our theoretical results with
simulations on real world data that show that even suboptimally set eager
reserve prices are preferred from a revenue standpoint.",monitoring personal pricing
http://arxiv.org/abs/1208.5258v2,"Personal data has value to both its owner and to institutions who would like
to analyze it. Privacy mechanisms protect the owner's data while releasing to
analysts noisy versions of aggregate query results. But such strict protections
of individual's data have not yet found wide use in practice. Instead, Internet
companies, for example, commonly provide free services in return for valuable
sensitive information from users, which they exploit and sometimes sell to
third parties.
  As the awareness of the value of the personal data increases, so has the
drive to compensate the end user for her private information. The idea of
monetizing private data can improve over the narrower view of hiding private
data, since it empowers individuals to control their data through financial
means.
  In this paper we propose a theoretical framework for assigning prices to
noisy query answers, as a function of their accuracy, and for dividing the
price amongst data owners who deserve compensation for their loss of privacy.
Our framework adopts and extends key principles from both differential privacy
and query pricing in data markets. We identify essential properties of the
price function and micro-payments, and characterize valid solutions.",monitoring personal pricing
http://arxiv.org/abs/1302.3820v1,"This paper explores using RSS measurements on many links in a wireless
network to estimate the breathing rate of a person, and the location where the
breathing is occurring, in a home, while the person is sitting, laying down,
standing, or sleeping. The main challenge in breathing rate estimation is that
""motion interference"", i.e., movements other than a person's breathing,
generally cause larger changes in RSS than inhalation and exhalation. We
develop a method to estimate breathing rate despite motion interference, and
demonstrate its performance during multiple short (3-7 minute) tests and during
a longer 66 minute test. Further, for the same experiments, we show the
location of the breathing person can be estimated, to within about 2 m average
error in a 56 square meter apartment. Being able to locate a breathing person
who is not otherwise moving, without calibration, is important for applications
in search and rescue, health care, and security.",monitoring personal pricing
http://arxiv.org/abs/1804.03178v1,"In practical crowdsourcing systems such as Amazon Mechanical Turk, posted
pricing is widely used due to its simplicity, where a task requester publishes
a pricing rule a priori, on which workers decide whether to accept and perform
the task or not, and are often paid according to the quality of their effort.
One of the key ingredients of a good posted pricing lies in how to recruit more
high-quality workers with less budget, for which the following two schemes are
considered: (i) personalized pricing by profiling users in terms of their
quality and cost, and (ii) additional bonus payment offered for more qualified
task completion. Despite their potential benefits in crowdsourced pricing, it
has been under-explored how much gain each or both of personalization and bonus
payment actually provides to the requester. In this paper, we study four
possible combinations of posted pricing made by pricing with/without
personalization and bonus. We aim at analytically quantifying when and how much
such two ideas contribute to the requester's utility. To this end, we first
derive the optimal personalized and common pricing schemes and analyze their
computational tractability. Next, we quantify the gap in the utility between
with and without bonus payment in both pricing schemes. We analytically prove
that the impact of bonus is negligible significantly marginal in personalized
pricing, whereas crucial in common pricing. Finally, we study the notion of
Price of Agnosticity that quantifies the utility gap between personalized and
common pricing policies. This implies that a complex personalized pricing with
privacy concerns can be replaced by a simple common pricing with bonus. We
validate our analytical findings through extensive simulations and real
experiments done in Amazon Mechanical Turk, and provide additional implications
that are useful in designing a pricing policy.",personal pricing
http://arxiv.org/abs/1705.02982v2,"A personal data market is a platform including three participants: data
owners (individuals), data buyers and market maker. Data owners who provide
personal data are compensated according to their privacy loss. Data buyers can
submit a query and pay for the result according to their desired accuracy.
Market maker coordinates between data owner and buyer. This framework has been
previously studied based on differential privacy. However, the previous study
assumes data owners can accept any level of privacy loss and data buyers can
conduct the transaction without regard to the financial budget. In this paper,
we propose a practical personal data trading framework that is able to strike a
balance between money and privacy. In order to gain insights on user
preferences, we first conducted an online survey on human attitude to- ward
privacy and interest in personal data trading. Second, we identify the 5 key
principles of personal data market, which is important for designing a
reasonable trading frame- work and pricing mechanism. Third, we propose a
reason- able trading framework for personal data which provides an overview of
how the data is traded. Fourth, we propose a balanced pricing mechanism which
computes the query price for data buyers and compensation for data owners
(whose data are utilized) as a function of their privacy loss. The main goal is
to ensure a fair trading for both parties. Finally, we will conduct an
experiment to evaluate the output of our proposed pricing mechanism in
comparison with other previously proposed mechanism.",personal pricing
http://arxiv.org/abs/1709.04767v1,"In this paper we examine information privacy from a value-centered angle. We
review and compare different personal data valuation approaches. We consider
the value of personal data now and in the near future, and we find that the
enduring part of personal data will soon become common knowledge and that their
price and value will drop substantially. Therefore the sector that is based on
personal data extraction will need to focus on new ways of monetization and on
the of the dynamic part of personal information.",personal pricing
http://arxiv.org/abs/1604.04157v1,"In \cite{EK10} the use of VCG in matching markets is motivated by saying that
in order to compute market clearing prices in a matching market, the auctioneer
needs to know the true valuations of the bidders. Hence VCG and corresponding
personalized prices are proposed as an incentive compatible mechanism. The same
line of argument pops up in several lecture sheets and other documents related
to courses based on Easley and Kleinberg's book, seeming to suggest that
computing market clearing prices and corresponding assignments were \emph{not}
incentive compatible. Main purpose of our note is to observe that, in contrast,
assignments based on buyer optimal market clearing prices are indeed incentive
compatible.",personal pricing
http://arxiv.org/abs/1905.01526v2,"We study the problem of computing personalized reserve prices in eager second
price auctions without having any assumption on valuation distributions. Here,
the input is a dataset that contains the submitted bids of $n$ buyers in a set
of auctions and the goal is to return personalized reserve prices $\textbf r$
that maximize the revenue earned on these auctions by running eager second
price auctions with reserve $\textbf r$. We present a novel LP formulation to
this problem and a rounding procedure which achieves a
$(1+2(\sqrt{2}-1)e^{\sqrt{2}-2})^{-1} \approx 0.684$-approximation. This
improves over the $\frac{1}{2}$-approximation Algorithm due to Roughgarden and
Wang. We show that our analysis is tight for this rounding procedure. We also
bound the integrality gap of the LP, which bounds the performance of any
algorithm based on this LP.",personal pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",personal pricing
http://arxiv.org/abs/physics/0611130v1,"The importance of the power law has been well realized in econophysics over
the last decade. For instance, the distribution of the rate of stock price
variation and of personal assets show the power law. While these results reveal
the striking scale invariance of financial markets, the behaviour of price in
real economy is less known in spite of its extreme importance. As an example of
markets in real economy, here we take up the price of precious stones which
increases with size while the amount of their production rapidly decreases with
size. We show for the first time that the price of natural precious stones
(quartz crystal ball, gemstones such as diamond, emerald, and sapphire) as a
function of weight obeys the power law. This indicates that the price is
determined by the same evaluation measure for different sizes. Our results
demonstrate that not only the distribution of an economical observable but also
the price itself obeys the power law. We anticipate our findings to be a
starting point for the quantitative study of scale invariance in real economy.
While the Black--Sholes model provided the framework for optimal pricing in
financial markets, our method of analysis prvides a new framework that
characterizes the market in real economy.",personal pricing
http://arxiv.org/abs/1407.0566v2,"In the context of a myriad of mobile apps which collect personally
identifiable information (PII) and a prospective market place of personal data,
we investigate a user-centric monetary valuation of mobile PII. During a 6-week
long user study in a living lab deployment with 60 participants, we collected
their daily valuations of 4 categories of mobile PII (communication, e.g.
phonecalls made/received, applications, e.g. time spent on different apps,
location and media, photos taken) at three levels of complexity (individual
data points, aggregated statistics and processed, i.e. meaningful
interpretations of the data). In order to obtain honest valuations, we employ a
reverse second price auction mechanism. Our findings show that the most
sensitive and valued category of personal information is location. We report
statistically significant associations between actual mobile usage, personal
dispositions, and bidding behavior. Finally, we outline key implications for
the design of mobile services and future markets of personal data.",personal pricing
http://arxiv.org/abs/1906.05457v2,"As personal data have been the new oil of the digital era, there is a growing
trend perceiving personal data as a commodity. Although some people are willing
to trade their personal data for money, they might still expect limited
individual privacy loss, and the maximum tolerable privacy loss varies with
each individual. In this paper, we propose a framework that enables individuals
to trade their location data streams under personalized privacy loss, which can
be bounded in w successive time points. However, the introduction of such
personalized bounds of individual privacy loss over streaming data raises
several technical challenges in the aspects of budget allocation, utility
estimation of personalized differentially private mechanism, and arbitrage-free
pricing. To deal with those challenges, we modularize three key modules in our
framework and propose arbitrage-free trading mechanisms by combining instances
of the modules. Finally, our experiments verify the effectiveness of the
proposed mechanisms.",personal pricing
http://arxiv.org/abs/1906.11657v1,"What fraction of the single item $n$ buyers setting's expected optimal
revenue MyeRev can the second price auction with reserves achieve? In the
special case where the buyers' valuation distributions are all drawn i.i.d. and
the distributions satisfy the regularity condition, the second price auction
with an anonymous reserve (ASP) is the optimal auction itself. As the setting
gets more complex, there are established upper bounds on the fraction of MyeRev
that ASP can achieve. On the contrary, no such upper bounds are known for the
fraction of MyeRev achievable by the second price auction with eager
personalized reserves (ESP). In particular, no separation was earlier known
between ESP's revenue and MyeRev even in the most general setting of
non-identical product distributions that don't satisfy the regularity
condition. In this paper we establish the first separation results for ESP: we
show that even in the case of distributions drawn i.i.d., but not necessarily
satisfying the regularity condition, the ESP cannot achieve more than a $0.778$
fraction of MyeRev in general. Combined with Correa et al.'s result (EC 2017)
that ESP can achieve at least a $0.745$ fraction of MyeRev, this nearly bridges
the gap between upper and lower bounds on ESP's approximation factor.",personal pricing
http://arxiv.org/abs/physics/0604179v1,"Involving effects of media, opinion leader and other agents on the opinion of
individuals of market society, a trader based model is developed and utilized
to simulate price via supply and demand. Pronounced effects are considered with
several weights and some personal differences between traders are taken into
account. Resulting time series and probabilty distribution function involving a
power law for price come out similar to the real ones.",personal pricing
http://arxiv.org/abs/1712.06236v2,"A smartphone user's personal hotspot (pH) allows him to share cellular
connection to another (e.g., a traveler) in the vicinity, but such sharing
consumes the limited data quota in his two-part tariff plan and may lead to
overage charge. This paper studies how to motivate such pH-enabled data-plan
sharing between local users and travelers in the ever-growing roaming markets,
and proposes pricing incentive for a data-plan buyer to reward surrounding pH
sellers (if any). The pricing scheme practically takes into account the
information uncertainty at the traveler side, including the random mobility and
the sharing cost distribution of selfish local users who potentially share pHs.
Though the pricing optimization problem is non-convex, we show that there
always exists a unique optimal price to tradeoff between the successful sharing
opportunity and the sharing price. We further generalize the optimal pricing to
the case of heterogeneous selling pHs who have diverse data usage behaviors in
the sharing cost distributions, and we show such diversity may or may not
benefit the traveler. Lacking selfish pHs' information, the traveler's expected
cost is higher than that under the complete information, but the gap diminishes
as the pHs' spatial density increases. Finally, we analyze the challenging
scenario that multiple travelers overlap for demanding data-plan sharing, by
resorting to a near-optimal pricing scheme. We show that a traveler suffers as
the travelers' spatial density increases.",personal pricing
http://arxiv.org/abs/1602.07720v1,"We study the question of setting and testing reserve prices in single item
auctions when the bidders are not identical. At a high level, there are two
generalizations of the standard second price auction: in the lazy version we
first determine the winner, and then apply reserve prices; in the eager version
we first discard the bidders not meeting their reserves, and then determine the
winner among the rest. We show that the two versions have dramatically
different properties: lazy reserves are easy to optimize, and A/B test in
production, whereas eager reserves always lead to higher welfare, but their
optimization is NP-complete, and naive A/B testing will lead to incorrect
conclusions. Despite their different characteristics, we show that the overall
revenue for the two scenarios is always within a factor of 2 of each other,
even in the presence of correlated bids. Moreover, we prove that the eager
auction dominates the lazy auction on revenue whenever the bidders are
independent or symmetric. We complement our theoretical results with
simulations on real world data that show that even suboptimally set eager
reserve prices are preferred from a revenue standpoint.",personal pricing
http://arxiv.org/abs/1208.5258v2,"Personal data has value to both its owner and to institutions who would like
to analyze it. Privacy mechanisms protect the owner's data while releasing to
analysts noisy versions of aggregate query results. But such strict protections
of individual's data have not yet found wide use in practice. Instead, Internet
companies, for example, commonly provide free services in return for valuable
sensitive information from users, which they exploit and sometimes sell to
third parties.
  As the awareness of the value of the personal data increases, so has the
drive to compensate the end user for her private information. The idea of
monetizing private data can improve over the narrower view of hiding private
data, since it empowers individuals to control their data through financial
means.
  In this paper we propose a theoretical framework for assigning prices to
noisy query answers, as a function of their accuracy, and for dividing the
price amongst data owners who deserve compensation for their loss of privacy.
Our framework adopts and extends key principles from both differential privacy
and query pricing in data markets. We identify essential properties of the
price function and micro-payments, and characterize valid solutions.",personal pricing
http://arxiv.org/abs/1909.13426v1,"Negotiation is a complex activity involving strategic reasoning, persuasion,
and psychology. An average person is often far from an expert in negotiation.
Our goal is to assist humans to become better negotiators through a
machine-in-the-loop approach that combines machine's advantage at data-driven
decision-making and human's language generation ability. We consider a
bargaining scenario where a seller and a buyer negotiate the price of an item
for sale through a text-based dialog. Our negotiation coach monitors messages
between them and recommends tactics in real time to the seller to get a better
deal (e.g., ""reject the proposal and propose a price"", ""talk about your
personal experience with the product""). The best strategy and tactics largely
depend on the context (e.g., the current price, the buyer's attitude).
Therefore, we first identify a set of negotiation tactics, then learn to
predict the best strategy and tactics in a given dialog context from a set of
human-human bargaining dialogs. Evaluation on human-human dialogs shows that
our coach increases the profits of the seller by almost 60%.",personal pricing
http://arxiv.org/abs/1905.00052v1,"We address the problem of personalization in the context of eCommerce search.
Specifically, we develop personalization ranking features that use in-session
context to augment a generic ranker optimized for conversion and relevance. We
use a combination of latent features learned from item co-clicks in historic
sessions and content-based features that use item title and price.
Personalization in search has been discussed extensively in the existing
literature. The novelty of our work is combining and comparing content-based
and content-agnostic features and showing that they complement each other to
result in a significant improvement of the ranker. Moreover, our technique does
not require an explicit re-ranking step, does not rely on learning user
profiles from long term search behavior, and does not involve complex modeling
of query-item-user features. Our approach captures item co-click propensity
using lightweight item embeddings. We experimentally show that our technique
significantly outperforms a generic ranker in terms of Mean Reciprocal Rank
(MRR). We also provide anecdotal evidence for the semantic similarity captured
by the item embeddings on the eBay search engine.",personal pricing
http://arxiv.org/abs/1203.3870v1,"Every time the customer (individual or company) has to release personal
information to its service provider (e.g., an online store or a cloud computing
provider), it faces a trade-off between the benefits gained (enhanced or
cheaper services) and the risks it incurs (identity theft and fraudulent uses).
The amount of personal information released is the major decision variable in
that trade-off problem, and has a proxy in the maximum loss the customer may
incur. We find the conditions for a unique optimal solution to exist for that
problem as that maximizing the customer's surplus. We also show that the
optimal amount of personal information is influenced most by the immediate
benefits the customer gets, i.e., the price and the quantity of service offered
by the service provider, rather than by maximum loss it may incur. Easy
spenders take larger risks with respect to low-spenders, but an increase in
price drives customers towards a more careful risk-taking attitude anyway. A
major role is also played by the privacy level, which the service provider
employs to regulate the benefits released to the customers. We also provide a
closed form solution for the limit case of a perfectly secure provider, showing
that the results do not differ significantly from those obtained in the general
case. The trade-off analysis may be employed by the customer to determine its
level of exposure in the relationship with its service provider.",personal pricing
http://arxiv.org/abs/1906.02635v1,"This paper proposes a method for estimating consumer preferences among
discrete choices, where the consumer chooses at most one product in a category,
but selects from multiple categories in parallel. The consumer's utility is
additive in the different categories. Her preferences about product attributes
as well as her price sensitivity vary across products and are in general
correlated across products. We build on techniques from the machine learning
literature on probabilistic models of matrix factorization, extending the
methods to account for time-varying product attributes and products going out
of stock. We evaluate the performance of the model using held-out data from
weeks with price changes or out of stock products. We show that our model
improves over traditional modeling approaches that consider each category in
isolation. One source of the improvement is the ability of the model to
accurately estimate heterogeneity in preferences (by pooling information across
categories); another source of improvement is its ability to estimate the
preferences of consumers who have rarely or never made a purchase in a given
category in the training data. Using held-out data, we show that our model can
accurately distinguish which consumers are most price sensitive to a given
product. We consider counterfactuals such as personally targeted price
discounts, showing that using a richer model such as the one we propose
substantially increases the benefits of personalization in discounts.",personal pricing
http://arxiv.org/abs/1501.04850v1,"With the rapid development of applications in open distributed environments
such as eCommerce, privacy of information is becoming a critical issue. Today,
many online companies are gathering information and have assembled
sophisticated databases that know a great deal about many people, generally
without the knowledge of those people. Such information changes hands or
ownership as a normal part of eCommerce transactions, or through strategic
decisions that often includes the sale of users' information to other firms.
The key commercial value of users' personal information derives from the
ability of firms to identify consumers and charge them personalized prices for
goods and services they have previously used or may wish to use in the future.
A look at present-day practices reveals that consumers' profile data is now
considered as one of the most valuable assets owned by online businesses. In
this thesis, we argue the following: if consumers' private data is such a
valuable asset, should they not be entitled to commercially benefit from their
asset as well? The scope of this thesis is on developing architecture for
privacy payoff as a means of rewarding consumers for sharing their personal
information with online businesses. The architecture is a multi-agent system in
which several agents employ various requirements for personal information
valuation and interaction capabilities that most users cannot do on their own.
The agents in the system bear the responsibility of working on behalf of
consumers to categorize their personal data objects, report to consumers on
online businesses' trustworthiness and reputation, determine the value of their
compensation using risk-based financial models, and, finally, negotiate for a
payoff value in return for the dissemination of users' information.",personal pricing
http://arxiv.org/abs/1812.09234v1,"A firm is selling a product to different types (based on the features such as
education backgrounds, ages, etc.) of customers over a finite season with
non-replenishable initial inventory. The type label of an arriving customer can
be observed but the demand function associated with each type is initially
unknown. The firm sets personalized prices dynamically for each type and
attempts to maximize the revenue over the season. We provide a learning
algorithm that is near-optimal when the demand and capacity scale in
proportion. The algorithm utilizes the primal-dual formulation of the problem
and learns the dual optimal solution explicitly. It allows the algorithm to
overcome the curse of dimensionality (the rate of regret is independent of the
number of types) and sheds light on novel algorithmic designs for learning
problems with resource constraints.",personal pricing
http://arxiv.org/abs/1508.07292v1,"Uber has recently been introducing novel practices in urban taxi transport.
Journey prices can change dynamically in almost real time and also vary
geographically from one area to another in a city, a strategy known as surge
pricing. In this paper, we explore the power of the new generation of open
datasets towards understanding the impact of the new disruption technologies
that emerge in the area of public transport. With our primary goal being a more
transparent economic landscape for urban commuters, we provide a direct price
comparison between Uber and the Yellow Cab company in New York. We discover
that Uber, despite its lower standard pricing rates, effectively charges higher
fares on average, especially during short in length, but frequent in
occurrence, taxi journeys. Building on this insight, we develop a smartphone
application, OpenStreetCab, that offers a personalized consultation to mobile
users on which taxi provider is cheaper for their journey. Almost five months
after its launch, the app has attracted more than three thousand users in a
single city. Their journey queries have provided additional insights on the
potential savings similar technologies can have for urban commuters, with a
highlight being that on average, a user in New York saves 6 U.S. Dollars per
taxi journey if they pick the cheapest taxi provider. We run extensive
experiments to show how Uber's surge pricing is the driving factor of higher
journey prices and therefore higher potential savings for our application's
users. Finally, motivated by the observation that Uber's surge pricing is
occurring more frequently that intuitively expected, we formulate a prediction
task where the aim becomes to predict a geographic area's tendency to surge.
Using exogenous to Uber datasets we show how it is possible to estimate
customer demand within an area, and by extension surge pricing, with high
accuracy.",personal pricing
http://arxiv.org/abs/physics/0608087v3,"Foreign exchange markets show that currency units (= accounting or nominal
price units) are variables. Technical and economic progress evidence that the
consumer baskets (= purchasing power units or real price units) are also
variables. In contrast, all physical measurement units are constants and either
defined in the SI (= metric) convention or based upon natural constants (=
""natural"" or Planck units). Econophysics can identify a constant natural value
scale or vaue unit (natural numaraire) based upon Planck energy. In honour of
the economist L. Walras, this ""Planck value"" could be called walras (Wal),
thereby using the SI naming convention. One Wal can be shown to have a
physiological and an economic interpretation in that it is equal to the annual
minimal real cost of physiological life of a reference person at minimal
activity. The price of one Wal in terms of any currency can be estimated by
hedonic regression techniques used in inflation measurement (axiometry). This
pilot research uses official disaggregated Swiss Producer and Consumer Price
Index data and estimates the hedonic walras price (HWP), quoted in Swiss francs
in 2003, and its inverse, the physical purchasing power (PhPP) of the Swiss
franc in 2003.",personal pricing
http://arxiv.org/abs/0801.2931v1,"We consider the ""Offline Ad Slot Scheduling"" problem, where advertisers must
be scheduled to ""sponsored search"" slots during a given period of time.
Advertisers specify a budget constraint, as well as a maximum cost per click,
and may not be assigned to more than one slot for a particular search.
  We give a truthful mechanism under the utility model where bidders try to
maximize their clicks, subject to their personal constraints. In addition, we
show that the revenue-maximizing mechanism is not truthful, but has a Nash
equilibrium whose outcome is identical to our mechanism. As far as we can tell,
this is the first treatment of sponsored search that directly incorporates both
multiple slots and budget constraints into an analysis of incentives.
  Our mechanism employs a descending-price auction that maintains a solution to
a certain machine scheduling problem whose job lengths depend on the price, and
hence is variable over the auction. The price stops when the set of bidders
that can afford that price pack exactly into a block of ad slots, at which
point the mechanism allocates that block and continues on the remaining slots.
To prove our result on the equilibrium of the revenue-maximizing mechanism, we
first show that a greedy algorithm suffices to solve the revenue-maximizing
linear program; we then use this insight to prove that bidders allocated in the
same block of our mechanism have no incentive to deviate from bidding the fixed
price of that block.",personal pricing
http://arxiv.org/abs/1409.0069v1,"There is a relatively small amount of research covering urban freight
movements. Most research dealing with the subject of urban mobility focuses on
passenger vehicles, not commercial vehicles hauling freight. However, in many
ways, urban freight transport contributes to congestion, air pollution, noise,
accident and more fuel consumption which raises logistic costs, and hence the
price of products. The main focus of this paper is to propose a new solution
for congestion in order to improve the distribution process of goods in urban
areas and optimize transportation cost, time of delivery, fuel consumption, and
environmental impact, while guaranteeing the safety of goods and passengers. A
novel technique for personalization in itinerary search based on city logistics
ontology and rules is proposed to overcome this problem. The integration of
personalization plays a key role in capturing or inferring the needs of each
stakeholder (user), and then satisfying these needs in a given context. The
proposed approach is implemented to an itinerary search problem for freight
transportation in urban areas to demonstrate its ability in facilitating
intelligent decision support by retrieving the best itinerary that satisfies
the most users preferences (stakeholders).",personal pricing
http://arxiv.org/abs/0712.1598v4,"Uncoordinated individuals in human society pursuing their personally optimal
strategies do not always achieve the social optimum, the most beneficial state
to the society as a whole. Instead, strategies form Nash equilibria which are
often socially suboptimal. Society, therefore, has to pay a price of anarchy
for the lack of coordination among its members. Here we assess this price of
anarchy by analyzing the travel times in road networks of several major cities.
Our simulation shows that uncoordinated drivers possibly waste a considerable
amount of their travel time. Counterintuitively,simply blocking certain streets
can partially improve the traffic conditions. We analyze various complex
networks and discuss the possibility of similar paradoxes in physics.",personal pricing
http://arxiv.org/abs/1810.10900v1,"In this paper we study the single-leg revenue management problem, with no
information given about the demand trajectory over time. The competitive ratio
for this problem has been established by Ball and Queyranne (2009) under the
assumption of independent demand, i.e., demand for higher fare classes does not
spill over to lower fare classes. We extend their results to general demand
models to account for the buying-down phenomenon, by incorporating the
price-skimming technique from Eren and Maglaras (2010). That is, we derive
state-dependent price-skimming policies, which stochastically increase their
price distributions as the inventory level decreases, in a way that yields the
best-possible competitive ratio. Furthermore, our policies have the benefit
that they can be easily adapted to exploit available demand information, such
as the personal characteristics of an incoming online customer, while
maintaining the competitive ratio guarantee. A key technical ingredient in our
paper is a new `valuation tracking' subroutine, which tracks the possible
values for the optimum, and follows the most inventory-conservative control
which maintains the desired competitive ratio.",personal pricing
http://arxiv.org/abs/1905.00844v1,"The Keynesian Beauty Contest is a classical game in which strategic agents
seek to both accurately guess the true state of the world as well as the
average action of all agents. We study an augmentation of this game where
agents are concerned about revealing their private information and additionally
suffer a loss based on how well an observer can infer their private signals. We
solve for an equilibrium of this augmented game and quantify the loss of social
welfare as a result of agents acting to obscure their private information,
which we call the 'price of privacy'. We analyze two versions of this this
price: one from the perspective of the agents measuring their diminished
ability to coordinate due to acting to obscure their information and another
from the perspective of an aggregator whose statistical estimate of the true
state of the world is of lower precision due to the agents adding random noise
to their actions. We show that these quantities are high when agents care very
strongly about protecting their personal information and low when the quality
of the signals the agents receive is poor.",personal pricing
http://arxiv.org/abs/1802.01346v1,"Multi-camera full-body pose capture of humans and animals in outdoor
environments is a highly challenging problem. Our approach to it involves a
team of cooperating micro aerial vehicles (MAVs) with on-board cameras only.
The key enabling-aspect of our approach is the on-board person detection and
tracking method. Recent state-of-the-art methods based on deep neural networks
(DNN) are highly promising in this context. However, real time DNNs are
severely constrained in input data dimensions, in contrast to available camera
resolutions. Therefore, DNNs often fail at objects with small scale or far away
from the camera, which are typical characteristics of a scenario with aerial
robots. Thus, the core problem addressed in this paper is how to achieve
on-board, real-time, continuous and accurate vision-based detections using DNNs
for visual person tracking through MAVs. Our solution leverages cooperation
among multiple MAVs. First, each MAV fuses its own detections with those
obtained by other MAVs to perform cooperative visual tracking. This allows for
predicting future poses of the tracked person, which are used to selectively
process only the relevant regions of future images, even at high resolutions.
Consequently, using our DNN-based detector we are able to continuously track
even distant humans with high accuracy and speed. We demonstrate the efficiency
of our approach through real robot experiments involving two aerial robots
tracking a person, while maintaining an active perception-driven formation. Our
solution runs fully on-board our MAV's CPU and GPU, with no remote processing.
ROS-based source code is provided for the benefit of the community.",personal pricing
http://arxiv.org/abs/1106.1577v1,"A dynamical model is introduced for the formation of a bullish or bearish
trends driving an asset price in a given market. Initially, each agent decides
to buy or sell according to its personal opinion, which results from the
combination of its own private information, the public information and its own
analysis. It then adjusts such opinion through the market as it observes
sequentially the behavior of a group of random selection of other agents. Its
choice is then determined by a local majority rule including itself. Whenever
the selected group is at a tie, i.e., it is undecided on what to do, the choice
is determined by the local group belief with respect to the anticipated trend
at that time. These local adjustments create a dynamic that leads the market
price formation. In case of balanced anticipations the market is found to be
efficient in being successful to make the ""right price"" to emerge from the
sequential aggregation of all the local individual informations which all
together contain the fundamental value. However, when a leading optimistic
belief prevails, the same efficient market mechanisms are found to produce a
bullish dynamic even though most agents have bearish private informations. The
market yields then a wider and wider discrepancy between the fundamental value
and the market value, which in turn creates a speculative bubble. Nevertheless,
there exists a limit in the growing of the bubble where private opinions take
over again and at once invert the trend, originating a sudden bearish trend.
Moreover, in the case of a drastic shift in the collective expectations, a huge
drop in price levels may also occur extremely fast and puts the market out of
control, it is a market crash.",personal pricing
http://arxiv.org/abs/0911.1619v1,"If you recommend a product to me and I buy it, how much should you be paid by
the seller? And if your sole interest is to maximize the amount paid to you by
the seller for a sequence of recommendations, how should you recommend
optimally if I become more inclined to ignore you with each irrelevant
recommendation you make? Finding an answer to these questions is a key
challenge in all forms of marketing that rely on and explore social ties;
ranging from personal recommendations to viral marketing.
  In the first part of this paper, we show that there can be no pricing
mechanism that is ""truthful"" with respect to the seller, and we use solution
concepts from coalitional game theory, namely the Core, the Shapley Value, and
the Nash Bargaining Solution, to derive provably ""fair"" prices for settings
with one or multiple recommenders. We then investigate pricing mechanisms for
the setting where recommenders have different ""purchase arguments"". Here we
show that it might be beneficial for the recommenders to withhold some of their
arguments, unless anonymity-proof solution concepts, such as the
anonymity-proof Shapley value, are used.
  In the second part of this paper, we analyze the setting where the
recommendee loses trust in the recommender for each irrelevant recommendation.
Here we prove that even if the recommendee regains her initial trust on each
successful recommendation, the expected total profit the recommender can make
over an infinite period is bounded. This can only be overcome when the
recommendee also incrementally regains trust during periods without any
recommendation. Here, we see an interesting connection to ""banner blindness"",
suggesting that showing fewer ads can lead to a higher long-term profit.",personal pricing
http://arxiv.org/abs/1808.07621v1,"Internet market makers are always facing intense competitive environment,
where personalized price reductions or discounted coupons are provided for
attracting more customers. Participants in such a price war scenario have to
invest a lot to catch up with other competitors. However, such a huge cost of
money may not always lead to an improvement of market share. This is mainly due
to a lack of information about others' strategies or customers' willingness
when participants develop their strategies.
  In order to obtain this hidden information through observable data, we study
the relationship between companies and customers in the Internet price war.
Theoretically, we provide a formalization of the problem as a stochastic game
with imperfect and incomplete information. Then we develop a variant of Latent
Dirichlet Allocation (LDA) to infer latent variables under the current market
environment, which represents the preferences of customers and strategies of
competitors. To our best knowledge, it is the first time that LDA is applied to
game scenario.
  We conduct simulated experiments where our LDA model exhibits a significant
improvement on finding strategies in the Internet price war by including all
available market information of the market maker's competitors. And the model
is applied to an open dataset for real business. Through comparisons on the
likelihood of prediction for users' behavior and distribution distance between
inferred opponent's strategy and the real one, our model is shown to be able to
provide a better understanding for the market environment.
  Our work marks a successful learning method to infer latent information in
the environment of price war by the LDA modeling, and sets an example for
related competitive applications to follow.",personal pricing
http://arxiv.org/abs/0904.3243v1,"The music industry has huge troubles adapting to the new technologies. As
many pointed out, when copying music is essentially free and socially accepted
it becomes increasingly tempting for users to infringe copyrights and copy
music from one person to another. The answer of the music industry is to outlaw
a majority of citizens. This article describes how the music industry should
reinvent itself and adapt to a world where the network is ubiquitous and
exchanging information is essentially free. It relies on adapting prices to the
demand and lower costs of electronic documents in a dramatic way.",personal pricing
http://arxiv.org/abs/1810.01982v2,"While E-commerce has been growing explosively and online shopping has become
popular and even dominant in the present era, online transaction fraud control
has drawn considerable attention in business practice and academic research.
Conventional fraud control considers mainly the interactions of two major
involved decision parties, i.e. merchants and fraudsters, to make fraud
classification decision without paying much attention to dynamic looping effect
arose from the decisions made by other profit-related parties. This paper
proposes a novel fraud control framework that can quantify interactive effects
of decisions made by different parties and can adjust fraud control strategies
using data analytics, artificial intelligence, and dynamic optimization
techniques. Three control models, Naive, Myopic and Prospective Controls, were
developed based on the availability of data attributes and levels of label
maturity. The proposed models are purely data-driven and self-adaptive in a
real-time manner. The field test on Microsoft real online transaction data
suggested that new systems could sizably improve the company's profit.",e-commerce fraud pricing
http://arxiv.org/abs/1805.00464v1,"The e-commerce share in the global retail spend is showing a steady increase
over the years indicating an evident shift of consumer attention from bricks
and mortar to clicks in retail sector. In recent years, online marketplaces
have become one of the key contributors to this growth. As the business model
matures, the number and types of frauds getting reported in the area is also
growing on a daily basis. Fraudulent e-commerce buyers and their transactions
are being studied in detail and multiple strategies to control and prevent them
are discussed. Another area of fraud happening in marketplaces are on the
seller side and is called merchant fraud. Goods/services offered and sold at
cheap rates, but never shipped is a simple example of this type of fraud. This
paper attempts to suggest a framework to detect such fraudulent sellers with
the help of machine learning techniques. The model leverages the historic data
from the marketplace and detect any possible fraudulent behaviours from sellers
and alert to the marketplace.",e-commerce fraud pricing
http://arxiv.org/abs/cs/0110006v1,"This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07407v1,"With the explosive growth of e-commerce and the booming of e-payment,
detecting online transaction fraud in real time has become increasingly
important to Fintech business. To tackle this problem, we introduce the TitAnt,
a transaction fraud detection system deployed in Ant Financial, one of the
largest Fintech companies in the world. The system is able to predict online
real-time transaction fraud in mere milliseconds. We present the problem
definition, feature extraction, detection methods, implementation and
deployment of the system, as well as empirical effectiveness. Extensive
experiments have been conducted on large real-world transaction data to show
the effectiveness and the efficiency of the proposed system.",e-commerce fraud pricing
http://arxiv.org/abs/1811.06109v1,"In Business Intelligence, accurate predictive modeling is the key for
providing adaptive decisions. We studied predictive modeling problems in this
research which was motivated by real-world cases that Microsoft data scientists
encountered while dealing with e-commerce transaction fraud control decisions
using transaction streaming data in an uncertain probabilistic decision
environment. The values of most online transactions related features can return
instantly, while the true fraud labels only return after a stochastic delay.
Using partially mature data directly for predictive modeling in an uncertain
probabilistic decision environment would lead to significant inaccuracy on risk
decision-making. To improve accurate estimation of the probabilistic prediction
environment, which leads to more accurate predictive modeling, two frameworks,
Current Environment Inference (CEI) and Future Environment Inference (FEI), are
proposed. These frameworks generated decision environment related features
using long-term fully mature and short-term partially mature data, and the
values of those features were estimated using varies of learning methods,
including linear regression, random forest, gradient boosted tree, artificial
neural network, and recurrent neural network. Performance tests were conducted
using some e-commerce transaction data from Microsoft. Testing results
suggested that proposed frameworks significantly improved the accuracy of
decision environment estimation.",e-commerce fraud pricing
http://arxiv.org/abs/1207.4292v1,"Many reports regarding online fraud in varieties media create skepticism for
conducting transactions online, especially through an open network such as the
Internet, which offers no security whatsoever. Therefore, encryption technology
is vitally important to support secure e-commerce on the Internet. Two
well-known encryption representing symmetric and asymmetric cryptosystems as
well as their applications are discussed in this paper. Encryption is a key
technology to secure electronic transactions. However, there are several
challenges such as crytoanalysis or code breaker as well as US export
restrictions on encryption. The future threat is the development of quantum
computers, which makes the existing encryption technology cripple.",e-commerce fraud pricing
http://arxiv.org/abs/1711.01434v3,"Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.",e-commerce fraud pricing
http://arxiv.org/abs/1709.04129v2,"On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.",e-commerce fraud pricing
http://arxiv.org/abs/1811.02196v2,"Often the challenge associated with tasks like fraud and spam detection is
the lack of all likely patterns needed to train suitable supervised learning
models. This problem accentuates when the fraudulent patterns are not only
scarce, they also change over time. Change in fraudulent pattern is because
fraudsters continue to innovate novel ways to circumvent measures put in place
to prevent fraud. Limited data and continuously changing patterns makes
learning significantly difficult. We hypothesize that good behavior does not
change with time and data points representing good behavior have consistent
spatial signature under different groupings. Based on this hypothesis we are
proposing an approach that detects outliers in large data sets by assigning a
consistency score to each data point using an ensemble of clustering methods.
Our main contribution is proposing a novel method that can detect outliers in
large datasets and is robust to changing patterns. We also argue that area
under the ROC curve, although a commonly used metric to evaluate outlier
detection methods is not the right metric. Since outlier detection problems
have a skewed distribution of classes, precision-recall curves are better
suited because precision compares false positives to true positives (outliers)
rather than true negatives (inliers) and therefore is not affected by the
problem of class imbalance. We show empirically that area under the
precision-recall curve is a better than ROC as an evaluation metric. The
proposed approach is tested on the modified version of the Landsat satellite
dataset, the modified version of the ann-thyroid dataset and a large real world
credit card fraud detection dataset available through Kaggle where we show
significant improvement over the baseline methods.",e-commerce fraud pricing
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",e-commerce fraud pricing
http://arxiv.org/abs/1606.01428v1,"Affiliate Marketing (AM) has become an important and cost effective tool for
e-commerce. There are numerous risks and vulnerabilities that are typically
associated with AM. Though a well-planned AM model can greatly benefit the
e-commerce strategies of an enterprise, a haphazardly implemented system can
expose a business enterprise to major risks and vulnerabilities, which can lead
to great financial losses through fraudulent activities. This
research-in-progress has identified some of the risks and the technical
background of those scenarios. The research will now move on to build a
functional prototype of an AM network to design and test solutions to control
the identified risks.",e-commerce fraud pricing
http://arxiv.org/abs/1707.03367v1,"In the e-commerce world, the follow-up of prices in detail web pages is of
great interest for things like buying a product when it falls below some
threshold. For doing this task, instead of bookmarking the pages and revisiting
them, in this paper we propose a novel web data extraction system, called
Wextractor. It consists of an extraction method and a web app for listing the
retrieved prices. As for the final user, the main feature of Wextractor is
usability because (s)he only has to signal the pages of interest and our system
automatically extracts the price from the page.",e-commerce fraud pricing
http://arxiv.org/abs/1906.07974v1,"Providers of online marketplaces are constantly combatting against
problematic transactions, such as selling illegal items and posting fictive
items, exercised by some of their users. A typical approach to detect fraud
activity has been to analyze registered user profiles, user's behavior, and
texts attached to individual transactions and the user. However, this
traditional approach may be limited because malicious users can easily conceal
their information. Given this background, network indices have been exploited
for detecting frauds in various online transaction platforms. In the present
study, we analyzed networks of users of an online consumer-to-consumer
marketplace in which a seller and the corresponding buyer of a transaction are
connected by a directed edge. We constructed egocentric networks of each of
several hundreds of fraudulent users and those of a similar number of normal
users. We calculated eight local network indices based on up to connectivity
between the neighbors of the focal node. Based on the present descriptive
analysis of these network indices, we fed twelve features that we constructed
from the eight network indices to random forest classifiers with the aim of
distinguishing between normal users and fraudulent users engaged in each one of
the four types of problematic transactions. We found that the classifier
accurately distinguished the fraudulent users from normal users and that the
classification performance did not depend on the type of problematic
transaction.",e-commerce fraud pricing
http://arxiv.org/abs/1806.05799v2,"As the largest e-commerce platform, Taobao helps advertisers reach billions
of search queries each day via sponsored search, which has also contributed
considerable revenue to the platform. An efficient bidding strategy to cater to
diverse advertiser demands while balancing platform revenue and consumer
experience is significant to a healthy and sustainable marketing ecosystem. In
this paper we propose \emph{Customer Intelligent Agent (CIA)}, a bidding
optimization framework which implements an impression-level bidding to reflect
advertisers' conversion willingness and budget control. In this way, CIA is
capable of fulfilling various e-commerce advertiser demands on different
levels, such as Gross Merchandise Volume optimization, style comparison etc.
Additionally, a replay based simulation system is designed to predict the
performance of different take-rate. CIA unifies the benefits of three parties
in the marketing ecosystem without changing the Generalized Second Price
mechanism. Our extensive offline simulations and large-scale online experiments
on \emph{Taobao Search Advertising (TSA)} platform verify the high
effectiveness of the CIA framework. Moreover, CIA has been deployed online as a
major bidding tool in TSA.",e-commerce fraud pricing
http://arxiv.org/abs/1806.09793v1,"With the considerable development of customer-to-customer (C2C) e-commerce in
the recent years, there is a big demand for an effective recommendation system
that suggests suitable websites for users to sell their items with some
specified needs. Nonetheless, e-commerce recommendation systems are mostly
designed for business-to-customer (B2C) websites, where the systems offer the
consumers the products that they might like to buy. Almost none of the related
research works focus on choosing selling sites for target items. In this paper,
we introduce an approach that recommends the selling websites based upon the
item's description, category, and desired selling price. This approach employs
NoSQL data-based machine learning techniques for building and training topic
models and classification models. The trained models can then be used to rank
the websites dynamically with respect to the user needs. The experimental
results with real-world datasets from Vietnam C2C websites will demonstrate the
effectiveness of our proposed method.",e-commerce fraud pricing
http://arxiv.org/abs/1909.13221v2,"Online advertising in E-commerce platforms provides sellers an opportunity to
achieve potential audiences with different target goals. Ad serving systems
(like display and search advertising systems) that assign ads to pages should
satisfy objectives such as plenty of audience for branding advertisers, clicks
or conversions for performance-based advertisers, at the same time try to
maximize overall revenue of the platform. In this paper, we propose an approach
based on linear programming subjects to constraints in order to optimize the
revenue and improve different performance goals simultaneously. We have
validated our algorithm by implementing an offline simulation system in Alibaba
E-commerce platform and running the auctions from online requests which takes
system performance, ranking and pricing schemas into account. We have also
compared our algorithm with related work, and the results show that our
algorithm can effectively improve campaign performance and revenue of the
platform.",e-commerce fraud pricing
http://arxiv.org/abs/1804.03910v1,"With the advent of e-commerce and online banking it has become extremely
important that the websites of the financial institutes (especially, banks)
implement up-to-date measures of cyber security (in accordance with the
recommendations of the regulatory authority) and thus circumvent the
possibilities of financial frauds that may occur due to vulnerabilities of the
website. Here, we systematically investigate whether Indian banks are following
the above requirement. To perform the investigation, recommendations of Reserve
Bank of India (RBI), National Institute of Standards and Technology (NIST),
European Union Agency for Network and Information Security (ENISA) and Internet
Engineering Task Force (IETF) are considered as the benchmarks. Further, the
validity and quality of the security certificates of various Indian banks have
been tested with the help of a set of tools (e.g., SSL Certificate Checker
provided by Digicert and SSL server test provided by SSL Labs). The analysis
performed by using these tools and a comparison with the benchmarks, have
revealed that the security measures taken by a set of Indian banks are not
up-to-date and are vulnerable under some known attacks.",e-commerce fraud pricing
http://arxiv.org/abs/physics/0608232v1,"We characterize the statistical properties of a large number of online
auctions run on eBay. Both stationary and dynamic properties, like
distributions of prices, number of bids etc., as well as relations between
these quantities are studied. The analysis of the data reveals surprisingly
simple distributions and relations, typically of power-law form. Based on these
findings we introduce a simple method to identify suspicious auctions that
could be influenced by a form of fraud known as shill bidding. Furthermore the
influence of bidding strategies is discussed. The results indicate that the
observed behavior is related to a mixture of agents using a variety of
strategies.",e-commerce fraud pricing
http://arxiv.org/abs/1905.04770v1,"Motivated by the dynamic assortment offerings and item pricings occurring in
e-commerce, we study a general problem of allocating finite inventories to
heterogeneous customers arriving sequentially. We analyze this problem under
the framework of competitive analysis, where the sequence of customers is
unknown and does not necessarily follow any pattern. Previous work in this
area, studying online matching, advertising, and assortment problems, has
focused on the case where each item can only be sold at a single price,
resulting in algorithms which achieve the best-possible competitive ratio of
1-1/e.
  In this paper, we extend all of these results to allow for items having
multiple feasible prices. Our algorithms achieve the best-possible
weight-dependent competitive ratios, which depend on the sets of feasible
prices given in advance. Our algorithms are also simple and intuitive; they are
based on constructing a class of universal ``value functions'' which integrate
the selection of items and prices offered.
  Finally, we test our algorithms on the publicly-available hotel data set of
Bodea et al. (2009), where there are multiple items (hotel rooms) each with
multiple prices (fares at which the room could be sold). We find that applying
our algorithms, as a ``hybrid'' with algorithms which attempt to forecast and
learn the future transactions, results in the best performance.",e-commerce fraud pricing
http://arxiv.org/abs/1711.02661v1,"In recent years, many new and interesting models of successful online
business have been developed, including competitive models such as auctions,
where the product price tends to rise, and group-buying, where users cooperate
obtaining a dynamic price that tends to go down. We propose the e-fair as a
business model for social commerce, where both sellers and buyers are grouped
to maximize benefits. e-Fairs extend the group-buying model aggregating demand
and supply for price optimization as well as consolidating shipments and
optimize withdrawals for guaranteeing additional savings. e-Fairs work upon
multiple dimensions: time to aggregate buyers, their geographical distribution,
price/quantity curves provided by sellers, and location of withdrawal points.
We provide an analytical model for time and spatial optimization and simulate
realistic scenarios using both real purchase data from an Italian marketplace
and simulated ones. Experimental results demonstrate the potentials offered by
e-fairs and show benefits for all the involved actors.",e-commerce fraud pricing
http://arxiv.org/abs/1211.3148v1,"Retailers in Saudi Arabia have been reserved in their adoption of
electronically delivered aspects of their business. This paper reports research
that identifies and explores key issues to enhance the diffusion of online
retailing in Saudi Arabia. Despite the fact that Saudi Arabia has the largest
and fastest growth of ICT marketplaces in the Arab region, e-commerce
activities are not progressing at the same speed. Only very few Saudi
companies, mostly medium and large companies from the manufacturing sector, are
involved in e-commerce implementation. Based on qualitative data collected by
conducting interviews with 16 retailers and 16 potential customers in Saudi
Arabia, 7 key drivers to online retailing diffusion in Saudi Arabia are
identified. These key drivers are government support, providing trustworthy and
secure online payments options, provision of individual house mailboxes,
providing high speed Internet connection at low cost, providing educational
programs, the success of bricks-and-clicks model, and competitive prices.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07607v3,"We study the problem of allocating impressions to sellers in e-commerce
websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue
generated by the platform. We employ a general framework of reinforcement
mechanism design, which uses deep reinforcement learning to design efficient
algorithms, taking the strategic behaviour of the sellers into account.
Specifically, we model the impression allocation problem as a Markov decision
process, where the states encode the history of impressions, prices,
transactions and generated revenue and the actions are the possible impression
allocations in each round. To tackle the problem of continuity and
high-dimensionality of states and actions, we adopt the ideas of the DDPG
algorithm to design an actor-critic policy gradient algorithm which takes
advantage of the problem domain in order to achieve convergence and stability.
We evaluate our proposed algorithm, coined IA(GRU), by comparing it against
DDPG, as well as several natural heuristics, under different rationality models
for the sellers - we assume that sellers follow well-known no-regret type
strategies which may vary in their degree of sophistication. We find that
IA(GRU) outperforms all algorithms in terms of the total revenue.",e-commerce fraud pricing
http://arxiv.org/abs/1708.07946v1,"Sales forecast is an essential task in E-commerce and has a crucial impact on
making informed business decisions. It can help us to manage the workforce,
cash flow and resources such as optimizing the supply chain of manufacturers
etc. Sales forecast is a challenging problem in that sales is affected by many
factors including promotion activities, price changes, and user preferences
etc. Traditional sales forecast techniques mainly rely on historical sales data
to predict future sales and their accuracies are limited. Some more recent
learning-based methods capture more information in the model to improve the
forecast accuracy. However, these methods require case-by-case manual feature
engineering for specific commercial scenarios, which is usually a difficult,
time-consuming task and requires expert knowledge. To overcome the limitations
of existing methods, we propose a novel approach in this paper to learn
effective features automatically from the structured data using the
Convolutional Neural Network (CNN). When fed with raw log data, our approach
can automatically extract effective features from that and then forecast sales
using those extracted features. We test our method on a large real-world
dataset from CaiNiao.com and the experimental results validate the
effectiveness of our method.",e-commerce fraud pricing
http://arxiv.org/abs/1808.08809v1,"The exponential growth of wireless-based solutions, such as those related to
the mobile smart devices (e.g., smart-phones and tablets) and Internet of
Things (IoT) devices, has lead to countless advantages in every area of our
society. Such a scenario has transformed the world a few decades back,
dominated by latency, into a new world based on an efficient real-time
interaction paradigm.Recently, cryptocurrency have contributed to this
technological revolution, the fulcrum of which are a decentralization model and
a certification function offered by the so-called blockchain infrastructure,
which make it possible to certify the financial transactions, anonymously.
However, it should be observed how this challenging scenario has generated new
security problems directly related to the involved new technologies (e.g.,
e-commerce frauds, mobile bot-net attacks, blockchain DoS attacks,
cryptocurrency scams, etc.). In this context, we can acknowledge that the
scientific community efforts are usually oriented toward specific solutions,
instead to exploit all the available technologies, synergistically, in order to
define more efficient security paradigms. This paper aims to indicate a
possible approach able to improve the security of people and things by
introducing a novel paradigm to security defined Internet of Entities (IoE). It
is a mechanism for the localization of people and things, which exploits both
the huge number of existing wireless-based devices and the blockchain-based
distributed ledger technology, overcoming the limits of traditional
localization approaches, but without jeopardizing the user privacy. Its
operation is based on two core elements with interchangeable roles, entities
and trackers, which can be very common elements such as smart-phones, tablets,
and IoT devices, and its implementation requires minimal efforts thanks to the
existing infrastructures and devices.",e-commerce fraud pricing
http://arxiv.org/abs/1309.0806v1,"With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.",e-commerce fraud pricing
http://arxiv.org/abs/1806.08910v1,"We introduce the fraud de-anonymization problem, that goes beyond fraud
detection, to unmask the human masterminds responsible for posting search rank
fraud in online systems. We collect and study search rank fraud data from
Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters
recruited from 6 crowdsourcing sites. We propose Dolos, a fraud
de-anonymization system that leverages traits and behaviors extracted from
these studies, to attribute detected fraud to crowdsourcing site fraudsters,
thus to real identities and bank accounts. We introduce MCDense, a min-cut
dense component detection algorithm to uncover groups of user accounts
controlled by different fraudsters, and leverage stylometry and deep learning
to attribute them to crowdsourcing site profiles. Dolos correctly identified
the owners of 95% of fraudster-controlled communities, and uncovered fraudsters
who promoted as many as 97.5% of fraud apps we collected from Google Play. When
evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6
months, Dolos identified 1,056 apps with suspicious reviewer groups. We report
orthogonal evidence of their fraud, including fraud duplicates and fraud
re-posts.",e-commerce fraud pricing
http://arxiv.org/abs/1409.6559v1,"Online auctions are among the most influential e-business applications. Their
impact on trading for businesses, as well as consumers, is both remarkable and
inevitable. There have been considerable efforts in setting up market places,
but, with respects to market volume, online trading is still in its early
stages. This chapter discusses the benefits of the concept of Internet
marketplaces, with the highest impact on pricing strategies, namely, the
conduction of online business auctions. We discuss their benefits, problems and
possible solutions. In addition, we sketch actions for suppliers to achieve a
better strategic position in the upcoming Internet market places.",e-commerce fraud pricing
http://arxiv.org/abs/1809.09621v1,"Complementary products recommendation is an important problem in e-commerce.
Such recommendations increase the average order price and the number of
products in baskets. Complementary products are typically inferred from basket
data. In this study, we propose the BB2vec model. The BB2vec model learns
vector representations of products by analyzing jointly two types of data -
Baskets and Browsing sessions (visiting web pages of products). These vector
representations are used for making complementary products recommendation. The
proposed model alleviates the cold start problem by delivering better
recommendations for products having few or no purchases. We show that the
BB2vec model has better performance than other models which use only basket
data.",e-commerce fraud pricing
http://arxiv.org/abs/1709.07534v1,"E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.",e-commerce fraud pricing
http://arxiv.org/abs/1901.02045v4,"Motivated by the application of real-time pricing in e-commerce platforms, we
consider the problem of revenue-maximization in a setting where the seller can
leverage contextual information describing the customer's history and the
product's type to predict her valuation of the product. However, her true
valuation is unobservable to the seller, only binary outcome in the form of
success-failure of a transaction is observed. Unlike in usual contextual bandit
settings, the optimal price/arm given a covariate in our setting is sensitive
to the detailed characteristics of the residual uncertainty distribution. We
develop a semi-parametric model in which the residual distribution is
non-parametric and provide the first algorithm which learns both regression
parameters and residual distribution with $\tilde O(\sqrt{n})$ regret. We
empirically test a scalable implementation of our algorithm and observe good
performance.",e-commerce fraud pricing
http://arxiv.org/abs/1902.09566v5,"Online retailers execute a very large number of price updates when compared
to brick-and-mortar stores. Even a few mis-priced items can have a significant
business impact and result in a loss of customer trust. Early detection of
anomalies in an automated real-time fashion is an important part of such a
pricing system. In this paper, we describe unsupervised and supervised anomaly
detection approaches we developed and deployed for a large-scale online pricing
system at Walmart. Our system detects anomalies both in batch and real-time
streaming settings, and the items flagged are reviewed and actioned based on
priority and business impact. We found that having the right architecture
design was critical to facilitate model performance at scale, and business
impact and speed were important factors influencing model selection, parameter
choice, and prioritization in a production environment for a large-scale
system. We conducted analyses on the performance of various approaches on a
test set using real-world retail data and fully deployed our approach into
production. We found that our approach was able to detect the most important
anomalies with high precision.",e-commerce fraud pricing
http://arxiv.org/abs/1806.07833v2,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",e-commerce fraud pricing
http://arxiv.org/abs/1709.01213v4,"Although mobile ad frauds have been widespread, state-of-the-art approaches
in the literature have mainly focused on detecting the so-called static
placement frauds, where only a single UI state is involved and can be
identified based on static information such as the size or location of ad
views. Other types of fraud exist that involve multiple UI states and are
performed dynamically while users interact with the app. Such dynamic
interaction frauds, although now widely spread in apps, have not yet been
explored nor addressed in the literature. In this work, we investigate a wide
range of mobile ad frauds to provide a comprehensive taxonomy to the research
community. We then propose, FraudDroid, a novel hybrid approach to detect ad
frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI
state transition graphs and collects their associated runtime network traffics,
which are then leveraged to check against a set of heuristic-based rules for
identifying ad fraudulent behaviours. We show empirically that FraudDroid
detects ad frauds with a high precision (93%) and recall (92%). Experimental
results further show that FraudDroid is capable of detecting ad frauds across
the spectrum of fraud types. By analysing 12,000 ad-supported Android apps,
FraudDroid identified 335 cases of fraud associated with 20 ad networks that
are further confirmed to be true positive results and are shared with our
fellow researchers to promote advanced ad fraud detection",e-commerce fraud pricing
http://arxiv.org/abs/1909.02398v1,"Automated fraud behaviors detection on electronic payment platforms is a
tough problem. Fraud users often exploit the vulnerability of payment platforms
and the carelessness of users to defraud money, steal passwords, do money
laundering, etc, which causes enormous losses to digital payment platforms and
users. There are many challenges for fraud detection in practice. Traditional
fraud detection methods require a large-scale manually labeled dataset, which
is hard to obtain in reality. Manually labeled data cost tremendous human
efforts. Besides, the continuous and rapid evolution of fraud users makes it
hard to find new fraud patterns based on existing detection rules. In our work,
we propose a real-world data oriented detection paradigm which can detect fraud
users and upgrade its detection ability automatically. Based on the new
paradigm, we design a novel fraud detection model, FraudJudger, to analyze
users behaviors on digital payment platforms and detect fraud users with fewer
labeled data in training. FraudJudger can learn the latent representations of
users from unlabeled data with the help of Adversarial Autoencoder (AAE).
Furthermore, FraudJudger can find new fraud patterns from unknown users by
cluster analysis. Our experiment is based on a real-world electronic payment
dataset. Comparing with other well-known fraud detection methods, FraudJudger
can achieve better detection performance with only 10% labeled data.",e-commerce fraud pricing
http://arxiv.org/abs/1309.3944v1,"With an upsurge in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection (FAFD) has become an
emerging topic of great importance for academic, research and industries. The
failure of internal auditing system of the organization in identifying the
accounting frauds has lead to use of specialized procedures to detect financial
accounting fraud, collective known as forensic accounting. Data mining
techniques are providing great aid in financial accounting fraud detection,
since dealing with the large data volumes and complexities of financial data
are big challenges for forensic accounting. This paper presents a comprehensive
review of the literature on the application of data mining techniques for the
detection of financial accounting fraud and proposes a framework for data
mining techniques based accounting fraud detection. The systematic and
comprehensive literature review of the data mining techniques applicable to
financial accounting fraud detection may provide a foundation to future
research in this field. The findings of this review show that data mining
techniques like logistic models, neural networks, Bayesian belief network, and
decision trees have been applied most extensively to provide primary solutions
to the problems inherent in the detection and classification of fraudulent
data.",e-commerce fraud pricing
http://arxiv.org/abs/1802.02996v1,"The difficulty of large scale monitoring of app markets affects our
understanding of their dynamics. This is particularly true for dimensions such
as app update frequency, control and pricing, the impact of developer actions
on app popularity, as well as coveted membership in top app lists. In this
paper we perform a detailed temporal analysis on two datasets we have collected
from the Google Play Store, one consisting of 160,000 apps and the other of
87,223 newly released apps. We have monitored and collected data about these
apps over more than 6 months. Our results show that a high number of these apps
have not been updated over the monitoring interval. Moreover, these apps are
controlled by a few developers that dominate the total number of app downloads.
We observe that infrequently updated apps significantly impact the median app
price. However, a changing app price does not correlate with the download
count. Furthermore, we show that apps that attain higher ranks have better
stability in top app lists. We show that app market analytics can help detect
emerging threat vectors, and identify search rank fraud and even malware.
Further, we discuss the research implications of app market analytics on
improving developer and user experiences.",e-commerce fraud pricing
http://arxiv.org/abs/cs/0109053v1,"Consumers value keeping some information about them private from potential
marketers. E-commerce dramatically increases the potential for marketers to
accumulate otherwise private information about potential customers. Online
marketers claim that this information enables them to better market their
products. Policy makers are currently drafting rules to regulate the way in
which these marketers can collect, store, and share this information. However,
there is little evidence yet either of consumers' valuation of their privacy or
of the benefits they might reap through better target marketing. We provide a
framework for measuring a portion of the benefits from allowing marketers to
make better use of consumer information. Target marketing is likely to reduce
consumer search costs, improve consumer product selection decisions, and lower
the marketing costs of goods sold. Our model allows us to estimate the value to
consumers of only the latter, price reductions from more efficient marketing.",e-commerce fraud pricing
http://arxiv.org/abs/1510.02377v3,"In a world where traditional notions of privacy are increasingly challenged
by the myriad companies that collect and analyze our data, it is important that
decision-making entities are held accountable for unfair treatments arising
from irresponsible data usage. Unfortunately, a lack of appropriate
methodologies and tools means that even identifying unfair or discriminatory
effects can be a challenge in practice. We introduce the unwarranted
associations (UA) framework, a principled methodology for the discovery of
unfair, discriminatory, or offensive user treatment in data-driven
applications. The UA framework unifies and rationalizes a number of prior
attempts at formalizing algorithmic fairness. It uniquely combines multiple
investigative primitives and fairness metrics with broad applicability,
granular exploration of unfair treatment in user subgroups, and incorporation
of natural notions of utility that may account for observed disparities. We
instantiate the UA framework in FairTest, the first comprehensive tool that
helps developers check data-driven applications for unfair user treatment. It
enables scalable and statistically rigorous investigation of associations
between application outcomes (such as prices or premiums) and sensitive user
attributes (such as race or gender). Furthermore, FairTest provides debugging
capabilities that let programmers rule out potential confounders for observed
unfair effects. We report on use of FairTest to investigate and in some cases
address disparate impact, offensive labeling, and uneven rates of algorithmic
error in four data-driven applications. As examples, our results reveal subtle
biases against older populations in the distribution of error in a predictive
health application and offensive racial labeling in an image tagger.",algorithm detect unfair pricing website
http://arxiv.org/abs/1806.11342v1,"In this paper, we consider a novel business model of video websites via
Membership-Advertising Mode in wireless network, where the video websites
provide three video services for mobile users: \textit{VIP-Member} service,
Regular-Member service and Non-Member service. The VIP-Member (Regular-Member)
service provides the highest level (middle level) quality and non-advertising
video service with high (low) price, while the Non-Member service provides the
lowest level quality and advertising-containing video service for free.
Meanwhile, the video websites sell their advertising spaces to the advertiser
to create extra revenues. We formulate the interactions among the advertiser,
video websites and mobile users as a three-stage Stackelberg game.
Specifically, in Stage I, the advertiser decides the advertising budget; in
Stage II, the video websites determine their advertising spaces selling
strategies for advertiser and the membership pricing strategies for mobile
users; in Stage III, the mobile users make their own decisions on video
watching strategies for each video website. We analyze the equilibrium of each
sub-game. Particularly, we derive the closed-form solutions of each mobile
user's optimal video watching strategies, each video website's optimal
membership price and the optimal advertising spaces selling number. In
addition, we also investigate the piece-wise structure of the advertiser's
utility function, and further propose an efficient algorithm to obtain the
optimal advertising budget. Finally, numerical results show the impacts of
different parameters' values on each entity's utility as well as the key
indicators.",algorithm detect unfair pricing website
http://arxiv.org/abs/1503.05414v3,"Social networks help to bond people who share similar interests all over the
world. As a complement, the Facebook ""Like"" button is an efficient tool that
bonds people with the online information. People click on the ""Like"" button to
express their fondness of a particular piece of information and in turn tend to
visit webpages with high ""Like"" count. The important fact of the Like count is
that it reflects the number of actual users who ""liked"" this information.
However, according to our study, one can easily exploit the defects of the
""Like"" button to counterfeit a high ""Like"" count. We provide a proof-of-concept
implementation of these exploits, and manage to generate 100 fake Likes in 5
minutes with a single account. We also reveal existing counterfeiting
techniques used by some online sellers to achieve unfair advantage for
promoting their products. To address this fake Like problem, we study the
varying patterns of Like count and propose an innovative fake Like detection
method based on clustering. To evaluate the effectiveness of our algorithm, we
collect the Like count history of more than 9,000 websites. Our experiments
successfully uncover 16 suspicious fake Like buyers that show abnormal Like
count increase patterns.",algorithm detect unfair pricing website
http://arxiv.org/abs/1211.0963v1,"Online rating systems are subject to malicious behaviors mainly by posting
unfair rating scores. Users may try to individually or collaboratively promote
or demote a product. Collaborating unfair rating 'collusion' is more damaging
than individual unfair rating. Although collusion detection in general has been
widely studied, identifying collusion groups in online rating systems is less
studied and needs more investigation. In this paper, we study impact of
collusion in online rating systems and asses their susceptibility to collusion
attacks. The proposed model uses a frequent itemset mining algorithm to detect
candidate collusion groups. Then, several indicators are used for identifying
collusion groups and for estimating how damaging such colluding groups might
be. Also, we propose an algorithm for finding possible collusive subgroup
inside larger groups which are not identified as collusive. The model has been
implemented and we present results of experimental evaluation of our
methodology.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7266v1,"Fake online pharmacies have become increasingly pervasive, constituting over
90% of online pharmacy websites. There is a need for fake website detection
techniques capable of identifying fake online pharmacy websites with a high
degree of accuracy. In this study, we compared several well-known link-based
detection techniques on a large-scale test bed with the hyperlink graph
encompassing over 80 million links between 15.5 million web pages, including
1.2 million known legitimate and fake pharmacy pages. We found that the QoC and
QoL class propagation algorithms achieved an accuracy of over 90% on our
dataset. The results revealed that algorithms that incorporate dual class
propagation as well as inlink and outlink information, on page-level or
site-level graphs, are better suited for detecting fake pharmacy websites. In
addition, site-level analysis yielded significantly better results than
page-level analysis for most algorithms evaluated.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0406034v1,"Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.",algorithm detect unfair pricing website
http://arxiv.org/abs/1712.10201v1,"High performance grid computing is a key enabler of large scale collaborative
computational science. With the promise of exascale computing, high performance
grid systems are expected to incur electricity bills that grow super-linearly
over time. In order to achieve cost effectiveness in these systems, it is
essential for the scheduling algorithms to exploit electricity price
variations, both in space and time, that are prevalent in the dynamic
electricity price markets. In this paper, we present a metascheduling algorithm
to optimize the placement of jobs in a compute grid which consumes electricity
from the day-ahead wholesale market. We formulate the scheduling problem as a
Minimum Cost Maximum Flow problem and leverage queue waiting time and
electricity price predictions to accurately estimate the cost of job execution
at a system. Using trace based simulation with real and synthetic workload
traces, and real electricity price data sets, we demonstrate our approach on
two currently operational grids, XSEDE and NorduGrid. Our experimental setup
collectively constitute more than 433K processors spread across 58 compute
systems in 17 geographically distributed locations. Experiments show that our
approach simultaneously optimizes the total electricity cost and the average
response time of the grid, without being unfair to users of the local batch
systems.",algorithm detect unfair pricing website
http://arxiv.org/abs/1205.3380v1,"Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.",algorithm detect unfair pricing website
http://arxiv.org/abs/1807.00787v1,"Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7261v1,"The ability to automatically detect fraudulent escrow websites is important
in order to alleviate online auction fraud. Despite research on related topics,
fake escrow website categorization has received little attention. In this study
we evaluated the effectiveness of various features and techniques for detecting
fake escrow websites. Our analysis included a rich set of features extracted
from web page text, image, and link information. We also proposed a composite
kernel tailored to represent the properties of fake websites, including content
duplication and structural attributes. Experiments were conducted to assess the
proposed features, techniques, and kernels on a test bed encompassing nearly
90,000 web pages derived from 410 legitimate and fake escrow sites. The
combination of an extended feature set and the composite kernel attained over
98% accuracy when differentiating fake sites from real ones, using the support
vector machines algorithm. The results suggest that automated web-based
information systems for detecting fake escrow sites could be feasible and may
be utilized as authentication mechanisms.",algorithm detect unfair pricing website
http://arxiv.org/abs/1408.1993v1,"Malicious websites are a major cyber attack vector, and effective detection
of them is an important cyber defense task. The main defense paradigm in this
regard is that the defender uses some kind of machine learning algorithms to
train a detection model, which is then used to classify websites in question.
Unlike other settings, the following issue is inherent to the problem of
malicious websites detection: the attacker essentially has access to the same
data that the defender uses to train its detection models. This 'symmetry' can
be exploited by the attacker, at least in principle, to evade the defender's
detection models. In this paper, we present a framework for characterizing the
evasion and counter-evasion interactions between the attacker and the defender,
where the attacker attempts to evade the defender's detection models by taking
advantage of this symmetry. Within this framework, we show that an adaptive
attacker can make malicious websites evade powerful detection models, but
proactive training can be an effective counter-evasion defense mechanism. The
framework is geared toward the popular detection model of decision tree, but
can be adapted to accommodate other classifiers.",algorithm detect unfair pricing website
http://arxiv.org/abs/1803.09967v1,"Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model's core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain's index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.",algorithm detect unfair pricing website
http://arxiv.org/abs/1711.06955v1,"Web spam is a big problem for search engine users in World Wide Web. They use
deceptive techniques to achieve high rankings. Although many researchers have
presented the different approach for classification and web spam detection
still it is an open issue in computer science. Analyzing and evaluating these
websites can be an effective step for discovering and categorizing the features
of these websites. There are several methods and algorithms for detecting those
websites, such as decision tree algorithm. In this paper, we present a
systematic framework based on CHAID algorithm and a modified string matching
algorithm (KMP) for extract features and analysis of these websites. We
evaluated our model and other methods with a dataset of Alexa Top 500 Global
Sites and Bing search engine results in 500 queries.",algorithm detect unfair pricing website
http://arxiv.org/abs/1309.7958v1,"Existing fake website detection systems are unable to effectively detect fake
websites. In this study, we advocate the development of fake website detection
systems that employ classification methods grounded in statistical learning
theory (SLT). Experimental results reveal that a prototype system developed
using SLT-based methods outperforms seven existing fake website detection
systems on a test bed encompassing 900 real and fake websites.",algorithm detect unfair pricing website
http://arxiv.org/abs/1907.12649v2,"In recent years, Header Bidding (HB) has gained popularity among web
publishers, challenging the status quo in the ad ecosystem. Contrary to the
traditional waterfall standard, HB aims to give back to publishers control of
their ad inventory, increase transparency, fairness and competition among
advertisers, resulting in higher ad-slot prices. Although promising, little is
known about how this ad protocol works: What are HB's possible implementations,
who are the major players, and what is its network and UX overhead? To address
these questions, we design and implement HBDetector: a novel methodology to
detect HB auctions on a website at real time. By crawling 35,000 top Alexa
websites, we collect and analyze a dataset of 800k auctions. We find that: (i)
14.28% of top websites utilize HB. (ii) Publishers prefer to collaborate with a
few Demand Partners who also dominate the waterfall market. (iii) HB latency
can be significantly higher (up to 3x in median case) than waterfall.",algorithm detect unfair pricing website
http://arxiv.org/abs/1208.1448v2,"In an emerging trend, more and more Internet users search for information
from Community Question and Answer (CQA) websites, as interactive communication
in such websites provides users with a rare feeling of trust. More often than
not, end users look for instant help when they browse the CQA websites for the
best answers. Hence, it is imperative that they should be warned of any
potential commercial campaigns hidden behind the answers. However, existing
research focuses more on the quality of answers and does not meet the above
need. In this paper, we develop a system that automatically analyzes the hidden
patterns of commercial spam and raises alarms instantaneously to end users
whenever a potential commercial campaign is detected. Our detection method
integrates semantic analysis and posters' track records and utilizes the
special features of CQA websites largely different from those in other types of
forums such as microblogs or news reports. Our system is adaptive and
accommodates new evidence uncovered by the detection algorithms over time.
Validated with real-world trace data from a popular Chinese CQA website over a
period of three months, our system shows great potential towards adaptive
online detection of CQA spams.",algorithm detect unfair pricing website
http://arxiv.org/abs/1808.07359v1,"Recent works showed that websites can detect browser extensions that users
install and websites they are logged into. This poses significant privacy
risks, since extensions and Web logins that reflect user's behavior, can be
used to uniquely identify users on the Web. This paper reports on the first
large-scale behavioral uniqueness study based on 16,393 users who visited our
website. We test and detect the presence of 16,743 Chrome extensions, covering
28% of all free Chrome extensions. We also detect whether the user is connected
to 60 different websites.
  We analyze how unique users are based on their behavior, and find out that
54.86% of users that have installed at least one detectable extension are
unique; 19.53% of users are unique among those who have logged into one or more
detectable websites; and 89.23% are unique among users with at least one
extension and one login. We use an advanced fingerprinting algorithm and show
that it is possible to identify a user in less than 625 milliseconds by
selecting the most unique combinations of extensions.
  Because privacy extensions contribute to the uniqueness of users, we study
the trade-off between the amount of trackers blocked by such extensions and how
unique the users of these extensions are. We have found that privacy extensions
should be considered more useful than harmful. The paper concludes with
possible countermeasures.",algorithm detect unfair pricing website
http://arxiv.org/abs/1805.01217v2,"Terms of service of on-line platforms too often contain clauses that are
potentially unfair to the consumer. We present an experimental study where
machine learning is employed to automatically detect such potentially unfair
clauses. Results show that the proposed system could provide a valuable tool
for lawyers and consumers alike.",algorithm detect unfair pricing website
http://arxiv.org/abs/1708.01348v4,"While page views are often sold instantly through real-time auctions when
users visit websites, they can also be sold in advance via guaranteed
contracts. In this paper, we present a dynamic programming model to study how
an online publisher should optimally allocate and price page views between
guaranteed and spot markets. The problem is challenging because the allocation
and pricing of guaranteed contracts affect advertisers' purchase between the
two markets, and the terminal value of the model is endogenously determined by
the updated dual force of supply and demand in auctions. We take the
advertisers' purchasing behaviour into consideration, i.e., risk aversion and
stochastic demand arrivals, and present a scalable and efficient algorithm for
the optimal solution. The model is also empirically validated with a commercial
dataset. The experimental results show that selling page views via both
guaranteed contracts and auctions can increase the publisher's expected total
revenue, and the optimal pricing and allocation strategies are robust to
different market and advertiser types.",algorithm detect unfair pricing website
http://arxiv.org/abs/1308.1382v1,"We consider the optimal pricing problem for a model of the rich media
advertisement market, as well as other related applications. In this market,
there are multiple buyers (advertisers), and items (slots) that are arranged in
a line such as a banner on a website. Each buyer desires a particular number of
{\em consecutive} slots and has a per-unit-quality value $v_i$ (dependent on
the ad only) while each slot $j$ has a quality $q_j$ (dependent on the position
only such as click-through rate in position auctions). Hence, the valuation of
the buyer $i$ for item $j$ is $v_iq_j$. We want to decide the allocations and
the prices in order to maximize the total revenue of the market maker.
  A key difference from the traditional position auction is the advertiser's
requirement of a fixed number of consecutive slots. Consecutive slots may be
needed for a large size rich media ad. We study three major pricing mechanisms,
the Bayesian pricing model, the maximum revenue market equilibrium model and an
envy-free solution model. Under the Bayesian model, we design a polynomial time
computable truthful mechanism which is optimum in revenue. For the market
equilibrium paradigm, we find a polynomial time algorithm to obtain the maximum
revenue market equilibrium solution. In envy-free settings, an optimal solution
is presented when the buyers have the same demand for the number of consecutive
slots. We conduct a simulation that compares the revenues from the above
schemes and gives convincing results.",algorithm detect unfair pricing website
http://arxiv.org/abs/cs/0108015v1,"Recent trends reveal the search by companies for a legal hook to prevent the
undesired and unauthorized copying of information posted on websites. In the
center of this controversy are metasites, websites that display prices for a
variety of vendors. Metasites function by implementing shopbots, which extract
pricing data from other vendors' websites. Technological mechanisms have proved
unsuccessful in blocking shopbots, and in response, websites have asserted a
variety of legal claims. Two recent cases, which rely on the troublesome
trespass to chattels doctrine, suggest that contract law may provide a less
demanding legal method of preventing the search of websites by data robots. If
blocking collection of pricing data is as simple as posting an online contract,
the question arises whether this end result is desirable and legally viable.",algorithm detect unfair pricing website
http://arxiv.org/abs/1905.07026v1,"Machine Learning techniques have become pervasive across a range of different
applications, and are now widely used in areas as disparate as recidivism
prediction, consumer credit-risk analysis and insurance pricing. The prevalence
of machine learning techniques has raised concerns about the potential for
learned algorithms to become biased against certain groups. Many definitions
have been proposed in the literature, but the fundamental task of reasoning
about probabilistic events is a challenging one, owing to the intractability of
inference.
  The focus of this paper is taking steps towards the application of tractable
models to fairness. Tractable probabilistic models have emerged that guarantee
that conditional marginal can be computed in time linear in the size of the
model. In particular, we show that sum product networks (SPNs) enable an
effective technique for determining the statistical relationships between
protected attributes and other training variables. If a subset of these
training variables are found by the SPN to be independent of the training
attribute then they can be considered `safe' variables, from which we can train
a classification model without concern that the resulting classifier will
result in disparate outcomes for different demographic groups.
  Our initial experiments on the `German Credit' data set indicate that this
processing technique significantly reduces disparate treatment of male and
female credit applicants, with a small reduction in classification accuracy
compared to state of the art. We will also motivate the concept of ""fairness
through percentile equivalence"", a new definition predicated on the notion that
individuals at the same percentile of their respective distributions should be
treated equivalently, and this prevents unfair penalisation of those
individuals who lie at the extremities of their respective distributions.",algorithm detect unfair pricing website
http://arxiv.org/abs/1811.09539v1,"Machine learning is becoming an ever present part in our lives as many
decisions, e.g. to lend a credit, are no longer made by humans but by machine
learning algorithms. However those decisions are often unfair and
discriminating individuals belonging to protected groups based on race or
gender. With the recent General Data Protection Regulation (GDPR) coming into
effect, new awareness has been raised for such issues and with computer
scientists having such a large impact on peoples lives it is necessary that
actions are taken to discover and prevent discrimination. This work aims to
give an introduction into discrimination, legislative foundations to counter it
and strategies to detect and prevent machine learning algorithms from showing
such behavior.",algorithm detect unfair pricing website
http://arxiv.org/abs/1708.07607v3,"We study the problem of allocating impressions to sellers in e-commerce
websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue
generated by the platform. We employ a general framework of reinforcement
mechanism design, which uses deep reinforcement learning to design efficient
algorithms, taking the strategic behaviour of the sellers into account.
Specifically, we model the impression allocation problem as a Markov decision
process, where the states encode the history of impressions, prices,
transactions and generated revenue and the actions are the possible impression
allocations in each round. To tackle the problem of continuity and
high-dimensionality of states and actions, we adopt the ideas of the DDPG
algorithm to design an actor-critic policy gradient algorithm which takes
advantage of the problem domain in order to achieve convergence and stability.
We evaluate our proposed algorithm, coined IA(GRU), by comparing it against
DDPG, as well as several natural heuristics, under different rationality models
for the sellers - we assume that sellers follow well-known no-regret type
strategies which may vary in their degree of sophistication. We find that
IA(GRU) outperforms all algorithms in terms of the total revenue.",algorithm detect unfair pricing website
http://arxiv.org/abs/1711.05098v1,"Recent industry reports assure the rise of web robots which comprise more
than half of the total web traffic. They not only threaten the security,
privacy and efficiency of the web but they also distort analytics and metrics,
doubting the veracity of the information being promoted. In the academic
publishing domain, this can cause articles to be faulty presented as prominent
and influential. In this paper, we present our approach on detecting web robots
in academic publishing websites. We use different supervised learning
algorithms with a variety of characteristics deriving from both the log files
of the server and the content served by the website. Our approach relies on the
assumption that human users will be interested in specific domains or articles,
while web robots crawl a web library incoherently. We experiment with features
adopted in previous studies with the addition of novel semantic characteristics
which derive after performing a semantic analysis using the Latent Dirichlet
Allocation (LDA) algorithm. Our real-world case study shows promising results,
pinpointing the significance of semantic features in the web robot detection
problem.",algorithm detect unfair pricing website
http://arxiv.org/abs/1002.1185v1,"There is a considerable body of work on sequence mining of Web Log Data. We
are using One Pass frequent Episode discovery (or FED) algorithm, takes a
different approach than the traditional apriori class of pattern detection
algorithms. In this approach significant intervals for each Website are
computed first (independently) and these interval used for detecting frequent
patterns/Episode and then the Analysis is performed on Significant Intervals
and frequent patterns That can be used to forecast the user's behavior using
previous trends and this can be also used for advertising purpose. This type of
applications predicts the Website interest. In this approach, time-series data
are folded over a periodicity (day, week, etc.) Which are used to form the
Interval? Significant intervals are discovered from these time points that
satisfy the criteria of minimum confidence and maximum interval length
specified by the user.",algorithm detect unfair pricing website
http://arxiv.org/abs/1410.5111v1,"The vulnerability of false data injection attacks on real-time electricity
pricing for the power grid market has been recently explored. Previous work has
focused on the impact caused by attackers that compromise pricing signals and
send false prices to a subset of consumers. In this paper we extend previous
work by considering a more powerful and general adversary model, a new analysis
method based on sensitivity functions, and by proposing several countermeasures
that can mitigate the negative impact of these attacks. Countermeasures include
adding a low-pass filter to the pricing signals, selecting the time interval
between price updates, selecting parameters of the controller, designing robust
control algorithms, and by detecting anomalies in the behavior of the system.",algorithm detect unfair pricing website
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",price discrimination
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination
http://arxiv.org/abs/1804.00480v1,"We consider the simplest and most fundamental problem of selling a single
item to a number of buyers whose values are drawn from known independent and
regular distributions. There are four most widely-used and widely-studied
mechanisms in this literature: Anonymous Posted-Pricing (AP), Second-Price
Auction with Anonymous Reserve (AR), Sequential Posted-Pricing (SPM), and
Myerson Auction (OPT). Myerson Auction is optimal but complicated, which also
suffers a few issues in practice such as fairness; AP is the simplest one but
its revenue is also the lowest among these four; AR and SPM are of intermediate
complexity and revenue. We study the revenue gaps among these four mechanisms,
which is defined as the largest ratio between revenues from two mechanisms. We
establish two tight ratios and one tighter bound:
  1. SPM/AP. This ratio studies the power of discrimination in pricing schemes.
We obtain the tight ratio of roughly $2.62$, closing the previous known bounds
$[e / (e - 1), e]$.
  2. AR/AP. This ratio studies the relative power of auction vs. pricing
schemes, when no discrimination is allowed. We get the tight ratio of $\pi^2 /
6 \approx 1.64$, closing the previous known bounds $[e / (e - 1), e]$.
  3. OPT/AR. This ratio studies the power of discrimination in auctions.
Previously, the revenue gap is known to be in interval $[2, e]$, and the
lower-bound of $2$ is conjectured to be tight~\cite{HR09,H13,AHNPY15}. We
disprove this conjecture by obtaining a better lower-bound of $2.15$.",price discrimination
http://arxiv.org/abs/1803.06797v5,"I consider the optimal hourly (or per-unit-time in general) pricing problem
faced by a freelance worker (or a service provider) on an on-demand service
platform. Service requests arriving while the worker is busy are lost forever.
Thus, the optimal hourly prices need to capture the average hourly opportunity
costs incurred by accepting jobs. Due to potential asymmetries in these costs,
price discrimination across jobs based on duration, characteristics of the
arrival process, etc., may be necessary for optimality, even if the customers'
hourly willingness to pay is identically distributed. I first establish that
such price discrimination is not necessary if the customer arrival process is
Poisson: in this case, the optimal policy charges an identical hourly rate for
all jobs. This result holds even if the earnings are discounted over time. I
then consider the case where the customers belong to different classes that are
differentiated in their willingness to pay. I present a simple and practical
iterative procedure to compute the optimal prices in this case under standard
regularity assumptions on the distributions of customer valuations. I finally
show that these insights continue to hold in the presence of competition
between multiple quality-differentiated workers, assuming a natural customer
choice model in which a customer always chooses the best available worker that
she can afford.",price discrimination
http://arxiv.org/abs/1405.5189v3,"There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling.",price discrimination
http://arxiv.org/abs/1710.10695v1,"There has been a great effort to transfer linear discriminant techniques that
operate on vector data to high-order data, generally referred to as Multilinear
Discriminant Analysis (MDA) techniques. Many existing works focus on maximizing
the inter-class variances to intra-class variances defined on tensor data
representations. However, there has not been any attempt to employ
class-specific discrimination criteria for the tensor data. In this paper, we
propose a multilinear subspace learning technique suitable for applications
requiring class-specific tensor models. The method maximizes the discrimination
of each individual class in the feature space while retains the spatial
structure of the input. We evaluate the efficiency of the proposed method on
two problems, i.e. facial image analysis and stock price prediction based on
limit order book data.",price discrimination
http://arxiv.org/abs/1703.06660v1,"We present in this work an economic analysis of ransomware, with relevant
data from Cryptolocker, CryptoWall, TeslaCrypt and other major strands. We
include a detailed study of the impact that different price discrimination
strategies can have on the success of a ransomware family, examining uniform
pricing, optimal price discrimination and bargaining strategies and analysing
their advantages and limitations. In addition, we present results of a
preliminary survey that can helps in estimating an optimal ransom value. We
discuss at each stage whether the different schemes we analyse have been
encountered already in existing malware, and the likelihood of them being
implemented and becoming successful. We hope this work will help to gain some
useful insights for predicting how ransomware may evolve in the future and be
better prepared to counter its current and future threat.",price discrimination
http://arxiv.org/abs/1411.1381v1,"We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or ""buy-it-now"" pricing (BIN),
and ""pay-per-play"" (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0.",price discrimination
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination
http://arxiv.org/abs/1703.00980v3,"This paper analyzes the impact of peer effects on electricity consumption of
a network of rational, utility-maximizing users. Users derive utility from
consuming electricity as well as consuming less energy than their neighbors.
However, a disutility is incurred for consuming more than their neighbors. To
maximize the profit of the load-serving entity that provides electricity to
such users, we develop a two-stage game-theoretic model, where the entity sets
the prices in the first stage. In the second stage, consumers decide on their
demand in response to the observed price set in the first stage so as to
maximize their utility. To this end, we derive theoretical statements under
which such peer effects reduce aggregate user consumption. Further, we obtain
expressions for the resulting electricity consumption and profit of the load
serving entity for the case of perfect price discrimination and a single price
under complete information, and approximations under incomplete information.
Simulations suggest that exposing only a selected subset of all users to peer
effects maximizes the entity's profit.",price discrimination
http://arxiv.org/abs/cs/0508028v1,"We present a mechanism for reservations of bursty resources that is both
truthful and robust. It consists of option contracts whose pricing structure
induces users to reveal the true likelihoods that they will purchase a given
resource. Users are also allowed to adjust their options as their likelihood
changes. This scheme helps users save cost and the providers to plan ahead so
as to reduce the risk of under-utilization and overbooking. The mechanism
extracts revenue similar to that of a monopoly provider practicing temporal
pricing discrimination with a user population whose preference distribution is
known in advance.",price discrimination
http://arxiv.org/abs/1304.6806v2,"We study scenarios where multiple sellers of a homogeneous good compete on
prices, where each seller can only sell to some subset of the buyers.
Crucially, sellers cannot price-discriminate between buyers. We model the
structure of the competition by a graph (or hyper-graph), with nodes
representing the sellers and edges representing populations of buyers. We study
equilibria in the game between the sellers, prove that they always exist, and
present various structural, quantitative, and computational results about them.
We also analyze the equilibria completely for a few cases. Many questions are
left open.",price discrimination
http://arxiv.org/abs/1706.01131v2,"We study the optimal pricing strategy of a monopolist selling homogeneous
goods to customers over multiple periods. The customers choose their time of
purchase to maximize their payoff that depends on their valuation of the
product, the purchase price, and the utility they derive from past purchases of
others, termed the network effect. We first show that the optimal price
sequence is non-decreasing. Therefore, by postponing purchase to future rounds,
customers trade-off a higher utility from the network effects with a higher
price. We then show that a customer's equilibrium strategy can be characterized
by a threshold rule in which at each round a customer purchases the product if
and only if her valuation exceeds a certain threshold. This implies that
customers face an inference problem regarding the valuations of others, i.e.,
observing that a customer has not yet purchased the product, signals that her
valuation is below a threshold. We consider a block model of network
interactions, where there are blocks of buyers subject to the same network
effect. A natural benchmark, this model allows us to provide an explicit
characterization of the optimal price sequence asymptotically as the number of
agents goes to infinity, which notably is linearly increasing in time with a
slope that depends on the network effect through a scalar given by the sum of
entries of the inverse of the network weight matrix. Our characterization shows
that increasing the ""imbalance"" in the network defined as the difference
between the in and out degree of the nodes increases the revenue of the
monopolist. We further study the effects of price discrimination and show that
in earlier periods monopolist offers lower prices to blocks with higher
Bonacich centrality to encourage them to purchase, which in turn further
incentivizes other customers to buy in subsequent periods.",price discrimination
http://arxiv.org/abs/1903.11469v1,"Increased data gathering capacity, together with the spreading of data
analytics techniques, has allowed an unprecedented concentration of information
related to the individuals' preferences in the hands of a few gatekeepers. In
such context, the traditional economic literature has been attempting to frame
all the data-driven economy features. Such features, although being able to
bring about a more efficient matching of people and relevant purchase
opportunities, also result into distortions and disequilibria, up to market
failures. Data-economy market disequilibria can be decrypted by leveraging on
some of the known network properties, thus obtaining general results suitable
for building a new theoretical framework for economic phenomena. Starting from
the hypothesis that a digital company can always benefit from an underlying
network of consumers or items related to its market, their representation can
indeed provide significant competitive advantages, also enhancing the
platforms' capacity to implement discriminatory practices by means of an
increased ability to estimate individuals' preferences. In the present paper,
we propose a measure called Information Patrimony, considering the amount of
information available within the system and we look into how platforms may
exploit data stemming from connected profiles within a network, with a view to
obtaining competitive advantages. Such information flow may eventually allow to
envisage the emergence of a new hybrid price discrimination pattern, through
which platforms may influence and steer individuals' purchase choices, as well
as to apply different prices to different customers.",price discrimination
http://arxiv.org/abs/1708.00754v1,"Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.",price discrimination
http://arxiv.org/abs/1803.04376v2,"One property that remains lacking in image captions generated by contemporary
methods is discriminability: being able to tell two images apart given the
caption for one of them. We propose a way to improve this aspect of caption
generation. By incorporating into the captioning training objective a loss
component directly related to ability (by a machine) to disambiguate
image/caption matches, we obtain systems that produce much more discriminative
caption, according to human evaluation. Remarkably, our approach leads to
improvement in other aspects of generated captions, reflected by a battery of
standard scores such as BLEU, SPICE etc. Our approach is modular and can be
applied to a variety of model/loss combinations commonly proposed for image
captioning.",price discrimination
http://arxiv.org/abs/1805.03403v1,"Unlike traditional learning to rank models that depend on hand-crafted
features, neural representation learning models learn higher level features for
the ranking task by training on large datasets. Their ability to learn new
features directly from the data, however, may come at a price. Without any
special supervision, these models learn relationships that may hold only in the
domain from which the training data is sampled, and generalize poorly to
domains not observed during training. We study the effectiveness of adversarial
learning as a cross domain regularizer in the context of the ranking task. We
use an adversarial discriminator and train our neural ranking model on a small
set of domains. The discriminator provides a negative feedback signal to
discourage the model from learning domain specific representations. Our
experiments show consistently better performance on held out domains in the
presence of the adversarial discriminator---sometimes up to 30% on precision@1.",price discrimination
http://arxiv.org/abs/1302.1105v1,"Discussions of the economics of scholarly communication are usually devoted
to Open Access, rising journal prices, publisher profits, and boycotts. That
ignores what seems a much more important development in this market.
Publishers, through the oft-reviled ""Big Deal"" packages, are providing much
greater and more egalitarian access to the journal literature, an approximation
to true Open Access. In the process they are also marginalizing libraries, and
obtaining a greater share of the resources going into scholarly communication.
This is enabling a continuation of publisher profits as well as of what for
decades has been called ""unsustainable journal price escalation"". It is also
inhibiting the spread of Open Access, and potentially leading to an oligopoly
of publishers controlling distribution through large-scale licensing.
  The ""Big Deal"" practices are worth studying for several general reasons. The
degree to which publishers succeed in diminishing the role of libraries may be
an indicator of the degree and speed at which universities transform
themselves. More importantly, these ""Big Deals"" appear to point the way to the
future of the whole economy, where progress is characterized by declining
privacy, increasing price discrimination, increasing opaqueness in pricing,
increasing reliance on low-paid or upaid work of others for profits, and
business models that depend on customer inertia.",price discrimination
http://arxiv.org/abs/cs/0308037v1,"A very complex vision system is developed to detect luminosity variations
connected with the discovery of new planets in the Universe. The traditional
imaging system can not manage a so large load. A private net is implemented to
perform an automatic vision and decision architecture. It lets to carry out an
on-line discrimination of interesting events by using two levels of triggers.
This system can even manage many Tbytes of data per day. The architecture
avails itself of a distributed parallel network system based on a maximum of
256 standard workstations with Microsoft Window as OS.",price discrimination
http://arxiv.org/abs/1905.05922v1,"An effective way for a Mobile network operator (MNO) to improve its revenue
is price discrimination, i.e., providing different combinations of data caps
and subscription fees. Rollover data plan (allowing the unused data in the
current month to be used in the next month) is an innovative data mechanism
with time flexibility. In this paper, we study the MNO's optimal multi-cap data
plans with time flexibility in a realistic asymmetric information scenario.
Specifically, users are associated with multi-dimensional private information,
and the MNO designs a contract (with different data caps and subscription fees)
to induce users to truthfully reveal their private information. This problem is
quite challenging due to the multi-dimensional private information. We address
the challenge in two aspects. First, we find that a feasible contract
(satisfying incentive compatibility and individual rationality) should allocate
the data caps according to users' willingness-to-pay (captured by the slopes of
users' indifference curves). Second, for the non-convex data cap allocation
problem, we propose a Dynamic Quota Allocation Algorithm, which has a low
complexity and guarantees the global optimality. Numerical results show that
the time-flexible data mechanisms increase both the MNO's profit (25% on
average) and users' payoffs (8.2% on average) under price discrimination.",price discrimination
http://arxiv.org/abs/1006.3894v1,"Hahn and Wallsten wrote that network neutrality ""usually means that broadband
service providers charge consumers only once for Internet access, do not favor
one content provider over another, and do not charge content providers for
sending information over broadband lines to end users."" In this paper we study
the implications of non-neutral behaviors under a simple model of linear
demand-response to usage-based prices. We take into account advertising
revenues and consider both cooperative and non-cooperative scenarios. In
particular, we model the impact of side-payments between service and content
providers. We also consider the effect of service discrimination by access
providers, as well as an extension of our model to non-monopolistic content
providers.",price discrimination
http://arxiv.org/abs/cs/0308037v1,"A very complex vision system is developed to detect luminosity variations
connected with the discovery of new planets in the Universe. The traditional
imaging system can not manage a so large load. A private net is implemented to
perform an automatic vision and decision architecture. It lets to carry out an
on-line discrimination of interesting events by using two levels of triggers.
This system can even manage many Tbytes of data per day. The architecture
avails itself of a distributed parallel network system based on a maximum of
256 standard workstations with Microsoft Window as OS.",price discrimination detection
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination detection
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination detection
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination detection
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination detection
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination detection
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",price discrimination detection
http://arxiv.org/abs/1508.03928v1,"In this paper, we propose a novel deep neural network framework embedded with
low-level features (LCNN) for salient object detection in complex images. We
utilise the advantage of convolutional neural networks to automatically learn
the high-level features that capture the structured information and semantic
context in the image. In order to better adapt a CNN model into the saliency
task, we redesign the network architecture based on the small-scale datasets.
Several low-level features are extracted, which can effectively capture
contrast and spatial information in the salient regions, and incorporated to
compensate with the learned high-level features at the output of the last fully
connected layer. The concatenated feature vector is further fed into a
hinge-loss SVM detector in a joint discriminative learning manner and the final
saliency score of each region within the bounding box is obtained by the linear
combination of the detector's weights. Experiments on three challenging
benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be
effective and superior than most low-level oriented state-of-the-arts in terms
of P-R curves, F-measure and mean absolute errors.",price discrimination detection
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination detection
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",price discrimination detection
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination detection
http://arxiv.org/abs/1804.00480v1,"We consider the simplest and most fundamental problem of selling a single
item to a number of buyers whose values are drawn from known independent and
regular distributions. There are four most widely-used and widely-studied
mechanisms in this literature: Anonymous Posted-Pricing (AP), Second-Price
Auction with Anonymous Reserve (AR), Sequential Posted-Pricing (SPM), and
Myerson Auction (OPT). Myerson Auction is optimal but complicated, which also
suffers a few issues in practice such as fairness; AP is the simplest one but
its revenue is also the lowest among these four; AR and SPM are of intermediate
complexity and revenue. We study the revenue gaps among these four mechanisms,
which is defined as the largest ratio between revenues from two mechanisms. We
establish two tight ratios and one tighter bound:
  1. SPM/AP. This ratio studies the power of discrimination in pricing schemes.
We obtain the tight ratio of roughly $2.62$, closing the previous known bounds
$[e / (e - 1), e]$.
  2. AR/AP. This ratio studies the relative power of auction vs. pricing
schemes, when no discrimination is allowed. We get the tight ratio of $\pi^2 /
6 \approx 1.64$, closing the previous known bounds $[e / (e - 1), e]$.
  3. OPT/AR. This ratio studies the power of discrimination in auctions.
Previously, the revenue gap is known to be in interval $[2, e]$, and the
lower-bound of $2$ is conjectured to be tight~\cite{HR09,H13,AHNPY15}. We
disprove this conjecture by obtaining a better lower-bound of $2.15$.",price discrimination detection
http://arxiv.org/abs/1505.01546v2,"There is a key research issue to accurately select out neutron signals and
discriminate gamma signals from a mixed radiation field in the neutron
detection. This paper proposes a fractal spectrum discrimination approach by
means of different spectrum characteristics of neutron and gamma. Figure of
merit and average discriminant error ratio are adopted together to evaluate the
discriminant effects. Different neutron and gamma signals with various noises
and pulse pile-ups are simulated according to real data in the literature. The
proposed approach is compared with the digital charge integration and pulse
gradient methods. It is found that the fractal approach exhibits the best
discriminant performance among three methods. The fractal spectrum approach is
not sensitive to the high frequency noises and pulse pile-ups. It means that
the proposed approach takes the advantages of anti-noises and high discriminant
ability, and can be used to better discriminate neutron and gamma in neutron
detection.",price discrimination detection
http://arxiv.org/abs/1803.06797v5,"I consider the optimal hourly (or per-unit-time in general) pricing problem
faced by a freelance worker (or a service provider) on an on-demand service
platform. Service requests arriving while the worker is busy are lost forever.
Thus, the optimal hourly prices need to capture the average hourly opportunity
costs incurred by accepting jobs. Due to potential asymmetries in these costs,
price discrimination across jobs based on duration, characteristics of the
arrival process, etc., may be necessary for optimality, even if the customers'
hourly willingness to pay is identically distributed. I first establish that
such price discrimination is not necessary if the customer arrival process is
Poisson: in this case, the optimal policy charges an identical hourly rate for
all jobs. This result holds even if the earnings are discounted over time. I
then consider the case where the customers belong to different classes that are
differentiated in their willingness to pay. I present a simple and practical
iterative procedure to compute the optimal prices in this case under standard
regularity assumptions on the distributions of customer valuations. I finally
show that these insights continue to hold in the presence of competition
between multiple quality-differentiated workers, assuming a natural customer
choice model in which a customer always chooses the best available worker that
she can afford.",price discrimination detection
http://arxiv.org/abs/1405.5189v3,"There are two major ways of selling impressions in display advertising. They
are either sold in spot through auction mechanisms or in advance via guaranteed
contracts. The former has achieved a significant automation via real-time
bidding (RTB); however, the latter is still mainly done over the counter
through direct sales. This paper proposes a mathematical model that allocates
and prices the future impressions between real-time auctions and guaranteed
contracts. Under conventional economic assumptions, our model shows that the
two ways can be seamless combined programmatically and the publisher's revenue
can be maximized via price discrimination and optimal allocation. We consider
advertisers are risk-averse, and they would be willing to purchase guaranteed
impressions if the total costs are less than their private values. We also
consider that an advertiser's purchase behavior can be affected by both the
guaranteed price and the time interval between the purchase time and the
impression delivery date. Our solution suggests an optimal percentage of future
impressions to sell in advance and provides an explicit formula to calculate at
what prices to sell. We find that the optimal guaranteed prices are dynamic and
are non-decreasing over time. We evaluate our method with RTB datasets and find
that the model adopts different strategies in allocation and pricing according
to the level of competition. From the experiments we find that, in a less
competitive market, lower prices of the guaranteed contracts will encourage the
purchase in advance and the revenue gain is mainly contributed by the increased
competition in future RTB. In a highly competitive market, advertisers are more
willing to purchase the guaranteed contracts and thus higher prices are
expected. The revenue gain is largely contributed by the guaranteed selling.",price discrimination detection
http://arxiv.org/abs/1710.10695v1,"There has been a great effort to transfer linear discriminant techniques that
operate on vector data to high-order data, generally referred to as Multilinear
Discriminant Analysis (MDA) techniques. Many existing works focus on maximizing
the inter-class variances to intra-class variances defined on tensor data
representations. However, there has not been any attempt to employ
class-specific discrimination criteria for the tensor data. In this paper, we
propose a multilinear subspace learning technique suitable for applications
requiring class-specific tensor models. The method maximizes the discrimination
of each individual class in the feature space while retains the spatial
structure of the input. We evaluate the efficiency of the proposed method on
two problems, i.e. facial image analysis and stock price prediction based on
limit order book data.",price discrimination detection
http://arxiv.org/abs/1703.06660v1,"We present in this work an economic analysis of ransomware, with relevant
data from Cryptolocker, CryptoWall, TeslaCrypt and other major strands. We
include a detailed study of the impact that different price discrimination
strategies can have on the success of a ransomware family, examining uniform
pricing, optimal price discrimination and bargaining strategies and analysing
their advantages and limitations. In addition, we present results of a
preliminary survey that can helps in estimating an optimal ransom value. We
discuss at each stage whether the different schemes we analyse have been
encountered already in existing malware, and the likelihood of them being
implemented and becoming successful. We hope this work will help to gain some
useful insights for predicting how ransomware may evolve in the future and be
better prepared to counter its current and future threat.",price discrimination detection
http://arxiv.org/abs/1906.05740v2,"Information transfer between time series is calculated by using the
asymmetric information-theoretic measure known as transfer entropy. Geweke's
autoregressive formulation of Granger causality is used to find linear transfer
entropy, and Schreiber's general, non-parametric, information-theoretic
formulation is used to detect non-linear transfer entropy.
  We first validate these measures against synthetic data. Then we apply these
measures to detect causality between social sentiment and cryptocurrency
prices. We perform significance tests by comparing the information transfer
against a null hypothesis, determined via shuffled time series, and calculate
the Z-score. We also investigate different approaches for partitioning in
nonparametric density estimation which can improve the significance of results.
  Using these techniques on sentiment and price data over a 48-month period to
August 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple
(XRP), litecoin (LTC) and ethereum (ETH), we detect significant information
transfer, on hourly timescales, in directions of both sentiment to price and of
price to sentiment. We report the scale of non-linear causality to be an order
of magnitude greater than linear causality.",price discrimination detection
http://arxiv.org/abs/1411.1381v1,"We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or ""buy-it-now"" pricing (BIN),
and ""pay-per-play"" (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0.",price discrimination detection
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination detection
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination detection
http://arxiv.org/abs/1703.00980v3,"This paper analyzes the impact of peer effects on electricity consumption of
a network of rational, utility-maximizing users. Users derive utility from
consuming electricity as well as consuming less energy than their neighbors.
However, a disutility is incurred for consuming more than their neighbors. To
maximize the profit of the load-serving entity that provides electricity to
such users, we develop a two-stage game-theoretic model, where the entity sets
the prices in the first stage. In the second stage, consumers decide on their
demand in response to the observed price set in the first stage so as to
maximize their utility. To this end, we derive theoretical statements under
which such peer effects reduce aggregate user consumption. Further, we obtain
expressions for the resulting electricity consumption and profit of the load
serving entity for the case of perfect price discrimination and a single price
under complete information, and approximations under incomplete information.
Simulations suggest that exposing only a selected subset of all users to peer
effects maximizes the entity's profit.",price discrimination detection
http://arxiv.org/abs/cs/0508028v1,"We present a mechanism for reservations of bursty resources that is both
truthful and robust. It consists of option contracts whose pricing structure
induces users to reveal the true likelihoods that they will purchase a given
resource. Users are also allowed to adjust their options as their likelihood
changes. This scheme helps users save cost and the providers to plan ahead so
as to reduce the risk of under-utilization and overbooking. The mechanism
extracts revenue similar to that of a monopoly provider practicing temporal
pricing discrimination with a user population whose preference distribution is
known in advance.",price discrimination detection
http://arxiv.org/abs/1304.6806v2,"We study scenarios where multiple sellers of a homogeneous good compete on
prices, where each seller can only sell to some subset of the buyers.
Crucially, sellers cannot price-discriminate between buyers. We model the
structure of the competition by a graph (or hyper-graph), with nodes
representing the sellers and edges representing populations of buyers. We study
equilibria in the game between the sellers, prove that they always exist, and
present various structural, quantitative, and computational results about them.
We also analyze the equilibria completely for a few cases. Many questions are
left open.",price discrimination detection
http://arxiv.org/abs/0905.2480v1,"We empirically investigate fluctuations in product prices in online markets
by using a tick-by-tick price data collected from a Japanese price comparison
site, and find some similarities and differences between product and asset
prices. The average price of a product across e-retailers behaves almost like a
random walk, although the probability of price increase/decrease is higher
conditional on the multiple events of price increase/decrease. This is quite
similar to the property reported by previous studies about asset prices.
However, we fail to find a long memory property in the volatility of product
price changes. Also, we find that the price change distribution for product
prices is close to an exponential distribution, rather than a power law
distribution. These two findings are in a sharp contrast with the previous
results regarding asset prices. We propose an interpretation that these
differences may stem from the absence of speculative activities in product
markets; namely, e-retailers seldom repeat buy and sell of a product, unlike
traders in asset markets.",product price change
http://arxiv.org/abs/physics/0607151v1,"We investigate an economic system in which one large agent - the Japan
government changes the environment of numerous smaller agents - the Japan
agriculture producers by indirect regulation of prices of agriculture goods.
The reason for this intervention was that before the oil crisis in 1974 Japan
agriculture production prices exhibited irregular and large amplitude changes.
By means of analysis of correlations and a combination of singular spectrum
analysis (SSA), principal component analysis (PCA), and time delay phase space
construction (TDPSC) we study the influence of the government measures on the
domestic piglet prices and production in Japan. We show that the government
regulation politics was successful and leaded (i) to a decrease of the
nonstationarities and to increase of predictability of the piglet price; (ii)
to a coupling of the price and production cycles; (iii) to increase of
determinism of the dynamics of the fluctuations of piglet price around the year
average price. The investigated case is an example confirming the thesis that a
large agent can change in a significant way the environment of the small agents
in complex (economic or financial) systems which can be crucial for their
survival or extinction.",product price change
http://arxiv.org/abs/1304.4618v1,"We show an auction-based algorithm to compute market equilibrium prices in a
production model, where consumers purchase items under separable nonlinear
utility concave functions which satisfy W.G.S(Weak Gross Substitutes);
producers produce items with multiple linear production constraints. Our
algorithm differs from previous approaches in that the prices are allowed to
both increase and decrease to handle changes in the production. This provides a
t^atonnement style algorithm which converges and provides a PTAS. The algorithm
can also be extended to arbitrary convex production regions and the
Arrow-Debreu model. The convergence is dependent on the behavior of the
marginal utility of the concave function.",product price change
http://arxiv.org/abs/1701.03537v2,"We consider a firm that sells a large number of products to its customers in
an online fashion. Each product is described by a high dimensional feature
vector, and the market value of a product is assumed to be linear in the values
of its features. Parameters of the valuation model are unknown and can change
over time. The firm sequentially observes a product's features and can use the
historical sales data (binary sale/no sale feedbacks) to set the price of
current product, with the objective of maximizing the collected revenue. We
measure the performance of a dynamic pricing policy via regret, which is the
expected revenue loss compared to a clairvoyant that knows the sequence of
model parameters in advance.
  We propose a pricing policy based on projected stochastic gradient descent
(PSGD) and characterize its regret in terms of time $T$, features dimension
$d$, and the temporal variability in the model parameters, $\delta_t$. We
consider two settings. In the first one, feature vectors are chosen
antagonistically by nature and we prove that the regret of PSGD pricing policy
is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}\delta_t)$. In the second
setting (referred to as stochastic features model), the feature vectors are
drawn independently from an unknown distribution. We show that in this case,
the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T
t\delta_t/d)$.",product price change
http://arxiv.org/abs/1906.02635v1,"This paper proposes a method for estimating consumer preferences among
discrete choices, where the consumer chooses at most one product in a category,
but selects from multiple categories in parallel. The consumer's utility is
additive in the different categories. Her preferences about product attributes
as well as her price sensitivity vary across products and are in general
correlated across products. We build on techniques from the machine learning
literature on probabilistic models of matrix factorization, extending the
methods to account for time-varying product attributes and products going out
of stock. We evaluate the performance of the model using held-out data from
weeks with price changes or out of stock products. We show that our model
improves over traditional modeling approaches that consider each category in
isolation. One source of the improvement is the ability of the model to
accurately estimate heterogeneity in preferences (by pooling information across
categories); another source of improvement is its ability to estimate the
preferences of consumers who have rarely or never made a purchase in a given
category in the training data. Using held-out data, we show that our model can
accurately distinguish which consumers are most price sensitive to a given
product. We consider counterfactuals such as personally targeted price
discounts, showing that using a richer model such as the one we propose
substantially increases the benefits of personalization in discounts.",product price change
http://arxiv.org/abs/1804.07557v1,"Goals to reduce carbon emissions and changing electricity prices due to
increasing penetrations of wind power generation affect the planning and
operation of district heating production systems. Through extensive
multivariate sensitivity analysis, this study estimates the robustness of
future cost-optimal heat production systems under changing electricity prices,
fuel cost and investment cost. Optimal production capacities are installed
choosing from a range of well-established production and storage technologies
including boilers, combined heat and power (CHP) units, power-to-heat
technologies and heat storages. The optimal heat production system is
characterized in three different electricity pricing scenarios: Historical,
wind power dominated and demand dominated. Coal CHP, large heat pumps and heat
storages dominate the optimal system if fossil fuels are allowed. Heat pumps
and storages take over if fossil fuels are excluded. The capacity allocation
between CHP and heat pumps is highly dependent on cost assumptions in the
fossil fuel scenario, but the optimal capacities become much more robust if
fossil fuels are not included. System cost becomes less robust in a fossil free
scenario. If the electricity pricing is dominated by wind power generation or
by the electricity demand, heat pumps become more favorable compared to
cogeneration units. The need for heat storage more than doubles, if fossil
fuels are not included, as the heating system becomes more closely coupled to
the electricity system.",product price change
http://arxiv.org/abs/1909.12227v1,"Financial markets have a vital role in the development of modern society.
They allow the deployment of economic resources. Changes in stock prices
reflect changes in the market. In this study, we focus on predicting stock
prices by deep learning model. This is a challenge task, because there is much
noise and uncertainty in information that is related to stock prices. So this
work uses sparse autoencoders with one-dimension (1-D) residual convolutional
networks which is a deep learning model, to de-noise the data. Long-short term
memory (LSTM) is then used to predict the stock price. The prices, indices and
macroeconomic variables in past are the features used to predict the next day's
price. Experiment results show that 1-D residual convolutional networks can
de-noise data and extract deep features better than a model that combines
wavelet transforms (WT) and stacked autoencoders (SAEs). In addition, we
compare the performances of model with two different forecast targets of stock
price: absolute stock price and price rate of change. The results show that
predicting stock price through price rate of change is better than predicting
absolute prices directly.",product price change
http://arxiv.org/abs/1706.00330v2,"The paradox of the energy transition is that the low marginal costs of new
renewable energy sources (RES) drag electricity prices down and discourage
investments in flexible productions that are needed to compensate for the lack
of dispatchability of the new RES. The energy transition thus discourages the
investments that are required for its own harmonious expansion. To investigate
how this paradox can be overcome, we argue that, under certain assumptions,
future electricity prices are rather accurately modeled from the residual load
obtained by subtracting non-flexible productions from the load. Armed with the
resulting economic indicator, we investigate future revenues for European power
plants with various degree of flexibility. We find that, if neither carbon
taxes nor fuel prices change, flexible productions would be financially
rewarded better and sooner if the energy transition proceeds faster but at more
or less constant total production, i.e. by reducing the production of thermal
power plants at the same rate as the RES production increases. Less flexible
productions, on the other hand, would see their revenue grow more moderately.
Our results indicate that a faster energy transition with a quicker withdrawal
of thermal power plants would reward flexible productions faster.",product price change
http://arxiv.org/abs/cs/0304032v1,"An acceleration in the growth of communications bandwidth in use and a rapid
reduction in bandwidth prices have not accompanied the U.S. economy's strong
performance in the second half of the 1990s. Overall U.S. bandwidth in use has
grown robustly throughout the 1990s, but growth has not significantly
accelerated in the second half of 1990s. Average prices for U.S. bandwidth in
use have fallen little in nominal terms in the second half of the 1990s. Policy
makers and policy analysts should recognize that institutional change, rather
than more competitors of established types, appears to be key to dramatic
improvements in bandwidth growth and prices. Such a development could provide a
significant additional impetus to aggregate growth and productivity.",product price change
http://arxiv.org/abs/1606.01371v3,"The assortment problem in revenue management is the problem of deciding which
subset of products to offer to consumers in order to maximise revenue. A simple
and natural strategy is to select the best assortment out of all those that are
constructed by fixing a threshold revenue $\pi$ and then choosing all products
with revenue at least $\pi$. This is known as the revenue-ordered assortments
strategy. In this paper we study the approximation guarantees provided by
revenue-ordered assortments when customers are rational in the following sense:
the probability of selecting a specific product from the set being offered
cannot increase if the set is enlarged. This rationality assumption, known as
regularity, is satisfied by almost all discrete choice models considered in the
revenue management and choice theory literature, and in particular by random
utility models. The bounds we obtain are tight and improve on recent results in
that direction, such as for the Mixed Multinomial Logit model by
Rusmevichientong et al. (2014). An appealing feature of our analysis is its
simplicity, as it relies only on the regularity condition.
  We also draw a connection between assortment optimisation and two pricing
problems called unit demand envy-free pricing and Stackelberg minimum spanning
tree: These problems can be restated as assortment problems under discrete
choice models satisfying the regularity condition, and moreover revenue-ordered
assortments correspond then to the well-studied uniform pricing heuristic. When
specialised to that setting, the general bounds we establish for
revenue-ordered assortments match and unify the best known results on uniform
pricing.",product price change
http://arxiv.org/abs/1009.1407v1,"In the insurance industry, spreadsheets have emerged as an invaluable tool to
for product pricing, because it is relatively straightforward to create and
maintain complex pricing models using Excel. In fact, Excel is often preferred
to ""hard-code"" whenever there are frequent changes to the calculations and
business logic which under-pin the pricing of an insurance product. However,
problems arise as soon as spreadsheets are deployed to end-users: version
control, security of intellectual property, and ensuring correct usage are
obvious issues; frequently, integration with other systems is also a
requirement. Zurich Financial Services Group is a leading financial services
provider; several possible solutions to these problems have been evaluated, and
EASA has been selected as the preferred technology. Other spreadsheet
collaboration approaches which were considered include Excel Services, and/or
custom-built software; however, EASA has provided clear benefits over these
strategies.",product price change
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",product price change
http://arxiv.org/abs/1708.09020v1,"As a firm varies the price of a product, consumers exhibit reference effects,
making purchase decisions based not only on the prevailing price but also the
product's price history. We consider the problem of learning such behavioral
patterns as a monopolist releases, markets, and prices products. This context
calls for pricing decisions that intelligently trade off between maximizing
revenue generated by a current product and probing to gain information for
future benefit. Due to dependence on price history, realized demand can reflect
delayed consequences of earlier pricing decisions. As such, inference entails
attribution of outcomes to prior decisions and effective exploration requires
planning price sequences that yield informative future outcomes. Despite the
considerable complexity of this problem, we offer a tractable systematic
approach. In particular, we frame the problem as one of reinforcement learning
and leverage Thompson sampling. We also establish a regret bound that provides
graceful guarantees on how performance improves as data is gathered and how
this depends on the complexity of the demand model. We illustrate merits of the
approach through simulations.",product price change
http://arxiv.org/abs/1301.6334v1,"We present the results of study of a possible relationship between the space
weather and terrestrial markets of agricultural products. It is shown that to
implement the possible effect of space weather on the terrestrial harvests and
prices, a simultaneous fulfillment of three conditions is required: 1)
sensitivity of local weather (cloud cover, atmospheric circulation) to the
state of space weather; 2) sensitivity of the area of specific agricultural
crops to the weather anomalies (belonging to the area of risk farming); 3)
relative isolation of the market, making it difficult to damp the price hikes
by the external food supplies. Four possible scenarios of the market response
to the modulations of local terrestrial weather via the solar activity are
described. The data sources and analysis methods applied to detect this
relationship are characterized. We describe the behavior of 22 European markets
during the medieval period, in particular, during the Maunder minimum
(1650-1715). We demonstrate a reliable manifestation of the influence of space
weather on prices, discovered in the statistics of intervals between the price
hikes and phase price asymmetry. We show that the effects of phase price
asymmetry persist even during the early modern period in the U.S. in the
production of the durum wheat. Within the proposed approach, we analyze the
statistics of depopulation in the eighteenth and nineteenth century Iceland,
induced by the famine due to a sharp livestock reduction owing to, in its turn,
the lack of foodstuff due to the local weather anomalies. A high statistical
significance of temporal matching of these events with the periods of extreme
solar activity is demonstrated. We discuss the possible consequences of the
observed global climate change in the formation of new areas of risk farming,
sensitive to space weather.",product price change
http://arxiv.org/abs/physics/0606064v1,"We analyse, following recent work of Roehner, changes in house prices for
both the UK and Ireland. We conclude that prices in London have reached a
tipping point and prices relative to inflation are set to fall over the next
few years. If inflation does not rise then a hard landing seems likely. House
prices in the Irish Republic are shown to have broken away from the moderate
rise still to be found in Northern Ireland and Dublin has emerged as another
global 'hot spot'. An evolution of Dublin house prices similar to that in
London can be anticipated. Keywords: Econophysics, house prices, real estate,
prediction PACS: 89.65.Gh, 89.90.+n",product price change
http://arxiv.org/abs/1909.12227v1,"Financial markets have a vital role in the development of modern society.
They allow the deployment of economic resources. Changes in stock prices
reflect changes in the market. In this study, we focus on predicting stock
prices by deep learning model. This is a challenge task, because there is much
noise and uncertainty in information that is related to stock prices. So this
work uses sparse autoencoders with one-dimension (1-D) residual convolutional
networks which is a deep learning model, to de-noise the data. Long-short term
memory (LSTM) is then used to predict the stock price. The prices, indices and
macroeconomic variables in past are the features used to predict the next day's
price. Experiment results show that 1-D residual convolutional networks can
de-noise data and extract deep features better than a model that combines
wavelet transforms (WT) and stacked autoencoders (SAEs). In addition, we
compare the performances of model with two different forecast targets of stock
price: absolute stock price and price rate of change. The results show that
predicting stock price through price rate of change is better than predicting
absolute prices directly.",price changes detection
http://arxiv.org/abs/0905.2480v1,"We empirically investigate fluctuations in product prices in online markets
by using a tick-by-tick price data collected from a Japanese price comparison
site, and find some similarities and differences between product and asset
prices. The average price of a product across e-retailers behaves almost like a
random walk, although the probability of price increase/decrease is higher
conditional on the multiple events of price increase/decrease. This is quite
similar to the property reported by previous studies about asset prices.
However, we fail to find a long memory property in the volatility of product
price changes. Also, we find that the price change distribution for product
prices is close to an exponential distribution, rather than a power law
distribution. These two findings are in a sharp contrast with the previous
results regarding asset prices. We propose an interpretation that these
differences may stem from the absence of speculative activities in product
markets; namely, e-retailers seldom repeat buy and sell of a product, unlike
traders in asset markets.",price changes detection
http://arxiv.org/abs/1709.01268v4,"Nowadays, with the availability of massive amount of trade data collected,
the dynamics of the financial markets pose both a challenge and an opportunity
for high frequency traders. In order to take advantage of the rapid, subtle
movement of assets in High Frequency Trading (HFT), an automatic algorithm to
analyze and detect patterns of price change based on transaction records must
be available. The multichannel, time-series representation of financial data
naturally suggests tensor-based learning algorithms. In this work, we
investigate the effectiveness of two multilinear methods for the mid-price
prediction problem against other existing methods. The experiments in a large
scale dataset which contains more than 4 millions limit orders show that by
utilizing tensor representation, multilinear models outperform vector-based
approaches and other competing ones.",price changes detection
http://arxiv.org/abs/1712.03031v1,"Price differentiation describes a marketing strategy to determine the price
of goods on the basis of a potential customer's attributes like location,
financial status, possessions, or behavior. Several cases of online price
differentiation have been revealed in recent years. For example, different
pricing based on a user's location was discovered for online office supply
chain stores and there were indications that offers for hotel rooms are priced
higher for Apple users compared to Windows users at certain online booking
websites. One potential source for relevant distinctive features are
\emph{system fingerprints}, i.\,e., a technique to recognize users' systems by
identifying unique attributes such as the source IP address or system
configuration. In this paper, we shed light on the ecosystem of pricing at
online platforms and aim to detect if and how such platform providers make use
of price differentiation based on digital system fingerprints. We designed and
implemented an automated price scanner capable of disguising itself as an
arbitrary system, leveraging real-world system fingerprints, and searched for
price differences related to different features (e.\,g., user location,
language setting, or operating system). This system allows us to explore price
differentiation cases and expose those characteristic features of a system that
may influence a product's price.",price changes detection
http://arxiv.org/abs/physics/0606064v1,"We analyse, following recent work of Roehner, changes in house prices for
both the UK and Ireland. We conclude that prices in London have reached a
tipping point and prices relative to inflation are set to fall over the next
few years. If inflation does not rise then a hard landing seems likely. House
prices in the Irish Republic are shown to have broken away from the moderate
rise still to be found in Northern Ireland and Dublin has emerged as another
global 'hot spot'. An evolution of Dublin house prices similar to that in
London can be anticipated. Keywords: Econophysics, house prices, real estate,
prediction PACS: 89.65.Gh, 89.90.+n",price changes detection
http://arxiv.org/abs/1512.03485v1,"Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.",price discrimination algorithm
http://arxiv.org/abs/1506.00682v5,"We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.",price discrimination algorithm
http://arxiv.org/abs/1001.0393v2,"Identical products being sold at different prices in different locations is a
common phenomenon. Price differences might occur due to various reasons such as
shipping costs, trade restrictions and price discrimination. To model such
scenarios, we supplement the classical Fisher model of a market by introducing
{\em transaction costs}. For every buyer $i$ and every good $j$, there is a
transaction cost of $\cij$; if the price of good $j$ is $p_j$, then the cost to
the buyer $i$ {\em per unit} of $j$ is $p_j + \cij$. This allows the same good
to be sold at different (effective) prices to different buyers.
  We provide a combinatorial algorithm that computes $\epsilon$-approximate
equilibrium prices and allocations in
$O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations -
where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum
of the budgets of all the buyers.",price discrimination algorithm
http://arxiv.org/abs/1708.00754v1,"Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.",price discrimination algorithm
http://arxiv.org/abs/1007.1501v2,"In revenue maximization of selling a digital product in a social network, the
utility of an agent is often considered to have two parts: a private valuation,
and linearly additive influences from other agents. We study the incomplete
information case where agents know a common distribution about others' private
valuations, and make decisions simultaneously. The ""rational behavior"" of
agents in this case is captured by the well-known Bayesian Nash equilibrium.
  Two challenging questions arise: how to compute an equilibrium and how to
optimize a pricing strategy accordingly to maximize the revenue assuming agents
follow the equilibrium? In this paper, we mainly focus on the natural model
where the private valuation of each agent is sampled from a uniform
distribution, which turns out to be already challenging.
  Our main result is a polynomial-time algorithm that can exactly compute the
equilibrium and the optimal price, when pairwise influences are non-negative.
If negative influences are allowed, computing any equilibrium even
approximately is PPAD-hard. Our algorithm can also be used to design an FPTAS
for optimizing discriminative price profile.",price discrimination algorithm
http://arxiv.org/abs/1507.02615v1,"For selling a single item to agents with independent but non-identically
distributed values, the revenue optimal auction is complex. With respect to it,
Hartline and Roughgarden (2009) showed that the approximation factor of the
second-price auction with an anonymous reserve is between two and four. We
consider the more demanding problem of approximating the revenue of the ex ante
relaxation of the auction problem by posting an anonymous price (while supplies
last) and prove that their worst-case ratio is e. As a corollary, the
upper-bound of anonymous pricing or anonymous reserves versus the optimal
auction improves from four to $e$. We conclude that, up to an $e$ factor,
discrimination and simultaneity are unimportant for driving revenue in
single-item auctions.",price discrimination algorithm
http://arxiv.org/abs/1307.4531v1,"After years of speculation, price discrimination in e-commerce driven by the
personal information that users leave (involuntarily) online, has started
attracting the attention of privacy researchers, regulators, and the press. In
our previous work we demonstrated instances of products whose prices varied
online depending on the location and the characteristics of perspective online
buyers. In an effort to scale up our study we have turned to crowd-sourcing.
Using a browser extension we have collected the prices obtained by an initial
set of 340 test users as they surf the web for products of their interest. This
initial dataset has permitted us to identify a set of online stores where price
variation is more pronounced. We have focused on this subset, and performed a
systematic crawl of their products and logged the prices obtained from
different vantage points and browser configurations. By analyzing this dataset
we see that there exist several retailers that return prices for the same
product that vary by 10%-30% whereas there also exist isolated cases that may
vary up to a multiplicative factor, e.g., x2. To the best of our efforts we
could not attribute the observed price gaps to currency, shipping, or taxation
differences.",price discrimination algorithm
http://arxiv.org/abs/1010.4281v1,"Recent results, establishing evidence of intractability for such restrictive
utility functions as additively separable, piecewise-linear and concave, under
both Fisher and Arrow-Debreu market models, have prompted the question of
whether we have failed to capture some essential elements of real markets,
which seem to do a good job of finding prices that maintain parity between
supply and demand.
  The main point of this paper is to show that even non-separable, quasiconcave
utility functions can be handled efficiently in a suitably chosen, though
natural, realistic and useful, market model; our model allows for perfect price
discrimination. Our model supports unique equilibrium prices and, for the
restriction to concave utilities, satisfies both welfare theorems.",price discrimination algorithm
http://arxiv.org/abs/1905.05922v1,"An effective way for a Mobile network operator (MNO) to improve its revenue
is price discrimination, i.e., providing different combinations of data caps
and subscription fees. Rollover data plan (allowing the unused data in the
current month to be used in the next month) is an innovative data mechanism
with time flexibility. In this paper, we study the MNO's optimal multi-cap data
plans with time flexibility in a realistic asymmetric information scenario.
Specifically, users are associated with multi-dimensional private information,
and the MNO designs a contract (with different data caps and subscription fees)
to induce users to truthfully reveal their private information. This problem is
quite challenging due to the multi-dimensional private information. We address
the challenge in two aspects. First, we find that a feasible contract
(satisfying incentive compatibility and individual rationality) should allocate
the data caps according to users' willingness-to-pay (captured by the slopes of
users' indifference curves). Second, for the non-convex data cap allocation
problem, we propose a Dynamic Quota Allocation Algorithm, which has a low
complexity and guarantees the global optimality. Numerical results show that
the time-flexible data mechanisms increase both the MNO's profit (25% on
average) and users' payoffs (8.2% on average) under price discrimination.",price discrimination algorithm
http://arxiv.org/abs/1508.05347v3,"Data as a commodity has always been purchased and sold. Recently, web
services that are data marketplaces have emerged that match data buyers with
data sellers. So far there are no guidelines how to price queries against a
database. We consider the recently proposed query-based pricing framework of
Koutris et al and ask the question of computing optimal input prices in this
framework by formulating a buyer utility model.
  We establish the interesting and deep equivalence between arbitrage-freeness
in the query-pricing framework and envy-freeness in pricing theory for
appropriately chosen buyer valuations. Given the approximation hardness results
from envy-free pricing we then develop logarithmic approximation pricing
algorithms exploiting the max flow interpretation of the arbitrage-free pricing
for the restricted query language proposed by Koutris et al. We propose a novel
polynomial-time logarithmic approximation pricing scheme and show that our new
scheme performs better than the existing envy-free pricing algorithms
instance-by-instance. We also present a faster pricing algorithm that is always
greater than the existing solutions, but worse than our previous scheme. We
experimentally show how our pricing algorithms perform with respect to the
existing envy-free pricing algorithms and to the optimal exponentially
computable solution, and our experiments show that our approximation algorithms
consistently arrive at about 99% of the optimal.",price discrimination algorithm
http://arxiv.org/abs/1609.06844v1,"In the quest for market mechanisms that are easy to implement, yet close to
optimal, few seem as viable as posted pricing. Despite the growing body of
impressive results, the performance of most posted price mechanisms however,
rely crucially on price discrimination when multiple copies of a good are
available. For the more general case with non-linear production costs on each
good, hardly anything is known for general multi-good markets. With this in
mind, we study a Bayesian setting where the seller can produce any number of
copies of a good but faces convex production costs for the same, and buyers
arrive sequentially. Our main contribution is a framework for
non-discriminatory pricing in the presence of production costs: the framework
yields posted price mechanisms with O(1)-approximation factors for fractionally
subadditive (XoS) buyers, logarithmic approximations for subadditive buyers,
and also extends to settings where the seller is oblivious to buyer valuations.
Our work presents the first known results for Bayesian settings with production
costs and is among the few posted price mechanisms that do not charge buyers
differently for the same good.",price discrimination algorithm
http://arxiv.org/abs/1202.2840v2,"Consider the following toy problem. There are $m$ rectangles and $n$ points
on the plane. Each rectangle $R$ is a consumer with budget $B_R$, who is
interested in purchasing the cheapest item (point) inside R, given that she has
enough budget. Our job is to price the items to maximize the revenue. This
problem can also be defined on higher dimensions. We call this problem the
geometric pricing problem.
  In this paper, we study a new class of problems arising from a geometric
aspect of the pricing problem. It intuitively captures typical real-world
assumptions that have been widely studied in marketing research, healthcare
economics, etc. It also helps classify other well-known pricing problems, such
as the highway pricing problem and the graph vertex pricing problem on planar
and bipartite graphs. Moreover, this problem turns out to have close
connections to other natural geometric problems such as the geometric versions
of the unique coverage and maximum feasible subsystem problems.
  We show that the low dimensionality arising in this pricing problem does lead
to improved approximation ratios, by presenting sublinear-approximation
algorithms for two central versions of the problem: unit-demand uniform-budget
min-buying and single-minded pricing problems. Our algorithm is obtained by
combining algorithmic pricing and geometric techniques. These results suggest
that considering geometric aspect might be a promising research direction in
obtaining improved approximation algorithms for such pricing problems. To the
best of our knowledge, this is one of very few problems in the intersection
between geometry and algorithmic pricing areas. Thus its study may lead to new
algorithmic techniques that could benefit both areas.",price discrimination algorithm
http://arxiv.org/abs/0910.0110v1,"We consider the Stackelberg shortest-path pricing problem, which is defined
as follows. Given a graph G with fixed-cost and pricable edges and two distinct
vertices s and t, we may assign prices to the pricable edges. Based on the
predefined fixed costs and our prices, a customer purchases a cheapest s-t-path
in G and we receive payment equal to the sum of prices of pricable edges
belonging to the path. Our goal is to find prices maximizing the payment
received from the customer. While Stackelberg shortest-path pricing was known
to be APX-hard before, we provide the first explicit approximation threshold
and prove hardness of approximation within 2-o(1).",price discrimination algorithm
http://arxiv.org/abs/1709.07534v1,"E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.",price discrimination algorithm
http://arxiv.org/abs/0908.2834v1,"Most recent papers addressing the algorithmic problem of allocating
advertisement space for keywords in sponsored search auctions assume that
pricing is done via a first-price auction, which does not realistically model
the Generalized Second Price (GSP) auction used in practice. Towards the goal
of more realistically modeling these auctions, we introduce the Second-Price Ad
Auctions problem, in which bidders' payments are determined by the GSP
mechanism. We show that the complexity of the Second-Price Ad Auctions problem
is quite different than that of the more studied First-Price Ad Auctions
problem. First, unlike the first-price variant, for which small constant-factor
approximations are known, it is NP-hard to approximate the Second-Price Ad
Auctions problem to any non-trivial factor. Second, this discrepancy extends
even to the 0-1 special case that we call the Second-Price Matching problem
(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This stands
in contrast to the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a classic result on the
performance of the ""Ranking"" algorithm for online bipartite matching.",price discrimination algorithm
http://arxiv.org/abs/1611.02442v3,"We study approximation algorithms for revenue maximization based on static
item pricing, where a seller chooses prices for various goods in the market,
and then the buyers purchase utility-maximizing bundles at these given prices.
We formulate two somewhat general techniques for designing good pricing
algorithms for this setting: Price Doubling and Item Halving. Using these
techniques, we unify many of the existing results in the item pricing
literature under a common framework, as well as provide several new item
pricing algorithms for approximating both revenue and social welfare. More
specifically, for a variety of settings with item pricing, we show that it is
possible to deterministically obtain a log-approximation for revenue and a
constant-approximation for social welfare simultaneously: thus one need not
sacrifice revenue if the goal is to still have decent welfare guarantees. %In
addition, we provide a new black-box reduction from revenue to welfare based on
item pricing, which immediately gives us new revenue-approximation algorithms
(e.g., for gross substitutes valuations).
  The main technical contribution of this paper is a $O((\log m + \log
k)^2)$-approximation algorithm for revenue maximization based on the Item
Halving technique, for settings where buyers have XoS valuations, where $m$ is
the number of goods and $k$ is the average supply. Surprisingly, ours is the
first known item pricing algorithm with polylogarithmic approximations for such
general classes of valuations, and partially resolves an important open
question from the algorithmic pricing literature about the existence of item
pricing algorithms with logarithmic factors for general valuations. We also use
the Item Halving framework to form envy-free item pricing mechanisms for the
popular setting of multi-unit markets, providing a log-approximation to revenue
in this case.",price discrimination algorithm
http://arxiv.org/abs/1805.02574v1,"We study revenue optimization pricing algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation. We show that, in the case when both the seller and the
buyer have the same discounting in their cumulative utilities (revenue and
surplus), there exist two optimal algorithms. The first one constantly offers
the Myerson price, while the second pricing proposes a ""big deal"": pay for all
goods in advance (at the first round) or get nothing. However, when there is an
imbalance between the seller and the buyer in the patience to wait for utility,
we find that the constant pricing, surprisingly, is no longer optimal. First,
it is outperformed by the pricing algorithm ""big deal"", when the seller's
discount rate is lower than the one of the buyer. Second, in the inverse case
of a less patient buyer, we reduce the problem of finding an optimal algorithm
to a multidimensional optimization problem (a multivariate analogue of the
functional used to determine Myerson's price) that does not admit a closed form
solution in general, but can be solved by numerical optimization techniques
(e.g., gradient ones). We provide extensive analysis of numerically found
optimal algorithms to demonstrate that they are non-trivial, may be
non-consistent, and generate larger expected revenue than the constant pricing
with the Myerson price.",price discrimination algorithm
http://arxiv.org/abs/1610.08890v2,"Parking prices in cities are uniform over large areas and do not reflect
spatially heterogeneous parking supply and demand. Underpricing results in high
parking occupancy in the subareas where the demand exceeds supply and long
search for the vacant parking, whereas overpricing leads to low occupancy and
hampered economic vitality. We present Nearest Pocket for Prices Algorithm
(NPPA), a spatially explicit algorithm for establishing on-and off-street
parking prices that guarantee a predetermined uniform level of occupation over
the entire parking space. We apply NPPA for establishing heterogeneous parking
prices that guarantee 90% parking occupancy in the Israeli city of Bat Yam.",price discrimination algorithm
http://arxiv.org/abs/1407.5699v1,"This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.",price discrimination algorithm
http://arxiv.org/abs/1611.00123v1,"The concept of device-to-device (D2D) communications underlaying cellular
networks opens up potential benefits for improving system performance but also
brings new challenges such as interference management. In this paper, we
propose a pricing framework for interference management from the D2D users to
the cellular system, where the base station (BS) protects itself (or its
serving cellular users) by pricing the crosstier interference caused from the
D2D users. A Stackelberg game is formulated to model the interactions between
the BS and D2D users. Specifically, the BS sets prices to a maximize its
revenue (or any desired utility) subject to an interference temperature
constraint. For given prices, the D2D users competitively adapt their power
allocation strategies for individual utility maximization. We first analyze the
competition among the D2D users by noncooperative game theory and an iterative
based distributed power allocation algorithm is proposed. Then, depending on
how much network information the BS knows, we develop two optimal algorithms,
one for uniform pricing with limited network information and the other for
differentiated pricing with global network information. The uniform pricing
algorithm can be implemented by a fully distributed manner and requires minimum
information exchange between the BS and D2D users, and the differentiated
pricing algorithm is partially distributed and requires no iteration between
the BS and D2D users. Then a suboptimal differentiated pricing scheme is
proposed to reduce complexity and it can be implemented in a fully distributed
fashion. Extensive simulations are conducted to verify the proposed framework
and algorithms.",price discrimination algorithm
http://arxiv.org/abs/cs/0109057v2,"Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either ""yes"" or ""no,"" due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are ""locked-in"" and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&T's incentive to charge a higher price to ""locked-in"" consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.",price discrimination algorithm
http://arxiv.org/abs/cs/0102003v1,"This paper develops three polynomial-time pricing techniques for European
Asian options with provably small errors, where the stock prices follow
binomial trees or trees of higher-degree. The first technique is the first
known Monte Carlo algorithm with analytical error bounds suitable for pricing
single-stock options with meaningful confidence and speed. The second technique
is a general recursive bucketing-based scheme that can use the
Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and
possibly others as the base-case subroutine. This scheme enables robust
trade-offs between accuracy and time over subtrees of different sizes. For
long-term options or high frequency price averaging, it can price single-stock
options with smaller errors in less time than the base-case algorithms
themselves. The third technique combines Fast Fourier Transform with
bucketing-based schemes for pricing basket options. This technique takes
polynomial time in the number of days and the number of stocks, and does not
add any errors to those already incurred in the companion bucketing scheme.
This technique assumes that the price of each underlying stock moves
independently.",price discrimination algorithm
http://arxiv.org/abs/1408.6292v1,"Given a batch of human computation tasks, a commonly ignored aspect is how
the price (i.e., the reward paid to human workers) of these tasks must be set
or varied in order to meet latency or cost constraints. Often, the price is set
up-front and not modified, leading to either a much higher monetary cost than
needed (if the price is set too high), or to a much larger latency than
expected (if the price is set too low). Leveraging a pricing model from prior
work, we develop algorithms to optimally set and then vary price over time in
order to meet a (a) user-specified deadline while minimizing total monetary
cost (b) user-specified monetary budget constraint while minimizing total
elapsed time. We leverage techniques from decision theory (specifically, Markov
Decision Processes) for both these problems, and demonstrate that our
techniques lead to upto 30\% reduction in cost over schemes proposed in prior
work. Furthermore, we develop techniques to speed-up the computation, enabling
users to leverage the price setting algorithms on-the-fly.",price discrimination algorithm
http://arxiv.org/abs/1811.02994v1,"Algorithmic discrimination is an important aspect when data is used for
predictive purposes. This paper analyzes the relationships between
discrimination and classification, data set partitioning, and decision models,
as well as correlation. The paper uses real world data sets to demonstrate the
existence of discrimination and the independence between the discrimination of
data sets and the discrimination of classification models.",price discrimination algorithm
http://arxiv.org/abs/1709.03221v1,"This paper defines software fairness and discrimination and develops a
testing-based method for measuring if and how much software discriminates,
focusing on causality in discriminatory behavior. Evidence of software
discrimination has been found in modern software systems that recommend
criminal sentences, grant access to financial products, and determine who is
allowed to participate in promotions. Our approach, Themis, generates efficient
test suites to measure discrimination. Given a schema describing valid system
inputs, Themis generates discrimination tests automatically and does not
require an oracle. We evaluate Themis on 20 software systems, 12 of which come
from prior work with explicit focus on avoiding discrimination. We find that
(1) Themis is effective at discovering software discrimination, (2)
state-of-the-art techniques for removing discrimination from algorithms fail in
many situations, at times discriminating against as much as 98% of an input
subdomain, (3) Themis optimizations are effective at producing efficient test
suites for measuring discrimination, and (4) Themis is more efficient on
systems that exhibit more discrimination. We thus demonstrate that fairness
testing is a critical aspect of the software development cycle in domains with
possible discrimination and provide initial tools for measuring software
discrimination.",price discrimination algorithm
http://arxiv.org/abs/0909.0892v2,"We consider auctions in which greedy algorithms, paired with first-price or
critical-price payment rules, are used to resolve multi-parameter combinatorial
allocation problems. We study the price of anarchy for social welfare in such
auctions. We show for a variety of equilibrium concepts, including Bayes-Nash
equilibrium and correlated equilibrium, the resulting price of anarchy bound is
close to the approximation factor of the underlying greedy algorithm.",price discrimination algorithm
http://arxiv.org/abs/1508.03928v1,"In this paper, we propose a novel deep neural network framework embedded with
low-level features (LCNN) for salient object detection in complex images. We
utilise the advantage of convolutional neural networks to automatically learn
the high-level features that capture the structured information and semantic
context in the image. In order to better adapt a CNN model into the saliency
task, we redesign the network architecture based on the small-scale datasets.
Several low-level features are extracted, which can effectively capture
contrast and spatial information in the salient regions, and incorporated to
compensate with the learned high-level features at the output of the last fully
connected layer. The concatenated feature vector is further fed into a
hinge-loss SVM detector in a joint discriminative learning manner and the final
saliency score of each region within the bounding box is obtained by the linear
combination of the detector's weights. Experiments on three challenging
benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be
effective and superior than most low-level oriented state-of-the-arts in terms
of P-R curves, F-measure and mean absolute errors.",price discrimination algorithm
http://arxiv.org/abs/1709.01654v2,"Hormozgan Province, located in the south of Iran, faces several challenges
regarding water resources management. The first one is the discharge of a
massive volume of water to the Persian Gulf because of the concentration of the
annual rainfalls in a short period of time and the narrow distance between the
headwater and the coast. The second one is the unbalanced development of
economic sectors in comparison with distribution of fresh water resources.
Finally, long-term drought is also common in this area. The construction of a
carry-over dam (Esteghlal Dam) and several conveyance pipelines and withdrawing
of the surface water and groundwater resources were considered as the solution
to deal with those challenges. During recent drought, severe overdraft and
inefficient use of recourses confirmed the fact that all done before are not
enough. During this period, there was a tendency to store water in reservoir in
order to meet the demand of the urban sector. Therefore, the agricultural
demand was the first victim of water allocation policy. It caused over
exploitation of the groundwater resources (to meet the agricultural demand) and
considerable losses (evaporation and leakage) from the reservoir. All of the
above-mentioned problems confirm the necessity of the development of a
conjunctive use policy. In this paper, all demand related to the Esteghlal Dam
and the Minab Aquifer (Bandarabbas City and agriculture in the Minab Plain)
were considered as the case study. The main objective was to find the best
applicable conjunctive policy which as well guarantees the conservation of the
Minab Aquifer. Alternative water allocation policies have been developed based
on the present capacities and the experience of local operating staff.",short withdrawal period
http://arxiv.org/abs/1209.5147v2,"We study numerically the hydrodynamics of dip coating from a suspension and
report a mechanism for colloidal assembly and pattern formation on smooth and
uniform substrates. Below a critical withdrawal speed of the substrate,
capillary forces required to deform the meniscus prevent colloidal particles
from entering the coating film. Capillary forces are overcome by hydrodynamic
drag only after a minimum number of particles organize in a close-packed
formation within the meniscus. Once within the film, the formed assembly moves
at nearly the withdrawal speed and rapidly separates from the next assembly.
The interplay between hydrodynamic and capillary forces can thus produce
periodic and regular structures within the curved meniscus that extends below
the withdrawn film. The hydrodynamically-driven assembly documented here is
consistent with stripe pattern formations observed experimentally in the
so-called thin-film entrainment regime.",short withdrawal period
http://arxiv.org/abs/1807.05782v2,"Rabin encryption and a secure ownership transfer protocol based on the
difficulty of factorization of a public key use a small public exponent. Such
encryption requires random number padding. The Coppersmith's shortpad attack
works effectively on short padding, thereby allowing an adversary to extract
the secret message. However, the criteria for determining the appropriate
padding size remains unclear. In this paper, we derived the processing-time
formula for the shortpad attack and determined the optimal random-padding size
in order to achieve the desired security.",short withdrawal period
http://arxiv.org/abs/1407.2137v1,"The above article, published online on 28 April 2014 in Wiley Online Library
(wileyonlinelibrary.com), has been withdrawn with agreement from the journal
Editor-in-Chief, Blaise Cronin, and Wiley Periodicals, Inc. The withdrawal has
been agreed for legal reasons.",short withdrawal period
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",short withdrawal period
http://arxiv.org/abs/1708.00157v1,"An existing pseudo-commodity and a smart contracts framework allow the
creation of a purely automatic and self-sufficient price-stable cryptocurrency,
without human intervention. This new currency, we denominated Toroid or TRD,
can be used more extensively for commerce than pseudo commodity
cryptocurrencies due to its lower volatility. Also, is suitable for investment,
as the tokens in each account multiply, return interest, when the market grows.
Like the controlled fiat money of a central bank plus the benefits of an
inflation-adjusted perpetuity bond. Collateral in base coin, for example BTC or
ETH, can be added to bootstrap your own Toroid investment or withdrawed after a
very small investment period. So, the Toroids are not created from nothing nor
have a limited monetary base. The minimum investment period can be very small,
for example one day, and you keep the interest but you can return the Toroids
and refund your collateral. That is a one-side only peg to a deflationary
crypto-commodity. The stability is guaranteed by endogenous measurements of
number of transactions and wallet pro-rated rebasement of balance to reduce
volatility of price. Each account has its own rebasement due to the account
creation timestamp. Rebasement control mechanism is progressive during initial
bootstrap period because price manipulation protection is more severe when the
capital involved is smaller. Rebasement has a quick positive start to
incentivize early adopters that see only big growth in their TRD account during
bootstrap period. Finally, the new rebasement control makes it economically
infeasible for an attacker targeting the coin with manipulated transaction
volume if we set the minimum rebasement greater than profits from massive
currency manipulation.",short withdrawal period
http://arxiv.org/abs/1002.3916v1,"Short-interval water level measurements using automatic water level recorder
in a deep well in an unconfined crystalline rock aquifer at the campus of NGRI,
near Hyderabad shows a cyclic fluctuation in the water levels. The observed
values clearly show the principal trend due to rainfall recharge. Spectral
analysis was carried out to evaluate correlation of the cyclic fluctuation to
the synthetic earth tides as well as groundwater withdrawal time series in the
surrounding. It was found that these fluctuations have considerably high
correlation with earth tides whereas groundwater pumping does not show any
significant correlation with water table fluctuations. It is concluded that
earth tides cause the fluctuation in the water table. These fluctuations were
hitherto unobserved during manual observations made over larger time intervals.
It indicates that the unconfined aquifer is characterised by a low porosity.",short withdrawal period
http://arxiv.org/abs/1606.01925v3,"In the Proper Interval Vertex Deletion problem (PIVD for short), we are given
a graph $G$ and an integer parameter $k>0$, and the question is whether there
are at most $k$ vertices in $G$ whose removal results in a proper interval
graph. It is known that the PIVD problem is fixed-parameter tractable and
admits a polynomial but ""unreasonably"" large kernel of $O(k^{53})$ vertices. A
natural question is whether the problem admits a polynomial kernel of
""reasonable"" size. In this paper, we answer this question by deriving an
$O(k^7)$-vertex kernel for the PIVD problem. Our kernelization is based on
several new observations and a refined analysis of the kernelization.",short withdrawal period
http://arxiv.org/abs/1312.3230v1,"BitCoin transactions are malleable in a sense that given a transaction an
adversary can easily construct an equivalent transaction which has a different
hash. This can pose a serious problem in some BitCoin distributed contracts in
which changing a transaction's hash may result in the protocol disruption and a
financial loss. The problem mostly concerns protocols, which use a ""refund""
transaction to withdraw a deposit in a case of the protocol interruption. In
this short note, we show a general technique for creating
malleability-resilient ""refund"" transactions, which does not require any
modification of the BitCoin protocol. Applying our technique to our previous
paper ""Fair Two-Party Computations via the BitCoin Deposits"" (Cryptology ePrint
Archive, 2013) allows to achieve fairness in any Two-Party Computation using
the BitCoin protocol in its current version.",short withdrawal period
http://arxiv.org/abs/1603.09633v2,"A high-accuracy numerical study on the evolution of two-dimensional unbounded
flows with the Hermite pseudo-spectral solver is presented. Our simulations
clearly show that the simple Oseen vortex always appears in the late stage for
every initial condition with non-zero circulation ($\Omega \neq 0$). In
general, the theoretical time adopted to describe the Oseen vortex and the
simulating time in numerical investigations are not the same, and their
difference ($T_{diff}$) is in inverse proportion to the viscosity for the same
initial condition. In particular, a perturbed monopole will also eventually
relax into an Oseen vortex which shows obvious difference from the original
monopole no matter how small the perturbation is. This difference can be well
represented by the time gap ($T_{gap}$) between the theoretical time of two
monopoles, and the type and amplitude of the perturbation determine the value
of $T_{gap}$.",short withdrawal period
http://arxiv.org/abs/1710.11271v2,"Most social platforms offer mechanisms allowing users to delete their posts,
and a significant fraction of users exercise this right to be forgotten.
However, ironically, users' attempt to reduce attention to sensitive posts via
deletion, in practice, attracts unwanted attention from stalkers specifically
to those posts. Thus, deletions may leave users more vulnerable to attacks on
their privacy in general. Users hoping to make their posts forgotten face a
""damned if I do, damned if I don't"" dilemma. Many are shifting towards
ephemeral social platform like Snapchat, which will deprive us of important
user-data archival. In the form of intermittent withdrawals, we present, Lethe,
a novel solution to this problem of forgetting the forgotten. If the
next-generation social platforms are willing to give up the uninterrupted
availability of non-deleted posts by a very small fraction, Lethe provides
privacy to the deleted posts over long durations. In presence of Lethe, an
adversarial observer becomes unsure if some posts are permanently deleted or
just temporarily withdrawn by Lethe; at the same time, the adversarial observer
is overwhelmed by a large number of falsely flagged undeleted posts. To
demonstrate the feasibility and performance of Lethe, we analyze large-scale
real data about users' deletion over Twitter and thoroughly investigate how to
choose time duration distributions for alternating between temporary
withdrawals and resurrections of non-deleted posts. We find a favorable
trade-off between privacy, availability and adversarial overhead in different
settings for users exercising their right to delete. We show that, even against
an ultimate adversary with an uninterrupted access to the entire platform,
Lethe offers deletion privacy for up to 3 months from the time of deletion,
while maintaining content availability as high as 95% and keeping the
adversarial precision to 20%.",withdrawal rights
http://arxiv.org/abs/1703.07521v5,"We show that for a relation $f\subseteq \{0,1\}^n\times \mathcal{O}$ and a
function $g:\{0,1\}^{m}\times \{0,1\}^{m} \rightarrow \{0,1\}$ (with $m= O(\log
n)$), $$\mathrm{R}_{1/3}(f\circ g^n) = \Omega\left(\mathrm{R}_{1/3}(f) \cdot
\left(\log\frac{1}{\mathrm{disc}(M_g)} - O(\log n)\right)\right),$$ where
$f\circ g^n$ represents the composition of $f$ and $g^n$, $M_g$ is the sign
matrix for $g$, $\mathrm{disc}(M_g)$ is the discrepancy of $M_g$ under the
uniform distribution and $\mathrm{R}_{1/3}(f)$ ($\mathrm{R}_{1/3}(f\circ g^n)$)
denotes the randomized query complexity of $f$ (randomized communication
complexity of $f\circ g^n$) with worst case error $\frac{1}{3}$.
  In particular, this implies that for a relation $f\subseteq \{0,1\}^n\times
\mathcal{O}$, $$\mathrm{R}_{1/3}(f\circ \mathrm{IP}_m^n) =
\Omega\left(\mathrm{R}_{1/3}(f) \cdot m\right),$$ where
$\mathrm{IP}_m:\{0,1\}^m\times \{0,1\}^m\rightarrow \{0,1\}$ is the Inner
Product (modulo $2$) function and $m= O(\log(n))$.",withdrawal rights
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",withdrawal rights
http://arxiv.org/abs/1810.00392v1,"We study the classical, two-sided stable marriage problem under pairwise
preferences. In the most general setting, agents are allowed to express their
preferences as comparisons of any two of their edges and they also have the
right to declare a draw or even withdraw from such a comparison. This freedom
is then gradually restricted as we specify six stages of orderedness in the
preferences, ending with the classical case of strictly ordered lists. We study
all cases occurring when combining the three known notions of stability---weak,
strong and super-stability---under the assumption that each side of the
bipartite market obtains one of the six degrees of orderedness. By designing
three polynomial algorithms and two NP-completeness proofs we determine the
complexity of all cases not yet known, and thus give an exact boundary in terms
of preference structure between tractable and intractable cases.",withdrawal rights
http://arxiv.org/abs/0707.1859v2,"After receiving useful peer comments, we would like to withdraw this paper.",withdrawal rights
http://arxiv.org/abs/1701.06989v4,"We consider an efficiently decodable non-adaptive group testing (NAGT)
problem that meets theoretical bounds. The problem is to find a few specific
items (at most $d$) satisfying certain characteristics in a colossal number of
$N$ items as quickly as possible. Those $d$ specific items are called
\textit{defective items}. The idea of NAGT is to pool a group of items, which
is called \textit{a test}, then run a test on them. If the test outcome is
\textit{positive}, there exists at least one defective item in the test, and if
it is \textit{negative}, there exists no defective items. Formally, a binary $t
\times N$ measurement matrix $\mathcal{M} = (m_{ij})$ is the representation for
$t$ tests where row $i$ stands for test $i$ and $m_{ij} = 1$ if and only if
item $j$ belongs to test $i$.
  There are three main objectives in NAGT: minimize the number of tests $t$,
construct matrix $\mathcal{M}$, and identify defective items as quickly as
possible. In this paper, we present a strongly explicit construction of
$\mathcal{M}$ for when the number of defective items is at most 2, with the
number of tests $t \simeq 16 \log{N} = O(\log{N})$. In particular, we need only
$K \simeq N \times 16\log{N} = O(N\log{N})$ bits to construct such matrices,
which is optimal. Furthermore, given these $K$ bits, any entry in the matrix
can be constructed in time $O \left(\ln{N}/ \ln{\ln{N}} \right)$. Moreover,
$\mathcal{M}$ can be decoded with high probability in time $O\left(
\frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$. When the number of defective items is
greater than 2, we present a scheme that can identify at least $(1-\epsilon)d$
defective items with $t \simeq 32 C(\epsilon) d \log{N} = O(d \log{N})$ in time
$O \left( d \frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$ for any close-to-zero
$\epsilon$, where $C(\epsilon)$ is a constant that depends only on $\epsilon$.",withdrawal rights
http://arxiv.org/abs/cs/0207050v2,"Constraint logic programming combines declarativity and efficiency thanks to
constraint solvers implemented for specific domains. Value withdrawal
explanations have been efficiently used in several constraints programming
environments but there does not exist any formalization of them. This paper is
an attempt to fill this lack. Furthermore, we hope that this theoretical tool
could help to validate some programming environments. A value withdrawal
explanation is a tree describing the withdrawal of a value during a domain
reduction by local consistency notions and labeling. Domain reduction is
formalized by a search tree using two kinds of operators: operators for local
consistency notions and operators for labeling. These operators are defined by
sets of rules. Proof trees are built with respect to these rules. For each
removed value, there exists such a proof tree which is the withdrawal
explanation of this value.",withdrawal rights
http://arxiv.org/abs/1208.6501v2,"We study a more powerful variant of false-name manipulation in Internet
auctions: an agent can submit multiple false-name bids, but then, once the
allocation and payments have been decided, withdraw some of her false-name
identities (have some of her false-name identities refuse to pay). While these
withdrawn identities will not obtain the items they won, their initial presence
may have been beneficial to the agent's other identities. We define a mechanism
to be false-name-proof with withdrawal (FNPW) if the aforementioned
manipulation is never beneficial. FNPW is a stronger condition than
false-name-proofness (FNP).",withdrawal rights
http://arxiv.org/abs/physics/0610102v3,"Recently, we proposed a classical communicator which was inspired by the
Kirchhoff-loop-Johnson-like-Noise (KLJN) communicator and was claimed totally
secure. Here we withdraw this claim and prove that, similarly to earlier
intuitive suspicions, a wave-based classical communicator can never be totally
secure.",withdrawal rights
http://arxiv.org/abs/1010.2871v2,"The authors have decided to withdraw this submission.
Clarifications/corrections, if any, may follow at a later date.",withdrawal rights
http://arxiv.org/abs/0804.4628v1,"Recently, Rawat and Saxena proposed a method for protecting data using
``Disclaimer Statement''. This paper presents some issues and several flaws in
their proposal.",withdrawal rights disclaimer
http://arxiv.org/abs/1710.11271v2,"Most social platforms offer mechanisms allowing users to delete their posts,
and a significant fraction of users exercise this right to be forgotten.
However, ironically, users' attempt to reduce attention to sensitive posts via
deletion, in practice, attracts unwanted attention from stalkers specifically
to those posts. Thus, deletions may leave users more vulnerable to attacks on
their privacy in general. Users hoping to make their posts forgotten face a
""damned if I do, damned if I don't"" dilemma. Many are shifting towards
ephemeral social platform like Snapchat, which will deprive us of important
user-data archival. In the form of intermittent withdrawals, we present, Lethe,
a novel solution to this problem of forgetting the forgotten. If the
next-generation social platforms are willing to give up the uninterrupted
availability of non-deleted posts by a very small fraction, Lethe provides
privacy to the deleted posts over long durations. In presence of Lethe, an
adversarial observer becomes unsure if some posts are permanently deleted or
just temporarily withdrawn by Lethe; at the same time, the adversarial observer
is overwhelmed by a large number of falsely flagged undeleted posts. To
demonstrate the feasibility and performance of Lethe, we analyze large-scale
real data about users' deletion over Twitter and thoroughly investigate how to
choose time duration distributions for alternating between temporary
withdrawals and resurrections of non-deleted posts. We find a favorable
trade-off between privacy, availability and adversarial overhead in different
settings for users exercising their right to delete. We show that, even against
an ultimate adversary with an uninterrupted access to the entire platform,
Lethe offers deletion privacy for up to 3 months from the time of deletion,
while maintaining content availability as high as 95% and keeping the
adversarial precision to 20%.",withdrawal rights disclaimer
http://arxiv.org/abs/1703.07521v5,"We show that for a relation $f\subseteq \{0,1\}^n\times \mathcal{O}$ and a
function $g:\{0,1\}^{m}\times \{0,1\}^{m} \rightarrow \{0,1\}$ (with $m= O(\log
n)$), $$\mathrm{R}_{1/3}(f\circ g^n) = \Omega\left(\mathrm{R}_{1/3}(f) \cdot
\left(\log\frac{1}{\mathrm{disc}(M_g)} - O(\log n)\right)\right),$$ where
$f\circ g^n$ represents the composition of $f$ and $g^n$, $M_g$ is the sign
matrix for $g$, $\mathrm{disc}(M_g)$ is the discrepancy of $M_g$ under the
uniform distribution and $\mathrm{R}_{1/3}(f)$ ($\mathrm{R}_{1/3}(f\circ g^n)$)
denotes the randomized query complexity of $f$ (randomized communication
complexity of $f\circ g^n$) with worst case error $\frac{1}{3}$.
  In particular, this implies that for a relation $f\subseteq \{0,1\}^n\times
\mathcal{O}$, $$\mathrm{R}_{1/3}(f\circ \mathrm{IP}_m^n) =
\Omega\left(\mathrm{R}_{1/3}(f) \cdot m\right),$$ where
$\mathrm{IP}_m:\{0,1\}^m\times \{0,1\}^m\rightarrow \{0,1\}$ is the Inner
Product (modulo $2$) function and $m= O(\log(n))$.",withdrawal rights disclaimer
http://arxiv.org/abs/0708.4293v1,"In viscous withdrawal, a converging flow imposed in an upper layer of viscous
liquid entrains liquid from a lower, stably stratified layer. Using the idea
that a thin tendril is entrained by a local straining flow, we propose a
scaling law for the volume flux of liquid entrained from miscible liquid
layers. A long-wavelength model including only local information about the
withdrawal flow is degenerate, with multiple tendril solutions for one
withdrawal condition. Including information about the global geometry of the
withdrawal flow removes the degeneracy while introducing only a logarithmic
dependence on the global flow parameters into the scaling law.",withdrawal rights disclaimer
http://arxiv.org/abs/1201.1188v3,"Biometrical authentication systems are often presented as the best and
simplest way to reach higher security levels. But a deeper analysis shows that
several risks are hidden and the service provider adopting those system has to
carefully check its liabilities before deploying them.",withdrawal rights disclaimer
http://arxiv.org/abs/1402.2757v1,"I have been asked to write brief, gentle introduction to the basic idea
behind the field of ""quantum gravity"" in 1500 words or less. Doing so appears
to be almost as great a challenge as coming up with a consistent theory of
quantum gravity. However, I will try. Disclaimer: \emph{The views expressed in
this article are my own and do not represent the consensus of the quantum
gravity community}.",withdrawal rights disclaimer
http://arxiv.org/abs/1810.00392v1,"We study the classical, two-sided stable marriage problem under pairwise
preferences. In the most general setting, agents are allowed to express their
preferences as comparisons of any two of their edges and they also have the
right to declare a draw or even withdraw from such a comparison. This freedom
is then gradually restricted as we specify six stages of orderedness in the
preferences, ending with the classical case of strictly ordered lists. We study
all cases occurring when combining the three known notions of stability---weak,
strong and super-stability---under the assumption that each side of the
bipartite market obtains one of the six degrees of orderedness. By designing
three polynomial algorithms and two NP-completeness proofs we determine the
complexity of all cases not yet known, and thus give an exact boundary in terms
of preference structure between tractable and intractable cases.",withdrawal rights disclaimer
http://arxiv.org/abs/0707.1859v2,"After receiving useful peer comments, we would like to withdraw this paper.",withdrawal rights disclaimer
http://arxiv.org/abs/1701.06989v4,"We consider an efficiently decodable non-adaptive group testing (NAGT)
problem that meets theoretical bounds. The problem is to find a few specific
items (at most $d$) satisfying certain characteristics in a colossal number of
$N$ items as quickly as possible. Those $d$ specific items are called
\textit{defective items}. The idea of NAGT is to pool a group of items, which
is called \textit{a test}, then run a test on them. If the test outcome is
\textit{positive}, there exists at least one defective item in the test, and if
it is \textit{negative}, there exists no defective items. Formally, a binary $t
\times N$ measurement matrix $\mathcal{M} = (m_{ij})$ is the representation for
$t$ tests where row $i$ stands for test $i$ and $m_{ij} = 1$ if and only if
item $j$ belongs to test $i$.
  There are three main objectives in NAGT: minimize the number of tests $t$,
construct matrix $\mathcal{M}$, and identify defective items as quickly as
possible. In this paper, we present a strongly explicit construction of
$\mathcal{M}$ for when the number of defective items is at most 2, with the
number of tests $t \simeq 16 \log{N} = O(\log{N})$. In particular, we need only
$K \simeq N \times 16\log{N} = O(N\log{N})$ bits to construct such matrices,
which is optimal. Furthermore, given these $K$ bits, any entry in the matrix
can be constructed in time $O \left(\ln{N}/ \ln{\ln{N}} \right)$. Moreover,
$\mathcal{M}$ can be decoded with high probability in time $O\left(
\frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$. When the number of defective items is
greater than 2, we present a scheme that can identify at least $(1-\epsilon)d$
defective items with $t \simeq 32 C(\epsilon) d \log{N} = O(d \log{N})$ in time
$O \left( d \frac{\ln^2{N}}{\ln^2{\ln{N}}} \right)$ for any close-to-zero
$\epsilon$, where $C(\epsilon)$ is a constant that depends only on $\epsilon$.",withdrawal rights disclaimer
http://arxiv.org/abs/1701.01103v4,"The redundancy for universal lossless compression of discrete memoryless
sources in Campbell's setting is characterized as a minimax R\'enyi divergence,
which is shown to be equal to the maximal $\alpha$-mutual information via a
generalized redundancy-capacity theorem. Special attention is placed on the
analysis of the asymptotics of minimax R\'enyi divergence, which is determined
up to a term vanishing in blocklength.",withdrawal rights disclaimer
http://arxiv.org/abs/1909.01825v1,"This document is an Internet Appendix of paper entitled ""Sequential
Bargaining Based Incentive Mechanism for Collaborative Internet Access"". It
includes information about LTE signal metrics, results of idle state
experiments, and linear regression assumptions of the models presented in the
related paper.",Internet accessibility
http://arxiv.org/abs/cs/0412119v1,"The rapid growth of the internet in general and of bandwidth capacity at
internet clients in particular poses increasing computation and bandwidth
demands on internet servers. Internet access technologies like ADSL [DSL],
Cable Modem and Wireless modem allow internet clients to access the internet
with orders of magnitude more bandwidth than using traditional modems. We
present CDTP a distributed transfer protocol that allows clients to cooperate
and therefore remove the strain from the internet server thus achieving much
better performance than traditional transfer protocols (e.g. FTP [FTP]). The
CDTP server and client tools are presented also as well as results of
experiments. Finally a bandwidth measurement technique is presented. CDTP tools
use this technique to differentiate between slow and fast clients.",Internet accessibility
http://arxiv.org/abs/cs/0609149v1,"In this article, we first provide a taxonomy of dynamic spectrum access. We
then focus on opportunistic spectrum access, the overlay approach under the
hierarchical access model of dynamic spectrum access. we aim to provide an
overview of challenges and recent developments in both technological and
regulatory aspects of opportunistic spectrum access.",Internet accessibility
http://arxiv.org/abs/1406.2516v1,"Unlike telephone operators, which pay termination fees to reach the users of
another network, Internet Content Providers (CPs) do not pay the Internet
Service Providers (ISPs) of users they reach. While the consequent cross
subsidization to CPs has nurtured content innovations at the edge of the
Internet, it reduces the investment incentives for the access ISPs to expand
capacity. As potential charges for terminating CPs' traffic are criticized
under the net neutrality debate, we propose to allow CPs to voluntarily
subsidize the usagebased fees induced by their content traffic for end-users.
We model the regulated subsidization competition among CPs under a neutral
network and show how deregulation of subsidization could increase an access
ISP's utilization and revenue, strengthening its investment incentives.
Although the competition might harm certain CPs, we find that the main cause
comes from high access prices rather than the existence of subsidization. Our
results suggest that subsidization competition will increase the
competitiveness and welfare of the Internet content market; however, regulators
might need to regulate access prices if the access ISP market is not
competitive enough. We envision that subsidization competition could become a
viable model for the future Internet.",Internet accessibility
http://arxiv.org/abs/1604.08243v1,"Recent years have witnessed several initiatives on enabling Internet access
to the next three billion people. Access to the Internet necessarily translates
to access to its services. This means that the goal of providing Internet
access requires ac- cess to its critical service infrastructure, which are
currently hosted in the cloud. However, recent works have pointed out that the
current cloud centric nature of the Internet is a fundamental barrier for
Internet access in rural/remote areas as well as in developing regions. It is
important to explore (low cost) solutions such as micro cloud infrastructures
that can provide services at the edge of the network (potentially on demand),
right near the users. In this paper, we present Cloudrone- a preliminary idea
of deploying a lightweight micro cloud infrastructure in the sky using
indigenously built low cost drones, single board computers and lightweight
Operating System virtualization technologies. Our paper lays out the
preliminary ideas on such a system that can be instantaneously deployed on
demand. We describe an initial design of the Cloudrone and provide a
preliminary evaluation of the proposed system mainly focussed on the
scalability issues of supporting multiple services and users.",Internet accessibility
http://arxiv.org/abs/1907.04570v1,"End-users and governments force network operators to deploy faster Internet
access services everywhere. Access technologies such as FTTx, VDSL2, DOCSIS3.0
can provide such services in cities. However, it is not cost-effective for
network operators to deploy them in less densely populated regions. The
recently proposed Hybrid Access Networks allow to boost xDSL networks by using
the available capacity in existing LTE networks. We first present the three
architectures defined by the Broadband Forum for such Hybrid Access Networks.
Then we describe our experience with the implementation and the deployment of
Multipath TCP-based Hybrid Access Networks.",Internet accessibility
http://arxiv.org/abs/1603.07431v1,"Decades of experience have shown that there is no single one-size-fits-all
solution that can be used to provision Internet globally and that invariably
there are tradeoffs in the design of Internet. Despite the best efforts of
networking researchers and practitioners, an ideal Internet experience is
inaccessible to an overwhelming majority of people the world over, mainly due
to the lack of cost efficient ways of provisioning high-performance global
Internet. In this paper, we argue that instead of an exclusive focus on a
utopian goal of universally accessible ""ideal networking"" (in which we have
high throughput and quality of service as well as low latency and congestion),
we should consider providing ""approximate networking"" through the adoption of
context-appropriate tradeoffs. Approximate networking can be used to implement
a pragmatic tiered global access to the Internet for all (GAIA) system in which
different users the world over have different context-appropriate (but still
contextually functional) Internet experience.",Internet accessibility
http://arxiv.org/abs/cs/0109113v1,"This focused study on state-level policy and access patterns contributes to a
fuller understanding of how these invisible barriers work to structure access
and define rural communities. Combining both quantitative and qualitative data,
this study examines the role of geo-policy barriers in one of the largest and
most rural states in the nation.
  Expanded Area Service policies are state policies wherein phone customers can
expand their local calling area. Because useful Internet access requires a
flat-price connection, EAS policies can play a crucial role in connecting
citizens to one another. EAS policies (including Texas') tend to vary along
five dimensions (community of interest, customer scope, directionality, pricing
mechanism and policy scope). EAS policies that rely on regulated market
boundaries for definition can generate gross inequities in rural Internet
access. Interviews with Internet Service Providers in a case study of 25 rural
communities reveals that LATA and exchange boundaries, along with
geographically restricted infrastructure investments, curtail service provision
in remote areas. A statistical analysis of 1300 telephone exchanges, including
208 rural telephone exchanges in Texas reveals that the farther a community
lies from a metropolitan area the less likely they are to have reliable
Internet access",Internet accessibility
http://arxiv.org/abs/1001.4191v1,"Broadband communications consists of the technologies and equipment required
to deliver packet-based digital voice, video, and data services to end users.
Broadband affords end users high-speed, always-on access to the Internet while
affording service providers the ability to offer value-added services to
increase revenues. Due to the growth of the Internet, there has been tremendous
buildout of high-speed, inter-city communications links that connect population
centers and Internet service providers (ISPs) points of presence (PoPs) around
the world. This build out of the backbone infrastructure or core network has
occurred primarily via optical transport technology. Broadband access
technologies are being deployed to address the bandwidth bottleneck for the
""last mile,"" the connection of homes and small businesses to this
infrastructure. One important aspect of broadband access to the home is that it
allows people to telecommute effectively by providing a similar environment as
when they are physically present in their office: simultaneous telephone and
computer access, high-speed Internet and intranet access for e-mail, file
sharing, and access to corporate servers.",Internet accessibility
http://arxiv.org/abs/cs/0701198v1,"We consider the RIPE WHOIS Internet data as characterized by the Cooperative
Association for Internet Data Analysis (CAIDA), and show that the Tempered
Preferential Attachment model [1] provides an excellent fit to this data.
  [1] D'Souza, Borgs, Chayes, Berger and Kleinberg, to appear PNAS USA, 2007.",Internet accessibility
http://arxiv.org/abs/1610.01065v1,"Cheating is a real problem in the Internet of Things. The fundamental
question that needs to be answered is how we can trust the validity of the data
being generated in the first place. The problem, however, isn't inherent in
whether or not to embrace the idea of an open platform and open-source
software, but to establish a methodology to verify the trustworthiness and
control any access. This paper focuses on building an access control model and
system based on trust computing. This is a new field of access control
techniques which includes Access Control, Trust Computing, Internet of Things,
network attacks, and cheating technologies. Nevertheless, the target access
control systems can be very complex to manage. This paper presents an overview
of the existing work on trust computing, access control models and systems in
IoT. It not only summarizes the latest research progress, but also provides an
understanding of the limitations and open issues of the existing work. It is
expected to provide useful guidelines for future research.",Internet accessibility
http://arxiv.org/abs/cs/0109059v1,"The Internet is changing rapidly the way people around the world communicate,
learn, and work. Yet the tremendous benefits of the Internet are not shared
equally by all. One way to close the gap of the ""digital divide"" is to ensure
Internet access to all schools from an early age. While both the USA and EU
have embraced the promotion of Internet access to schools, the two have decided
to finance it differently. This paper shows that the main costs of Internet
access to schools are not communications-related (telecommunications and
Internet services) but rather non-communications-related (hardware, educational
training, software). This paper goes on to discuss whether the identified costs
should be financed in any way by the universal service obligations funded by
the telecommunications industry/sector/consumers (sector specific) or a general
governmental budget (educational budget).",Internet accessibility
http://arxiv.org/abs/1610.04459v1,"Uniform and affordable Internet is emerging as one of the fundamental civil
rights in developing countries. However in India, the connectivity is far from
uniform across the regions, where the disparity is evident in the
infrastructure, the cost of access and telecommunication services to provide
Internet facilities among different economic classes. In spite of having a
large mobile user base, the mobile Internet are still remarkably slower in some
of the developing countries. Especially in India, it falls below 50% even in
comparison with the performance of its developing counterparts!
  This essay presents a study of connectivity and performance trends based on
an exploratory analysis of mobile Internet measurement data from India. In
order to assess the state of mobile networks and its readiness in adopting the
different mobile standards (2G, 3G, and 4G) for commercial use, we discuss the
spread, penetration, interoperability and the congestion trends. Based on our
analysis, we argue that the network operators have taken negligible measures to
scale the mobile Internet. Affordable Internet is definitely for everyone. But,
the affordability of the Internet in terms of cost does not necessarily imply
the rightful access to Internet services.
  Chota recharge is possibly leading us to chota (shrunken) Internet!",Internet accessibility
http://arxiv.org/abs/cs/0109049v1,"Many people expect the Internet to change American politics, most likely in
the direction of increasing direct citizen participation and forcing government
officials to respond more quickly to voter concerns. A recent California
initiative with these objectives would authorize use of encrypted digital
signatures over the Internet to qualify candidates, initiatives, and other
ballot measures. Proponents of Internet signature gathering say it will
significantly lower the cost of qualifying initiatives and thereby reduce the
influence of organized, well-financed interest groups. They also believe it
will increase both public participation in the political process and public
understanding about specific measures. However, opponents question whether
Internet security is adequate to prevent widespread abuse and argue that the
measure would create disadvantages for those who lack access to the Internet.
Beyond issues of security, cost, and access lie larger questions about the
effects of Internet signature gathering on direct democracy. Would it encourage
greater and more informed public participation in the political process? Or
would it flood voters with ballot measures and generally worsen current
problems with the initiative process itself? Because we lack good data on these
questions, answers to them today are largely conjectural. We can be fairly
sure, however, that Internet petition signing, like Internet voting, will have
unintended consequences.",Internet accessibility
http://arxiv.org/abs/1007.0126v1,"In this paper, we propose a cognitive radio based Internet access framework
for disaster response network deployment in challenged environments. The
proposed architectural framework is designed to help the existent but partially
damaged networks to restore their connectivity and to connect them to the
global Internet. This architectural framework provides the basis to develop
algorithms and protocols for the future cognitive radio network deployments in
challenged environments.",Internet accessibility
http://arxiv.org/abs/1807.06077v1,"Arpanet, Internet, Internet of Services, Internet of Things, Internet of
Skills. What next? We conjecture that in 15-20 years from now we will have the
Internet of Neurons, a new Internet paradigm in which humans will be able to
connect bi-directionally to the net using only their brain. The Internet of
Neurons will provide new, tremendous opportunities thanks to constant access to
unlimited information. It will empower all those outside of the technical
industry, actually it will empower all human beings, to access and use
technological products and services as everybody will be able to connect, even
without possessing a laptop, a tablet or a smartphone. The Internet of Neurons
will thus ultimately complete the currently still immature democratization of
knowledge and technology. But it will also bring along several enormous
challenges, especially concerning security (as well as privacy and trust).
  In this paper we speculate on the worldwide deployment of the Internet of
Neurons by 2038 and brainstorm about its disruptive impact, discussing the main
technological (and neurological) breakthroughs required to enable it, the new
opportunities it provides and the security challenges it raises. We also
elaborate on the novel system models, threat models and security properties
that are required to reason about privacy, security and trust in the Internet
of Neurons.",Internet accessibility
http://arxiv.org/abs/cs/0109064v1,"This paper explores commonalities between the creation of the Rural
Electrification Administration and the similar dilemma of providing an
affordable infrastructure for high-speed Internet access in places where profit
incentives do not exist. In the case of the R.E.A., the necessity for an
aggressive federal initiative to wire rural America, where the market for
electricity had failed, is revisited as the missing incentives are identified
and explored. We then examine the incentive-poor similarities between rural
electrification and rural high-speed Internet access through how consumers
currently and prospectively gain access to broadband Internet service. The
regulatory environment created by the Telecommunications Act of 1996 and the
Federal Communications Commission is considered. Although the FCC is required
(Section 254.b.3) to take regulatory measures to ensure comparable and
affordable access to the Internet for all Americans, the historical
similarities and comparative analysis of rural electrification and high-speed
Internet access suggests the goal of universal service is unlikely to be met in
the near future. Regulatory disincentives to build such networks are present,
driven in part by market realities and in part by competitive restrictions in
the Telecommunications Act of 1996. Finally, we pose the question of whether a
federal effort equivalent to the R.E.A. is needed to ensure that residents of
sparsely populated areas, like their predecessors in the 1930s, are not
comparatively disadvantaged in the first decades of the 21st century. The paper
concludes with a proposal to accelerate the deployment of broadband
infrastructure in rural America.",Internet accessibility
http://arxiv.org/abs/1802.04410v1,"This paper investigates a critical access control issue in the Internet of
Things (IoT). In particular, we propose a smart contract-based framework, which
consists of multiple access control contracts (ACCs), one judge contract (JC)
and one register contract (RC), to achieve distributed and trustworthy access
control for IoT systems. Each ACC provides one access control method for a
subject-object pair, and implements both static access right validation based
on predefined policies and dynamic access right validation by checking the
behavior of the subject. The JC implements a misbehavior-judging method to
facilitate the dynamic validation of the ACCs by receiving misbehavior reports
from the ACCs, judging the misbehavior and returning the corresponding penalty.
The RC registers the information of the access control and misbehavior-judging
methods as well as their smart contracts, and also provides functions (e.g.,
register, update and delete) to manage these methods. To demonstrate the
application of the framework, we provide a case study in an IoT system with one
desktop computer, one laptop and two Raspberry Pi single-board computers, where
the ACCs, JC and RC are implemented based on the Ethereum smart contract
platform to achieve the access control.",Internet accessibility
http://arxiv.org/abs/1603.09537v1,"Internet has shown itself to be a catalyst for economic growth and social
equity but its potency is thwarted by the fact that the Internet is off limits
for the vast majority of human beings. Mobile phones---the fastest growing
technology in the world that now reaches around 80\% of humanity---can enable
universal Internet access if it can resolve coverage problems that have
historically plagued previous cellular architectures (2G, 3G, and 4G). These
conventional architectures have not been able to sustain universal service
provisioning since these architectures depend on having enough users per cell
for their economic viability and thus are not well suited to rural areas (which
are by definition sparsely populated). The new generation of mobile cellular
technology (5G), currently in a formative phase and expected to be finalized
around 2020, is aimed at orders of magnitude performance enhancement. 5G offers
a clean slate to network designers and can be molded into an architecture also
amenable to universal Internet provisioning. Keeping in mind the great social
benefits of democratizing Internet and connectivity, we believe that the time
is ripe for emphasizing universal Internet provisioning as an important goal on
the 5G research agenda. In this paper, we investigate the opportunities and
challenges in utilizing 5G for global access to the Internet for all (GAIA). We
have also identified the major technical issues involved in a 5G-based GAIA
solution and have set up a future research agenda by defining open research
problems.",Internet accessibility
http://arxiv.org/abs/cs/0110016v1,"Advanced services require more reliable bandwidth than currently provided by
the Internet Protocol, even with the reliability enhancements provided by TCP.
More reliable bandwidth will be provided through QoS (quality of service), as
currently discussed widely. Yet QoS has some implications beyond providing
ubiquitous access to advance Internet service, which are of interest from a
policy perspective. In particular, what are the implications for price of
Internet services? Further, how will these changes impact demand and universal
service for the Internet. This paper explores the relationship between
certainty of bandwidth and certainty of price for Internet services over a
statistically shared network and finds that these are mutually exclusive goals.",Internet accessibility
http://arxiv.org/abs/1005.4028v1,"Internet Banking System refers to systems that enable bank customers to
access accounts and general information on bank products and services through a
personal computer or other intelligent device. Internet banking products and
services can include detailed account information for corporate customers as
well as account summery and transfer money. Ultimately, the products and
services obtained through Internet Banking may mirror products and services
offered through other bank delivery channels. In this paper, Internet Banking
System Prototype has been proposed in order to illustrate the services which is
provided by the Bank online services.",Internet accessibility
http://arxiv.org/abs/1308.2454v1,"We introduce a comprehensive analytical framework to compare between open
access and closed access in two-tier femtocell networks, with regard to uplink
interference and outage. Interference at both the macrocell and femtocell
levels is considered. A stochastic geometric approach is employed as the basis
for our analysis. We further derive sufficient conditions for open access and
closed access to outperform each other in terms of the outage probability,
leading to closed-form expressions to upper and lower bound the difference in
the targeted received power between the two access modes. Simulations are
conducted to validate the accuracy of the analytical model and the correctness
of the bounds.",Internet accessibility
http://arxiv.org/abs/1210.2911v1,"Ad Hoc and Mesh networks are good samples of multi agent systems, where their
nodes access the channel through carrier sense multiple access method, while a
node channel access influence the access of neighbor nodes to the channel.
Hence, game theory is a strong tool for studying this kind of networks. Carrier
sense multiple access parameters such as minimum and maximum size of contention
window and persistence factor can be modified based on game theoretic methods.
In this study different games for tuning the parameters is investigated and
different challenges are examined.",Internet accessibility
http://arxiv.org/abs/1401.1513v1,"In this paper, we study the stability of two interacting queues under random
multiple access in which the queues leverage the feedback information. We
derive the stability region under random multiple access where one of the two
queues exploits the feedback information and backs off under negative
acknowledgement (NACK) and the other, higher priority, queue will access the
channel with probability one. We characterize the stability region of this
feedback-based random access protocol and prove that this derived stability
region encloses the stability region of the conventional random access (RA)
scheme that does not exploit the feedback information.",Internet accessibility
http://arxiv.org/abs/1901.07100v1,"A new multiple access method, namely, delta-orthogonal multiple access
(D-OMA) is introduced for massive access in future generation 6G cellular
networks. D-OMA is based on the concept of distributed large coordinated
multipoint transmission-enabled non-orthogonal multiple access (NOMA) using
partially overlapping sub-bands for NOMA clusters. Performance of this scheme
is demonstrated in terms of outage capacity for different degrees of
overlapping of NOMA sub-bands. D-OMA can also be used for enhanced security
provisioning in both uplink and downlink wireless access networks. Practical
implementation issues and open challenges for optimizing D-OMA are also
discussed.",Internet accessibility
http://arxiv.org/abs/1607.05017v1,"Rateless Multiple Access (RMA) is a novel non-orthogonal multiple access
framework that is promising for massive access in Internet of Things (IoT) due
to its high efficiency and low complexity. In the framework, after certain
\emph{registration}, each active user respectively transmits to the access
point (AP) randomly based on an assigned random access control function (RACf)
until receiving an acknowledgement (ACK). In this work, by exploiting the
intrinsic access pattern of each user, we propose a grant-free RMA scheme,
which no longer needs the registration process as in the original RMA, thus
greatly reduces the signalling overhead and system latency. Furthermore, we
propose a low-complexity joint iterative detection and decoding algorithm in
which the channel estimation, active user detection, and information decoding
are done simultaneously. Finally, we propose a method based on density
evolution (DE) to evaluate the system performance.",Internet accessibility
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",Internet accessibility
http://arxiv.org/abs/1701.00220v1,"Today, smartphone devices are owned by a large portion of the population and
have become a very popular platform for accessing the Internet. Smartphones
provide the user with immediate access to information and services. However,
they can easily expose the user to many privacy risks. Applications that are
installed on the device and entities with access to the device's Internet
traffic can reveal private information about the smartphone user and steal
sensitive content stored on the device or transmitted by the device over the
Internet. In this paper, we present a method to reveal various demographics and
technical computer skills of smartphone users by their Internet traffic
records, using machine learning classification models. We implement and
evaluate the method on real life data of smartphone users and show that
smartphone users can be classified by their gender, smoking habits, software
programming experience, and other characteristics.",Internet accessibility
http://arxiv.org/abs/1012.2177v3,"Since the beginning of the Internet thirty years ago, we have witnessed a
number of changes in the application of communication technologies. Today, the
Internet can be described to a large extent as a ubiquitous infrastructure that
is always accessible. After the era of connecting places and connecting people,
the Internet of the future will also connect things. The idea behind the
resulting Internet of Things is to seamlessly gather and use information about
objects of the real world during their entire lifecycle. In this paper, we
consider different approaches to technological protection of user data privacy
in the world of Internet of Things. In particular,we consider what kind of
security problems are being faced and what level of protection can be provided
by applying approaches based on secure multi-party computations.",Internet accessibility
http://arxiv.org/abs/cs/0109046v1,"While traditional radio stations are subject to extensive government
regulations, Internet radio stations remain largely unregulated. As Internet
radio usage has increased certain stakeholders have begun to argue that these
Internet radio broadcasters are providing significant and diverse programming
to American audiences and that government regulation of spectrum-using radio
station ownership may be further relaxed.
  One of the primary justifications for regulation of ownership has been to
protect diversity in broadcasting. This study hypothesizes that Internet radio
broadcasting does add diversity to the radio broadcasting industry and that it
should be considered as relevant by regulators.
  This study evaluates the role of Internet radio broadcasters according to
five criteria intended to gauge the level of diversity being delivered to
listeners online. By measuring the levels of format, channel, ownership,
location and language diversity among Internet radio stations, it is possible
to draw benchmark lessons about the new medium's ability to provide Americans
with diverse broadcasting options.
  The study finds that Internet radio broadcasters are in fact adding
measurable diversity to the radio broadcasting industry. Internet broadcasters
are providing audiences with access to an increasing number of stations,
owners, formats, and language choices, and it is likely that technologies
aiding in the mobility of access as well as broadband evolution will reinforce
these findings.",Internet accessibility
http://arxiv.org/abs/cs/0109110v1,"This paper explains why, despite a marked increase in available political
information on cable television and the Internet, citizens' levels of political
knowledge have, at best, remained stagnant (Delli Carpini & Keeter, 1996).
Since the availability of entertainment content has increased too, the effect
of new media on knowledge and vote likelihood should be determined by people's
relative preferences for entertainment and information. Access to new media
should increase knowledge and vote likelihood among people who prefer news. At
the same time, it is hypothesized to have a negative effect on knowledge and
turnout for people who prefer entertainment content. Hypotheses are tested by
building a measure of Relative Entertainment Preference (REP) from existing NES
and Pew survey data. Results support the predicted interaction effect of media
environment (cable and/or Internet access) and motivation (REP) on political
knowledge and turnout. In particular, people who prefer entertainment to news
and have access to cable television and the Internet are less knowledgeable and
less likely to vote than any other group of people.",Internet accessibility
http://arxiv.org/abs/1610.00355v2,"The Internet Protocol (IP) is the lifeblood of the modern Internet. Its
simplicity and universality have fueled the unprecedented and lasting global
success of the current Internet. Nonetheless, some limitations of IP have been
emerging in recent years. Its original design envisaged supporting perhaps tens
of thousands of static hosts operating in a friendly academic-like setting,
mainly in order to facilitate email communication and remote access to scarce
computing resources. At present IP interconnects billions of static and mobile
devices (ranging from supercomputers to IoT gadgets) with a large and dynamic
set of popular applications. Starting in mid-1990s, the advent of mobility,
wirelessness and the web substantially shifted Internet usage and communication
paradigms. This accentuated long-term concerns about the current Internet
architecture and prompted interest in alternative designs.
  The U.S. National Science Foundation (NSF) has been one of the key supporters
of efforts to design a set of candidate next-generation Internet architectures.
As a prominent design requirement, NSF emphasized ""security and privacy by
design"" in order to avoid the long and unhappy history of incremental patching
and retrofitting that characterizes the current Internet architecture. To this
end, as a result of a competitive process, four prominent research projects
were funded by the NSF in 2010: Nebula, Named-Data Networking (NDN),
MobilityFirst (MF), and Expressive Internet Architecture (XIA). This paper
provides a comprehensive and neutral analysis of salient security and privacy
features (and issues) in these NSF-funded Future Internet Architectures. It
also compares the four candidate designs with the current IP-based architecture
and discusses similarities, differences, and possible improvements.",Internet accessibility
http://arxiv.org/abs/1301.5931v2,"This paper analyses the performance of TCP over random and dedicated access
methods in the context of DVB-RCS2. Random access methods introduce a lower
connection delay compared to dedicated methods. We investigate the poten- tial
to improve the performance of short flows in regards to transmission delay,
over random access methods for DVB- RCS2 that is currently under development.
Our simulation experiments show that the transmission of the first ten IP
datagrams of each TCP flow can be 500 ms faster with ran- dom access than with
dedicated access making the former of interest to carry Internet traffic. Such
methods, however, are less efficient in regards to bandwidth usage than
dedicated access mecanisms and less reliable in overloaded network conditions.
Two aspects of channel usage optimization can be distinguished: reducing the
duration of ressource utiliza- tion with random access methods, or increasing
the spec- trum efficiency with dedicated access methods. This article argues
that service providers may let low-cost users exploit the DVB-RCS2 to browse
the web by introducing different services, which choice is based on the channel
access method.",Internet accessibility
http://arxiv.org/abs/1112.5760v1,"The paper analyses current versions of top three used Internet browsers and
compare their security levels to a research done in 2006. The security is
measured by analyzing how user data is stored. Data recorded during different
browsing sessions and by different password management functions it is
considered sensitive data. The paper describes how the browser protects the
sensitive data and how an attacker or a forensic analyst can access it.",Internet accessibility
http://arxiv.org/abs/1508.02063v2,"Wireless communications along with the Internet has been the most
transformative technology in the past 50 years. We expect that wireless data
growth driven by new mobile applications, need to connect all humankind (not
just 1/3) as well as Billions of things to the Internet will require Terabit/s
shared links for ground based local area and wide area wireless access, for
wireless backhaul as well as access via unmanned aerial vehicles (UAVs) and
satellites. We present a new scalable radio architecture that we refer to
multi-comm-core (MCC) to enable low-cost ultra-high speed wireless
communications using both traditional and millimeter wave spectrum.",Internet accessibility
http://arxiv.org/abs/1706.09787v1,"Nowadays, Machine-to-Machine (M2M) and Internet of Things (IoT) traffic rate
is increasing at a fast pace. The use of satellites is expected to play a large
role in delivering such a traffic. In this work, we investigate the use of two
of the most common M2M/IoT protocols stacks on a satellite Random Access (RA)
channel, based on DVB-RCS2 standard. The metric under consideration is the
completion time, in order to identify the protocol stack that can provide the
best performance level.",Internet accessibility
http://arxiv.org/abs/1709.09349v1,"In this paper, we empirically demonstrate the growing importance of
reliability by measuring its effect on user behavior. We present an approach
for broadband reliability characterization using data collected by many
emerging national initiatives to study broadband and apply it to the data
gathered by the Federal Communications Commission's Measuring Broadband America
project. Motivated by our findings, we present the design, implementation, and
evaluation of a practical approach for improving the reliability of broadband
Internet access with multihoming.",Internet accessibility
http://arxiv.org/abs/0901.1257v1,"We have developed an Internet-based audience response system (called ARSBO).
In this way we combine the advantages of common audience response systems using
handheld devices and the easy and cheap access to the Internet. Evaluations of
audience response systems in the literature have shown their success:
encouraging participation of the students as well as immediate feedback to
answers to the whole group for evaluational purposes of the teacher. However,
commercial systems are relatively expensive and the number of students in such
a teaching-learning scenario is limited. ARSBO solves these problems. Using the
Internet (e.g. in computer rooms or by wireless Internet access) there are no
special costs and the number of participating students is not limited. ARSBO is
very easy to use for students as well as for the construction of new questions
with possible answers and for the visualization of statistical results to
questions.",Internet accessibility
http://arxiv.org/abs/1302.5491v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a study of Sri Lankan hotel websites.
A web survey and a web content analysis were conducted as the methods to elicit
data on web accessibility. Factors preventing accessibility were hypothesized
as an initial experiment. Affecting design elements are identified through web
content analysis, the results of which are utilized to develop specific
implications for improving web accessibility. The hypothesis tests show that
there is no significant correlation between accessibility and geographical or
economic factors. However, physical impairments of users have a considerable
influence on the accessibility of web page user interface if it has been
designed without full consideration of the needs of all users. Especially,
visual and mobility impaired users experience poor accessibility. Poor
readability and less navigable page designs are two observable issues, which
pose threats to accessibility. The lack of conformance to W3C accessibility
guidelines and the poor design process are the specific shortcomings which
reduce the overall accessibility. Guidelines aim to improve the accessibility
of sites with a strategic focus. Further enhancements are suggested with
adherence to principles, user centered design and developing customizable web
portals compatible for connections with differing speeds. Re-ordering search
results has been suggested as one of the finest step towards making the web
content accessible for users with differing needs. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1908.02804v1,"The web is the prominent way information is exchanged in the 21st century.
However, ensuring web-based information is accessible is complicated,
particularly with web applications that rely on JavaScript and other
technologies to deliver and build representations; representations are often
the HTML, images, or other code a server delivers for a web resource. Static
representations are becoming rarer and assessing the accessibility of web-based
information to ensure it is available to all users is increasingly difficult
given the dynamic nature of representations.
  In this work, we survey three ongoing research threads that can inform web
accessibility solutions: assessing web accessibility, modeling web user
activity, and web application crawling. Current web accessibility research is
continually focused on increasing the percentage of automatically testable
standards, but still relies heavily upon manual testing for complex interactive
applications. Along-side web accessibility research, there are mechanisms
developed by researchers that replicate user interactions with web pages based
on usage patterns. Crawling web applications is a broad research domain;
exposing content in web applications is difficult because of incompatibilities
in web crawlers and the technologies used to create the applications. We
describe research on crawling the deep web by exercising user forms. We close
with a thought exercise regarding the convergence of these three threads and
the future of automated, web-based accessibility evaluation and assurance
through a use case in web archiving. These research efforts provide insight
into how users interact with websites, how to automate and simulate user
interactions, how to record the results of user interactions, and how to
analyze, evaluate, and map resulting website content to determine its relative
accessibility.",web accessibility
http://arxiv.org/abs/1112.5728v1,"Web Accessibility for disabled people has posed a challenge to the civilized
societies that claim to uphold the principles of equal opportunity and
nondiscrimination. Certain concrete measures have been taken to narrow down the
digital divide between normal and disabled users of Internet technology. The
efforts have resulted in enactment of legislations and laws and mass awareness
about the discriminatory nature of the accessibility issue, besides the efforts
have resulted in the development of commensurate technological tools to develop
and test the Web accessibility. World Wide Web consortium's (W3C) Web
Accessibility Initiative (WAI) has framed a comprehensive document comprising
of set of guidelines to make the Web sites accessible to the users with
disabilities. This paper is about the issues and aspects surrounding Web
Accessibility. The details and scope are kept limited to comply with the aim of
the paper which is to create awareness and to provide basis for in-depth
investigation.",web accessibility
http://arxiv.org/abs/1405.7868v1,"Today most of the information in all areas is available over the web. It
increases the web utilization as well as attracts the interest of researchers
to improve the effectiveness of web access and web utilization. As the number
of web clients gets increased, the bandwidth sharing is performed that
decreases the web access efficiency. Web page prefetching improves the
effectiveness of web access by availing the next required web page before the
user demand. It is an intelligent predictive mining that analyze the user web
access history and predict the next page. In this work, vague improved markov
model is presented to perform the prediction. In this work, vague rules are
suggested to perform the pruning at different levels of markov model. Once the
prediction table is generated, the association mining will be implemented to
identify the most effective next page. In this paper, an integrated model is
suggested to improve the prediction accuracy and effectiveness.",web accessibility
http://arxiv.org/abs/1004.1257v1,"World Wide Web is a huge repository of web pages and links. It provides
abundance of information for the Internet users. The growth of web is
tremendous as approximately one million pages are added daily. Users' accesses
are recorded in web logs. Because of the tremendous usage of web, the web log
files are growing at a faster rate and the size is becoming huge. Web data
mining is the application of data mining techniques in web data. Web Usage
Mining applies mining techniques in log data to extract the behavior of users
which is used in various applications like personalized services, adaptive web
sites, customer profiling, prefetching, creating attractive web sites etc., Web
usage mining consists of three phases preprocessing, pattern discovery and
pattern analysis. Web log data is usually noisy and ambiguous and preprocessing
is an important process before mining. For discovering patterns sessions are to
be constructed efficiently. This paper reviews existing work done in the
preprocessing stage. A brief overview of various data mining techniques for
discovering patterns, and pattern analysis are discussed. Finally a glimpse of
various applications of web usage mining is also presented.",web accessibility
http://arxiv.org/abs/1309.4009v1,"Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.",web accessibility
http://arxiv.org/abs/1302.5198v1,"This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a subjective study of Sri Lankan
hotel websites. A web survey and a web content analysis were conducted as the
methods to elicit data on web accessibility. Factors preventing accessibility
were hypothesized as an initial experiment. Hazardous design elements are
identified through web content analysis, the results of which are utilized to
develop specific implications for improving web accessibility. The hypothesis
tests show that there is no significant correlation between accessibility and
geographical or economic factors. However, physical impairments of users have a
considerable influence on the accessibility. Especially, visual and mobility
impaired users experience poor accessibility. Poor readability and less
navigable page designs are two observable issues, which pose threats to
accessibility. The lack of conformance to W3C accessibility guidelines and the
poor design process are the specific shortcomings which reduce the overall
accessibility. Guidelines aim to improve the accessibility of sites with a
strategic focus. Further enhancements are suggested with adherence to
principles and user centered design and developing customizable web portals
compatible for connections with differing speeds. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.",web accessibility
http://arxiv.org/abs/1105.0141v1,"Semantic Web is an open, distributed, and dynamic environment where access to
resources cannot be controlled in a safe manner unless the access decision
takes into account during discovery of web services. Security becomes the
crucial factor for the adoption of the semantic based web services. An access
control means that the users must fulfill certain conditions in order to gain
access over web services. Access control is important in both perspectives i.e.
legal and security point of view. This paper discusses important requirements
for effective access control in semantic web services which have been extracted
from the literature surveyed. I have also discussed open research issues in
this context, focusing on access control policies and models in this paper.",web accessibility
http://arxiv.org/abs/1511.04493v1,"Mobile devices that connect to the Internet via cellular networks are rapidly
becoming the primary medium for accessing Web content. Cellular service
providers (CSPs) commonly deploy Web proxies and other middleboxes for
security, performance optimization and traffic engineering reasons. However,
the prevalence and policies of these Web proxies are generally opaque to users
and difficult to measure without privileged access to devices and servers. In
this paper, we present a methodology to detect the presence of Web proxies
without requiring access to low-level packet traces on a device, nor access to
servers being contacted. We demonstrate the viability of this technique using
controlled experiments, and present the results of running our approach on
several production networks and popular Web sites. Next, we characterize the
behaviors of these Web proxies, including caching, redirecting, and content
rewriting. Our analysis can identify how Web proxies impact network
performance, and inform policies for future deployments. Last, we release an
Android app called Proxy Detector on the Google Play Store, allowing average
users with unprivileged (non-rooted) devices to understand Web proxy
deployments and contribute to our IRB-approved study. We report on results of
using this app on 11 popular carriers from the US, Canada, Austria, and China.",web accessibility
http://arxiv.org/abs/1710.07899v1,"Web portals are being considered as excellent means for conducting teaching
and learning activities electronically. The number of online services such as
course enrollment, tutoring through online course materials, evaluation and
even certification through web portals is increasing day by day. However, the
effectiveness of an educational web portal depends on its accessibility to a
wide range of students irrespective of their age, and physical abilities.
Accessibility of web portals largely depends on their userfriendliness in terms
of design, contents, assistive features, and online support. In this paper, we
have critically analyzed the web portals of thirty Indian Universities of
different categories based on the WCAG 2.0 guidelines. The purpose of this
study is to point out the deficiencies that are commonly observed in web
portals and help web designers to remove such deficiencies from the academic
web portals with a view to enhance their accessibility.",web accessibility
http://arxiv.org/abs/0907.5433v1,"World Wide Web is a huge data repository and is growing with the explosive
rate of about 1 million pages a day. As the information available on World Wide
Web is growing the usage of the web sites is also growing. Web log records each
access of the web page and number of entries in the web logs is increasing
rapidly. These web logs, when mined properly can provide useful information for
decision-making. The designer of the web site, analyst and management
executives are interested in extracting this hidden information from web logs
for decision making. Web access pattern, which is the frequently used sequence
of accesses, is one of the important information that can be mined from the web
logs. This information can be used to gather business intelligence to improve
sales and advertisement, personalization for a user, to analyze system
performance and to improve the web site organization. There exist many
techniques to mine access patterns from the web logs. This paper describes the
powerful algorithm that mines the web logs efficiently. Proposed algorithm
firstly converts the web access data available in a special doubly linked tree.
Each access is called an event. This tree keeps the critical mining related
information in very compressed form based on the frequent event count. Proposed
recursive algorithm uses this tree to efficiently find all access patterns that
satisfy user specified criteria. To prove that our algorithm is efficient from
the other GSP (Generalized Sequential Pattern) algorithms we have done
experimental studies on sample data.",web accessibility
http://arxiv.org/abs/1305.5959v2,"Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.",web accessibility
http://arxiv.org/abs/1408.5460v1,"The World Wide Web is the most wide known information source that is easily
available and searchable. It consists of billions of interconnected documents
Web pages are authored by millions of people. Accesses made by various users to
pages are recorded inside web logs. These log files exist in various formats.
Because of increase in usage of web, size of web log files is increasing at a
much faster rate. Web mining is application of data mining technique to these
log files. It can be of three types Web usage mining, Web structure mining and
Web content mining. Web Usage mining is mining of usage patterns of users which
can then be used to personalize web sites and create attractive web sites. It
consists of three main phases: Preprocessing, Pattern discovery and Pattern
analysis. In this paper we focus on Data cleaning and IP Address identification
stages of preprocessing. Methodology has been proposed for both the stages. At
the end conclusion is made about number of users left after IP address
identification.",web accessibility
http://arxiv.org/abs/1204.2225v1,"This paper support the concept of a community Web directory, as a Web
directory that is constructed according to the needs and interests of
particular user communities. Furthermore, it presents the complete method for
the construction of such directories by using web usage data. User community
models take the form of thematic hierarchies and are constructed by employing
clustering approach. We applied our methodology to the ODP directory and also
to an artificial Web directory, which was generated by clustering Web pages
that appear in the access log of an Internet Service Provider. For the
discovery of the community models, we introduced a new criterion that combines
a priori thematic informativeness of the Web directory categories with the
level of interest observed in the usage data. In this context, we introduced
and evaluated new clustering method. We have tested the methodology using
access log files which are collected from the proxy servers of an Internet
Service Provider and provided results that indicates the usability of the
community Web directories. The proposed clustering methodology is evaluated
both on a specialized artificial and a community Web directory, indicating its
value to the user of the web.",web accessibility
http://arxiv.org/abs/1312.3060v1,"Expert System is developed as consulting service for users spread or public
requires affordable access. The Internet has become a medium for such services,
but presence of mobile devices make the access becomes more widespread by
utilizing mobile web and WAP (Wireless Application Protocol). Applying expert
systems applications over the web and WAP requires a knowledge base
representation that can be accessed simultaneously. This paper proposes single
database to accommodate the knowledge representation with decision tree mapping
approach. Because of the database exist, consulting application through both
web and WAP can access it to provide expert system services options for more
affordable for public.",web accessibility
http://arxiv.org/abs/1803.09585v1,"The paper presents a flexible and efficient method to secure the access to a
Web site implemented in PHP script language. The algorithm is based on the PHP
session mechanism. The proposed method is a general one and offers the
possibility to implement a PHP based secured access to a Web site, through a
portal page and using an additional script included in any site page, which is
required to be accessed only by registered users. This paper presents the
design, implementation and integration of the algorithm on any generic WEB
site.",web accessibility
http://arxiv.org/abs/1506.05628v1,"We analyze 18 million rows of Wi-Fi access logs collected over a one year
period from over 120,000 anonymized users at an inner-city shopping mall. The
anonymized dataset gathered from an opt-in system provides users' approximate
physical location, as well as Web browsing and some search history. Such data
provides a unique opportunity to analyze the interaction between people's
behavior in physical retail spaces and their Web behavior, serving as a proxy
to their information needs. We find: (1) the use of Wi-Fi network maps the
opening hours of the mall; (2) there is a weekly periodicity in users' visits
to the mall; (3) around 60% of registered Wi-Fi users actively browse the Web
and around 10% of them use Wi-Fi for accessing Web search engines; (4) people
are likely to spend a relatively constant amount of time browsing the Web while
their visiting duration may vary; (5) people tend to visit similar mall
locations and Web content during their repeated visits to the mall; (6) the
physical spatial context has a small but significant influence on the Web
content that indoor users browse; (7) accompanying users tend to access
resources from the same Web domains.",web accessibility
http://arxiv.org/abs/1208.1679v1,"Colors play a particularly important role in both designing and accessing Web
pages. A well-designed color scheme improves Web pages' visual aesthetic and
facilitates user interactions. As far as we know, existing color assessment
studies focus on images; studies on color assessment and editing for Web pages
are rare. This paper investigates color assessment for Web pages based on
existing online color theme-rating data sets and applies this assessment to Web
color edit. This study consists of three parts. First, we study the extraction
of a Web page's color theme. Second, we construct color assessment models that
score the color compatibility of a Web page by leveraging machine learning
techniques. Third, we incorporate the learned color assessment model into a new
application, namely, color transfer for Web pages. Our study combines
techniques from computer graphics, Web mining, computer vision, and machine
learning. Experimental results suggest that our constructed color assessment
models are effective, and useful in the color transfer for Web pages, which has
received little attention in both Web mining and computer graphics communities.",web accessibility
http://arxiv.org/abs/1111.2530v1,"With the rapid growth of internet technologies, Web has become a huge
repository of information and keeps growing exponentially under no editorial
control. However the human capability to read, access and understand Web
content remains constant. This motivated researchers to provide Web
personalized online services such as Web recommendations to alleviate the
information overload problem and provide tailored Web experiences to the Web
users. Recent studies show that Web usage mining has emerged as a popular
approach in providing Web personalization. However conventional Web usage based
recommender systems are limited in their ability to use the domain knowledge of
the Web application. The focus is only on Web usage data. As a consequence the
quality of the discovered patterns is low. In this paper, we propose a novel
framework integrating semantic information in the Web usage mining process.
Sequential Pattern Mining technique is applied over the semantic space to
discover the frequent sequential patterns. The frequent navigational patterns
are extracted in the form of Ontology instances instead of Web page views and
the resultant semantic patterns are used for generating Web page
recommendations to the user. Experimental results shown are promising and
proved that incorporating semantic information into Web usage mining process
can provide us with more interesting patterns which consequently make the
recommendation system more functional, smarter and comprehensive.",web accessibility
http://arxiv.org/abs/1204.5267v1,"Though World Wide Web is the single largest source of information, it is
ill-equipped to serve the people with vision related problems. With the
prolific increase in the interest to make the web accessible to all sections of
the society, solving this accessibility problem becomes mandatory. This paper
presents a technique for making web pages accessible for people with low vision
issues. A model for making web pages accessible, WILI (Web Interface for people
with Low-vision Issues) has been proposed. The approach followed in this work
is to automatically replace the existing display style of a web page with a new
skin following the guidelines given by Clear Print Booklet provided by Royal
National Institute of Blind. ""Single Click Solution"" is one of the primary
advantages provided by WILI. A prototype using the WILI model is implemented
and various experiments are conducted. The results of experiments conducted on
WILI indicate 82% effective conversion rate.",web accessibility
http://arxiv.org/abs/1408.2695v1,"This paper addresses web object size which is one of important performance
measures and affects to service time in multiple access environment. Since
packets arrive according to Poission distribution and web service time has
arbitrary distribution, M/G/1 model can be used to describe the behavior of the
web server system. In the time division multiplexing (TDM), we can use M/D/1
with vacations model, because service time is constant and server may have a
vacation. We derive the mean web object size satisfying the constraint such
that mean waiting time by round-robin scheduling in multiple access environment
is equal to the mean queueing delay of M/D/1 with vacations model in TDM and
M/H2/1 model, respectively. Performance evaluation shows that the mean web
object size increases as the link utilization increases at the given maximum
segment size (MSS), but converges on the lower bound when the number of
embedded objects included in a web page is beyond the threshold. Our results
can be applied to the economic design and maintenance of web service.",web accessibility
http://arxiv.org/abs/1309.1792v1,"The broad proliferation of mobile devices in recent years has drastically
changed the means of accessing the World Wide Web. Describing a shift away from
the desktop computer era for content consumption, predictions indicate that the
main access of web-based content will come from mobile devices. Concurrently,
the manner of content presentation has changed as well; web artifacts are
allowing for richer media and higher levels of user interaction which is
enabled through increasing access networks speeds. This article provides an
overview of more than two years of high level web page characteristics by
comparing the desktop and mobile client versions. Our study is the first
long-term evaluation of differences as seen by desktop and mobile web browser
clients. We showcase the main differentiating factors with respect to the
number of web page object requests, their sizes, relationships, and web page
object caching. We additionally highlight long-term trends and discuss their
future implications.",web accessibility
http://arxiv.org/abs/1310.2375v1,"Web usage mining: automatic discovery of patterns in clickstreams and
associated data collected or generated as a result of user interactions with
one or more Web sites. This paper describes web usage mining for our college
log files to analyze the behavioral patterns and profiles of users interacting
with a Web site. The discovered patterns are represented as clusters that are
frequently accessed by groups of visitors with common interests. In this paper,
the visitors and hits were forecasted to predict the further access statistics.",web accessibility
http://arxiv.org/abs/1508.02127v1,"Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results.",web accessibility
http://arxiv.org/abs/1103.5046v1,"The Semantic Web initiative puts emphasis not primarily on putting data on
the Web, but rather on creating links in a way that both humans and machines
can explore the Web of data. When such users access the Web, they leave a trail
as Web servers maintain a history of requests. Web usage mining approaches have
been studied since the beginning of the Web given the log's huge potential for
purposes such as resource annotation, personalization, forecasting etc.
However, the impact of any such efforts has not really gone beyond generating
statistics detailing who, when, and how Web pages maintained by a Web server
were visited.",web accessibility
http://arxiv.org/abs/1806.00871v1,"Personal and private Web archives are proliferating due to the increase in
the tools to create them and the realization that Internet Archive and other
public Web archives are unable to capture personalized (e.g., Facebook) and
private (e.g., banking) Web pages. We introduce a framework to mitigate issues
of aggregation in private, personal, and public Web archives without
compromising potential sensitive information contained in private captures. We
amend Memento syntax and semantics to allow TimeMap enrichment to account for
additional attributes to be expressed inclusive of the requirements for
dereferencing private Web archive captures. We provide a method to involve the
user further in the negotiation of archival captures in dimensions beyond time.
We introduce a model for archival querying precedence and short-circuiting, as
needed when aggregating private and personal Web archive captures with those
from public Web archives through Memento. Negotiation of this sort is novel to
Web archiving and allows for the more seamless aggregation of various types of
Web archives to convey a more accurate picture of the past Web.",web accessibility
http://arxiv.org/abs/1103.5002v1,"The paper proposes an approach to modeling users of large Web sites based on
combining different data sources: access logs and content of the accessed pages
are combined with semantic information about the Web pages, the users and the
accesses of the users to the Web site. The assumption is that we are dealing
with a large Web site providing content to a large number of users accessing
the site. The proposed approach represents each user by a set of features
derived from the different data sources, where some feature values may be
missing for some users. It further enables user modeling based on the provided
characteristics of the targeted user subset. The approach is evaluated on
real-world data where we compare performance of the automatic assignment of a
user to a predefined user segment when different data sources are used to
represent the users.",web accessibility
http://arxiv.org/abs/1104.1892v1,"In this paper we present clustering method is very sensitive to the initial
center values, requirements on the data set too high, and cannot handle noisy
data the proposal method is using information entropy to initialize the cluster
centers and introduce weighting parameters to adjust the location of cluster
centers and noise problems.The navigation datasets which are sequential in
nature, Clustering web data is finding the groups which share common interests
and behavior by analyzing the data collected in the web servers, this improves
clustering on web data efficiently using improved fuzzy c-means(FCM)
clustering. Web usage mining is the application of data mining techniques to
web log data repositories. It is used in finding the user access patterns from
web access log. Web data Clusters are formed using on MSNBC web navigation
dataset.",web accessibility
http://arxiv.org/abs/1405.0749v1,"Web crawlers visit internet applications, collect data, and learn about new
web pages from visited pages. Web crawlers have a long and interesting history.
Early web crawlers collected statistics about the web. In addition to
collecting statistics about the web and indexing the applications for search
engines, modern crawlers can be used to perform accessibility and vulnerability
checks on the application. Quick expansion of the web, and the complexity added
to web applications have made the process of crawling a very challenging one.
Throughout the history of web crawling many researchers and industrial groups
addressed different issues and challenges that web crawlers face. Different
solutions have been proposed to reduce the time and cost of crawling.
Performing an exhaustive crawl is a challenging question. Additionally
capturing the model of a modern web application and extracting data from it
automatically is another open question. What follows is a brief history of
different technique and algorithms used from the early days of crawling up to
the recent days. We introduce criteria to evaluate the relative performance of
web crawlers. Based on these criteria we plot the evolution of web crawlers and
compare their performance",web accessibility
http://arxiv.org/abs/1105.1929v1,"The World Wide Web no longer consists just of HTML pages. Our work sheds
light on a number of trends on the Internet that go beyond simple Web pages.
The hidden Web provides a wealth of data in semi-structured form, accessible
through Web forms and Web services. These services, as well as numerous other
applications on the Web, commonly use XML, the eXtensible Markup Language. XML
has become the lingua franca of the Internet that allows customized markups to
be defined for specific domains. On top of XML, the Semantic Web grows as a
common structured data source. In this work, we first explain each of these
developments in detail. Using real-world examples from scientific domains of
great interest today, we then demonstrate how these new developments can assist
the managing, harvesting, and organization of data on the Web. On the way, we
also illustrate the current research avenues in these domains. We believe that
this effort would help bridge multiple database tracks, thereby attracting
researchers with a view to extend database technology.",web accessibility
http://arxiv.org/abs/1108.0748v2,"Web mining is the nontrivial process to discover valid, novel, potentially
useful knowledge from web data using the data mining techniques or methods. It
may give information that is useful for improving the services offered by web
portals and information access and retrieval tools. With the rapid development
of biclustering, more researchers have applied the biclustering technique to
different fields in recent years. When biclustering approach is applied to the
web usage data it automatically captures the hidden browsing patterns from it
in the form of biclusters. In this work, swarm intelligent technique is
combined with biclustering approach to propose an algorithm called Binary
Particle Swarm Optimization (BPSO) based Biclustering for Web Usage Data. The
main objective of this algorithm is to retrieve the global optimal bicluster
from the web usage data. These biclusters contain relationships between web
users and web pages which are useful for the E-Commerce applications like web
advertising and marketing. Experiments are conducted on real dataset to prove
the efficiency of the proposed algorithms.",web accessibility
http://arxiv.org/abs/1805.01392v1,"Web tracking technologies are pervasive and operated by a few large
technology companies. This technology, and the use of the collected data has
been implicated in influencing elections, fake news, discrimination, and even
health decisions. Little is known about how this technology is deployed on
hospital or other health related websites. The websites of the 210 public
hospitals in the state of Illinois, USA were evaluated with a web tracker
identification tool. Web trackers were identified on 94% of hospital webs
sites, with an average of 3.5 trackers on the websites of general hospitals.
The websites of smaller critical access hospitals used an average of 2 web
trackers. The most common web tracker identified was Google Analytics, found on
74% of Illinois hospital websites. Of the web trackers discovered, 88% were
operated by Google and 26% by Facebook. In light of revelations about how web
browsing profiles have been used and misused, search bubbles, and the potential
for algorithmic discrimination hospital leadership and policy makers must
carefully consider if it is appropriate to use third party tracking technology
on hospital web sites.",web accessibility
http://arxiv.org/abs/cs/0610056v1,"The ongoing paradigm change in the scholarly publication system ('science is
turning to e-science') makes it necessary to construct alternative evaluation
criteria/metrics which appropriately take into account the unique
characteristics of electronic publications and other research output in digital
formats. Today, major parts of scholarly Open Access (OA) publications and the
self-archiving area are not well covered in the traditional citation and
indexing databases. The growing share and importance of freely accessible
research output demands new approaches/metrics for measuring and for evaluating
of these new types of scientific publications. In this paper we propose a
simple quantitative method which establishes indicators by measuring the
access/download pattern of OA documents and other web entities of a single web
server. The experimental indicators (search engine, backlink and direct access
indicator) are constructed based on standard local web usage data. This new
type of web-based indicator is developed to model the specific demand for
better study/evaluation of the accessibility, visibility and interlinking of
open accessible documents. We conclude that e-science will need new stable
e-indicators.",web accessibility
http://arxiv.org/abs/1407.5732v1,"A large amount of data on the WWW remains inaccessible to crawlers of Web
search engines because it can only be exposed on demand as users fill out and
submit forms. The Hidden web refers to the collection of Web data which can be
accessed by the crawler only through an interaction with the Web-based search
form and not simply by traversing hyperlinks. Research on Hidden Web has
emerged almost a decade ago with the main line being exploring ways to access
the content in online databases that are usually hidden behind search forms.
The efforts in the area mainly focus on designing hidden Web crawlers that
focus on learning forms and filling them with meaningful values. The paper
gives an insight into the various Hidden Web crawlers developed for the purpose
giving a mention to the advantages and shortcoming of the techniques employed
in each.",web accessibility
http://arxiv.org/abs/1906.01418v1,"The trend towards mobile devices usage has put more than ever the Web as a
ubiquitous platform where users perform all kind of tasks. In some cases, users
access the Web with 'native' mobile applications developed for well-known
sites, such as LinkedIn, Facebook, Twitter, etc. These native applications
might offer further (e.g. location-based) functionalities to their users in
comparison with their corresponding Web sites, because they were developed with
mobile features in mind. However, most Web applications have not this native
mobile counterpart and users access them using browsers in the mobile device.
Users might eventually want to add mobile features on these Web sites even
though those features were not supported originally. In this paper we present a
novel approach to allow end users to augment their preferred Web sites with
mobile features. This end-user approach is supported by a framework for mobile
Web augmentation that we describe in the paper. We also present a set of
supporting tools and a validation experiment with end users.",web accessibility
http://arxiv.org/abs/1711.10553v1,"Semantic web technologies represent much richer forms of relationships among
users, resources and actions among different web applications such as clouding
computing. However, Semantic web applications pose new requirements for
security mechanisms especially in the access control models. This paper
addresses existing access control methods and presents a semantic based access
control model which considers semantic relations among different entities in
cloud computing environment. We have enriched the research for semantic web
technology with role-based access control that is able to be applied in the
field of medical information system or e-Healthcare system. This work shows how
the semantic web technology provides efficient solutions for the management of
complex and distributed data in heterogeneous systems, and it can be used in
the medical information systems as well.",web accessibility
http://arxiv.org/abs/1701.08256v1,"Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.",web accessibility
http://arxiv.org/abs/1903.09756v1,"Access control is an important component for web services such as a cloud.
Current clouds tend to design the access control mechanism together with the
policy language on their own. It leads to two issues: (i) a cloud user has to
learn different policy languages to use multiple clouds, and (ii) a cloud
service provider has to customize an authorization mechanism based on its
business requirement, which brings high development cost. In this work, a new
access control policy language called PERM modeling language (PML) is proposed
to express various access control models such as access control list (ACL),
role-based access control (RBAC) and attribute-based access control (ABAC),
etc. PML's enforcement mechanism is designed in an interpreter-on-interpreter
manner, which not only secures the authorization code with sandboxing, but also
extends PML to all programming languages that support Lua. PML is already
adopted by real-world projects such as Intel's RMD, VMware's Dispatch, Orange's
Gobis and so on, which proves PML's usability. The performance evaluation on
OpenStack, CloudStack and Amazon Web Services (AWS) shows PML's enforcement
overhead per request is under 5.9us.",web accessibility
http://arxiv.org/abs/1810.12379v1,"The accessibility of content for all has been a key goal of the Web since its
conception. However, true accessibility -- access to relevant content in the
global context -- has been elusive for reasons that extend beyond physical
accessibility issues. Among them are the spoken languages, literacy levels,
expertise, and culture. These issues are highly significant, since information
may not reach those who are the most in need of it. For example, the minimum
wage laws that are published in legalese on government sites and the
low-literate and immigrant populations. While some organizations and volunteers
work on bridging such gaps by creating and disseminating alternative versions
of such content, Web scale solutions much be developed to take advantage of its
distributed dissemination capabilities. This work examines content
accessibility from the perspective of inclusiveness. For this purpose, a human
in the loop approach for renarrating Web content is proposed, where a
renarrator creates an alternative narrative of some Web content with the intent
of extending its reach. A renarration relates some Web content with an
alternative version by means of transformations like simplification,
elaboration, translation, or production of audio and video material. This work
presents a model and a basic architecture for supporting renarrations along
with various scenarios. We also discuss the potentials of the W3C specification
for Web Annotation Data Model towards a more inclusive and decentralized social
web.",web accessibility
http://arxiv.org/abs/1312.4794v1,"Given that semantic Web realization is based on the critical mass of metadata
accessibility and the representation of data with formal knowledge, it needs to
generate metadata that is specific, easy to understand and well-defined.
However, semantic annotation of the web documents is the successful way to make
the Semantic Web vision a reality. This paper introduces the Semantic Web and
its vision (stack layers) with regard to some concept definitions that helps
the understanding of semantic annotation. Additionally, this paper introduces
the semantic annotation categories, tools, domains and models.",web accessibility
http://arxiv.org/abs/1908.07965v1,"Recent developments in online tracking make it harder for individuals to
detect and block trackers. Some sites have deployed indirect tracking methods,
which attempt to uniquely identify a device by asking the browser to perform a
seemingly-unrelated task. One type of indirect tracking, Canvas fingerprinting,
causes the browser to render a graphic recording rendering statistics as a
unique identifier. In this work, we observe how indirect device fingerprinting
methods are disclosed in privacy policies, and consider whether the disclosures
are sufficient to enable website visitors to block the tracking methods. We
compare these disclosures to the disclosure of direct fingerprinting methods on
the same websites.
  Our case study analyzes one indirect fingerprinting technique, Canvas
fingerprinting. We use an existing automated detector of this fingerprinting
technique to conservatively detect its use on Alexa Top 500 websites that cater
to United States consumers, and we examine the privacy policies of the
resulting 28 websites. Disclosures of indirect fingerprinting vary in
specificity. None described the specific methods with enough granularity to
know the website used Canvas fingerprinting. Conversely, many sites did provide
enough detail about usage of direct fingerprinting methods to allow a website
visitor to reliably detect and block those techniques.
  We conclude that indirect fingerprinting methods are often difficult to
detect and are not identified with specificity in privacy policies. This makes
indirect fingerprinting more difficult to block, and therefore risks disturbing
the tentative armistice between individuals and websites currently in place for
direct fingerprinting. This paper illustrates differences in fingerprinting
approaches, and explains why technologists, technology lawyers, and
policymakers need to appreciate the challenges of indirect fingerprinting.",mandatory disclosures detection
http://arxiv.org/abs/1711.02828v1,"Supervisory Control and Data Acquisition (SCADA) systems face the absence of
a protection technique that can beat different types of intrusions and protect
the data from disclosure while handling this data using other applications,
specifically Intrusion Detection System (IDS). The SCADA system can manage the
critical infrastructure of industrial control environments. Protecting
sensitive information is a difficult task to achieve in reality with the
connection of physical and digital systems. Hence, privacy preservation
techniques have become effective in order to protect sensitive/private
information and to detect malicious activities, but they are not accurate in
terms of error detection, sensitivity percentage of data disclosure. In this
paper, we propose a new Privacy Preservation Intrusion Detection (PPID)
technique based on the correlation coefficient and Expectation Maximisation
(EM) clustering mechanisms for selecting important portions of data and
recognizing intrusive events. This technique is evaluated on the power system
datasets for multiclass attacks to measure its reliability for detecting
suspicious activities. The experimental results outperform three techniques in
the above terms, showing the efficiency and effectiveness of the proposed
technique to be utilized for current SCADA systems.",mandatory disclosures detection
http://arxiv.org/abs/0709.3504v1,"A forty-four pass fibre optic surface plasmon resonance sensor that enhances
detection sensitivity according to the number of passes is demonstrated for the
first time. The technique employs a fibre optic recirculation loop that passes
the detection spot forty- four times, thus enhancing sensitivity by a factor of
forty-four. Presently, the total number of passes is limited by the onset of
lasing action of the recirculation loop. This technique offers a significant
sensitivity improvement for various types of plasmon resonance sensors that may
be used in chemical and biomolecule detections.",mandatory disclosures detection
http://arxiv.org/abs/1402.3198v1,"Certain methods of analysis require the knowledge of the spatial distances
between entities whose data are stored in a microdata table. For instance, such
knowledge is necessary and sufficient to perform data mining tasks such as
nearest neighbour searches or clustering. However, when inter-record distances
are published in addition to the microdata for research purposes, the risk of
identity disclosure has to be taken into consideration again. In order to
tackle this problem, we introduce a flexible graph model for microdata in a
metric space and propose a linkage attack based on realistic assumptions of a
data snooper's background knowledge. This attack is based on the idea of
finding a maximum approximate common subgraph of two vertex-labelled and
edge-weighted graphs. By adapting a standard argument from algorithmic graph
theory to our setup, this task is transformed to the maximum clique detection
problem in a corresponding product graph. Using a toy example and experimental
results on simulated data show that publishing even approximate distances could
increase the risk of identity disclosure unreasonably.",mandatory disclosures detection
http://arxiv.org/abs/1208.2486v1,"Plagiarism is a burning problem that academics have been facing in all of the
varied levels of the educational system. With the advent of digital content,
the challenge to ensure the integrity of academic work has been amplified. This
paper discusses on defining a precise definition of plagiarized computer code,
various solutions available for detecting plagiarism and building a cloud
platform for plagiarism disclosure. 'CodeAliker', our application thus
developed automates the submission of assignments and the review process
associated for essay text as well as computer code. It has been made available
under the GNU's General Public License as a Free and Open Source Software.",mandatory disclosures detection
http://arxiv.org/abs/1901.07311v1,"A tremendous amount of individual-level data is generated each day, of use to
marketing, decision makers, and machine learning applications. This data often
contain private and sensitive information about individuals, which can be
disclosed by adversaries. An adversary can recognize the underlying
individual's identity for a data record by looking at the values of
quasi-identifier attributes, known as identity disclosure, or can uncover
sensitive information about an individual through attribute disclosure. In
Statistical Disclosure Control, multiple disclosure risk measures have been
proposed. These share two drawbacks: they do not consider identity and
attribute disclosure concurrently in the risk measure, and they make
restrictive assumptions on an adversary's knowledge by assuming certain
attributes are quasi-identifiers and there is a clear boundary between
quasi-identifiers and sensitive information. In this paper, we present a novel
disclosure risk measure that addresses these limitations, by presenting a
single combined metric of identity and attribute disclosure risk, and providing
flexibility in modeling adversary's knowledge. We have developed an efficient
algorithm for computing the proposed risk measure and evaluated the feasibility
and performance of our approach on a real-world data set from the domain of
social work.",mandatory disclosures detection
http://arxiv.org/abs/1809.00620v2,"Online advertisements that masquerade as non-advertising content pose
numerous risks to users. Such hidden advertisements appear on social media
platforms when content creators or ""influencers"" endorse products and brands in
their content. While the Federal Trade Commission (FTC) requires content
creators to disclose their endorsements in order to prevent deception and harm
to users, we do not know whether and how content creators comply with the FTC's
guidelines. In this paper, we studied disclosures within affiliate marketing,
an endorsement-based advertising strategy used by social media content
creators. We examined whether content creators follow the FTC's disclosure
guidelines, how they word the disclosures, and whether these disclosures help
users identify affiliate marketing content as advertisements. To do so, we
first measured the prevalence of and identified the types of disclosures in
over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a
user study with 1,791 participants to test the efficacy of these disclosures.
Our findings reveal that only about 10% of affiliate marketing content on both
platforms contains any disclosures at all. Further, users fail to understand
shorter, non-explanatory disclosures. Based on our findings, we make various
design and policy suggestions to help improve advertising disclosure practices
on social media platforms.",mandatory disclosures detection
http://arxiv.org/abs/1302.2028v1,"Protecting sensitive information from unauthorized disclosure is a major
concern of every organization. As an organizations employees need to access
such information in order to carry out their daily work, data leakage detection
is both an essential and challenging task. Whether caused by malicious intent
or an inadvertent mistake, data loss can result in significant damage to the
organization. Fingerprinting is a content-based method used for detecting data
leakage. In fingerprinting, signatures of known confidential content are
extracted and matched with outgoing content in order to detect leakage of
sensitive content. Existing fingerprinting methods, however, suffer from two
major limitations. First, fingerprinting can be bypassed by rephrasing (or
minor modification) of the confidential content, and second, usually the whole
content of document is fingerprinted (including non-confidential parts),
resulting in false alarms. In this paper we propose an extension to the
fingerprinting approach that is based on sorted k-skip-n-grams. The proposed
method is able to produce a fingerprint of the core confidential content which
ignores non-relevant (non-confidential) sections. In addition, the proposed
fingerprint method is more robust to rephrasing and can also be used to detect
a previously unseen confidential document and therefore provide better
detection of intentional leakage incidents.",mandatory disclosures detection
http://arxiv.org/abs/1308.4806v2,"We have developed a new method to measure krypton traces in xenon at
unprecedented low concentrations. This is a mandatory task for many near-future
low-background particle physics detectors. Our system separates krypton from
xenon using cryogenic gas chromatography. The amount of krypton is then
quantified using a mass spectrometer. We demonstrate that the system has
achieved a detection limit of 8 ppq (parts per quadrillion) and present results
of distilled xenon with krypton concentrations below 1 ppt.",mandatory disclosures detection
http://arxiv.org/abs/1803.08488v2,"While disclosures relating to various forms of Internet advertising are well
established and follow specific formats, endorsement marketing disclosures are
often open-ended in nature and written by individual publishers. Because such
marketing often appears as part of publishers' actual content, ensuring that it
is adequately disclosed is critical so that end-users can identify it as such.
In this paper, we characterize disclosures relating to affiliate marketing---a
type of endorsement based marketing---on two popular social media platforms:
YouTube & Pinterest. We find that only roughly one-tenth of affiliate content
on both platforms contains disclosures. Based on our findings, we make policy
recommendations geared towards various stakeholders in the affiliate marketing
industry, highlighting how both social media platforms and affiliate companies
can enable better disclosure practices.",mandatory disclosures detection
http://arxiv.org/abs/1504.08043v2,"From buying books to finding the perfect partner, we share our most intimate
wants and needs with our favourite online systems. But how far should we accept
promises of privacy in the face of personal profiling? In particular we ask how
can we improve detection of sensitive topic profiling by online systems? We
propose a definition of privacy disclosure we call
{\epsilon}-indistinguishability from which we construct scalable, practical
tools to assess an adversaries learning potential. We demonstrate our results
using openly available resources, detecting a learning rate in excess of 98%
for a range of sensitive topics during our experiments.",mandatory disclosures detection
http://arxiv.org/abs/1605.06466v1,"Web services are software systems designed for supporting interoperable
dynamic cross-enterprise interactions. The result of attacks to Web services
can be catastrophic and causing the disclosure of enterprises' confidential
data. As new approaches of attacking arise every day, anomaly detection systems
seem to be invaluable tools in this context. The aim of this work has been to
target the attacks that reside in the Web service layer and the extensible
markup language (XML)-structured simple object access protocol (SOAP) messages.
After studying the shortcomings of the existing solutions, a new approach for
detecting anomalies in Web services is outlined. More specifically, the
proposed technique illustrates how to identify anomalies by employing mining
methods on XML-structured SOAP messages. This technique also takes the
advantages of tree-based association rule mining to extract knowledge in the
training phase, which is used in the test phase to detect anomalies. In
addition, this novel composition of techniques brings nearly low false alarm
rate while maintaining the detection rate reasonably high, which is shown by a
case study.",mandatory disclosures detection
http://arxiv.org/abs/1009.5823v1,"Hyperspectral imaging has proven its efficiency for target detection
applications but the acquisition mode and the data rate are major issues when
dealing with real-time detection applications. It can be useful to use snapshot
spectral imagers able to acquire all the spectral channels simultaneously on a
single image sensor. Such snapshot spectral imagers suffer from the lack of
spectral resolution. It is then mandatory to carefully select the spectral
content of the acquired image with respect to the proposed application. We
present a novel approach of hyperspectral band selection for target detection
which maximizes the contrast between the background and the target by proper
optimization of positions and linewidths of a limited number of filters. Based
on a set of tunable band-pass filters such as Fabry-Perot filters, the device
should be able to adapt itself to the current scene and the target looked for.
Simulations based on real hyperspectral images show that such snapshot imagers
could compete well against hyperspectral imagers in terms of detection
efficiency while allowing snapshot acquisition, and real-time detection.",mandatory disclosures detection
http://arxiv.org/abs/1811.08212v1,"The automatic detection of frauds in banking transactions has been recently
studied as a way to help the analysts finding fraudulent operations. Due to the
availability of a human feedback, this task has been studied in the framework
of active learning: the fraud predictor is allowed to sequentially call on an
oracle. This human intervention is used to label new examples and improve the
classification accuracy of the latter. Such a setting is not adapted in the
case of fraud detection with financial data in European countries. Actually, as
a human verification is mandatory to consider a fraud as really detected, it is
not necessary to focus on improving the classifier. We introduce the setting of
'Computer-assisted fraud detection' where the goal is to minimize the number of
non fraudulent operations submitted to an oracle. The existing methods are
applied to this task and we show that a simple meta-algorithm provides
competitive results in this scenario on benchmark datasets.",mandatory disclosures detection
http://arxiv.org/abs/1412.0008v1,"We live and work in environments that are inundated with cameras embedded in
devices such as phones, tablets, laptops, and monitors. Newer wearable devices
like Google Glass, Narrative Clip, and Autographer offer the ability to quietly
log our lives with cameras from a `first person' perspective. While capturing
several meaningful and interesting moments, a significant number of images
captured by these wearable cameras can contain computer screens. Given the
potentially sensitive information that is visible on our displays, there is a
need to guard computer screens from undesired photography. People need
protection against photography of their screens, whether by other people's
cameras or their own cameras.
  We present ScreenAvoider, a framework that controls the collection and
disclosure of images with computer screens and their sensitive content.
ScreenAvoider can detect images with computer screens with high accuracy and
can even go so far as to discriminate amongst screen content. We also introduce
a ScreenTag system that aids in the identification of screen content, flagging
images with highly sensitive content such as messaging applications or email
webpages. We evaluate our concept on realistic lifelogging datasets, showing
that ScreenAvoider provides a practical and useful solution that can help users
manage their privacy.",mandatory disclosures detection
http://arxiv.org/abs/1702.01160v2,"Mobile applications (apps) often transmit sensitive data through network with
various intentions. Some transmissions are needed to fulfill the app's
functionalities. However, transmissions with malicious receivers may lead to
privacy leakage and tend to behave stealthily to evade detection. The problem
is twofold: how does one unveil sensitive transmissions in mobile apps, and
given a sensitive transmission, how does one determine if it is legitimate?
  In this paper, we propose LeakSemantic, a framework that can automatically
locate abnormal sensitive network transmissions from mobile apps. LeakSemantic
consists of a hybrid program analysis component and a machine learning
component. Our program analysis component combines static analysis and dynamic
analysis to precisely identify sensitive transmissions. Compared to existing
taint analysis approaches, LeakSemantic achieves better accuracy with fewer
false positives and is able to collect runtime data such as network traffic for
each transmission. Based on features derived from the runtime data, machine
learning classifiers are built to further differentiate between the legal and
illegal disclosures. Experiments show that LeakSemantic achieves 91% accuracy
on 2279 sensitive connections from 1404 apps.",mandatory disclosures detection
http://arxiv.org/abs/1405.2911v1,"Humanoid robots are designed to operate in human centered environments where
they execute a multitude of challenging tasks, each differing in complexity,
resource requirements, and execution time. In such highly dynamic surroundings
it is desirable to anticipate upcoming situations in order to predict future
resource requirements such as CPU or memory usage. Resource prediction
information is essential for detecting upcoming resource bottlenecks or
conflicts and can be used enhance resource negotiation processes or to perform
speculative resource allocation.
  In this paper we present a prediction model based on Markov chains for
predicting the behavior of the humanoid robot ARMAR-III in human robot
interaction scenarios. Robot state information required by the prediction
algorithm is gathered through self-monitoring and combined with environmental
context information. Adding resource profiles allows generating probability
distributions of possible future resource demands. Online learning of model
parameters is made possible through disclosure mechanisms provided by the robot
framework ArmarX.",mandatory disclosures detection
http://arxiv.org/abs/1704.02385v1,"An-ever increasing number of social media websites, electronic newspapers and
Internet forums allow visitors to leave comments for others to read and
interact. This exchange is not free from participants with malicious
intentions, which do not contribute with the written conversation. Among
different communities users adopt strategies to handle such users. In this
paper we present a comprehensive categorization of the trolling phenomena
resource, inspired by politeness research and propose a model that jointly
predicts four crucial aspects of trolling: intention, interpretation, intention
disclosure and response strategy. Finally, we present a new annotated dataset
containing excerpts of conversations involving trolls and the interactions with
other users that we hope will be a useful resource for the research community.",mandatory disclosures detection
http://arxiv.org/abs/1906.09829v1,"Privacy is of the utmost concern when it comes to releasing data to third
parties. Data owners rely on anonymization approaches to safeguard the released
datasets against re-identification attacks. However, even with strict
anonymization in place, re-identification attacks are still a possibility and
in many cases a reality. Prior art has focused on providing better
anonymization algorithms with minimal loss of information and how to prevent
data disclosure attacks. Our approach tries to tackle the issue of tracing
re-identification attacks based on the concept of honeytokens, decoy or ""bait""
records with the goal to lure malicious users. While the concept of honeytokens
has been widely used in the security domain, this is the first approach to
apply the concept on the data privacy domain. Records with high
re-identification risk, called AnonTokens, are inserted into anonymized
datasets. This work demonstrates the feasibility, detectability and usability
of AnonTokens and provides promising results for data owners who want to apply
our approach to real use cases. We evaluated our concept with real large-scale
population datasets. The results show that the introduction of decoy tokens is
feasible without significant impact on the released dataset.",mandatory disclosures detection
http://arxiv.org/abs/1410.4617v2,"We view a distributed system as a graph of active locations with
unidirectional channels between them, through which they pass messages. In this
context, the graph structure of a system constrains the propagation of
information through it.
  Suppose a set of channels is a cut set between an information source and a
potential sink. We prove that, if there is no disclosure from the source to the
cut set, then there can be no disclosure to the sink. We introduce a new
formalization of partial disclosure, called *blur operators*, and show that the
same cut property is preserved for disclosure to within a blur operator. This
cut-blur property also implies a compositional principle, which ensures limited
disclosure for a class of systems that differ only beyond the cut.",mandatory disclosures detection
http://arxiv.org/abs/1807.05738v1,"Opinion polls suggest that the public value their privacy, with majorities
calling for greater control of their data. However, individuals continue to use
online services which place their personal information at risk, comprising a
Privacy Paradox. Previous work has analysed this phenomenon through
after-the-fact comparisons, but not studied disclosure behaviour during
questioning. We physically surveyed UK cities to study how the British public
regard privacy and how perceptions differ between demographic groups. Through
analysis of optional data disclosure, we empirically examined whether those who
claim to value their privacy act privately with their own data. We found that
both opinions and self-reported actions have little effect on disclosure, with
over 99\% of individuals revealing private data needlessly. We show that not
only do individuals act contrary to their opinions, they disclose information
needlessly even whilst describing themselves as private. We believe our
findings encourage further analysis of data disclosure, as a means of studying
genuine privacy behaviour.",mandatory disclosures detection
http://arxiv.org/abs/1809.09682v2,"Motivated by applications where privacy is important, we consider planning
problems for robots acting in the presence of an observer. We first formulate
and then solve planning problems subject to stipulations on the information
divulged during plan execution --- the appropriate solution concept being both
a plan and an information disclosure policy. We pose this class of problem
under a worst-case model within the framework of procrustean graphs,
formulating the disclosure policy as a particular type of map on edge labels.
We devise algorithms that, given a planning problem supplemented with an
information stipulation, can find a plan, associated disclosure policy, or both
if some exists. Both the plan and associated disclosure policy may depend
subtlety on additional information available to the observer, such as whether
the observer knows the robot's plan (e.g., leaked via a side-channel). Our
implementation finds a plan and a suitable disclosure policy, jointly, when any
such pair exists, albeit for small problem instances.",mandatory disclosures detection
http://arxiv.org/abs/1902.11065v1,"The increased need for unattended authentication in multiple scenarios has
motivated a wide deployment of biometric systems in the last few years. This
has in turn led to the disclosure of security concerns specifically related to
biometric systems. Among them, Presentation Attacks (PAs, i.e., attempts to log
into the system with a fake biometric characteristic or presentation attack
instrument) pose a severe threat to the security of the system: any person
could eventually fabricate or order a gummy finger or face mask to impersonate
someone else. The biometrics community has thus made a considerable effort to
the development of automatic Presentation Attack Detection (PAD) mechanisms,
for instance through the international LivDet competitions.
  In this context, we present a novel fingerprint PAD scheme based on $i)$ a
new capture device able to acquire images within the short wave infrared (SWIR)
spectrum, and $ii)$ an in-depth analysis of several state-of-the-art techniques
based on both handcrafted and deep learning features. The approach is evaluated
on a database comprising over 4700 samples, stemming from 562 different
subjects and 35 different presentation attack instrument (PAI) species. The
results show the soundness of the proposed approach with a detection equal
error rate (D-EER) as low as 1.36\% even in a realistic scenario where five
different PAI species are considered only for testing purposes (i.e., unknown
attacks).",mandatory disclosures detection
http://arxiv.org/abs/1710.00101v1,"Traffic analysis is a type of attack on secure communications systems, in
which the adversary extracts useful patterns and information from the observed
traffic. This paper improves and extends an efficient traffic analysis attack,
called ""statistical disclosure attack."" Moreover, we propose a solution to
defend against the improved (and, a fortiori, the original) statistical
disclosure attack. Our solution delays the attacker considerably, meaning that
he should gather significantly more observations to be able to deduce
meaningful information from the traffic.",mandatory disclosures detection
http://arxiv.org/abs/1904.01711v1,"Perfect data privacy seems to be in fundamental opposition to the economical
and scientific opportunities associated with extensive data exchange. Defying
this intuition, this paper develops a framework that allows the disclosure of
collective properties of datasets without compromising the privacy of
individual data samples. We present an algorithm to build an optimal disclosure
strategy/mapping, and discuss it fundamental limits on finite and
asymptotically large datasets. Furthermore, we present explicit expressions to
the asymptotic performance of this scheme in some scenarios, and study cases
where our approach attains maximal efficiency. We finally discuss suboptimal
schemes to provide sample privacy guarantees to large datasets with a reduced
computational cost.",mandatory disclosures detection
http://arxiv.org/abs/physics/0102063v4,"The hierarchy of exact equations is given that relates two-spatial-point
velocity structure functions of arbitrary order with other statistics. Because
no assumption is used, the exact statistical equations can apply to any flow
for which the Navier-Stokes equations are accurate, and they apply no matter
how small the number of samples in the ensemble. The exact statistical
equations can be used to verify DNS computations and to detect their
limitations. For example,if DNS data are used to evaluate the exact statistical
equations, then the equations should balance to within numerical precision,
otherwise a computational problem is indicated. The equations allow
quantification of the approach to local homogeneity and to local isotropy.
Testing the balance of the equations allows detection of scaling ranges for
quantification of scaling-range exponents. The second-order equations lead to
Kolmogorov's equation. All higher-order equations contain a statistic composed
of one factor of the two-point difference of the pressure gradient multiplied
by factors of velocity difference. Investigation of this
pressure-gradient-difference statistic can reveal much about two issues: 1)
whether or not different components of the velocity structure function of given
order have differing exponents in the inertial range, and 2) the increasing
deviation of those exponents from Kolmogorov scaling as the order increases.
Full disclosure of the mathematical methods is in
xxx.lanl.gov/list/physics.flu-dyn/0102055.",mandatory disclosures detection
http://arxiv.org/abs/1701.08470v1,"The application of automatic theorem provers to discharge proof obligations
is necessary to apply formal methods in an efficient manner. Tools supporting
formal methods, such as Atelier~B, generate proof obligations fully
automatically. Consequently, such proof obligations are often cluttered with
information that is irrelevant to establish their validity.
  We present iapa, an ""Interface to Automatic Proof Agents"", a new tool that is
being integrated to Atelier~B, through which the user will access proof
obligations, apply operations to simplify these proof obligations, and then
dispatch the resulting, simplified, proof obligations to a portfolio of
automatic theorem provers.",non-compliance with information obligations
http://arxiv.org/abs/cs/0604081v1,"We consider the interpretations of notions of access control (permissions,
interdictions, obligations, and user rights) as run-time properties of
information systems specified as event systems with fairness. We give proof
rules for verifying that an access control policy is enforced in a system, and
consider preservation of access control by refinement of event systems. In
particular, refinement of user rights is non-trivial; we propose to combine
low-level user rights and system obligations to implement high-level user
rights.",non-compliance with information obligations
http://arxiv.org/abs/1210.6857v1,"We show that time complexity analysis of higher-order functional programs can
be effectively reduced to an arguably simpler (although computationally
equivalent) verification problem, namely checking first-order inequalities for
validity. This is done by giving an efficient inference algorithm for linear
dependent types which, given a PCF term, produces in output both a linear
dependent type and a cost expression for the term, together with a set of proof
obligations. Actually, the output type judgement is derivable iff all proof
obligations are valid. This, coupled with the already known relative
completeness of linear dependent types, ensures that no information is lost,
i.e., that there are no false positives or negatives. Moreover, the procedure
reflects the difficulty of the original problem: simple PCF terms give rise to
sets of proof obligations which are easy to solve. The latter can then be put
in a format suitable for automatic or semi-automatic verification by external
solvers. Ongoing experimental evaluation has produced encouraging results,
which are briefly presented in the paper.",non-compliance with information obligations
http://arxiv.org/abs/1606.00339v1,"We present a general formal argumentation system for dealing with the
detachment of conditional obligations. Given a set of facts, constraints, and
conditional obligations, we answer the question whether an unconditional
obligation is detachable by considering reasons for and against its detachment.
For the evaluation of arguments in favor of detaching obligations we use a
Dung-style argumentation-theoretical semantics. We illustrate the modularity of
the general framework by considering some extensions, and we compare the
framework to some related approaches from the literature.",non-compliance with information obligations
http://arxiv.org/abs/1206.5174v3,"We recently introduced p-automata, automata that read discrete-time Markov
chains. We used turn-based stochastic parity games to define acceptance of
Markov chains by a subclass of p-automata. Definition of acceptance required a
cumbersome and complicated reduction to a series of turn-based stochastic
parity games. The reduction could not support acceptance by general p-automata,
which was left undefined as there was no notion of games that supported it.
  Here we generalize two-player games by adding a structural acceptance
condition called obligations. Obligations are orthogonal to the linear winning
conditions that define winning. Obligations are a declaration that player 0 can
achieve a certain value from a configuration. If the obligation is met, the
value of that configuration for player 0 is 1.
  One cannot define value in obligation games by the standard mechanism of
considering the measure of winning paths on a Markov chain and taking the
supremum of the infimum of all strategies. Mainly because obligations need
definition even for Markov chains and the nature of obligations has the flavor
of an infinite nesting of supremum and infimum operators. We define value via a
reduction to turn-based games similar to Martin's proof of determinacy of
Blackwell games with Borel objectives. Based on this definition, we show that
games are determined. We show that for Markov chains with Borel objectives and
obligations, and finite turn-based stochastic parity games with obligations
there exists an alternative and simpler characterization of the value function.
Based on this simpler definition we give an exponential time algorithm to
analyze finite turn-based stochastic parity games with obligations. Finally, we
show that obligation games provide the necessary framework for reasoning about
p-automata and that they generalize the previous definition.",non-compliance with information obligations
http://arxiv.org/abs/cs/0106010v1,"This paper concentrates on the representation of the legal relations that
obtain between parties once they have entered a contractual agreement and their
evolution as the agreement progresses through time. Contracts are regarded as
process and they are analysed in terms of the obligations that are active at
various points during their life span. An informal notation is introduced that
summarizes conveniently the states of an agreement as it evolves over time.
Such a representation enables us to determine what the status of an agreement
is, given an event or a sequence of events that concern the performance of
actions by the agents involved. This is useful both in the context of contract
drafting (where parties might wish to preview how their agreement might evolve)
and in the context of contract performance monitoring (where parties might with
to establish what their legal positions are once their agreement is in force).
The discussion is based on an example that illustrates some typical patterns of
contractual obligations.",non-compliance with information obligations
http://arxiv.org/abs/1603.02964v1,"Some organizations use software applications to manage their customers'
personal, medical, or financial information. In the United States, those
software applications are obligated to preserve users' privacy and to comply
with the United States federal privacy laws and regulations. To formally
guarantee compliance with those regulations, it is essential to extract and
model the privacy rules from the text of the law using a formal framework. In
this work we propose a goal-oriented framework for modeling and extracting the
privacy requirements from regulatory text using natural language processing
techniques.",non-compliance with information obligations
http://arxiv.org/abs/1905.08819v1,"Data cooperatives with fiduciary obligations to members provide a promising
direction for the empowerment of individuals through their own personal data. A
data cooperative can manage, curate and protect access to the personal data of
citizen members. Furthermore, the data cooperative can run internal analytics
in order to obtain insights regarding the well-being of its members. Armed with
these insights, the data cooperative would be in a good position to negotiate
better services and discounts for its members. Credit Unions and similar
institutions can provide a suitable realization of data cooperatives.",non-compliance with information obligations
http://arxiv.org/abs/1402.4741v1,"In this paper, we provide more evidence for the contention that logical
consequence should be understood in normative terms. Hartry Field and John
MacFarlane covered the classical case. We extend their work, examining what it
means for an agent to be obliged to infer a conclusion when faced with
uncertain information or reasoning within a non-monotonic, defeasible, logical
framework (which allows e. g. for inference to be drawn from premises
considered true unless evidence to the contrary is presented).",non-compliance with information obligations
http://arxiv.org/abs/1607.01485v1,"Normative texts are documents based on the deontic notions of obligation,
permission, and prohibition. Our goal is to model such texts using the C-O
Diagram formalism, making them amenable to formal analysis, in particular
verifying that a text satisfies properties concerning causality of actions and
timing constraints. We present an experimental, semi-automatic aid to bridge
the gap between a normative text and its formal representation. Our approach
uses dependency trees combined with our own rules and heuristics for extracting
the relevant components. The resulting tabular data can then be converted into
a C-O Diagram.",non-compliance with information obligations
http://arxiv.org/abs/1012.1131v1,"Nowadays we are faced with an increasing popularity of social software
including wikis, blogs, micro-blogs and online social networks such as Facebook
and MySpace. Unfortunately, the mostly used social services are centralized and
personal information is stored at a single vendor. This results in potential
privacy problems as users do not have much control over how their private data
is disseminated. To overcome this limitation, some recent approaches envisioned
replacing the single authority centralization of services by a peer-to-peer
trust-based approach where users can decide with whom they want to share their
private data. In this peer-to-peer collaboration it is very difficult to ensure
that after data is shared with other peers, these peers will not misbehave and
violate data privacy. In this paper we propose a mechanism that addresses the
issue of data privacy violation due to data disclosure to malicious peers. In
our approach trust values between users are adjusted according to their
previous activities on the shared data. Users share their private data by
specifying some obligations the receivers must follow. We log modifications
done by users on the shared data as well as the obligations that must be
followed when data is shared. By a log-auditing mechanism we detect users that
misbehaved and we adjust their associated trust values by using any existing
decentralized trust model.",non-compliance with information obligations
http://arxiv.org/abs/1208.0944v1,"In a global and technology oriented world the requirements that products and
services have to fulfill are increasing and are getting more complicated.
Research and development (R&D) is becoming increasingly important in creating
the knowledge that makes research and business more competitive. Companies are
obliged to produce more rapidly, more effectively and more efficiently. In
order to meet these requirements and to secure the viability of business
processes, services and products R&D teams need to access and retrieve
information from as many sources as possible. From the other perspective
virtual teams are important mechanisms for organizations seeking to leverage
scarce resources across geographic and other boundaries moreover; virtual
collaboration has become vital for most organizations. This is particularly
true in the context of designing new product and service innovation. Such
collaboration often involves a network of partners located around the world.
However at the R&D project level, dealing with such distributed teams
challenges both managers and specialists. In new product development, it is
necessary to put together the growing different capabilities and services with
the goal, through cooperation between suppliers and customers, service
providers and scientific institutions to achieve innovations of high quality.
In this paper based on comprehensive literature review of recent articles, at
the first step provides an primary definition and characterization of virtual
R&D team; next, the potential value created by virtual R&D teams for new
product development is explored and lastly along with a guide line for future
study, it is argued that the establishing of virtual R&D teams should be given
consideration in the management of R&D projects.",non-compliance with information obligations
http://arxiv.org/abs/1311.2023v1,"Social networks can have asymmetric relationships. In the online social
network Twitter, a follower receives tweets from a followed person but the
followed person is not obliged to subscribe to the channel of the follower.
Thus, it is natural to consider the dissemination of information in directed
networks. In this work we use the mean-field approach to derive differential
equations that describe the dissemination of information in a social network
with asymmetric relationships. In particular, our model reflects the impact of
the degree distribution on the information propagation process. We further show
that for an important subclass of our model, the differential equations can be
solved analytically.",non-compliance with information obligations
http://arxiv.org/abs/1811.07271v2,"Visualizations have a potentially enormous influence on how data are used to
make decisions across all areas of human endeavor. However, it is not clear how
this power connects to ethical duties: what obligations do we have when it
comes to visualizations and visual analytics systems, beyond our duties as
scientists and engineers? Drawing on historical and contemporary examples, I
address the moral components of the design and use of visualizations, identify
some ongoing areas of visualization research with ethical dilemmas, and propose
a set of additional moral obligations that we have as designers, builders, and
researchers of visualizations.",non-compliance with information obligations
http://arxiv.org/abs/1206.5132v2,"In many countries, information and communication technology (ICT) has a lucid
impact on the development of educational curriculum. This is the era of
Information Communication Technology, so to perk up educational planning it is
indispensable to implement the ICT in Education sector. Student can perform
well throughout the usage of ICT. ICT helps the students to augment their
knowledge skills as well as to improve their learning skills. To know with
reference to the usage and Impact of ICT in Education sector of Pakistan, we
accumulate data from 429 respondents from 5 colleges and universities, we use
convenient sampling to accumulate the data from district Rawalpindi of
Pakistan. The consequences show that Availability and Usage of ICT improves the
knowledge and learning skills of students. This indicates that existence of ICT
is improving the educational efficiency as well as obliging for making policies
regarding education sector.",non-compliance with information obligations
http://arxiv.org/abs/physics/0309091v1,"The electric charge of the quantization condition of Dirac's monopole may
have any value, we are not obliged to identify it with the electron charge.
Consequently the magnetic charge of the monopole is quite arbitrary: Dirac's
monopole is a mere object of science fiction.",non-compliance with information obligations
http://arxiv.org/abs/1407.6124v1,"We consider the problem of automated reasoning about dynamically manipulated
data structures. The state-of-the-art methods are limited to the
unfold-and-match (U+M) paradigm, where predicates are transformed via
(un)folding operations induced from their definitions before being treated as
uninterpreted. However, proof obligations from verifying programs with
iterative loops and multiple function calls often do not succumb to this
paradigm. Our contribution is a proof method which -- beyond U+M -- performs
automatic formula re-writing by treating previously encountered obligations in
each proof path as possible induction hypotheses. This enables us, for the
first time, to systematically reason about a wide range of obligations, arising
from practical program verification. We demonstrate the power of our proof
rules on commonly used lemmas, thereby close the remaining gaps in existing
state-of-the-art systems. Another impact, probably more important, is that our
method regains the power of compositional reasoning, and shows that the usage
of user-provided lemmas is no longer needed for the existing set of benchmarks.
This not only removes the burden of coming up with the appropriate lemmas, but
also significantly boosts up the verification process, since lemma
applications, coupled with unfolding, often induce very large search space.",non-compliance with information obligations
http://arxiv.org/abs/0811.1914v1,"We describe an extension to the TLA+ specification language with constructs
for writing proofs and a proof environment, called the Proof Manager (PM), to
checks those proofs. The language and the PM support the incremental
development and checking of hierarchically structured proofs. The PM translates
a proof into a set of independent proof obligations and calls upon a collection
of back-end provers to verify them. Different provers can be used to verify
different obligations. The currently supported back-ends are the tableau prover
Zenon and Isabelle/TLA+, an axiomatisation of TLA+ in Isabelle/Pure. The proof
obligations for a complete TLA+ proof can also be used to certify the theorem
in Isabelle/TLA+.",non-compliance with information obligations
http://arxiv.org/abs/1809.05369v1,"Data protection regulations generally afford individuals certain rights over
their personal data, including the rights to access, rectify, and delete the
data held on them. Exercising such rights naturally requires those with data
management obligations (service providers) to be able to match an individual
with their data. However, many mobile apps collect personal data, without
requiring user registration or collecting details of a user's identity (email
address, names, phone number, and so forth). As a result, a user's ability to
exercise their rights will be hindered without means for an individual to link
themselves with this 'nameless' data. Current approaches often involve those
seeking to exercise their legal rights having to give the app's provider more
personal information, or even to register for a service; both of which seem
contrary to the spirit of data protection law. This paper explores these
concerns, and indicates simple means for facilitating data subject rights
through both application and mobile platform (OS) design.",non-compliance with information obligations
http://arxiv.org/abs/cs/0701142v1,"We introduce the concept of knowledge states; many well-known algorithms can
be viewed as knowledge state algorithms. The knowledge state approach can be
used to to construct competitive randomized online algorithms and study the
tradeoff between competitiveness and memory. A knowledge state simply states
conditional obligations of an adversary, by fixing a work function, and gives a
distribution for the algorithm. When a knowledge state algorithm receives a
request, it then calculates one or more ""subsequent"" knowledge states, together
with a probability of transition to each. The algorithm then uses randomization
to select one of those subsequents to be the new knowledge state. We apply the
method to the paging problem. We present optimally competitive algorithm for
paging for the cases where the cache sizes are k=2 and k=3. These algorithms
use only a very limited number of bookmarks.",non-compliance with information obligations
http://arxiv.org/abs/1108.2349v1,"Current approaches for the discovery, specification, and provision of
services ignore the relationship between the service contract and the
conditions in which the service can guarantee its contract. Moreover, they do
not use formal methods for specifying services, contracts, and compositions.
Without a formal basis it is not possible to justify through formal
verification the correctness conditions for service compositions and the
satisfaction of contractual obligations in service provisions. We remedy this
situation in this paper. We present a formal definition of services with
context-dependent contracts. We define a composition theory of services with
context-dependent contracts taking into consideration functional,
nonfunctional, legal and contextual information. Finally, we present a formal
verification approach that transforms the formal specification of service
composition into extended timed automata that can be verified using the model
checking tool UPPAAL.",non-compliance with information obligations
http://arxiv.org/abs/1405.0650v1,"Software as a Service (SaaS) becomes in this decade the focus of many
enterprises and research. SaaS provides software application as Web based
delivery to server many customers. This sharing of infrastructure and
application provided by Saas has a great benefit to customers, since it reduces
costs, minimizes risks, improves their competitive positioning, as well as
seeks out innovative. SaaS application is generally developed with standardized
software functionalities to serve as many customers as possible. However many
customers ask to change the standardized provided functions according to their
specific business needs, and this can be achieve through the configuration and
customization provided by the SaaS vendor. Allowing many customers to change
software configurations without impacting others customers and with preserving
security and efficiency of the provided services, becomes a big challenge to
SaaS vendors, who are oblige to design new strategies and architectures.
Multi-tenancy (MT) architectures allow multiple customers to be consolidated
into the same operational system without changing anything in the vendor source
code. In this paper, we will present how the configuration can be done on an
ERP web application in a Multi-Tenancy SaaS environment.",non-compliance with information obligations
http://arxiv.org/abs/1812.09241v1,"Bibliometric indicators are increasingly used in support of decisions for
recruitment, career advancement, rewarding and selective funding for
scientists. Given the importance of the applications, bibliometricians are
obligated to carry out empirical testing of the robustness of the indicators,
in simulations of real contexts. In this work we compare the results of
national-scale research assessments at the individual level, based on three
different indexes: the h-index, g-index and ""fractional scientific strength"",
or FSS, an indicator previously proposed by the authors. For each index, we
construct and compare rankings lists of all Italian academic researchers
working in the hard sciences over the period 2001-2005. The analysis quantifies
the shifts in ranks that occur when researchers' productivity rankings by
simple indicators such as h- or g-index are compared with that by more accurate
FSS.",non-compliance with information obligations
http://arxiv.org/abs/1707.07866v1,"We present HornDroid, a new tool for the static analysis of information flow
properties in Android applications. The core idea underlying HornDroid is to
use Horn clauses for soundly abstracting the semantics of Android applications
and to express security properties as a set of proof obligations that are
automatically discharged by an off-the-shelf SMT solver. This approach makes it
possible to fine-tune the analysis in order to achieve a high degree of
precision while still using off-the-shelf verification tools, thereby
leveraging the recent advances in this field. As a matter of fact, HornDroid
outperforms state-of-the-art Android static analysis tools on benchmarks
proposed by the community. Moreover, HornDroid is the first static analysis
tool for Android to come with a formal proof of soundness, which covers the
core of the analysis technique: besides yielding correctness assurances, this
proof allowed us to identify some critical corner-cases that affect the
soundness guarantees provided by some of the previous static analysis tools for
Android.",non-compliance with information obligations
http://arxiv.org/abs/1806.03513v1,"The Android platform was introduced by Google in 2008 as an operating system
for mobile devices. Android's SDK provides a wide support for programming and
extensive examples and documentation. Reliability is an increasing concern for
Smart Phone applications since they often feature personal information and
data. Therefore, techniques and tools for checking the correct behavior of apps
are required. This paper shows how the Event-B method can be used to reason and
to verify the design of Android apps and how this can be used to document
implementation decisions. Our approach consists in modeling the core
functionality of the app in Event-B and using the evidence shown by the Proof
Obligations generated to reason about the design and the implementation of the
app. Although we do not propose a novel approach, we prove that heavyweight
Formal Methods (FMs) techniques with Event-B can effectively be used to support
the development of correct Android apps. We present a case study in which we
design the core functionality of WhatsApp in Event-B, we encode it over three
machine refinements modeling basic functionality (chatting, deleting content,
forwarding content, deleting a chat session, etc.), read and unread status of
chat sessions, and implementation details, respectively. We report and discuss
on underlying challenges in the design and implementation of the core
functionality.",non-compliance with information obligations
http://arxiv.org/abs/1903.11516v2,"In a paper ( posthumously ) co-authored by Isaac Newton himself, the primacy
of geometric notions in pedagogical expositions of centripetal acceleration has
been clearly asserted. In the present paper we demonstrate how this pedagogical
prerogative can inform the design of an experiment involving an
accelerometer-equipped smartphone rotating uniformly in a horizontal plane.
Specifically, the location of the sensor itself within the body of the
smartphone will be determined using a technique that is purely geometrical in
nature, relying on nothing more than the notion that centripetal accelerations
are centrally-pointing. The complete absence of algebraic manipulations obliges
students to focus exclusively on the development of their geometrical reasoning
abilities. In particular, it provides a healthy challenge for those
algebraically-accomplished students for whom equations, calculations and data
tables represent a means of avoiding a direct confrontation with the imposing
spectre of material that is otherwise purely conceptual in nature.",non-compliance with information obligations
http://arxiv.org/abs/1805.05887v1,"Today's emerging Industrial Internet of Things (IIoT) scenarios are
characterized by the exchange of data between services across enterprises.
Traditional access and usage control mechanisms are only able to determine if
data may be used by a subject, but lack an understanding of how it may be used.
The ability to control the way how data is processed is however crucial for
enterprises to guarantee (and provide evidence of) compliant processing of
critical data, as well as for users who need to control if their private data
may be analyzed or linked with additional information - a major concern in IoT
applications processing personal information. In this paper, we introduce
LUCON, a data-centric security policy framework for distributed systems that
considers data flows by controlling how messages may be routed across services
and how they are combined and processed. LUCON policies prevent information
leaks, bind data usage to obligations, and enforce data flows across services.
Policy enforcement is based on a dynamic taint analysis at runtime and an
upfront static verification of message routes against policies. We discuss the
semantics of these two complementing enforcement models and illustrate how
LUCON policies are compiled from a simple policy language into a first-order
logic representation. We demonstrate the practical application of LUCON in a
real-world IoT middleware and discuss its integration into Apache Camel.
Finally, we evaluate the runtime impact of LUCON and discuss performance and
scalability aspects.",non-compliance with information obligations
http://arxiv.org/abs/1007.3589v1,"Registries play a key role in service-oriented applications. Originally, they
were neutral players between service providers and clients. The UDDI Business
Registry (UBR) was meant to foster these concepts and provide a common
reference for companies interested in Web services. The more Web services were
used, the more companies started create their own local registries: more
efficient discovery processes, better control over the quality of published
information, and also more sophisticated publication policies motivated the
creation of private repositories. The number and heterogeneity of the different
registries - besides the decision to close the UBR are pushing for new and
sophisticated means to make different registries cooperate. This paper proposes
DIRE (DIstributed REgistry), a novel approach based on a publish and subscribe
(P/S) infrastructure to federate different heterogeneous registries and make
them exchange information about published services. The paper discusses the
main motivations for the P/S-based infrastructure, proposes an integrated
service model, introduces the main components of the framework, and exemplifies
them on a simple case study.",do-not-call registry
http://arxiv.org/abs/cs/0605111v1,"The NSDL Metadata Registry is designed to provide humans and machines with
the means to discover, create, access and manage metadata schemes, schemas,
application profiles, crosswalks and concept mappings. This paper describes the
general goals and architecture of the NSDL Metadata Registry as well as issues
encountered during the first year of the project's implementation.",do-not-call registry
http://arxiv.org/abs/1609.09211v1,"Advancements in technology have transformed mobile devices from being mere
communication widgets to versatile computing devices. Proliferation of these
hand held devices has made them a common means to access and process digital
information. Most web based applications are today available in a form that can
conveniently be accessed over mobile devices. However, webservices
(applications meant for consumption by other applications rather than humans)
are not as commonly provided and consumed over mobile devices. Facilitating
this and in effect realizing a service-oriented system over mobile devices has
the potential to further enhance the potential of mobile devices. One of the
major challenges in this integration is the lack of an efficient service
registry system that caters to issues associated with the dynamic and volatile
mobile environments. Existing service registry technologies designed for
traditional systems fall short of accommodating such issues. In this paper, we
propose a novel approach to manage service registry systems provided 'solely'
over mobile devices, and thus realising an SOA without the need for high-end
computing systems. The approach manages a dynamic service registry system in
the form of light weight and distributed registries. We assess the feasibility
of our approach by engineering and deploying a working prototype of the
proposed registry system over actual mobile devices. A comparative study of the
proposed approach and the traditional UDDI (Universal Description, Discovery,
and Integration) registry is also included. The evaluation of our framework has
shown propitious results in terms of battery cost, scalability, hindrance with
native applications.",do-not-call registry
http://arxiv.org/abs/cs/0212052v1,"In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.",do-not-call registry
http://arxiv.org/abs/1608.01019v1,"The entity registry system (ERS) is a decentralized entity registry that can
be used to replace the Web as a platform for publishing linked data when the
latter is not available. In developing countries, where off-line is the default
mode of operation, centralized linked data solutions fail to address the needs
of the communities. Although the features are mostly completed, the system is
not yet ready for deployment. This project aims to provide extensive tests and
scalability investigations that would make it ready for a real scenario.",do-not-call registry
http://arxiv.org/abs/1111.5733v1,"The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.",do-not-call registry
http://arxiv.org/abs/1809.01756v1,"Token curated registries (TCRs) have been proposed recently as an approach to
create and maintain high quality lists of resources or recommendations in a
decentralized manner. Applications range from maintaining registries of web
domains for advertising purposes (e.g., adChain) or restaurants, consumer
products, etc. The registry is maintained through a combination of candidate
applications requiring a token deposit, challenges based on token staking and
token-weighted votes with a redistribution of tokens occurring as a consequence
of the vote. We present a simplified mathematical model of a TCR and its
challenge and voting process analyze it from a game-theoretic perspective. We
derive some insights into conditions with respect to the quality of a candidate
under which challenges occur, and under which the outcome is reject or accept.
We also show that there are conditions under which the outcome may not be
entirely predictable in the sense that everyone voting for accept and everyone
voting for reject could both be Nash Equilibria outcomes. For such conditions,
we also explore when a particular strategy profile may be payoff dominant. We
identify ways in which our modeling can be extended and also some implications
of our model with respect to the composition of TCRs.",do-not-call registry
http://arxiv.org/abs/1602.03681v1,"The public package registry npm is one of the biggest software registry. With
its 216 911 software packages, it forms a big network of software dependencies.
In this paper we evaluate various methods for finding similar packages in the
npm network, using only the structure of the graph. Namely, we want to find a
way of categorizing similar packages, which would be useful for recommendation
systems. This size enables us to compute meaningful results, as it softened the
particularities of the graph. Npm is also quite famous as it is the default
package repository of Node.js. We believe that it will make our results
interesting for more people than a less used package repository. This makes it
a good subject of analysis of software networks.",do-not-call registry
http://arxiv.org/abs/1512.08612v1,"At dry and clean material junctions of rigid materials the corrugation of the
sliding energy landscape is dominated by variations of Pauli repulsions. These
occur when electron clouds centered around atoms in adjacent layers overlap as
they slide across each other. In such cases there exists a direct relation
between interfacial surface (in)commensurability and superlubricity, a
frictionless and wearless tribological state. The Registry Index is a purely
geometrical parameter that quantifies the degree of interlayer
commensurability, thus providing a simple and intuitive method for the
prediction of sliding energy landscapes at rigid material interfaces. In the
present study, we extend the applicability of the Registry Index to
non-parallel surfaces, using a model system of nanotubes motion on flat
hexagonal materials. Our method successfully reproduces sliding energy
landscapes of carbon nanotubes on Graphene calculated using a Lennard-Jones
type and the Kolmogorov-Crespi interlayer potentials. Furthermore, it captures
the sliding energy corrugation of a boron nitride nanotube on hexagonal boron
nitride calculated using the h-BN ILP. Finally, we use the Registry Index to
predict the sliding energy landscapes of the heterogeneous junctions of a
carbon nanotubes on hexagonal boron nitride and of boron nitride nanotubes on
graphene that are shown to exhibit a significantly reduced corrugation. For
such rigid interfaces this is expected to be manifested by superlubric motion.",do-not-call registry
http://arxiv.org/abs/1811.09680v1,"Token Curated Registries (TCR) are decentralized recommendation systems that
can be implemented using Blockchain smart contracts. They allow participants to
vote for or against adding items to a list through a process that involves
staking tokens intrinsic to the registry, with winners receiving the staked
tokens for each vote. A TCR aims to provide incentives to create a well-curated
list. In this work, we consider a challenge for these systems - incentivizing
token-holders to actually engage and participate in the voting process. We
propose a novel token-inflation mechanism for enhancing engagement, whereby
only voting participants see their token supply increased by a pre-defined
multiple after each round of voting. To evaluate this proposal, we propose a
simple 4-class model of voters that captures all possible combinations of two
key dimensions: whether they are engaged (likely to vote at all for a given
item) or disengaged, and whether they are informed (likely to vote in a way
that increases the quality of the list) or uninformed, and a simple metric to
evaluate the quality of the list as a function of the vote outcomes. We conduct
simulations using this model of voters and show that implementing
token-inflation results in greater wealth accumulation for engaged voters. In
particular, when the number of informed voters is sufficiently high, our
simulations show that voters that are both informed and engaged see the
greatest benefits from participating in the registry when our proposed
token-inflation mechanism is employed. We further validate this finding using a
simplified mathematical analysis.",do-not-call registry
http://arxiv.org/abs/1308.3357v1,"Linked Data applications often assume that connectivity to data repositories
and entity resolution services are always available. This may not be a valid
assumption in many cases. Indeed, there are about 4.5 billion people in the
world who have no or limited Web access. Many data-driven applications may have
a critical impact on the life of those people, but are inaccessible to those
populations due to the architecture of today's data registries. In this paper,
we propose and evaluate a new open-source system that can be used as a
general-purpose entity registry suitable for deployment in poorly-connected or
ad-hoc environments.",do-not-call registry
http://arxiv.org/abs/1903.03061v1,"This paper presents DIALOG (Digital Investigation Ontology); a framework for
the management, reuse, and analysis of Digital Investigation knowledge. DIALOG
provides a general, application independent vocabulary that can be used to
describe an investigation at different levels of detail. DIALOG is defined to
encapsulate all concepts of the digital forensics field and the relationships
between them. In particular, we concentrate on the Windows Registry, where
registry keys are modeled in terms of both their structure and function.
Registry analysis software tools are modeled in a similar manner and we
illustrate how the interpretation of their results can be done using the
reasoning capabilities of ontology",do-not-call registry
http://arxiv.org/abs/1906.03300v1,"In this study, we aim to incorporate the expertise of anonymous curators into
a token-curated registry (TCR), a decentralized recommender system for
collecting a list of high-quality content. This registry is important, because
previous studies on TCRs have not specifically focused on technical content,
such as academic papers and patents, whose effective curation requires
expertise in relevant fields. To measure expertise, curation in our model
focuses on both the content and its citation relationships, for which curator
assignment uses the Personalized PageRank (PPR) algorithm while reward
computation uses a multi-task peer-prediction mechanism. Our proposed CitedTCR
bridges the literature on network-based and token-based recommender systems and
contributes to the autonomous development of an evolving citation graph for
high-quality content. Moreover, we experimentally confirm the incentive for
registration and curation in CitedTCR using the simplification of a one-to-one
correspondence between users and content (nodes).",do-not-call registry
http://arxiv.org/abs/1611.01820v1,"Today, full-texts of scientific articles are often stored in different
locations than the used datasets. Dataset registries aim at a closer
integration by making datasets citable but authors typically refer to datasets
using inconsistent abbreviations and heterogeneous metadata (e.g. title,
publication year). It is thus hard to reproduce research results, to access
datasets for further analysis, and to determine the impact of a dataset.
Manually detecting references to datasets in scientific articles is
time-consuming and requires expert knowledge in the underlying research
domain.We propose and evaluate a semi-automatic three-step approach for finding
explicit references to datasets in social sciences articles.We first extract
pre-defined special features from dataset titles in the da|ra registry, then
detect references to datasets using the extracted features, and finally match
the references found with corresponding dataset titles. The approach does not
require a corpus of articles (avoiding the cold start problem) and performs
well on a test corpus. We achieved an F-measure of 0.84 for detecting
references in full-texts and an F-measure of 0.83 for finding correct matches
of detected references in the da|ra dataset registry.",do-not-call registry
http://arxiv.org/abs/1603.01979v1,"In this work, we compare GDELT and Event Registry, which monitor news
articles worldwide and provide big data to researchers regarding scale, news
sources, and news geography. We found significant differences in scale and news
sources, but surprisingly, we observed high similarity in news geography
between the two datasets.",do-not-call registry
http://arxiv.org/abs/1910.00286v1,"Detection and Analysis of a potential malware specifically, used for ransom
is a challenging task. Recently, intruders are utilizing advance cryptographic
techniques to get hold of digital assets and then demand ransom. It is believed
that generally, the files comprise of some attributes, states, and patterns
that can be recognized by a machine learning technique. This work thus focuses
on detection of Ransomware by performing feature engineering, which helps in
analyzing vital attributes and behaviors of the malware. The main contribution
of this work is the identification of important and distinct characteristics of
Ransomware that can help in detecting them. Finally, based on the selected
features, both conventional machine learning techniques and Transfer Learning
based Deep Convolutional Neural Networks have been used to detect Ransomware.
In order to perform feature engineering and analysis, two separate datasets
(static and dynamic) were generated. The static dataset has 3646 samples (1700
Ransomware and 1946 Goodware). On the other hand, the dynamic dataset comprised
of 3444 samples (1455 Ransomware and 1989 Goodware). Through various
experiments, it is observed that the Registry changes, API calls, and DLLs are
the most important features for Ransomware detection. Additionally, important
sequences are found with the help of N Gram technique. It is also observed that
in case of Registry Delete operation, if a malicious file tries to delete
registries, it follows a specific and repeated sequence. However for the benign
file, it doesnt follow any specific sequence or repetition. Similarly, an
interesting observation made through this study is that there is no common
Registry deleted sequence between malicious and benign file. And thus this
discernible fact can be readily exploited for Ransomware detection. The
relevant Python code and dataset are available at github.",do-not-call registry
http://arxiv.org/abs/cs/0212051v1,"Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.",do-not-call registry
http://arxiv.org/abs/0711.1836v2,"The size distribution of land plots is a result of land allocation processes
in the past. In the absence of regulation this is a Markov process leading an
equilibrium described by a probabilistic equation used commonly in the
insurance and financial mathematics. We support this claim by analyzing the
distribution of two plot types, garden and build-up areas, in the Czech Land
Registry pointing out the coincidence with the distribution of prime number
factors described by Dickman function in the first case.",do-not-call registry
http://arxiv.org/abs/1007.3631v1,"The advanced features of today's smart phones and hand held devices, like the
increased memory and processing capabilities, allowed them to act even as
information providers. Thus a smart phone hosting web services is not a fancy
anymore. But the relevant discovery of these services provided by the smart
phones has became quite complex, because of the volume of services possible
with each Mobile Host providing some services. Centralized registries have
severe drawbacks in such a scenario and alternate means of service discovery
are to be addressed. P2P domain with it resource sharing capabilities comes
quite handy and here in this paper we provide an alternate approach to UDDI
registry for discovering mobile web services. The services are published into
the P2P network as JXTA modules and the discovery issues of these module
advertisements are addressed. The approach also provides alternate means of
identifying the Mobile Host.",do-not-call registry
http://arxiv.org/abs/1301.1886v1,"The demand of transparency of clinical research results, the need of
accelerating the process of transferring innovation in the daily medical
practice as well as assuring patient safety and product efficacy make it
necessary to extend the functionality of traditional trial registries. These
new systems should combine different functionalities to track the information
exchange, support collaborative work, manage regulatory documents and monitor
the entire clinical investigation (CIV) lifecycle. This is the approach used to
develop MEDIS, a Medical Device Information System, described in this paper
under the perspective of the business process, and the underlining
architecture. Moreover, MEDIS was designed on the basis of Health Level 7 (HL7)
v.3 standards and methodology to make it interoperable with similar registries,
but also to facilitate information exchange between different health
information systems.",do-not-call registry
http://arxiv.org/abs/1411.2649v3,"With the ongoing exhaustion of free address pools at the registries serving
the global demand for IPv4 address space, scarcity has become reality. Networks
in need of address space can no longer get more address allocations from their
respective registries.
  In this work we frame the fundamentals of the IPv4 address exhaustion
phenomena and connected issues. We elaborate on how the current ecosystem of
IPv4 address space has evolved since the standardization of IPv4, leading to
the rather complex and opaque scenario we face today. We outline the evolution
in address space management as well as address space use patterns, identifying
key factors of the scarcity issues. We characterize the possible solution space
to overcome these issues and open the perspective of address blocks as virtual
resources, which involves issues such as differentiation between address
blocks, the need for resource certification, and issues arising when
transferring address space between networks.",do-not-call registry
http://arxiv.org/abs/1410.3340v1,"Much interest has been taken in understanding the global routing structure of
the Internet, both to model and protect the current structures and to modify
the structure to improve resilience. These studies rely on trace-routes and
algorithmic inference to resolve individual IP addresses into connected
routers, yielding a network of routers. Using WHOIS registries, parsing of DNS
registries, as well as simple latency-based triangulation, these routers can
often be geolocated to at least their country of origin, if not specific
regions. In this work, we use node subgraph summary statistics to present
evidence that the router-level (IPv4) network is spatially embedded, with the
similarity (or dissimilarity) of a node from it's neighbor strongly correlating
with the attributes of other routers residing in the same country or region. We
discuss these results in context of the recently proposed gravity models of the
Internet, as well as the potential application to geolocation inferrence.",do-not-call registry
